
@article{_efficient_,
  title = {Efficient {{Global Optimization}} of {{Expensive Black}}-{{Box Functions}}},
  pages = {38},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  file = {/home/victor/Zotero/storage/JSEY6UYJ/Efficient Global Optimization of Expensive Black-B.pdf},
  language = {en}
}

@article{ablin_super-efficiency_2020,
  title = {Super-Efficiency of Automatic Differentiation for Functions Defined as a Minimum},
  author = {Ablin, Pierre and Peyr{\'e}, Gabriel and Moreau, Thomas},
  year = {2020},
  month = feb,
  abstract = {In min-min optimization or max-min optimization, one has to compute the gradient of a function defined as a minimum. In most cases, the minimum has no closed-form, and an approximation is obtained via an iterative algorithm. There are two usual ways of estimating the gradient of the function: using either an analytic formula obtained by assuming exactness of the approximation, or automatic differentiation through the algorithm. In this paper, we study the asymptotic error made by these estimators as a function of the optimization error. We find that the error of the automatic estimator is close to the square of the error of the analytic estimator, reflecting a super-efficiency phenomenon. The convergence of the automatic estimator greatly depends on the convergence of the Jacobian of the algorithm. We analyze it for gradient descent and stochastic gradient descent and derive convergence rates for the estimators in these cases. Our analysis is backed by numerical experiments on toy problems and on Wasserstein barycenter computation. Finally, we discuss the computational complexity of these estimators and give practical guidelines to chose between them.},
  archivePrefix = {arXiv},
  eprint = {2002.03722},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/99B3HQZC/Ablin et al. - 2020 - Super-efficiency of automatic differentiation for .pdf},
  journal = {arXiv:2002.03722 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{abramovich_wavelet_1998,
  title = {Wavelet Decomposition Approaches to Statistical Inverse Problems},
  author = {u Abramovich, F. and Silverman, B. W.},
  year = {1998},
  volume = {85},
  pages = {115--129},
  file = {/home/victor/Zotero/storage/NEFQHAGV/biometrika1998.pdf},
  journal = {Biometrika},
  keywords = {stochastic inverse problem,wavelet},
  number = {1}
}

@article{afshar_probabilistic_,
  title = {Probabilistic {{Inference}} in {{Piecewise Graphical Models}}},
  author = {Afshar, Hadi Mohasel},
  pages = {165},
  file = {/home/victor/Zotero/storage/PR9NVEI4/Afshar - Probabilistic Inference in Piecewise Graphical Mod.pdf}
}

@article{andreassian_all_2012,
  title = {All That Glitters Is Not Gold: The Case of Calibrating Hydrological Models: {{Invited Commentary}}},
  shorttitle = {All That Glitters Is Not Gold},
  author = {Andr{\'e}assian, Vazken and Le Moine, Nicolas and Perrin, Charles and Ramos, Maria-Helena and Oudin, Ludovic and Mathevet, Thibault and Lerat, Julien and Berthet, Lionel},
  year = {2012},
  month = jul,
  volume = {26},
  pages = {2206--2210},
  issn = {08856087},
  doi = {10.1002/hyp.9264},
  file = {/home/victor/Zotero/storage/UDKEPQMP/Andr√©assian et al. - 2012 - All that glitters is not gold the case of calibra.pdf},
  journal = {Hydrological Processes},
  language = {en},
  number = {14}
}

@book{andrieu_introduction_2003,
  title = {An {{Introduction}} to {{MCMC}} for {{Machine Learning}}},
  author = {Andrieu, Christophe and Freitas, Nando De and {al}, et},
  year = {2003},
  abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
  file = {/home/victor/Zotero/storage/WJ26GD3I/Andrieu et al. - 2003 - An Introduction to MCMC for Machine Learning.pdf;/home/victor/Zotero/storage/FVZ5A9HC/summary.html}
}

@article{angelikopoulos_x-tmcmc_2015,
  title = {X-{{TMCMC}}: {{Adaptive}} Kriging for {{Bayesian}} Inverse Modeling},
  shorttitle = {X-{{TMCMC}}},
  author = {Angelikopoulos, Panagiotis and Papadimitriou, Costas and Koumoutsakos, Petros},
  year = {2015},
  month = jun,
  volume = {289},
  pages = {409--428},
  issn = {00457825},
  doi = {10.1016/j.cma.2015.01.015},
  file = {/home/victor/Zotero/storage/YZ3W8HHH/angelikopoulos2015a.pdf},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  keywords = {Adaptative Kriging,Bayesian inference,Kriging},
  language = {en}
}

@article{apley_understanding_2006,
  title = {Understanding the {{Effects}} of {{Model Uncertainty}} in {{Robust Design With Computer Experiments}}},
  author = {Apley, Daniel W. and Liu, Jun and Chen, Wei},
  year = {2006},
  volume = {128},
  pages = {945},
  issn = {10500472},
  doi = {10.1115/1.2204974},
  file = {/home/victor/Zotero/storage/B344XSHR/Apley et al. - 2006 - Understanding the Effects of Model Uncertainty in .pdf},
  journal = {Journal of Mechanical Design},
  language = {en},
  number = {4}
}

@article{aravkin_estimating_2012,
  title = {Estimating {{Nuisance Parameters}} in {{Inverse Problems}}},
  author = {Aravkin, Aleksandr Y. and {van Leeuwen}, Tristan},
  year = {2012},
  month = nov,
  volume = {28},
  pages = {115016},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/28/11/115016},
  abstract = {Many inverse problems include nuisance parameters which, while not of direct interest, are required to recover primary parameters. Structure present in these problems allows efficient optimization strategies \textemdash{} a well known example is variable projection, where nonlinear least squares problems which are linear in some parameters can be very efficiently optimized. In this paper, we extend the idea of projecting out a subset over the variables to a broad class of maximum likelihood (ML) and maximum a posteriori likelihood (MAP) problems with nuisance parameters, such as variance or degrees of freedom. As a result, we are able to incorporate nuisance parameter estimation into large-scale constrained and unconstrained inverse problem formulations. We apply the approach to a variety of problems, including estimation of unknown variance parameters in the Gaussian model, degree of freedom (d.o.f.) parameter estimation in the context of robust inverse problems, automatic calibration, and optimal experimental design. Using numerical examples, we demonstrate improvement in recovery of primary parameters for several largescale inverse problems. The proposed approach is compatible with a wide variety of algorithms and formulations, and its implementation requires only minor modifications to existing algorithms.},
  archivePrefix = {arXiv},
  eprint = {1206.6532},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/72YYDB72/Aravkin et van Leeuwen - 2012 - Estimating Nuisance Parameters in Inverse Problems.pdf},
  journal = {Inverse Problems},
  keywords = {65K05; 65K10; 86-08,Mathematics - Numerical Analysis,Statistics - Computation},
  language = {en},
  number = {11}
}

@book{armagan_bayesian_2010,
  title = {Bayesian Generalized Double {{Pareto}} Shrinkage},
  author = {Armagan, A. and {al}, et},
  year = {2010},
  abstract = {We propose a generalized double Pareto prior for shrinkage estimation in linear models. The prior can be obtained via a scale mixture of Laplace or normal distributions, while forming a bridge between the Laplace and Normal-Jeffreys ' priors. While it has a spike at zero like the Laplace density, it also has a Student-t-like tail behavior. We show strong consistency of the posterior in regression models with a diverging number of parameters, providing a template to be used for other priors in similar settings. Bayesian computation is straightforward via a simple Gibbs sampling algorithm. We also investigate the properties of the maximum a posteriori estimator and reveal connections with some well-established regularization procedures. The performance of the new prior is tested through simulations.},
  file = {/home/victor/Zotero/storage/W6N3EVXB/Armagan et al - 2010 - Bayesian generalized double Pareto shrinkage.pdf;/home/victor/Zotero/storage/2UH922I9/summary.html},
  keywords = {Bayesian inference}
}

@inproceedings{arnaud_evaluation_2010,
  title = {{\'Evaluation d'un risque d'inondation fluviale par planification s\'equentielle d'exp\'eriences}},
  booktitle = {{42\`emes Journ\'ees de Statistique}},
  author = {Arnaud, Aur{\'e}lie and Bect, Julien and Couplet, Mathieu and Pasanisi, Alberto and Vazquez, Emmanuel},
  year = {2010},
  abstract = {Nous nous int\'eressons au risque d'inondation d'une zone habitable ou industrielle, situ\'ee \`a proximit\'e d'un fleuve. Le risque est \'evalu\'e \`a partir d'un mod\`ele de la ligne d'eau du fleuve en pr\'esence d'incertitudes sur le d\'ebit et les caract\'eristiques du lit fluvial. Comme l'\'evaluation du mod\`ele de la hauteur d'eau, pour un d\'ebit et des caract\'eristiques du lit fix\'es, est potentiellement co\^uteux en temps de calcul, l'estimation d'une probabilit\'e de d\'epassement de seuil ou d'un quantile de la hauteur d'eau doit en pratique \^etre conduite avec un budget r\'eduit de simulations. Dans cet article, nous nous int\'eressons sp\'ecifiquement \`a l'estimation d'un quantile et nous proposons une m\'ethode de planification d'exp\'eriences s\'equentielle qui construit une approximation du mod\`ele par krigeage en choisissant les points d'\'evaluation du mod\`ele de mani\`ere \`a r\'eduire la variance d'estimation du quantile.},
  file = {/home/victor/Zotero/storage/6LTDBWZX/Arnaud et al. - 2010 - √âvaluation d'un risque d'inondation fluviale par p.pdf;/home/victor/Zotero/storage/BNT2E33P/inria-00494767.html},
  language = {fr}
}

@article{artzner_coherent_1999,
  title = {Coherent {{Measures}} of {{Risk}}},
  author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean-Marc and Heath, David},
  year = {1999},
  month = jul,
  volume = {9},
  pages = {203--228},
  issn = {0960-1627, 1467-9965},
  doi = {10.1111/1467-9965.00068},
  abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties ``coherent.'' We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
  file = {/home/victor/Zotero/storage/2U9VWDRP/Artzner et al. - 1999 - Coherent Measures of Risk.pdf},
  journal = {Mathematical Finance},
  language = {en},
  number = {3}
}

@phdthesis{asri_etude_2014,
  title = {{\'Etude des M-estimateurs et leurs versions pond\'er\'ees pour des donn\'ees clusteris\'ees}},
  author = {Asri, Mohamed El},
  year = {2014},
  month = dec,
  abstract = {La classe des M-estimateurs engendre des estimateurs classiques d'un param\`etre de localisation multidimensionnel tels que l'estimateur du maximum de vraisemblance, la moyenne empirique et la m\'ediane spatiale. Huber (1964) introduit les M-estimateurs dans le cadre de l'\'etude des estimateurs robustes. Parmi la litt\'erature d\'edi\'ee \`a ces estimateurs, on trouve en particulier les ouvrages de Huber (1981) et de Hampel et al. (1986) sur le comportement asymptotique et la robustesse via le point de rupture et la fonction d'influence (voir Ruiz-Gazen (2012) pour une synth\`ese sur ces notions). Plus r\'ecemment, des r\'esultats sur la convergence et la normalit\'e asymptotique sont \'etablis par Van der Vaart (2000) dans le cadre multidimensionnel. Nevalainen et al. (2006, 2007) \'etudient le cas particulier de la m\'ediane spatiale pond\'er\'ee et non-pond\'er\'ee dans le cas clusteris\'e. Nous g\'en\'eralisons ces r\'esultats aux M-estimateurs pond\'er\'es. Nous \'etudions leur convergence presque s\^ure, leur normalit\'e asymptotique ainsi que leur robustesse dans le cas de donn\'ees clusteris\'ees.},
  file = {/home/victor/Zotero/storage/7NP8Z3XT/Asri - 2014 - √âtude des M-estimateurs et leurs versions pond√©r√©e.pdf;/home/victor/Zotero/storage/EJV47BT2/tel-01202540.html},
  language = {fr},
  school = {Universit\'e d'Avignon}
}

@article{atkins_implicit_2012,
  title = {Implicit {{Particle Methods}} and {{Their Connection}} with {{Variational Data Assimilation}}},
  author = {Atkins, Ethan and Morzfeld, Matthias and Chorin, Alexandre J.},
  year = {2012},
  month = nov,
  volume = {141},
  pages = {1786--1803},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-12-00145.1},
  abstract = {The implicit particle filter is a sequential Monte Carlo method for data assimilation that guides the particles to the high-probability regions via a sequence of steps that includes minimizations. A new and more general derivation of this approach is presented and the method is extended to particle smoothing as well as to data assimilation for perfect models. Minimizations required by implicit particle methods are shown to be similar to those that one encounters in variational data assimilation, and the connection of implicit particle methods with variational data assimilation is explored. In particular, it is argued that existing variational codes can be converted into implicit particle methods at a low additional cost, often yielding better estimates that are also equipped with quantitative measures of the uncertainty. A detailed example is presented.},
  file = {/home/victor/Zotero/storage/PXUNNLB6/Atkins et al. - 2012 - Implicit Particle Methods and Their Connection wit.pdf;/home/victor/Zotero/storage/WLSWGZPR/MWR-D-12-00145.html},
  journal = {Monthly Weather Review},
  number = {6}
}

@article{au_estimation_2001,
  title = {Estimation of Small Failure Probabilities in High Dimensions by Subset Simulation},
  author = {Au, Siu-Kui and Beck, James L.},
  year = {2001},
  month = oct,
  volume = {16},
  pages = {263--277},
  issn = {02668920},
  doi = {10.1016/S0266-8920(01)00019-4},
  abstract = {A new simulation approach, called `subset simulation', is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made suf\textregistered ciently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and ef\textregistered ciently estimated by means of simulation. The conditional probabilities cannot be estimated ef\textregistered ciently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and ef\textregistered cient in computing small probabilities. The ef\textregistered ciency of the method is demonstrated by calculating the \textregistered rst-excursion probabilities for a linear oscillator subjected to white noise excitation and for a \textregistered ve-story nonlinear hysteretic shear building under uncertain seismic excitation. q 2001 Elsevier Science Ltd. All rights reserved.},
  file = {/home/victor/Zotero/storage/9JP6H7FP/Au et Beck - 2001 - Estimation of small failure probabilities in high .pdf},
  journal = {Probabilistic Engineering Mechanics},
  language = {en},
  number = {4}
}

@article{augustin_trust-region_2017,
  title = {A Trust-Region Method for Derivative-Free Nonlinear Constrained Stochastic Optimization},
  author = {Augustin, F. and Marzouk, Y. M.},
  year = {2017},
  month = mar,
  abstract = {In this work we introduce the algorithm (S)NOWPAC (Stochastic Nonlinear Optimization With Path-Augmented Constraints) for stochastic nonlinear constrained derivative-free optimization. The algorithm extends the derivative-free optimizer NOWPAC to be applicable to nonlinear stochastic programming. It is based on a trust region framework, utilizing local fully linear surrogate models combined with Gaussian process surrogates to mitigate the noise in the objective function and constraint evaluations. We show several benchmark results that demonstrate (S)NOWPAC's efficiency and highlight the accuracy of the optimal solutions found.},
  archivePrefix = {arXiv},
  eprint = {1703.04156},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/LXXZ4ZT5/Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;/home/victor/Zotero/storage/PVWW83A3/Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;/home/victor/Zotero/storage/WYTGF3KZ/Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;/home/victor/Zotero/storage/5W93B5GQ/1703.html},
  journal = {arXiv:1703.04156 [math]},
  keywords = {9080; 90C15; 90C30; 90C56; 65K05; 60G15,G.1.6,Mathematics - Optimization and Control},
  primaryClass = {math}
}

@article{azais_distribution_2005,
  title = {On the Distribution of the Maximum of a Gaussian Field with d Parameters},
  author = {Azais, Jean-Marc and Wschebor, Mario},
  year = {2005},
  month = feb,
  volume = {15},
  pages = {254--278},
  issn = {1050-5164},
  doi = {10.1214/105051604000000602},
  abstract = {Let I be a compact d-dimensional manifold, let X:I\textbackslash to R be a Gaussian process with regular paths and let F\_I(u), u\textbackslash in R, be the probability distribution function of sup\_\{t\textbackslash in I\}X(t). We prove that under certain regularity and nondegeneracy conditions, F\_I is a C\^1-function and satisfies a certain implicit equation that permits to give bounds for its values and to compute its asymptotic behavior as u\textbackslash to +\textbackslash infty. This is a partial extension of previous results by the authors in the case d=1. Our methods use strongly the so-called Rice formulae for the moments of the number of roots of an equation of the form Z(t)=x, where Z:I\textbackslash to R\^d is a random field and x is a fixed point in R\^d. We also give proofs for this kind of formulae, which have their own interest beyond the present application.},
  archivePrefix = {arXiv},
  eprint = {math/0503475},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/AMLGSN9K/Azais et Wschebor - 2005 - On the distribution of the maximum of a gaussian f.pdf},
  journal = {The Annals of Applied Probability},
  keywords = {Mathematics - Probability},
  language = {en},
  number = {1A}
}

@book{azais_level_2009,
  title = {Level Sets and Extrema of Random Processes and Fields},
  author = {Aza{\"i}s, Jean-Marc and Wschebor, Mario},
  year = {2009},
  publisher = {{Wiley \& Sons}},
  doi = {10.1002/9780470434642},
  file = {/home/victor/Zotero/storage/XMVNYBKM/hal-00982795.html}
}

@book{azais_level_2009-1,
  title = {Level {{Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}: {{Aza\"is}}/{{Level Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}},
  shorttitle = {Level {{Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}},
  author = {Az{\"a}is, Jean-Marc and Wschebor, Mario},
  year = {2009},
  month = feb,
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{Hoboken, NJ, USA}},
  doi = {10.1002/9780470434642},
  file = {/home/victor/Zotero/storage/DY8I85EE/Az√§is et Wschebor - 2009 - Level Sets and Extrema of Random Processes and Fie.pdf},
  isbn = {978-0-470-43464-2 978-0-470-40933-6},
  language = {en}
}

@article{baccou_development_2019,
  title = {Development of Good Practice Guidance for Quantification of Thermal-Hydraulic Code Model Input Uncertainty},
  author = {Baccou, Jean and Zhang, Jinzhao and Fillion, Philippe and Damblin, Guillaume and Petruzzi, Alessandro and Mendiz{\'a}bal, Rafael and Revent{\'o}s, Francesc and Skorek, Tomasz and Couplet, Mathieu and Iooss, Bertrand and Oh, Deog-Yeon and Takeda, Takeshi},
  year = {2019},
  month = dec,
  volume = {354},
  pages = {110173},
  issn = {0029-5493},
  doi = {10.1016/j.nucengdes.2019.110173},
  abstract = {Taking into account uncertainties is a key issue in nuclear power plant safety analysis using best estimate plus uncertainty methodologies. It involves two main types of treatment depending on the variables of interest: input parameters or system response quantity. The OECD/NEA PREMIUM project devoted to the first type of variables has shown that inverse methods for input uncertainty quantification can exhibit strong user-effect. One of the main reasons was the lack of a clear guidance to perform a reliable analysis. This work is precisely devoted to the development of a first good practice guidance document for quantification of thermal-hydraulic code model input uncertainty. The developments have been done in the framework of the OECD/NEA SAPIUM project (January 2017\textendash September 2019). This paper provides a summary of the main project outcome. Recommendations and open issues for future developments are also given.},
  file = {/home/victor/Zotero/storage/SAMJNAM2/Baccou et al. - 2019 - Development of good practice guidance for quantifi.pdf;/home/victor/Zotero/storage/NKNHW23N/S0029549319301839.html},
  journal = {Nuclear Engineering and Design},
  keywords = {Good practice guidance,Inverse quantification of uncertainty,Model input uncertainty quantification,System approach,Thermal hydraulic code,Validation},
  series = {Special {{Issue}} on {{TRENDS AND PERSPECTIVES IN NUCLEAR THERMAL}}-{{HYDRAULICS}}}
}

@inproceedings{barron_bayesian_2014,
  title = {Bayesian Properties of Normalized Maximum Likelihood and Its Fast Computation},
  booktitle = {2014 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Barron, Andrew and Roos, Teemu and Watanabe, Kazuho},
  year = {2014},
  month = jun,
  pages = {1667--1671},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/ISIT.2014.6875117},
  file = {/home/victor/Zotero/storage/RGVPKKBE/Barron et al. - 2014 - Bayesian properties of normalized maximum likeliho.pdf},
  isbn = {978-1-4799-5186-4},
  language = {en}
}

@article{bassett_maximum_2019,
  title = {Maximum a {{Posteriori Estimators}} as a {{Limit}} of {{Bayes Estimators}}},
  author = {Bassett, Robert and Deride, Julio},
  year = {2019},
  month = mar,
  volume = {174},
  pages = {129--144},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/s10107-018-1241-0},
  abstract = {Maximum a posteriori and Bayes estimators are two common methods of point estimation in Bayesian Statistics. It is commonly accepted that maximum a posteriori estimators are a limiting case of Bayes estimators with 0-1 loss. In this paper, we provide a counterexample which shows that in general this claim is false. We then correct the claim that by providing a levelset condition for posterior densities such that the result holds. Since both estimators are defined in terms of optimization problems, the tools of variational analysis find a natural application to Bayesian point estimation.},
  archivePrefix = {arXiv},
  eprint = {1611.05917},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/FWDNEZYJ/Bassett et Deride - 2019 - Maximum a Posteriori Estimators as a Limit of Baye.pdf},
  journal = {Mathematical Programming},
  keywords = {62C10; 62F10; 62F15; 65K10,Mathematics - Optimization and Control,Mathematics - Statistics Theory},
  language = {en},
  number = {1-2}
}

@article{basu_analysis_2017,
  title = {Analysis of {{Thompson Sampling}} for {{Gaussian Process Optimization}} in the {{Bandit Setting}}},
  author = {Basu, Kinjal and Ghosh, Souvik},
  year = {2017},
  month = may,
  abstract = {We consider the global optimization of a function over a continuous domain. At every evaluation attempt, we can observe the function at a chosen point in the domain and we reap the reward of the value observed. We assume that drawing these observations are expensive and noisy. We frame it as a continuum-armed bandit problem with a Gaussian Process prior on the function. In this regime, most algorithms have been developed to minimize some form of regret. Contrary to this popular norm, in this paper, we study the convergence of the sequential point \$\textbackslash boldsymbol\{x\}\^t\$ to the global optimizer \$\textbackslash boldsymbol\{x\}\^*\$ for the Thompson Sampling approach. Under some assumptions and regularity conditions, we show an exponential rate of convergence to the true optimal.},
  archivePrefix = {arXiv},
  eprint = {1705.06808},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/VCV7B7YA/Basu et Ghosh - 2017 - Analysis of Thompson Sampling for Gaussian Process.pdf},
  journal = {arXiv:1705.06808 [stat]},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{basu_robust_1998,
  title = {Robust and Efficient Estimation by Minimising a Density Power Divergence},
  author = {Basu, Ayanendranath and Harris, Ian R. and Hjort, Nils L. and Jones, M. C.},
  year = {1998},
  volume = {85},
  pages = {549--559},
  file = {/home/victor/Zotero/storage/IMF8SAEK/1997-7.pdf},
  journal = {Biometrika},
  number = {3}
}

@phdthesis{baudoui_optimisation_2012,
  title = {Optimisation Robuste Multiobjectifs Par Mod\`eles de Substitution},
  author = {Baudoui, Vincent},
  year = {2012},
  file = {/home/victor/Zotero/storage/DIHXGHU4/2012_Baudoui_Vincent.pdf},
  keywords = {Optim Robuste,Th√®se},
  school = {Toulouse, ISAE}
}

@article{bayarri_framework_2007,
  title = {A Framework for Validation of Computer Models},
  author = {Bayarri, Maria J. and Berger, James O. and Paulo, Rui and Sacks, Jerry and Cafeo, John A. and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
  year = {2007},
  volume = {49},
  pages = {138--154},
  file = {/home/victor/Zotero/storage/6RNCHIIS/Bayarri et al. - 2007 - A framework for validation of computer models.pdf},
  journal = {Technometrics},
  keywords = {Validation,Verification},
  number = {2}
}

@article{beaumont_approximate_2010,
  title = {Approximate {{Bayesian Computation}} in {{Evolution}} and {{Ecology}}},
  author = {Beaumont, Mark A.},
  year = {2010},
  month = dec,
  volume = {41},
  pages = {379--406},
  issn = {1543-592X, 1545-2069},
  doi = {10.1146/annurev-ecolsys-102209-144621},
  abstract = {In the past 10 years a statistical technique, approximate Bayesian computation (ABC), has been developed that can be used to infer parameters and choose between models in the complicated scenarios that are often considered in the environmental sciences. For example, based on gene sequence and microsatellite data, the method has been used to choose between competing models of human demographic history as well as to infer growth rates, times of divergence, and other parameters. The method fits naturally in the Bayesian inferential framework, and a brief overview is given of the key concepts. Three main approaches to ABC have been developed, and these are described and compared. Although the method arose in population genetics, ABC is increasingly used in other fields, including epidemiology, systems biology, ecology, and agent-based modeling, and many of these applications are briefly described.},
  file = {/home/victor/Zotero/storage/3D67J548/Beaumont - 2010 - Approximate Bayesian Computation in Evolution and .pdf},
  journal = {Annual Review of Ecology, Evolution, and Systematics},
  language = {en},
  number = {1}
}

@article{bect_bayesian_2017,
  title = {Bayesian Subset Simulation},
  author = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
  year = {2017},
  month = jan,
  volume = {5},
  pages = {762--786},
  issn = {2166-2525},
  doi = {10.1137/16M1078276},
  abstract = {We consider the problem of estimating a probability of failure {$\alpha$}, defined as the volume of the excursion set of a function f : X {$\subseteq$} Rd \textrightarrow{} R above a given threshold, under a given probability measure on X. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate {$\alpha$} when the number of evaluations of f is very limited and {$\alpha$} is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of f above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on f is used to define the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of f to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves significant savings in the number of function evaluations with respect to other Monte Carlo approaches.},
  archivePrefix = {arXiv},
  eprint = {1601.02557},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/CRIYDVDY/Bect et al. - 2017 - Bayesian subset simulation.pdf},
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  keywords = {Statistics - Computation},
  language = {en},
  number = {1}
}

@article{bect_sequential_2012,
  title = {Sequential Design of Computer Experiments for the Estimation of a Probability of Failure},
  author = {Bect, Julien and Ginsbourger, David and Li, Ling and Picheny, Victor and Vazquez, Emmanuel},
  year = {2012},
  month = may,
  volume = {22},
  pages = {773--793},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-011-9241-4},
  abstract = {This paper deals with the problem of estimating the volume of the excursion set of a function f : Rd \textrightarrow{} R above a given threshold, under a probability measure on Rd that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian-theoretic formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of f and aim at performing evaluations of f as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure.},
  archivePrefix = {arXiv},
  eprint = {1009.5177},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/KMNZSABA/Bect et al. - 2012 - Sequential design of computer experiments for the .pdf},
  journal = {Statistics and Computing},
  keywords = {62L05; 62C10; 62P30,Statistics - Applications,Statistics - Computation},
  language = {en},
  number = {3}
}

@article{beirlant_nonparametric_1997,
  title = {Nonparametric Entropy Estimation: {{An}} Overview},
  shorttitle = {Nonparametric Entropy Estimation},
  author = {Beirlant, Jan and Dudewicz, Edward J. and Gy{\"o}rfi, L{\'a}szl{\'o} and {Van der Meulen}, Edward C.},
  year = {1997},
  volume = {6},
  pages = {17--39},
  journal = {International Journal of Mathematical and Statistical Sciences},
  number = {1}
}

@article{beland_bayesian_nodate,
  title = {Bayesian {{Optimization Under Uncertainty}}},
  author = {Beland, Justin J and Nair, Prasanth B},
  pages = {5},
  abstract = {We consider the problem of robust optimization, where it is sought to design a system such that it sustains a specified measure of performance under uncertainty. This problem is challenging since modeling a complex system under uncertainty can be expensive and for most real-world problems robust optimization will not be computationally viable. In this paper, we propose a Bayesian methodology to efficiently solve a class of robust optimization problems that arise in engineering design under uncertainty. The central idea is to use Gaussian process models of loss functions (or robustness metrics) together with appropriate acquisition functions to guide the search for a robust optimal solution. Numerical studies on a test problem are presented to demonstrate the efficacy of the proposed approach.},
  file = {/home/victor/Zotero/storage/VJ4T9MEE/Beland et Nair - Bayesian Optimization Under Uncertainty.pdf},
  language = {en}
}

@inproceedings{benneyan_probability_2006,
  title = {Probability Distributions and Variances of Quadratic Loss Functions},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Computers}} and {{Industrial Engineering}}},
  author = {Benneyan, James and Aksezer, {\c C}a{\u g}lar},
  year = {2006},
  pages = {2890--2899},
  file = {/home/victor/Zotero/storage/LTKZVHNL/5db30bc0fe22538899a233d598828eac3e88.pdf}
}

@article{bentzien_decomposition_2014,
  title = {Decomposition and Graphical Portrayal of the Quantile Score},
  author = {Bentzien, Sabrina and Friederichs, Petra},
  year = {2014},
  month = jul,
  volume = {140},
  pages = {1924--1934},
  issn = {1477-870X},
  doi = {10.1002/qj.2284},
  abstract = {This study expands the pool of verification methods for probabilistic weather and climate predictions by a decomposition of the quantile score (QS). The QS is a proper score function and evaluates pr...},
  file = {/home/victor/Zotero/storage/JAV7U7ER/Bentzien et Friederichs - 2014 - Decomposition and graphical portrayal of the quant.pdf;/home/victor/Zotero/storage/I3BNU8QJ/qj.html},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  language = {en},
  number = {683}
}

@article{bera_mm_2002,
  title = {The {{MM}}, {{ME}}, {{ML}}, {{EL}}, {{EF}} and {{GMM}} Approaches to Estimation: A Synthesis},
  shorttitle = {The {{MM}}, {{ME}}, {{ML}}, {{EL}}, {{EF}} and {{GMM}} Approaches to Estimation},
  author = {Bera, Anil K. and Bilias, Yannis},
  year = {2002},
  month = mar,
  volume = {107},
  pages = {51--86},
  issn = {0304-4076},
  doi = {10.1016/S0304-4076(01)00113-0},
  abstract = {The 20th century began on an auspicious statistical note with the publication of Karl Pearson's (Philos. Mag. Ser. 50 (1900) 157) goodness-of-fit test, which is regarded as one of the most important scientific breakthroughs. The basic motivation behind this test was to see whether an assumed probability model adequately described the data at hand. Pearson (Philos. Trans. Roy. Soc. London Ser. A 185 (1894) 71) also introduced a formal approach to statistical estimation through his method of moments (MM) estimation. Ronald A. Fisher, while he was a third year undergraduate at the Gonville and Caius College, Cambridge, suggested the maximum likelihood estimation (MLE) procedure as an alternative to Pearson's MM approach. In 1922 Fisher published a monumental paper that introduced such basic concepts as consistency, efficiency, sufficiency\textemdash and even the term ``parameter'' with its present meaning. Fisher (Philos. Trans. Roy. Soc. London Ser. A 222 (1922) 309) provided the analytical foundation of MLE and studied its efficiency relative to the MM estimator. Fisher (J. Roy. Statist. Soc. 87 (1924a) 442) established the asymptotic equivalence of minimum {$\chi$}2 and ML estimators and wrote in favor of using minimum {$\chi$}2 method rather than Pearson's MM approach. Recently, econometricians have found working under assumed likelihood functions restrictive, and have suggested using a generalized version of Pearson's MM approach, commonly known as the GMM estimation procedure as advocated in Hansen (Econometrica 50 (1982) 1029). Earlier, Godambe (Ann. Math. Statist. 31 (1960) 1208) and Durbin (J. Roy. Statist. Soc. Ser. B 22 (1960) 139) developed the estimating function (EF) approach to estimation that has been proven very useful for many statistical models. A fundamental result is that score is the optimum EF. Ferguson (Ann. Math. Statist. 29 (1958) 1046) considered an approach very similar to GMM and showed that estimation based on the Pearson {$\chi$}2 statistic is equivalent to efficient GMM. Golan et al. (Maximum Entropy Econometrics: Robust Estimation with Limited Data. Wiley, New York, 1996) developed entropy-based formulation that allowed them to solve a wide range of estimation and inference problems in econometrics. More recently, Imbens et al. (Econometrica 66 (1998) 333), Kitamura and Stutzer (Econometrica 65 (1997) 861) and Mittelhammer et al. (Econometric Foundations. Cambridge University Press, Cambridge, 2000) put GMM within the framework of empirical likelihood (EL) and maximum entropy (ME) estimation. It can be shown that many of these estimation techniques can be obtained as special cases of minimizing Cressie and Read (J. Roy. Statist. Soc. Ser. B 46 (1984) 440) power divergence criterion that comes directly from the Pearson (1900) {$\chi$}2 statistic. In this way we are able to assimilate a number of seemingly unrelated estimation techniques into a unified framework.},
  file = {/home/victor/Zotero/storage/GJL37QWG/Bera et Bilias - 2002 - The MM, ME, ML, EL, EF and GMM approaches to estim.pdf;/home/victor/Zotero/storage/PMR4TSV7/S0304407601001130.html},
  journal = {Journal of Econometrics},
  keywords = {Empirical likelihood,Entropy,Estimating function,Generalized method of moments,History of estimation,Karl Pearson's goodness-of-fit statistic,Likelihood,Method of moment,Power divergence criterion},
  language = {en},
  number = {1},
  series = {Information and {{Entropy Econometrics}}}
}

@article{berger_deriving_1992,
  title = {Deriving {{Generalized Means}} as {{Least Squares}} and {{Maximum Likelihood Estimates}}},
  author = {Berger, Roger L. and Casella, George},
  year = {1992},
  month = nov,
  volume = {46},
  pages = {279--282},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1992.10475904},
  abstract = {Functions called generalized means are of interest in statistics because they are simple to compute, have intuitive appeal, and can serve as reasonable parameter estimates. The well-known arithmetic, geometric, and harmonic means are all examples of generalized means. We show how generalized means can be derived in a unified way, as least squares estimates for a transformed data set. We also investigate models that have generalized means as their maximum likelihood estimates.},
  file = {/home/victor/Zotero/storage/54YTW8HV/BU-1133-M.pdf;/home/victor/Zotero/storage/G8XTTU2P/Berger et Casella - 1992 - Deriving Generalized Means as Least Squares and Ma.pdf;/home/victor/Zotero/storage/TQKPYYE7/00031305.1992.html},
  journal = {The American Statistician},
  keywords = {Arithmetic mean,Exponential family,Geometric mean,Harmonic mean},
  number = {4}
}

@article{berger_formal_2009,
  title = {The Formal Definition of Reference Priors},
  author = {Berger, James O. and Bernardo, Jos{\'e} M. and Sun, Dongchu},
  year = {2009},
  month = apr,
  volume = {37},
  pages = {905--938},
  issn = {0090-5364},
  doi = {10.1214/07-AOS587},
  abstract = {Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically.},
  archivePrefix = {arXiv},
  eprint = {0904.0156},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/WJGEVP2V/Berger et al. - 2009 - The formal definition of reference priors.pdf;/home/victor/Zotero/storage/YC35QS73/0904.html},
  journal = {The Annals of Statistics},
  keywords = {62F15 (Primary) 62A01; 62B10 (Secondary),Mathematics - Statistics Theory},
  number = {2}
}

@article{berger_integrated_1999,
  title = {Integrated Likelihood Methods for Eliminating Nuisance Parameters},
  author = {Berger, James O. and Liseo, Brunero and Wolpert, Robert L.},
  year = {1999},
  month = feb,
  volume = {14},
  pages = {1--28},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1009211804},
  abstract = {Elimination of nuisance parameters is a central problem in statistical inference and has been formally studied in virtually all approaches to inference. Perhaps the least studied approach is elimination of nuisance parameters through integration, in the sense that this is viewed as an almost incidental byproduct of Bayesian analysis and is hence not something which is deemed to require separate study. There is, however, considerable value in considering integrated likelihood on its own, especially versions arising from default or noninformative priors. In this paper, we review such common integrated likelihoods and discuss their strengths and weaknesses relative to other methods.},
  file = {/home/victor/Zotero/storage/K775LQTY/Berger et al. - 1999 - Integrated likelihood methods for eliminating nuis.pdf;/home/victor/Zotero/storage/KVZ6CQTS/1009211804.html},
  journal = {Statistical Science},
  keywords = {Marginal likelihood,nuisance parameters,profile likelihood,reference priors},
  language = {en},
  mrnumber = {MR1702200},
  number = {1},
  zmnumber = {1059.62521}
}

@article{berger_overview_1994,
  title = {An Overview of Robust {{Bayesian}} Analysis},
  author = {Berger, James O. and Moreno, El{\'i}as and Pericchi, Luis Raul and Bayarri, M. Jes{\'u}s and Bernardo, Jos{\'e} M. and Cano, Juan A. and {De la Horra}, Juli{\'a}n and Mart{\'i}n, Jacinto and {R{\'i}os-Ins{\'u}a}, David and Betr{\`o}, Bruno},
  year = {1994},
  volume = {3},
  pages = {5--124},
  file = {/home/victor/Zotero/storage/8IEPC4A3/tr93-53c.pdf;/home/victor/Zotero/storage/KN3BLS4A/BF02562676.html},
  journal = {Test},
  keywords = {Bayesian inference},
  number = {1}
}

@article{bertsimas_robust_2014,
  title = {Robust {{Sample Average Approximation}}},
  author = {Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
  year = {2014},
  month = aug,
  abstract = {Sample average approximation (SAA) is a widely popular approach to data-driven decisionmaking under uncertainty. Under mild assumptions, SAA is both tractable and enjoys strong asymptotic performance guarantees. Similar guarantees, however, do not typically hold in finite samples. In this paper, we propose a modification of SAA, which we term Robust SAA, which retains SAA's tractability and asymptotic properties and, additionally, enjoys strong finite-sample performance guarantees. The key to our method is linking SAA, distributionally robust optimization, and hypothesis testing of goodness-of-fit. Beyond Robust SAA, this connection provides a unified perspective enabling us to characterize the finite sample and asymptotic guarantees of various other data-driven procedures that are based upon distributionally robust optimization. This analysis provides insight into the practical performance of these various methods in real applications. We present examples from inventory management and portfolio allocation, and demonstrate numerically that our approach outperforms other data-driven approaches in these applications.},
  archivePrefix = {arXiv},
  eprint = {1408.4445},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/FUIBHXQN/Bertsimas et al. - 2014 - Robust Sample Average Approximation.pdf},
  journal = {arXiv:1408.4445 [math]},
  keywords = {Mathematics - Optimization and Control},
  language = {en},
  primaryClass = {math}
}

@article{bertsimas_theory_2010,
  title = {Theory and {{Applications}} of {{Robust Optimization}}},
  author = {Bertsimas, Dimitris and Brown, David B. and Caramanis, Constantine},
  year = {2010},
  month = oct,
  abstract = {In this paper we survey the primary research, both theoretical and applied, in the area of Robust Optimization (RO). Our focus is on the computational attractiveness of RO approaches, as well as the modeling power and broad applicability of the methodology. In addition to surveying prominent theoretical results of RO, we also present some recent results linking RO to adaptable models for multi-stage decision-making problems. Finally, we highlight applications of RO across a wide spectrum of domains, including finance, statistics, learning, and various areas of engineering.},
  archivePrefix = {arXiv},
  eprint = {1010.5445},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/T6QT64CY/Bertsimas et al. - 2010 - Theory and Applications of Robust Optimization.pdf;/home/victor/Zotero/storage/6DFX49PU/1010.html},
  journal = {arXiv:1010.5445 [cs, math]},
  keywords = {90C25,Computer Science - Computational Engineering; Finance; and Science,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@article{betancourt_conceptual_2017,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2017},
  month = jan,
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous under- standing of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is con- fined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any ex- haustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archivePrefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/97IXVFQ5/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;/home/victor/Zotero/storage/3VVTEM7T/1701.html},
  journal = {arXiv:1701.02434 [stat]},
  keywords = {Statistics - Methodology},
  primaryClass = {stat}
}

@phdthesis{bettinger_inversion_2009,
  title = {Inversion d'un Syst\`eme Par Krigeage: Application \`a La Synth\`ese Des Catalyseurs \`a Haut D\'ebit},
  shorttitle = {Inversion d'un Syst\`eme Par Krigeage},
  author = {Bettinger, R{\'e}gis},
  year = {2009},
  file = {/home/victor/Zotero/storage/3RXCDSE8/Bettinger.pdf},
  school = {Universit\'e de Nice Sophia Antipolis},
  type = {{{PhD Thesis}}}
}

@article{beyer_robust_2007,
  title = {Robust Optimization \textendash{} {{A}} Comprehensive Survey},
  author = {Beyer, Hans-Georg and Sendhoff, Bernhard},
  year = {2007},
  month = jul,
  volume = {196},
  pages = {3190--3218},
  issn = {00457825},
  doi = {10.1016/j.cma.2007.03.003},
  abstract = {This paper reviews the state-of-the-art in robust design optimization \textendash{} the search for designs and solutions which are immune with respect to production tolerances, parameter drifts during operation time, model sensitivities and others. Starting with a short glimps of Taguchi's robust design methodology, a detailed survey of approaches to robust optimization is presented. This includes a detailed discussion on how to account for design uncertainties and how to measure robustness (i.e., how to evaluate robustness). The main focus will be on the different approaches to perform robust optimization in practice including the methods of mathematical programming, deterministic nonlinear optimization, and direct search methods such as stochastic approximation and evolutionary computation. It discusses the strengths and weaknesses of the different methods, thus, providing a basis for guiding the engineer to the most appropriate techniques. It also addresses performance aspects and test scenarios for direct robust optimization techniques.},
  file = {/home/victor/Zotero/storage/CRUQZVV2/Beyer et Sendhoff - 2007 - Robust optimization ‚Äì A comprehensive survey.pdf},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  language = {en},
  number = {33-34}
}

@article{bichon_efficient_nodate,
  title = {{{EFFICIENT SURROGATE MODELING FOR RELIABILITY ANALYSIS AND DESIGN}}},
  author = {Bichon, Barron James},
  pages = {169},
  file = {/home/victor/Zotero/storage/PBX8ZJTE/Bichon - EFFICIENT SURROGATE MODELING FOR RELIABILITY ANALY.pdf},
  language = {en}
}

@article{bickel_fast_2005,
  title = {On a {{Fast}}, {{Robust Estimator}} of the {{Mode}}: {{Comparisons}} to {{Other Robust Estimators}} with {{Applications}}},
  shorttitle = {On a {{Fast}}, {{Robust Estimator}} of the {{Mode}}},
  author = {Bickel, David R. and Fruehwirth, Rudolf},
  year = {2005},
  month = may,
  abstract = {Advances in computing power enable more widespread use of the mode, which is a natural measure of central tendency since, as the most probable value, it is not influenced by the tails in the distribution. The properties of the half-sample mode, which is a simple and fast estimator of the mode of a continuous distribution, are studied. The half-sample mode is less sensitive to outliers than most other estimators of location, including many other low-bias estimators of the mode. Its breakdown point is one half, equal to that of the median. However, because of its finite rejection point, the half-sample mode is much less sensitive to outliers that are all either greater or less than the other values of the sample. This is confirmed by applying the mode estimator and the median to samples drawn from normal, lognormal, and Pareto distributions contaminated by outliers. It is also shown that the half-sample mode, in combination with a robust scale estimator, is a highly robust starting point for iterative robust location estimators such as Huber's M-estimator. The half-sample mode can easily be generalized to modal intervals containing more or less than half of the sample. An application of such an estimator to the finding of collision points in high-energy proton-proton interactions is presented.},
  archivePrefix = {arXiv},
  eprint = {math/0505419},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/QZFLB585/Bickel et Fruehwirth - 2005 - On a Fast, Robust Estimator of the Mode Compariso.pdf;/home/victor/Zotero/storage/M5EJDIBY/0505419.html},
  journal = {arXiv:math/0505419},
  keywords = {Mode estimation}
}

@article{bickel_robust_2003,
  title = {Robust and Efficient Estimation of the Mode of Continuous Data: The Mode as a Viable Measure of Central Tendency},
  shorttitle = {Robust and Efficient Estimation of the Mode of Continuous Data},
  author = {Bickel, David R.},
  year = {2003},
  month = dec,
  volume = {73},
  pages = {899--912},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/0094965031000097809},
  abstract = {Although a natural measure of the central tendency of a sample of continuous data is its mode (the most probable value), the mean and median are the most popular measures of location due to their simplicity and ease of estimation. The median is often used instead of the mean for asymmetric data because it is closer to the mode and is insensitive to extreme values in the sample. However, the mode itself can be reliably estimated by first transforming the data into approximately normal data by raising the values to a real power, and then estimating the mean and standard deviation of the transformed data. With this method, two estimators of the mode of the original data are proposed: a simple estimator based on estimating the mean by the sample mean and the standard deviation by the sample standard deviation, and a more robust estimator based on estimating the mean by the median and the standard deviation by the standardized median absolute deviation.},
  file = {/home/victor/Zotero/storage/54JEYDX5/Bickel - 2003 - Robust and efficient estimation of the mode of con.pdf},
  journal = {Journal of Statistical Computation and Simulation},
  language = {en},
  number = {12}
}

@article{bigoni_efficient_2016,
  title = {Efficient Uncertainty Quantification of a Fully Nonlinear and Dispersive Water Wave Model with Random Inputs},
  author = {Bigoni, Daniele and {Engsig-Karup}, Allan P. and Eskilsson, Claes},
  year = {2016},
  month = dec,
  volume = {101},
  pages = {87--113},
  issn = {0022-0833, 1573-2703},
  doi = {10.1007/s10665-016-9848-8},
  abstract = {A major challenge in next-generation industrial applications is to improve numerical analysis by quantifying uncertainties in predictions. In this work we present a formulation of a fully nonlinear and dispersive potential flow water wave model with random inputs for the probabilistic description of the evolution of waves. The model is analyzed using random sampling techniques and non-intrusive methods based on generalized Polynomial Chaos (PC). These methods allow to accurately and efficiently estimate the probability distribution of the solution and require only the computation of the solution in different points in the parameter space, allowing for the reuse of existing simulation software. The choice of the applied methods is driven by the number of uncertain input parameters and by the fact that finding the solution of the considered model is computationally intensive. We revisit experimental benchmarks often used for validation of deterministic water wave models. Based on numerical experiments and assumed uncertainties in boundary data, our analysis reveals that some of the known discrepancies from deterministic simulation in comparison with experimental measurements could be partially explained by the variability in the model input. We finally present a synthetic experiment studying the variance based sensitivity of the wave load on an offshore structure to a number of input uncertainties. In the numerical examples presented the PC methods have exhibited fast convergence, suggesting that the problem is amenable to being analyzed with such methods.},
  archivePrefix = {arXiv},
  eprint = {1410.6338},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/SMSSFESF/Bigoni et al. - 2016 - Efficient uncertainty quantification of a fully no.pdf},
  journal = {Journal of Engineering Mathematics},
  keywords = {65C99; 76B15,Physics - Computational Physics},
  language = {en},
  number = {1}
}

@article{bilionis_solution_2013,
  title = {Solution of Inverse Problems with Limited Forward Solver Evaluations: A {{Bayesian}} Perspective},
  shorttitle = {Solution of Inverse Problems with Limited Forward Solver Evaluations},
  author = {Bilionis, I. and Zabaras, N.},
  year = {2013},
  volume = {30},
  pages = {015004},
  file = {/home/victor/Zotero/storage/SUA66K2E/Bilionis et Zabaras - 2013 - Solution of inverse problems with limited forward .pdf;/home/victor/Zotero/storage/TMEH4W3B/Bilionis et Zabaras - 2013 - Solution of inverse problems with limited forward},
  journal = {Inverse Problems},
  number = {1}
}

@book{billingsley_probability_2008,
  title = {Probability and Measure},
  author = {Billingsley, Patrick},
  year = {2008},
  publisher = {{John Wiley \& Sons}},
  file = {/home/victor/Zotero/storage/7U9W5HAJ/Billingsley - 2008 - Probability and measure.pdf}
}

@article{binois_replication_2019,
  title = {Replication or {{Exploration}}? {{Sequential Design}} for {{Stochastic Simulation Experiments}}},
  shorttitle = {Replication or {{Exploration}}?},
  author = {Binois, Micka{\"e}l and Huang, Jiangeng and Gramacy, Robert B. and Ludkovski, Mike},
  year = {2019},
  month = jan,
  volume = {61},
  pages = {7--23},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2018.1469433},
  abstract = {We investigate the merits of replication, and provide methods for optimal design (including replicates), with the goal of obtaining globally accurate emulation of noisy computer simulation experiments. We first show that replication can be beneficial from both design and computational perspectives, in the context of Gaussian process surrogate modeling. We then develop a lookahead-based sequential design scheme that can determine if a new run should be at an existing input location (i.e., replicate) or at a new one (explore). When paired with a newly developed heteroscedastic Gaussian process model, our dynamic design scheme facilitates learning of signal and noise relationships which can vary throughout the input space. We show that it does so efficiently, on both computational and statistical grounds. In addition to illustrative synthetic examples, we demonstrate performance on two challenging real-data simulation experiments, from inventory management and epidemiology. Supplementary materials for the article are available online.},
  file = {/home/victor/Zotero/storage/WX3AA43G/Binois et al. - 2019 - Replication or Exploration Sequential Design for .pdf;/home/victor/Zotero/storage/PVDQMSZ5/00401706.2018.html},
  journal = {Technometrics},
  keywords = {Computer experiment,Gaussian process,Input-dependent noise,Lookahead,Replicated observations,Surrogate model},
  number = {1}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {{springer}},
  file = {/home/victor/Zotero/storage/DDVRV3WH/Bishop - 2006 - Pattern recognition and machine learning.pdf;/home/victor/Zotero/storage/W2MZ8V2M/Bishop - Pattern Recognition And Machine Learning - Springer 2006.pdf;/home/victor/Zotero/storage/XW2ADTWV/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{bissiri_general_2016,
  title = {A General Framework for Updating Belief Distributions},
  author = {Bissiri, P. G. and Holmes, C. C. and Walker, S. G.},
  year = {2016},
  volume = {78},
  pages = {1103--1130},
  issn = {1467-9868},
  doi = {10.1111/rssb.12158},
  abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered as a special case. Modern application areas make it increasingly challenging for Bayesians to attempt to model the true data-generating mechanism. For instance, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our framework uses loss functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.},
  file = {/home/victor/Zotero/storage/77RV5X3W/Bissiri et al. - 2016 - A general framework for updating belief distributi.pdf;/home/victor/Zotero/storage/53IJDAIQ/rssb.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Decision theory,General Bayesian updating,Generalized estimating equations,Gibbs posteriors,Information,Loss function,Maximum entropy,Provably approximately correct Bayes methods,Self-information loss function},
  language = {en},
  number = {5}
}

@inproceedings{blanchard_polynomial_,
  title = {A {{Polynomial Chaos}} Based {{Bayesian Approach}} for {{Estimating Uncertain Parameters}} of {{Mechanical Systems}}},
  author = {Blanchard, Emmanuel and Sandu, Corina and Sandu, Adrian},
  file = {/home/victor/Zotero/storage/WPFGV2WC/Blanchard_DETC2007_34600.pdf}
}

@misc{blanchet-scalliet_specific_2017,
  title = {A Specific Kriging Kernel for Dimensionality Reduction: {{Isotropic}} by Group Kernel},
  shorttitle = {A Specific Kriging Kernel for Dimensionality Reduction},
  author = {{Blanchet-Scalliet}, Christophette and Helbert, C{\'e}line and Ribaud, M{\'e}lina and Vial, C{\'e}line},
  year = {2017},
  month = mar,
  abstract = {In the context of computer experiments, metamodels are largely used to represent the output of computer codes. Among these models, Gaussian process regression (kriging) is very efficient see e.g Snelson (2008). In high dimension that is with a large number of input variables , but with few observations the classical anisotropic kriging becomes inefficient and sometimes completely wrong. One way to overcome this drawback is to use the isotropic kernel which is more robust because it estimates not as many parameters. However this model is too restrictive. The aim of this paper is to construct a model between these two, that is at the same time a robust and a flexible model. These two skills are necessary for a model in high dimension. We propose a kernel which is an answer to these requests and that we call isotropic by group kernel. This kernel is a tensor product of few isotropic kernels built on well-chosen subgroup of variables. The number and the composition of the groups are found by an algorithm which explores different structures. The choice of the best model is based on the quality of prediction.},
  file = {/home/victor/Zotero/storage/EVJFQIXZ/Blanchet-Scalliet et al. - 2017 - A specific kriging kernel for dimensionality reduc.pdf;/home/victor/Zotero/storage/GSUA7RYB/hal-01496521v1.html},
  language = {en}
}

@article{blomker_predictability_2006,
  title = {Predictability of the {{Burgers}} Dynamics under Model Uncertainty},
  author = {Bl{\"o}mker, Dirk and Duan, Jinqiao},
  year = {2006},
  month = jul,
  abstract = {Complex systems may be subject to various uncertainties. A great effort has been concentrated on predicting the dynamics under uncertainty in initial conditions. In the present work, we consider the well-known Burgers equation with random boundary forcing or with random body forcing. Our goal is to attempt to understand the stochastic Burgers dynamics by predicting or estimating the solution processes in various diagnostic metrics, such as mean length scale, correlation function and mean energy. First, for the linearized model, we observe that the important statistical quantities like mean energy or correlation functions are the same for the two types of random forcing, even though the solutions behave very differently. Second, for the full nonlinear model, we estimate the mean energy for various types of random body forcing, highlighting the different impact on the overall dynamics of space-time white noises, trace class white-in-time and colored-in-space noises, point noises, additive noises or multiplicative noises.},
  archivePrefix = {arXiv},
  eprint = {math/0607357},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/MKLWLZCZ/Bl√∂mker et Duan - 2006 - Predictability of the Burgers dynamics under model.pdf},
  journal = {arXiv:math/0607357},
  keywords = {Mathematics - Classical Analysis and ODEs,Mathematics - Probability},
  language = {en}
}

@article{bogunovic_robust_nodate,
  title = {Robust {{Adaptive Decision Making}}: {{Bayesian Optimization}} and {{Beyond}}},
  author = {Bogunovic, Ilija},
  pages = {200},
  file = {/home/victor/Zotero/storage/RAMIJHXE/Bogunovic - Robust Adaptive Decision Making Bayesian Optimiza.pdf},
  language = {en}
}

@article{bolton_applications_2019,
  title = {Applications of {{Deep Learning}} to {{Ocean Data Inference}} and {{Sub}}-{{Grid Parameterisation}}},
  author = {Bolton, Thomas and Zanna, Laure},
  year = {2019},
  month = jan,
  volume = {0},
  issn = {1942-2466},
  doi = {10.1029/2018MS001472},
  abstract = {Abstract Oceanographic observations are limited by sampling rates, while ocean models are limited by finite resolution and high viscosity and diffusion coefficients. Therefore both data from observations and ocean models lack information at small- and fast-scales. Methods are needed to either extract information, extrapolate, or up-scale existing oceanographic datasets, to account for or represent unresolved physical processes. Here we use machine learning to leverage observations and model data by predicting unresolved turbulent processes and sub-surface flow fields. As a proof-of-concept, we train convolutional neural networks on degraded-data from a high-resolution quasi-geostrophic ocean model. We demonstrate that convolutional neural networks successfully replicate the spatio-temporal variability of the sub-grid eddy momentum forcing, are capable of generalising to a range of dynamical behaviours, and can be forced to respect global momentum conservation. The training data of our convolutional neural networks can be sub-sampled to 10-20\% of the original size without a significant decrease in accuracy. We also show that the sub-surface flow field can be predicted using only information at the surface (e.g., using only satellite altimetry data). Our study indicates that data-driven approaches can be exploited to predict both sub-grid and large-scale processes, while respecting physical principles, even when data is limited to a particular region or external forcing. Our in-depth study presents evidence for the successful design of ocean eddy parameterisations for implementation in coarse-resolution climate models.},
  file = {/home/victor/Zotero/storage/D36QTPGZ/Bolton et Zanna - 2019 - Applications of Deep Learning to Ocean Data Infere.pdf;/home/victor/Zotero/storage/2EJ763U5/2018MS001472.html},
  journal = {Journal of Advances in Modeling Earth Systems},
  keywords = {Data Inference,Eddies,Machine learning,Oceanography,Turbulence},
  number = {ja}
}

@inproceedings{bond_parameterized_2005,
  title = {Parameterized Model Order Reduction of Nonlinear Dynamical Systems},
  booktitle = {Proceedings of the 2005 {{IEEE}}/{{ACM International}} Conference on {{Computer}}-Aided Design},
  author = {Bond, Brad and Daniel, Luca},
  year = {2005},
  pages = {487--494},
  publisher = {{IEEE Computer Society}},
  file = {/home/victor/Zotero/storage/SFL2S7Y6/pub219_000.pdf},
  keywords = {Moment matching,MOR methods}
}

@article{bonhomme_minimizing_nodate,
  title = {Minimizing {{Sensitivity}} to {{Model Misspecification}}},
  author = {Bonhomme, Stephane and Weidner, Martin},
  pages = {35},
  abstract = {We propose a framework to compute predictions based on an economic model when the model may be misspecified. Our approach relies on minimizing sensitivity of the estimates to the type of misspecification that is most influential for the parameter of interest. We rely on a local asymptotic approach where the degree of misspecification is indexed by the sample size. This results in simple rules to adjust the predictions from the reference model. We calibrate the degree of misspecification using a detection error probability approach, which allows us to perform systematic sensitivity analysis in both point-identified and partially-identified settings. We study three examples: demand analysis, treatment effects estimation under selection on observables, and panel data models where the distribution of individual effects may be misspecified and the number of time periods is small.},
  file = {/home/victor/Zotero/storage/P5MMJB99/Bonhomme et Weidner - Minimizing Sensitivity to Model MisspeciÔ¨Åcation.pdf},
  language = {en}
}

@book{booker_rigorous_1998,
  title = {A {{Rigorous Framework}} for {{Optimization}} of {{Expensive Functions}} by {{Surrogates}}},
  author = {Booker, Andrew J. and Jr, J. E. Dennis and Frank, Paul D. and Serafini, David B. and Torczon, Virginia and Trosset, Michael W.},
  year = {1998},
  abstract = {The goal of the research reported here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical. This paper presents and analyzes a framework for generating a sequence of approximations to the objective function and managing the use of these approximations as surrogates for optimization. The result is to obtain convergence to a minimizer of an expensive objective function subject to simple constraints. The approach is widely applicable because it does not require, or even explicitly approximate, derivatives of the objective. Numerical results are presented for a 31-variable helicopter rotor blade design example and for a standard optimization test example.},
  file = {/home/victor/Zotero/storage/A23KJ6LH/Booker et al. - 1998 - A Rigorous Framework for Optimization of Expensive.pdf;/home/victor/Zotero/storage/YPKX7WD3/summary.html}
}

@article{borman_expectation_2004,
  title = {The Expectation Maximization Algorithm-a Short Tutorial},
  author = {Borman, Sean},
  year = {2004},
  pages = {1--9},
  file = {/home/victor/Zotero/storage/HGJ62YFV/EM_algorithm.pdf},
  journal = {Submitted for publication},
  keywords = {EM algorithm}
}

@inproceedings{bossek_learning_2015,
  title = {Learning {{Feature}}-{{Parameter Mappings}} for {{Parameter Tuning}} via the {{Profile Expected Improvement}}},
  booktitle = {Proceedings of the 2015 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Bossek, Jakob and Bischl, Bernd and Wagner, Tobias and Rudolph, G{\"u}nter},
  year = {2015},
  pages = {1319--1326},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2739480.2754673},
  abstract = {The majority of algorithms can be controlled or adjusted by parameters. Their values can substantially affect the algorithms' performance. Since the manual exploration of the parameter space is tedious -- even for few parameters -- several automatic procedures for parameter tuning have been proposed. Recent approaches also take into account some characteristic properties of the problem instances, frequently termed instance features. Our contribution is the proposal of a novel concept for feature-based algorithm parameter tuning, which applies an approximating surrogate model for learning the continuous feature-parameter mapping. To accomplish this, we learn a joint model of the algorithm performance based on both the algorithm parameters and the instance features. The required data is gathered using a recently proposed acquisition function for model refinement in surrogate-based optimization: the profile expected improvement. This function provides an avenue for maximizing the information required for the feature-parameter mapping, i.e., the mapping from instance features to the corresponding optimal algorithm parameters. The approach is validated by applying the tuner to exemplary evolutionary algorithms and problems, for which theoretically grounded or heuristically determined feature-parameter mappings are available.},
  file = {/home/victor/Zotero/storage/CSL4V53D/Bossek et al. - 2015 - Learning Feature-Parameter Mappings for Parameter .pdf},
  isbn = {978-1-4503-3472-3},
  keywords = {evolutionary algorithms,model-based optimization,parameter tuning},
  series = {{{GECCO}} '15}
}

@book{boucheron_concentration_2013,
  title = {Concentration {{Inequalities}}: {{A Nonasymptotic Theory}} of {{Independence}}},
  shorttitle = {Concentration {{Inequalities}}},
  author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year = {2013},
  month = feb,
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199535255.001.0001},
  file = {/home/victor/Zotero/storage/7CQ8ZP24/Boucheron et al. - 2013 - Concentration Inequalities A Nonasymptotic Theory.pdf},
  isbn = {978-0-19-953525-5},
  language = {en}
}

@article{bouezmarni_nonparametric_2010,
  title = {Nonparametric Density Estimation for Multivariate Bounded Data},
  author = {Bouezmarni, Taoufik and Rombouts, Jeroen V. K.},
  year = {2010},
  month = jan,
  volume = {140},
  pages = {139--152},
  issn = {0378-3758},
  doi = {10.1016/j.jspi.2009.07.013},
  abstract = {We propose a new nonparametric estimator for the density function of multivariate bounded data. As frequently observed in practice, the variables may be partially bounded (e.g. nonnegative) or completely bounded (e.g. in the unit interval). In addition, the variables may have a point mass. We reduce the conditions on the underlying density to a minimum by proposing a nonparametric approach. By using a gamma, a beta, or a local linear kernel (also called boundary kernels), in a product kernel, the suggested estimator becomes simple in implementation and robust to the well known boundary bias problem. We investigate the mean integrated squared error properties, including the rate of convergence, uniform strong consistency and asymptotic normality. We establish consistency of the least squares cross-validation method to select optimal bandwidth parameters. A detailed simulation study investigates the performance of the estimators. Applications using lottery and corporate finance data are provided.},
  file = {/home/victor/Zotero/storage/3Z77C6KS/Bouezmarni et Rombouts - 2010 - Nonparametric density estimation for multivariate .pdf;/home/victor/Zotero/storage/UANRQMPP/S0378375809002249.html},
  journal = {Journal of Statistical Planning and Inference},
  keywords = {Asymmetric kernels,Asymptotic properties,Bandwidth selection,Density estimation,Least squares cross-validation,Multivariate boundary bias,Nonparametric multivariate density estimation},
  number = {1}
}

@article{bouhlel_gradient-enhanced_2017,
  title = {Gradient-Enhanced Kriging for High-Dimensional Problems},
  author = {Bouhlel, Mohamed Amine and Martins, Joaquim R. R. A.},
  year = {2017},
  month = aug,
  abstract = {Surrogate models provide a low computational cost alternative to evaluating expensive functions. The construction of accurate surrogate models with large numbers of independent variables is currently prohibitive because it requires a large number of function evaluations. Gradient-enhanced kriging has the potential to reduce the number of function evaluations for the desired accuracy when efficient gradient computation, such as an adjoint method, is available. However, current gradient-enhanced kriging methods do not scale well with the number of sampling points due to the rapid growth in the size of the correlation matrix where new information are added for each sampling point in each direction of the design space. They do not scale well with the number of independent variables either due to the increase in the number of hyperparameters that needs to be estimated. To address this issue, we develop a new gradient-enhanced surrogate model approach that drastically reduced the number of hyperparameters through the use of the partial-least squares method that maintains accuracy. In addition, this method is able to control the size of the correlation matrix by adding only relevant points defined through the information provided by the partial-least squares method. To validate our method, we compare the global accuracy of the proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions, as well as engineering problems of varied complexity with up to 15 dimensions. We show that the proposed method requires fewer sampling points than conventional methods to obtain a desired accuracy, or provides more accuracy for a fixed budget of sampling points. In some cases, we get over 3 times more accurate models than a bench of surrogate models from the literature, and also over 3200 times faster than standard gradient-enhanced kriging models.},
  archivePrefix = {arXiv},
  eprint = {1708.02663},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/L7NWKUIQ/Bouhlel et Martins - 2017 - Gradient-enhanced kriging for high-dimensional pro.pdf},
  journal = {arXiv:1708.02663 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bouhlel_gradient-enhanced_2019,
  title = {Gradient-Enhanced Kriging for High-Dimensional Problems},
  author = {Bouhlel, Mohamed A. and Martins, Joaquim R. R. A.},
  year = {2019},
  month = jan,
  volume = {35},
  pages = {157--173},
  issn = {1435-5663},
  doi = {10.1007/s00366-018-0590-x},
  abstract = {Surrogate models provide an affordable alternative to the evaluation of expensive deterministic functions. However, the construction of accurate surrogate models with many independent variables is currently prohibitive because they require a large number of function evaluations for the desired accuracy. Gradient-enhanced kriging has the potential to reduce the number of evaluations when efficient gradient computation, such as an adjoint method, is available. However, current gradient-enhanced kriging methods do not scale well with the number of sampling points because of the rapid growth in the size of the correlation matrix, where new information is added for each sampling point in each direction of the design space. Furthermore, they do not scale well with the number of independent variables because of the increase in the number of hyperparameters that must be estimated. To address this issue, we develop a new gradient-enhanced surrogate model approach that drastically reduces the number of hyperparameters through the use of the partial least squares method to maintain accuracy. In addition, this method is able to control the size of the correlation matrix by adding only relevant points defined by the information provided by the partial least squares method. To validate our method, we compare the global accuracy of the proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions, as well as engineering problems of varied complexity with up to 15 dimensions. We show that the proposed method requires fewer sampling points than conventional methods to obtain the desired accuracy, or it provides more accuracy for a fixed budget of sampling points. In some cases, we get models that are over three times more accurate than previously developed surrogate models for the same computational time, and over 3200 times faster than standard gradient-enhanced kriging models for the same accuracy.},
  file = {/home/victor/Zotero/storage/CEZELLGQ/Bouhlel et Martins - 2019 - Gradient-enhanced kriging for high-dimensional pro.pdf},
  journal = {Engineering with Computers},
  keywords = {GP,High dimension},
  language = {en},
  number = {1}
}

@article{bouhlel_improving_2016,
  title = {Improving Kriging Surrogates of High-Dimensional Design Models by {{Partial Least Squares}} Dimension Reduction},
  author = {Bouhlel, Mohamed Amine and Bartoli, Nathalie and Otsmane, Abdelkader and Morlier, Joseph},
  year = {2016},
  month = may,
  volume = {53},
  pages = {935--952},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-015-1395-9},
  abstract = {Engineering computer codes are often computationally expensive. To lighten this load, we exploit new covariance kernels to replace computationally expensive codes with surrogate models. For input spaces with large dimensions, using the kriging model in the standard way is computationally expensive because a large covariance matrix must be inverted several times to estimate the parameters of the model. We address this issue herein by constructing a covariance kernel that depends on only a few parameters. The new kernel is constructed based on information obtained from the Partial Least Squares method. Promising results are obtained for numerical examples with up to 100 dimensions, and significant computational gain is obtained while maintaining sufficient accuracy.},
  file = {/home/victor/Zotero/storage/94JUCVF2/Bouhlel et al. - 2016 - Improving kriging surrogates of high-dimensional d.pdf},
  journal = {Structural and Multidisciplinary Optimization},
  keywords = {GP,High dimension},
  language = {en},
  number = {5}
}

@phdthesis{boutet_estimation_2015,
  title = {Estimation Du Frottement Sur Le Fond Pour La Mod\'elisation de La Mar\'ee Barotrope},
  author = {Boutet, Martial},
  year = {2015},
  file = {/home/victor/Zotero/storage/CSL2KG7Y/these_martial_boutet.pdf},
  keywords = {Mar√©e,Th√®se},
  school = {Universit\'e d'Aix Marseille}
}

@book{boyd_convex_2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  file = {/home/victor/Zotero/storage/YZQWMY7F/Boyd et Vandenberghe - 2004 - Convex optimization.pdf},
  isbn = {978-0-521-83378-3},
  keywords = {Convex functions,Mathematical optimization},
  language = {en},
  lccn = {QA402.5 .B69 2004}
}

@phdthesis{brachet_schemas_2018,
  title = {Sch\'emas Compacts Hermitiens Sur La {{Sph\`ere}} : Applications En Climatologie et Oc\'eanographie Num\'erique},
  shorttitle = {Sch\'emas Compacts Hermitiens Sur La {{Sph\`ere}}},
  author = {Brachet, Matthieu},
  year = {2018},
  month = jul,
  abstract = {L'enjeu de la simulation de la dynamique atmosph\'erique et oc\'eanographique a pris ces derni\`eres ann\'ees une importance accrue avec la question du r\'echauffement climatique. Le mod\`ele \`a simuler est complexe. Il combine les \'equations de la m\'ecanique des fluides avec celles de la thermodynamique. Au 19\`eme si\`ecle, le math\'ematicien Adh\'emar Barr\'e de Saint-Venant formule un syst\`eme d'\'equations aux d\'eriv\'ees partielles d\'ecrivant les mouvements d'un fluide soumis \`a la gravit\'e et de faible \'epaisseur. Il s'agit des \'equations Shallow Water. L'objectif de cette th\`ese est de d\'evelopper et d'analyser un algorithme de r\'esolution des \'equations Shallow Water sur une sph\`ere en rotation. Dans un premier temps, j'\'etudie diff\'erents aspects math\'ematiques des op\'erateurs aux diff\'erences finis utilis\'es par la suite en g\'eom\'etrie sph\'erique. Les sch\'emas aux diff\'erences obtenus sont utilis\'es pour r\'esoudre l'\'equation de transport, l'\'equation des ondes et l'\'equation de Burgers. Les propri\'et\'es de stabilit\'e pr\'ecision et conservation sont analys\'ees. Dans un second temps, la grille Cubed-Sphere est introduite et analys\'ee.  La structure de ce maillage est analogue \`a celle d'un cube. L'interpr\'etation de la Cubed-Sphere \`a l'aide de grands cercles permet de construire des op\'erateurs sph\'eriques discrets gradient, divergence et vorticit\'e d'ordre au moins \'egal \`a 3 (en pratique d'ordre 4).  La troisi\`eme partie de la th\`ese est d\'edi\'ee \`a diff\'erents tests pour le syst\`eme d'\'equations Shallow Water ainsi que pour l'\'equation d'advection. Les r\'esultats d\'emontrent une pr\'ecision proche de celle obtenue par les algorithmes conservatifs d'ordre 4 les plus r\'ecents},
  collaborator = {Croisille, Jean-Pierre},
  copyright = {Licence Etalab},
  file = {/home/victor/Zotero/storage/CMPS6VLT/DDOC_T_2018_0111_BRACHET.pdf},
  keywords = {516.362,518,Barr√© de Saint-Venant; Adh√©mar-Jean-Claude (1797-1886) -- M√©thodologie,Compact scheme,Cubed-Sphere,Discr√©tisation (cartographie),Discr√©tisation en temps,√âquation Shallow Water,√âquations aux d√©riv√©es partielles,Sch√©mas compacts,Shallow Water equation,Sph√®re,Time discretisation},
  school = {Universit\'e de Lorraine},
  type = {These de Doctorat}
}

@article{bretin_approximation_2020,
  title = {Approximation of Surface Diffusion Flow: A Second Order Variational {{Cahn}}--{{Hilliard}} Model with Degenerate Mobilities},
  shorttitle = {Approximation of Surface Diffusion Flow},
  author = {Bretin, Elie and Masnou, Simon and Sengers, Arnaud and Terii, Garry},
  year = {2020},
  month = jul,
  abstract = {This paper tackles the approximation of surface diffusion flow using a Cahn--Hilliard-type model. We introduce and analyze a new second order variational phase field model which associates the classical Cahn--Hilliard energy with two degenerate mobilities. This association allows to gain an order of approximation of the sharp limit. In a second part, we propose some simple and efficient numerical schemes to approximate the solutions, and we provide numerical 2D and 3D experiments that illustrate the interest of our model in comparison with other Cahn--Hilliard models.},
  archivePrefix = {arXiv},
  eprint = {2007.03793},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/6BG937U3/Bretin et al. - 2020 - Approximation of surface diffusion flow a second .pdf;/home/victor/Zotero/storage/LCE3A7VB/2007.html},
  journal = {arXiv:2007.03793 [cs, math]},
  keywords = {74N20; 35A35; 53E10; 53E40; 65M32; 35A15,Mathematics - Analysis of PDEs,Mathematics - Numerical Analysis},
  primaryClass = {cs, math}
}

@article{brookes_comparison_,
  title = {A Comparison of {{Fuzzy}}, {{Bayesian}} and {{Weighted Average}} Formulations of an in-Stream Habitat Suitability Model.},
  author = {Brookes, C J and Kumar, Vikas and Lane, S N},
  pages = {8},
  abstract = {Three variations of a simple in-stream habitat suitability model were implemented and the effect on output for one organism at 22 sites on one short section of a river was examined. The model uses only two factors, depth and velocity, to calculate quality and a third factor, width to quantify utility. The implementations were based on 3 multi-criteria evaluation approaches: Weighted Average, Fuzzy Sets and Bayesian probability. There was broad agreement between the formulations but important differences in detail. The model outputs indicate that uncertainty arising from model formulation is significant and can have a bearing on planning decisions. There is a complex interaction between the formulations and the characteristics of the sites. Suitability models should be used thoughtfully and implemented in ways that: facilitate exploratory analysis; present ranges of possible outputs; present indicators of uncertainty; and facilitate back tracking to explain the outputs.},
  file = {/home/victor/Zotero/storage/TEAUII5N/Brookes et al. - A comparison of Fuzzy, Bayesian and Weighted Avera.pdf},
  keywords = {Bayesian,Fuzzy}
}

@article{brynjarsdottir_learning_2014,
  title = {Learning about Physical Parameters: The Importance of Model Discrepancy},
  shorttitle = {Learning about Physical Parameters},
  author = {Brynjarsd{\'o}ttir, Jenn{\'y} and O'Hagan, Anthony},
  year = {2014},
  month = nov,
  volume = {30},
  pages = {114007},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/30/11/114007},
  file = {/home/victor/Zotero/storage/97P352BQ/Learning-about-physical-parameters-The-importance-of-model-discrepancy.pdf},
  journal = {Inverse Problems},
  keywords = {Discrepancy,Model inadequacy},
  number = {11}
}

@article{buchholz_high_nodate,
  title = {High Dimensional {{Bayesian}} Computation},
  author = {Buchholz, Alexander},
  pages = {168},
  file = {/home/victor/Zotero/storage/9IIL2VQM/Buchholz - High dimensional Bayesian computation.pdf},
  language = {en}
}

@book{bucholtz_handbook_2013,
  title = {Handbook of {{Differential Entropy}}},
  author = {Bucholtz, Frank},
  year = {2013},
  month = nov,
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/b15991},
  file = {/home/victor/Zotero/storage/6UFL6ZZI/Bucholtz - 2013 - Handbook of Differential Entropy.pdf},
  isbn = {978-1-4665-8316-0 978-1-4665-8317-7},
  language = {en}
}

@inproceedings{buhmann_robust_2013,
  title = {Robust Optimization in the Presence of Uncertainty},
  booktitle = {Proceedings of the 4th Conference on {{Innovations}} in {{Theoretical Computer Science}} - {{ITCS}} '13},
  author = {Buhmann, Joachim M. and Mihalak, Matus and Sramek, Rastislav and Widmayer, Peter},
  year = {2013},
  pages = {505},
  publisher = {{ACM Press}},
  address = {{Berkeley, California, USA}},
  doi = {10.1145/2422436.2422491},
  abstract = {We study optimization in the presence of uncertainty such as noise in measurements, and advocate a novel approach of tackling it. The main difference to any existing approach is that we do not assume any knowledge about the nature of the uncertainty (such as for instance a probability distribution). Instead, we are given several instances of the same optimization problem as input, and, assuming they are typical w.r.t. the uncertainty, we make use of it in order to compute a solution that is good for the sample instances as well as for future (unknown) typical instances.},
  file = {/home/victor/Zotero/storage/NI3RV4WM/Buhmann et al. - 2013 - Robust optimization in the presence of uncertainty.pdf},
  isbn = {978-1-4503-1859-4},
  language = {en}
}

@article{buhmann_robust_2018,
  title = {Robust Optimization in the Presence of Uncertainty: {{A}} Generic Approach},
  shorttitle = {Robust Optimization in the Presence of Uncertainty},
  author = {Buhmann, J. M. and Gronskiy, A. Y. and Mihal{\'a}k, M. and Pr{\"o}ger, T. and {\v S}r{\'a}mek, R. and Widmayer, P.},
  year = {2018},
  month = jun,
  volume = {94},
  pages = {135--166},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2017.10.004},
  abstract = {We propose a novel approach for optimization under uncertainty. Our approach does not assume any particular noise model behind the measurements, and only requires two typical instances. We first propose a measure of similarity of instances (with respect to a given objective). Based on this measure, we then choose a solution randomly among all solutions that are near-optimum for both instances. The exact notion of near-optimum is intertwined with the proposed similarity measure. Our similarity measure also allows us to derive formal statements about the expected quality of the computed solution. Furthermore, we apply our approach to various optimization problems.},
  file = {/home/victor/Zotero/storage/PBY9I6I9/Buhmann et al. - 2018 - Robust optimization in the presence of uncertainty.pdf;/home/victor/Zotero/storage/J2ZPYUX7/S002200001730212X.html},
  journal = {Journal of Computer and System Sciences},
  keywords = {Instance similarity,Noise,Optimization,Robustness,Uncertainty},
  series = {Journal of {{Computer}} and {{System Science}}: 50 Years of Celebration. {{In}} Memory of {{Professor Edward Blum}}.}
}

@article{bui-thanh_model_2008,
  title = {Model Reduction for Large-Scale Systems with High-Dimensional Parametric Input Space},
  author = {{Bui-Thanh}, Tan and Willcox, Karen and Ghattas, Omar},
  year = {2008},
  volume = {30},
  pages = {3270--3288},
  file = {/home/victor/Zotero/storage/U7L8XC6G/bwg08.pdf},
  journal = {SIAM Journal on Scientific Computing},
  keywords = {MOR methods,reduction parameter space},
  number = {6}
}

@article{bull_convergence_nodate,
  title = {Convergence {{Rates}} of {{Efficient Global Optimization Algorithms}}},
  author = {Bull, Adam D and Bull, A},
  pages = {26},
  abstract = {In the efficient global optimization problem, we minimize an unknown function f , using as few observations f (x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions.},
  file = {/home/victor/Zotero/storage/HTC3Q7X6/Bull et Bull - Convergence Rates of EfÔ¨Åcient Global Optimization .pdf},
  language = {en}
}

@article{burnham_multimodel_2004,
  title = {Multimodel {{Inference}}: {{Understanding AIC}} and {{BIC}} in {{Model Selection}}},
  shorttitle = {Multimodel {{Inference}}},
  author = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  month = nov,
  volume = {33},
  pages = {261--304},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124104268644},
  file = {/home/victor/Zotero/storage/LJDVJXE3/Burnham et Anderson - 2004 - Multimodel Inference Understanding AIC and BIC in.pdf},
  journal = {Sociological Methods \& Research},
  language = {en},
  number = {2}
}

@article{calafiore_scenario_2006,
  title = {The {{Scenario Approach}} to {{Robust Control Design}}},
  author = {Calafiore, G.C. and Campi, M.C.},
  year = {2006},
  month = may,
  volume = {51},
  pages = {742--753},
  issn = {0018-9286},
  doi = {10.1109/TAC.2006.875041},
  abstract = {We propose a new probabilistic solution framework for robust control analysis and synthesis problems that can be expressed in the form of minimization of a linear objective subject to convex constraints parameterized by uncertainty terms. This includes for instance the wide class of NP-hard control problems representable by means of parameter-dependent linear matrix inequalities (LMIs).},
  file = {/home/victor/Zotero/storage/UCVEUFN8/Calafiore et Campi - 2006 - The Scenario Approach to Robust Control Design.pdf},
  journal = {IEEE Transactions on Automatic Control},
  language = {en},
  number = {5}
}

@article{capolei_waterflooding_2013,
  title = {Waterflooding Optimization in Uncertain Geological Scenarios},
  author = {Capolei, Andrea and Suwartadi, Eka and Foss, Bjarne and J{\o}rgensen, John Bagterp},
  year = {2013},
  month = dec,
  volume = {17},
  pages = {991--1013},
  issn = {1420-0597, 1573-1499},
  doi = {10.1007/s10596-013-9371-1},
  file = {/home/victor/Zotero/storage/WH6UF2IS/Capolei et al. - 2013 - Waterflooding optimization in uncertain geological.pdf},
  journal = {Computational Geosciences},
  language = {en},
  number = {6}
}

@book{casella_statistical_2002,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  year = {2002},
  volume = {2},
  publisher = {{Duxbury Pacific Grove, CA}},
  file = {/home/victor/Zotero/storage/SQK9PBTE/Casella et Berger - 2002 - Statistical inference.pdf;/home/victor/Zotero/storage/X9I3J2BZ/Casella et Berger - 2002 - Statistical inference.pdf}
}

@article{celeux_stochastic_1992,
  title = {On {{Stochastic Versions}} of the {{EM Algorithm}}},
  author = {Celeux, Gilles and Chauveau, Didier and Diebolt, Jean},
  year = {1992},
  month = dec,
  volume = {37},
  pages = {55--57},
  issn = {0759-1063, 2070-2779},
  doi = {10.1177/075910639203700105},
  abstract = {We compare three different stochastic versions of the EM algorithm: The SEM algorithm, the SAEM algorithm and the MCEM algorithm. We suggest that the most relevant contribution of the MCEM methodology is what we call the simulated annealing MCEM algorithm, which turns out to be very close to SAEM. We focus particularly on the mixture of distributions problem. In this context, we review the available theoretical results on the convergence of these algorithms and on the behavior of SEM as the sample size tends to infinity. The second part is devoted to intensive Monte Carlo numerical simulations and a real data study. We show that, for some particular mixture situations, the SEM algorithm is almost always preferable to the EM and simulated annealing versions SAEM and MCEM. For some very intricate mixtures, however, none of these algorithms can be confidently used. Then, SEM can be used as an efficient data exploratory tool for locating significant maxima of the likelihood function. In the real data case, we show that the SEM stationary distribution provides a contrasted view of the loglikelihood by emphasizing sensible maxima.},
  file = {/home/victor/Zotero/storage/BIUEYSS9/1992 - Institut national de recherche en informatique et .pdf},
  journal = {Bulletin of Sociological Methodology/Bulletin de M\'ethodologie Sociologique},
  language = {en},
  number = {1}
}

@article{chapelle_empirical_,
  title = {An {{Empirical Evaluation}} of {{Thompson Sampling}}},
  author = {Chapelle, Olivier and Li, Lihong},
  pages = {9},
  abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
  file = {/home/victor/Zotero/storage/TVUBSS3S/Chapelle et Li - An Empirical Evaluation of Thompson Sampling.pdf},
  language = {en}
}

@article{chen_beta_1999,
  title = {Beta Kernel Estimators for Density Functions},
  author = {Chen, Song Xi},
  year = {1999},
  month = aug,
  volume = {31},
  pages = {131--145},
  issn = {0167-9473},
  doi = {10.1016/S0167-9473(99)00010-9},
  abstract = {Kernel estimators using non-negative kernels are considered to estimate probability density functions with compact supports. The kernels are chosen from a family of beta densities. The beta kernel estimators are free of boundary bias, non-negative and achieve the optimal rate of convergence for the mean integrated squared error. The proposed beta kernel estimators have two features. One is that the different amount of smoothing is allocated by naturally varying kernel shape without explicitly changing the value of the smoothing bandwidth. Another feature is that the support of the beta kernels can match the support of the density function; this leads to larger effective sample sizes used in the density estimation and can produce density estimates that have smaller finite-sample variance than some other estimators.},
  file = {/home/victor/Zotero/storage/7GIVZIWE/Chen - 1999 - Beta kernel estimators for density functions.pdf;/home/victor/Zotero/storage/EHYXW4JH/S0167947399000109.html},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {Beta kernels,Boundary bias,Density estimation,Local linear estimators,Variable kernels},
  number = {2}
}

@incollection{chen_estimating_2000,
  title = {Estimating {{Marginal Posterior Densities}}},
  booktitle = {Monte {{Carlo Methods}} in {{Bayesian Computation}}},
  author = {Chen, Ming-Hui and Shao, Qi-Man and Ibrahim, Joseph G.},
  year = {2000},
  pages = {94--123},
  publisher = {{Springer, New York, NY}},
  doi = {10.1007/978-1-4612-1276-8_4},
  abstract = {In Bayesian inference, a joint posterior distribution is available through the likelihood function and a prior distribution. One purpose of Bayesian inference is to calculate and display marginal posterior densities because the marginal posterior densities provide complete information about parameters of interest. As shown in Chapter 2, a Markov chain Monte Carlo (MCMC) sampling algorithm, such as the Gibbs sampler or a Metropolis-Hastings algorithm, can be used to draw MCMC samples from the posterior distribution. Chapter 3 also demonstrates how we can easily obtain posterior quantities such as posterior means, posterior standard deviations, and other posterior quantities from MCMC samples. However, when a Bayesian model becomes complicated, it may be difficult to obtain a reliable estimator of a marginal posterior density based on the MCMC sample. A traditional method for estimating marginal posterior densities is kernel density estimation. Since the kernel density estimator is nonparametric, it may not be efficient. On the other hand, the kernel density estimator may not be applicable for some complicated Bayesian models. In the context of Bayesian inference, the joint posterior density is typically known up to a normalizing constant. Using the structure of a posterior density, a number of authors (e.g., Gelfand, Smith, and Lee 1992; Johnson 1992; Chen 1993 and 1994; Chen and Shao 1997c; Chib 1995; Verdinelli and Wasserman 1995) propose parametric marginal posterior density estimators based on the MCMC sample. In this chapter, we present several available Monte Carlo (MC) methods for computing marginal posterior density estimators, and we also discuss how well marginal posterior density estimation works using the Kullback\textemdash Leibler (K\textemdash L) divergence as a performance measure.},
  file = {/home/victor/Zotero/storage/56KWF5P8/Chen et al. - 2000 - Estimating Marginal Posterior Densities.pdf;/home/victor/Zotero/storage/YRAALCCA/978-1-4612-1276-8_4.html},
  isbn = {978-1-4612-7074-4 978-1-4612-1276-8},
  language = {en},
  series = {Springer {{Series}} in {{Statistics}}}
}

@article{chen_reduced_,
  title = {Reduced {{Collocation Methods}}: {{Reduced Basis Methods}} in the {{Collocation Framework}}},
  shorttitle = {Reduced {{Collocation Methods}}},
  author = {Chen, Yanlai and Gottlieb, Sigal},
  volume = {55},
  pages = {718--737},
  issn = {0885-7474},
  abstract = {Abstract: In this paper, we present \{\textbackslash{} em the first\} reduced basis method well-suited for the collocation framework. Two fundamentally different algorithms are presented: the so-called Least Squares Reduced Collocation Method (LSRCM) and Empirical},
  file = {/home/victor/Zotero/storage/BXLVDHK5/Reduced_Collocation_Methods_Reduced_Basis_Methods_in_the_Collocation_Framework.html},
  journal = {Journal of Scientific Computing},
  number = {3}
}

@article{cheng_general_2008,
  title = {General Frequentist Properties of the Posterior Profile Distribution},
  author = {Cheng, Guang and Kosorok, Michael R.},
  year = {2008},
  month = aug,
  volume = {36},
  pages = {1819--1853},
  issn = {0090-5364},
  doi = {10.1214/07-AOS536},
  abstract = {In this paper, inference for the parametric component of a semiparametric model based on sampling from the posterior profile distribution is thoroughly investigated from the frequentist viewpoint. The higher-order validity of the profile sampler obtained in Cheng and Kosorok [Ann. Statist. 36 (2008)] is extended to semiparametric models in which the infinite dimensional nuisance parameter may not have a root-\$n\$ convergence rate. This is a nontrivial extension because it requires a delicate analysis of the entropy of the semiparametric models involved. We find that the accuracy of inferences based on the profile sampler improves as the convergence rate of the nuisance parameter increases. Simulation studies are used to verify this theoretical result. We also establish that an exact frequentist confidence interval obtained by inverting the profile log-likelihood ratio can be estimated with higher-order accuracy by the credible set of the same type obtained from the posterior profile distribution. Our theory is verified for several specific examples.},
  archivePrefix = {arXiv},
  eprint = {math/0612191},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/MAKXMJ3I/Cheng et Kosorok - 2008 - General frequentist properties of the posterior pr.pdf},
  journal = {The Annals of Statistics},
  keywords = {62G20; 62F25 (Primary) 62F15; 62F12 (Secondary),Mathematics - Statistics Theory},
  language = {en},
  number = {4}
}

@inproceedings{cheng_robust_2009,
  title = {Robust Reliability Optimization of Mechanical Components Using Non-Probabilistic Interval Model},
  booktitle = {2009 {{IEEE}} 10th {{International Conference}} on {{Computer}}-{{Aided Industrial Design Conceptual Design}}},
  author = {Cheng, X.},
  year = {2009},
  month = nov,
  pages = {821--825},
  doi = {10.1109/CAIDCD.2009.5375184},
  abstract = {Mechanical components design is subjected to uncertainties in material and geometrical properties, loads and other variables. For reliability optimization design with uncertain parameters, based on the non-probabilistic reliability theory, robust design and optimal design method, the uncertain parameters of mechanical components are expressed by non-probabilistic interval variables, and a non-probabilistic measure and procedure for robust reliability computation is presented. Compared with the conventional probabilistic reliability optimization approach, the proposed method does not require a presumed probability distribution of the uncertain parameters and only the bounds or ranges of their variations are required. The optimal design for non-probabilistic robust reliability is formulated as a two level optimization problem, in which the first level minimizes the original robust optimal objective with the constraints of non-probabilistic reliability index, and the secondary level is used to identify the reliability index. The purpose of it is to get a tradeoff between the design objective and the robustness to uncertainties and satisfy the requirements for reliability. For instance, the robust reliability optimization of unidirectional wedge-typed overrunning clutch is proposed, and the results show that the proposed method is useful for mechanical components design and quality improvement.},
  file = {/home/victor/Zotero/storage/H9PVKPM9/5375184.html},
  keywords = {Constraint optimization,design engineering,Design methodology,Design optimization,geometrical properties,mechanical components design,Mechanical Components Design,Mechanical factors,mechanical products,Mechanical variables measurement,Non-probabilistic Interval Model,nonprobabilistic interval model,nonprobabilistic interval variables,nonprobabilistic measure,nonprobabilistic reliability index,nonprobabilistic reliability theory,nonprobabilistic robust reliability,optimal design,optimisation,Optimization methods,probabilistic reliability optimization,probability,probability distribution,Probability distribution,reliability,Reliability Optimization,reliability optimization design,Reliability theory,robust design,Robust Design,robust reliability optimization,Robustness,two level optimization problem,uncertain parameters,Uncertainty,unidirectional wedge-typed overrunning clutch}
}

@article{chevalier_estimating_nodate,
  title = {Estimating and Quantifying Uncertainties on Level Sets Using the {{Vorob}}'ev Expectation and Deviance with {{Gaussian}} Process Models},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David and Bect, Julien and Molchanov, Ilya},
  pages = {9},
  abstract = {Several methods based on Kriging have been recently proposed for calculating a probability of failure involving costly-to-evaluate functions. A closely related problem is to estimate the set of inputs leading to a response exceeding a given threshold. Now, estimating such level set \textendash{} and not solely its volume \textendash{} and quantifying uncertainties on it are not straightforward. Here we use notions from random set theory to obtain an estimate of the level set, together with a quantification of estimation uncertainty. We give explicit formulae in the Gaussian process set-up, and provide a consistency result. We then illustrate how space-filling versus adaptive design strategies may sequentially reduce level set estimation uncertainty.},
  file = {/home/victor/Zotero/storage/CCUH53KS/Chevalier et al. - Estimating and quantifying uncertainties on level .pdf},
  language = {en}
}

@article{chevalier_fast_,
  title = {Fast Uncertainty Reduction Strategies Relying on {{Gaussian}} Process Models},
  author = {Chevalier, Cl{\'e}ment},
  pages = {196},
  file = {/home/victor/Zotero/storage/9U4ENIG7/Chevalier - Fast uncertainty reduction strategies relying on G.pdf},
  language = {en}
}

@article{chevalier_kriginv_2014,
  title = {{{KrigInv}}: {{An}} Efficient and User-Friendly Implementation of Batch-Sequential Inversion Strategies Based on Kriging},
  shorttitle = {{{KrigInv}}},
  author = {Chevalier, Cl{\'e}ment and Picheny, Victor and Ginsbourger, David},
  year = {2014},
  month = mar,
  volume = {71},
  pages = {1021--1034},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.03.008},
  abstract = {Several strategies relying on kriging have recently been proposed for adaptively estimating contour lines and excursion sets of functions under severely limited evaluation budget. The recently released R package KrigInv3 is presented and offers a sound implementation of various sampling criteria for those kinds of inverse problems. KrigInv is based on the DiceKriging package, and thus benefits from a number of options concerning the underlying kriging models. Six implemented sampling criteria are detailed in a tutorial and illustrated with graphical examples. Different functionalities of KrigInv are gradually explained. Additionally, two recently proposed criteria for batch-sequential inversion are presented, enabling advanced users to distribute function evaluations in parallel on clusters or clouds of machines. Finally, auxiliary problems are discussed. These include the fine tuning of numerical integration and optimization procedures used within the computation and the optimization of the considered criteria.},
  file = {/home/victor/Zotero/storage/DKH97IKK/Chevalier et al. - 2014 - KrigInv An efficient and user-friendly implementa.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}

@article{cohn_introduction_1997,
  title = {An {{Introduction}} to {{Estimation Theory}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}: {{Theory}} and {{Practice}})},
  shorttitle = {An {{Introduction}} to {{Estimation Theory}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}},
  author = {Cohn, Stephen E.},
  year = {1997},
  volume = {75},
  pages = {257--288},
  file = {/home/victor/Zotero/storage/488E3CR6/Cohn - 1997 - An Introduction to Estimation Theory (gtSpecial Is.pdf;/home/victor/Zotero/storage/3ACGSNZS/ja.html},
  journal = {Journal of the Meteorological Society of Japan. Ser. II},
  number = {1B}
}

@phdthesis{cook_effective_2018,
  title = {Effective {{Formulations}} of {{Optimization Under Uncertainty}} for {{Aerospace Design}}},
  author = {Cook, Laurence William},
  year = {2018},
  month = jul,
  doi = {10.17863/CAM.23427},
  abstract = {Formulations of optimization under uncertainty (OUU) commonly used in 
aerospace design\textemdash those based on treating statistical moments of the quantity 
of interest (QOI) as separate objectives\textemdash can result in stochastically dominated 
designs. A stochastically dominated design is undesirable, because it is less likely 
than another design to achieve a QOI at least as good as a given value, for any 
given value. 
 
As a remedy to this limitation for the multi-objective formulation of moments, 
a novel OUU formulation is proposed\textemdash dominance optimization. This formulation 
seeks a set of solutions and makes use of global optimizers, so is useful for early 
stages of the design process when exploration of design space is important. 
 
Similarly, to address this limitation for the single-objective formulation of 
moments (combining moments via a weighted sum), a second novel formulation 
is proposed\textemdash horsetail matching. This formulation can make use of gradient- 
based local optimizers, so is useful for later stages of the design process when 
exploitation of a region of design space is important. Additionally, horsetail 
matching extends straightforwardly to different representations of uncertainty, 
and is flexible enough to emulate several existing OUU formulations. 
 
Existing multi-fidelity methods for OUU are not compatible with these novel 
formulations, so one such method\textemdash information reuse\textemdash is generalized to be 
compatible with these and other formulations. 
 
The proposed formulations, along with generalized information reuse, are 
compared to their most comparable equivalent in the current state-of-the-art 
on practical design problems: transonic aerofoil design, coupled aero-structural 
wing design, high-fidelity 3D wing design, and acoustic horn shape design. 
 
Finally, the two novel formulations are combined in a two-step design process, 
which is used to obtain a robust design in a challenging version of the acoustic horn 
design problem. Dominance optimization is given half the computational budget 
for exploration; then horsetail matching is given the other half for exploitation. 
Using exactly the same computational budget as a moment-based approach, the 
design obtained using the novel formulations is 95\% more likely to achieve a 
better QOI than the best value achievable by the moment-based design.},
  copyright = {Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)},
  file = {/home/victor/Zotero/storage/8W7UKHCR/Cook - 2018 - Effective Formulations of Optimization Under Uncer.pdf;/home/victor/Zotero/storage/RWIX948G/276146.html},
  language = {en},
  school = {University of Cambridge},
  type = {Thesis}
}

@article{cook_extending_2017,
  title = {Extending {{Horsetail Matching}} for {{Optimization Under Probabilistic}}, {{Interval}}, and {{Mixed Uncertainties}}},
  author = {Cook, Laurence W. and Jarrett, Jerome P. and Willcox, Karen E.},
  year = {2017},
  month = oct,
  volume = {56},
  pages = {849--861},
  issn = {0001-1452},
  doi = {10.2514/1.J056371},
  abstract = {This paper presents a new approach for optimization under uncertainty in the presence of probabilistic, interval, and mixed uncertainties, avoiding the need to specify probability distributions on uncertain parameters when such information is not readily available. Existing approaches for optimization under these types of uncertainty mostly rely on treating combinations of statistical moments as separate objectives, but this can give rise to stochastically dominated designs. Here, horsetail matching is extended for use with these types of uncertainties to overcome some of the limitations of existing approaches. The formulation delivers a single, differentiable metric as the objective function for optimization. It is demonstrated on algebraic test problems, the design of a wing using a low-fidelity coupled aerostructural code, and the aerodynamic shape optimization of a wing using computational fluid dynamics analysis.},
  file = {/home/victor/Zotero/storage/ZBT9TGQK/Cook et al. - 2017 - Extending Horsetail Matching for Optimization Unde.pdf;/home/victor/Zotero/storage/SJNAGEIJ/1.html},
  journal = {AIAA Journal},
  number = {2}
}

@article{cook_horsetail_2018,
  title = {Horsetail Matching: A Flexible Approach to Optimization under Uncertainty},
  shorttitle = {Horsetail Matching},
  author = {Cook, L. W. and Jarrett, J. P.},
  year = {2018},
  month = apr,
  volume = {50},
  pages = {549--567},
  issn = {0305-215X, 1029-0273},
  doi = {10.1080/0305215X.2017.1327581},
  abstract = {It is important to design engineering systems to be robust with respect to uncertainties in the design process. Often, this is done by considering statistical moments, but over-reliance on statistical moments when formulating a robust optimization can produce designs that are stochastically dominated by other feasible designs. This article instead proposes a formulation for optimization under uncertainty that minimizes the difference between a design's cumulative distribution function and a target. A standard target is proposed that produces stochastically non-dominated designs, but the formulation also offers enough flexibility to recover existing approaches for robust optimization. A numerical implementation is developed that employs kernels to give a differentiable objective function. The method is applied to algebraic test problems and a robust transonic airfoil design problem where it is compared to multi-objective, weighted-sum and density matching approaches to robust optimization; several advantages over these existing methods are demonstrated.},
  file = {/home/victor/Zotero/storage/QZQCNVQ7/Cook et Jarrett - 2018 - Horsetail matching a flexible approach to optimiz.pdf},
  journal = {Engineering Optimization},
  language = {en},
  number = {4}
}

@article{cooper_improving_nodate,
  title = {Improving {{Inundation Forecasting}} Using {{Data Assimilation}}},
  author = {Cooper, E S and Dance, S L and Nichols, N K and Smith, P J and {Garcia-Pintado}, J},
  pages = {20},
  abstract = {Fluvial flooding is a costly problem in the UK and worldwide. Real-time accurate inundation forecasting can help to reduce the damage caused by inundation events by alerting people to take necessary mitigating actions. This work is part of an effort to improve inundation forecasting using data assimilation (DA). DA is a method for combining a numerical model of a system with observations in order to best estimate the current state of the system. A river inundation model has been developed using numerical implementation of the shallow water equations with an inflow source term added; the model and implementation of the source term are described here. The model has then been used with idealised river valley topographies in order to investigate the sensitivities of the system to model parameters describing the effect of friction between water and the river channel, and the effect of the topography slope at the downstream boundary. Initial DA experiments using an ensemble Kalman Filter are also described. Identical twin experiments show that the DA as implemented in this domain can correct the water levels at the time of the observations, and that more observations lead to a better correction. However, by the time of the next observations very similar water levels are predicted, regardless of the number of observations used in the assimilation. This implies that the effective time for observations in this system is small compared to the time between observations.},
  file = {/home/victor/Zotero/storage/5UJ8F2JJ/Cooper et al. - Improving Inundation Forecasting using Data Assimi.pdf},
  language = {en}
}

@phdthesis{couderc_dassfow-shallow_2013,
  title = {Dassfow-Shallow, Variational Data Assimilation for Shallow-Water Models: {{Numerical}} Schemes, User and Developer Guides},
  shorttitle = {Dassfow-Shallow, Variational Data Assimilation for Shallow-Water Models},
  author = {Couderc, Fr{\'e}d{\'e}ric and Madec, Ronan and Monnier, J{\'e}r{\^o}me and Vila, Jean-Paul},
  year = {2013},
  file = {C\:\\Users\\Victor\\Dropbox\\INRIA 2017\\Litt√©rature\\DassFow-Shallow_Variational_Data_Assimilation_for_.pdf},
  school = {University of Toulouse, CNRS, IMT, INSA, ANR}
}

@article{cox_partial_1975,
  title = {Partial Likelihood},
  author = {Cox, D. R.},
  year = {1975},
  month = aug,
  volume = {62},
  pages = {269--276},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/62.2.269},
  abstract = {Abstract.  A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and infer},
  file = {/home/victor/Zotero/storage/5ASFZ34K/Cox - 1975 - Partial likelihood.pdf;/home/victor/Zotero/storage/M6HFBWG2/337051.html},
  journal = {Biometrika},
  language = {en},
  number = {2}
}

@article{craig_bayesian_2001,
  title = {Bayesian {{Forecasting}} for {{Complex Systems Using Computer Simulators}}},
  author = {Craig, Peter S and Goldstein, Michael and Rougier, Jonathan C and Seheult, Allan H},
  year = {2001},
  month = jun,
  volume = {96},
  pages = {717--729},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214501753168370},
  file = {/home/victor/Zotero/storage/4ZGE4AUB/Craig et al. - 2001 - Bayesian Forecasting for Complex Systems Using Com.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {454}
}

@article{cui_likelihood-informed_2014,
  title = {Likelihood-Informed Dimension Reduction for Nonlinear Inverse Problems},
  author = {Cui, Tiangang and Martin, James and Marzouk, Youssef M. and Solonen, Antti and Spantini, Alessio},
  year = {2014},
  month = nov,
  volume = {30},
  pages = {114015},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/30/11/114015},
  abstract = {The intrinsic dimensionality of an inverse problem is affected by prior information, the accuracy and number of observations, and the smoothing properties of the forward operator. From a Bayesian perspective, changes from the prior to the posterior may, in many problems, be confined to a relatively low-dimensional subspace of the parameter space. We present a dimension reduction approach that defines and identifies such a subspace, called the "likelihood-informed subspace" (LIS), by characterizing the relative influences of the prior and the likelihood over the support of the posterior distribution. This identification enables new and more efficient computational methods for Bayesian inference with nonlinear forward models and Gaussian priors. In particular, we approximate the posterior distribution as the product of a lower-dimensional posterior defined on the LIS and the prior distribution marginalized onto the complementary subspace. Markov chain Monte Carlo sampling can then proceed in lower dimensions, with significant gains in computational efficiency. We also introduce a Rao-Blackwellization strategy that de-randomizes Monte Carlo estimates of posterior expectations for additional variance reduction. We demonstrate the efficiency of our methods using two numerical examples: inference of permeability in a groundwater system governed by an elliptic PDE, and an atmospheric remote sensing problem based on Global Ozone Monitoring System (GOMOS) observations.},
  archivePrefix = {arXiv},
  eprint = {1403.4680},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/47LK8WJT/Cui et al. - 2014 - Likelihood-informed dimension reduction for nonlin.pdf;/home/victor/Zotero/storage/RNW3P6CB/1403.html},
  journal = {Inverse Problems},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Methodology},
  number = {11}
}

@article{cui_likelihood-informed_2014-1,
  title = {Likelihood-Informed Dimension Reduction for Nonlinear Inverse Problems},
  author = {Cui, Tiangang and Martin, James and Marzouk, Youssef M. and Solonen, Antti and Spantini, Alessio},
  year = {2014},
  month = nov,
  volume = {30},
  pages = {114015},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/30/11/114015},
  abstract = {The intrinsic dimensionality of an inverse problem is affected by prior information, the accuracy and number of observations, and the smoothing properties of the forward operator. From a Bayesian perspective, changes from the prior to the posterior may, in many problems, be confined to a relatively lowdimensional subspace of the parameter space. We present a dimension reduction approach that defines and identifies such a subspace, called the ``likelihoodinformed subspace'' (LIS), by characterizing the relative influences of the prior and the likelihood over the support of the posterior distribution. This identification enables new and more efficient computational methods for Bayesian inference with nonlinear forward models and Gaussian priors. In particular, we approximate the posterior distribution as the product of a lower-dimensional posterior defined on the LIS and the prior distribution marginalized onto the complementary subspace. Markov chain Monte Carlo sampling can then proceed in lower dimensions, with significant gains in computational efficiency. We also introduce a Rao-Blackwellization strategy that de-randomizes Monte Carlo estimates of posterior expectations for additional variance reduction. We demonstrate the efficiency of our methods using two numerical examples: inference of permeability in a groundwater system governed by an elliptic PDE, and an atmospheric remote sensing problem based on Global Ozone Monitoring System (GOMOS) observations.},
  archivePrefix = {arXiv},
  eprint = {1403.4680},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/UZR5MGG2/Cui et al. - 2014 - Likelihood-informed dimension reduction for nonlin.pdf},
  journal = {Inverse Problems},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Methodology},
  language = {en},
  number = {11}
}

@incollection{d._bates_uncertainty_2014,
  title = {Uncertainty in {{Flood Inundation Modelling}}},
  author = {D. Bates, Paul and Pappenberger, Florian and Romanowicz, Renata},
  year = {2014},
  month = mar,
  pages = {232--269},
  doi = {10.1142/9781848162716_0010},
  file = {/home/victor/Zotero/storage/BQGH65W8/D. Bates et al. - 2014 - Uncertainty in Flood Inundation Modelling.pdf},
  isbn = {978-1-84816-270-9}
}

@book{d._morris_bayesian_2012,
  title = {Bayesian {{Design}} and {{Analysis}} of {{Computer Experiments}}: {{Use}} of {{Derivatives}} in {{Surface Prediction}}},
  shorttitle = {Bayesian {{Design}} and {{Analysis}} of {{Computer Experiments}}},
  author = {D. Morris, Max and J. Mitchell, Toby and Ylvisaker, Don},
  year = {2012},
  month = mar,
  volume = {35},
  doi = {10.1080/00401706.1993.10485320},
  abstract = {The work of Currin et al. and others in developing fast predictive approximations'' of computer models is extended for the case in which derivatives of the output variable of interest with respect to input variables are available. In addition to describing the calculations required for the Bayesian analysis, the issue of experimental design is also discussed, and an algorithm is described for constructing maximin distance'' designs. An example is given based on a demonstration model of eight inputs and one output, in which predictions based on a maximin design, a Latin hypercube design, and two compromise'' designs are evaluated and compared. 12 refs., 2 figs., 6 tabs.}
}

@techreport{daks_golden_2017,
  title = {Do the {{Golden State Warriors Have Hot Hands}}?},
  author = {Daks, Alon and Desai, Nishant and Goldberg, Lisa R.},
  year = {2017},
  month = nov,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {Star Golden State Warriors Steph Curry, Klay Thompson, and Kevin Durant are great shooters but they are not streak shooters. Only rarely do they show signs of a hot hand. This conclusion is based on an empirical analysis of field goal and free throw data from the 82 regular season and 17 postseason games played by the Warriors in 2016-2017. Our analysis is inspired by the iconic 1985 hot-hand study by Thomas Gilovitch, Robert Vallone and Amos Tversky, but uses a permutation test to automatically account for Josh Miller and Adam Sanjurjo's recent small sample correction. In this study we show how long standing problems can be reexamined using nonparametric statistics to avoid faulty hypothesis tests due to misspecified distributions.},
  file = {/home/victor/Zotero/storage/VAXQU5TJ/papers.html},
  keywords = {conditional probability,hot hand,nonparametric test,permutation test,small sample correction,streak shooting},
  language = {en},
  number = {ID 2984615},
  type = {{{SSRN Scholarly Paper}}}
}

@article{damblin_adaptive_2015,
  title = {Adaptive Numerical Designs for the Calibration of Computer Codes},
  author = {Damblin, Guillaume and Barbillon, Pierre and Keller, Merlin and Pasanisi, Alberto and Parent, Eric},
  year = {2015},
  month = feb,
  abstract = {Making good predictions of a physical system using a computer code requires the inputs to be carefully specified. Some of these inputs called control variables have to reproduce physical conditions whereas other inputs, called parameters, are specific to the computer code and most often uncertain. The goal of statistical calibration consists in estimating these parameters with the help of a statistical model which links the code outputs with the field measurements. In a Bayesian setting, the posterior distribution of these parameters is normally sampled using MCMC methods. However, they are impractical when the code runs are high time-consuming. A way to circumvent this issue consists of replacing the computer code with a Gaussian process emulator, then sampling a cheap-to-evaluate posterior distribution based on it. Doing so, calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator. We aim at reducing this error by building a proper sequential design by means of the Expected Improvement criterion. Numerical illustrations in several dimensions assess the efficiency of such sequential strategies.},
  archivePrefix = {arXiv},
  eprint = {1502.07252},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/9YULXTPU/Damblin et al. - 2015 - Adaptive numerical designs for the calibration of .pdf;/home/victor/Zotero/storage/LVYNJDWT/1502.html},
  journal = {arXiv:1502.07252 [stat]},
  keywords = {Adaptive sampling,Calibration,Numerical design},
  primaryClass = {stat}
}

@article{damblin_adaptive_2018,
  title = {Adaptive Numerical Designs for the Calibration of Computer Codes},
  author = {Damblin, Guillaume and Barbillon, Pierre and Keller, Merlin and Pasanisi, Alberto and Parent, Eric},
  year = {2018},
  month = jan,
  volume = {6},
  pages = {151--179},
  issn = {2166-2525},
  doi = {10.1137/15M1033162},
  abstract = {Making good predictions of a physical system using a computer code requires the inputs to be carefully specified. Some of these inputs, called control variables, reproduce physical conditions whereas other inputs, called parameters, are specific to the computer code and most often uncertain. The goal of statistical calibration consists in reducing their uncertainty with the help of a statistical model which links the code outputs with the field measurements. In a Bayesian setting, the posterior distribution of these parameters is typically sampled using MCMC methods. However, they are impractical when the code runs are highly timeconsuming. A way to circumvent this issue consists of replacing the computer code with a Gaussian process emulator, then sampling a surrogate posterior distribution based on it. Doing so, calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator. Under the assumption that there is no code discrepancy, we aim to reduce this error by constructing a sequential design by means of the Expected Improvement criterion. Numerical illustrations in several dimensions assess the efficiency of such sequential strategies.},
  archivePrefix = {arXiv},
  eprint = {1502.07252},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/XCJPZ39M/Damblin et al. - 2018 - Adaptive numerical designs for the calibration of .pdf},
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  keywords = {Statistics - Computation},
  language = {en},
  number = {1}
}

@phdthesis{damblin_contributions_2015,
  title = {Contributions Statistiques Au Calage et \`a La Validation Des Codes de Calcul},
  author = {DAMBLIN, Guillaume},
  year = {2015},
  file = {/home/victor/Zotero/storage/I7YJVMRB/DAMBLIN - 2015 - DE L‚ÄôUNIVERSIT√â PARIS-SACLAY.pdf},
  school = {AgroParisTech},
  type = {{{PhD Thesis}}}
}

@article{dang_exact_nodate,
  title = {Exact Asymptotic Limit for Kernel Estimation of Regression Level Sets},
  author = {Dang, Dau and Lalo{\"e}, Thomas and Servien, R{\'e}mi},
  pages = {15},
  abstract = {The asymptotic behavior of a plug-in kernel estimator of the regression level sets is studied. The exact asymtotic rate in terms of the symmetric difference is derived for a given level. Then, we exhibit the exact asymptotic rate when the level corresponds to a fixed probability and is therefore unknown.},
  file = {/home/victor/Zotero/storage/C4N6JP9Q/Dang et al. - Exact asymptotic limit for kernel estimation of re.pdf},
  language = {en}
}

@article{das_estimation_1991,
  title = {On the Estimation of Parameters of Hydraulic Models by Assimilation of Periodic Tidal Data},
  author = {Das, S. K. and Lardner, R. W.},
  year = {1991},
  volume = {96},
  pages = {15187},
  issn = {0148-0227},
  doi = {10.1029/91JC01318},
  file = {/home/victor/Zotero/storage/GP9KM35M/Das et Lardner - 1991 - On the estimation of parameters of hydraulic model.pdf},
  journal = {Journal of Geophysical Research},
  language = {en},
  number = {C8}
}

@article{das_variational_1992,
  title = {Variational Parameter Estimation for a Two-Dimensional Numerical Tidal Model},
  author = {Das, S. K. and Lardner, R. W.},
  year = {1992},
  month = aug,
  volume = {15},
  pages = {313--327},
  issn = {1097-0363},
  doi = {10.1002/fld.1650150305},
  abstract = {It is shown that the parameters in a two-dimensional (depth-averaged) numerical tidal model can be estimated accurately by assimilation of data from tide gauges. The tidal model considered is a semi-linearized one in which kinematical non-linearities are neglected but non-linear bottom friction is included. The parameters to be estimated (bottom friction coefficient and water depth) are assumed to be position-dependent and are approximated by piecewise linear interpolations between certain nodal values. The numerical scheme consists of a two-level leapfrog method. The adjoint scheme is constructed on the assumption that a certain norm of the difference between computed and observed elevations at the tide gauges should be minimized. It is shown that a satisfactory numerical minimization can be completed using either the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton algorithm or Nash's truncated Newton algorithm. On the basis of a number of test problems, it is shown that very effective estimation of the nodal values of the parameters can be achieved provided the number of data stations is sufficiently large in relation to the number of nodes.},
  copyright = {Copyright \textcopyright{} 1992 John Wiley \& Sons, Ltd},
  file = {/home/victor/Zotero/storage/IUZLL882/Das et Lardner - 1992 - Variational parameter estimation for a two-dimensi.pdf;/home/victor/Zotero/storage/DICBZ35B/fld.html},
  journal = {International Journal for Numerical Methods in Fluids},
  keywords = {Data assimilation,Numerical tidal model,Optimal control,Parameter estimation},
  language = {en},
  number = {3}
}

@article{dashti_bayesian_2013,
  title = {The {{Bayesian Approach To Inverse Problems}}},
  author = {Dashti, Masoumeh and Stuart, Andrew M.},
  year = {2013},
  month = feb,
  abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications involving the blending of mathematical models with data.},
  archivePrefix = {arXiv},
  eprint = {1302.6989},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/WJAQBC8F/Dashti et Stuart - 2013 - The Bayesian Approach To Inverse Problems.pdf;/home/victor/Zotero/storage/TYUV9KQE/1302.html},
  journal = {arXiv:1302.6989 [math]},
  keywords = {Mathematics - Probability},
  primaryClass = {math}
}

@article{dawson_data_nodate,
  title = {Data Assimilation for Parameter Estimation in Coastal Ocean Hydrodynamics Modeling},
  author = {Dawson, Clint and Ghattas, Omar and Gonzalez, Oscar and Hoteit, Ibrahim and Tsai, Richard},
  pages = {193},
  file = {/home/victor/Zotero/storage/8RQXINSW/Dawson et al. - Data assimilation for parameter estimation in coas.pdf},
  language = {en}
}

@misc{december_11th_how_2013,
  title = {How {{Academia Resembles}} a {{Drug Gang}}},
  author = {December 11th and communication|75 Comments, 2013|Academic},
  year = {2013},
  month = dec,
  abstract = {Academic systems rely on the existence of a~supply of ``outsiders''~ready to forgo wages and employment security in exchange for the prospect of uncertain security, prestige, freedom and reasonably h\ldots},
  file = {/home/victor/Zotero/storage/Z9GJMRI5/how-academia-resembles-a-drug-gang.html},
  journal = {Impact of Social Sciences},
  language = {en-US}
}

@article{dellino_robust_2012,
  title = {Robust {{Optimization}} in {{Simulation}}: {{Taguchi}} and {{Krige Combined}}},
  shorttitle = {Robust {{Optimization}} in {{Simulation}}},
  author = {Dellino, Gabriella and Kleijnen, Jack P. C. and Meloni, Carlo},
  year = {2012},
  month = aug,
  volume = {24},
  pages = {471--484},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.1110.0465},
  file = {/home/victor/Zotero/storage/6RZ7KGTL/Dellinoetal_IJOC_24_3_2012.pdf},
  journal = {INFORMS Journal on Computing},
  keywords = {Optim Robuste},
  language = {en},
  number = {3}
}

@article{demortier_p_nodate,
  title = {P {{Values}} and {{Nuisance Parameters}}},
  author = {Demortier, Luc},
  pages = {11},
  abstract = {We review the definition and interpretation of p values, describe methods to incorporate systematic uncertainties in their calculation, and briefly discuss a non-regular but common problem caused by nuisance parameters that are unidentified under the null hypothesis.},
  file = {/home/victor/Zotero/storage/JR5P3ZB3/Demortier - P Values and Nuisance Parameters.pdf},
  language = {en}
}

@article{dempster_maximum_1977,
  title = {Maximum Likelihood from Incomplete Data via the {{EM}} Algorithm},
  author = {Dempster, Arthur P. and Laird, Nan M. and Rubin, Donald B.},
  year = {1977},
  pages = {1--38},
  file = {/home/victor/Zotero/storage/773M4BD9/Dempster et al. - Maximum Likelihood from Incomplete Data via the EM.pdf;/home/victor/Zotero/storage/CSLEN38G/Dempster et al. - 1977 - Maximum likelihood from incomplete data via the EM.pdf;/home/victor/Zotero/storage/7H9ZL662/2984875.html},
  journal = {Journal of the royal statistical society. Series B (methodological)}
}

@incollection{deodatis_robust_2014,
  title = {Robust Minimax Design from Costly Simulations},
  booktitle = {Safety, {{Reliability}}, {{Risk}} and {{Life}}-{{Cycle Performance}} of {{Structures}} and {{Infrastructures}}},
  author = {{Piet-Lahanier}, H and Marzat, J and Walter, E},
  editor = {Deodatis, George and Ellingwood, Bruce and Frangopol, Dan},
  year = {2014},
  month = jan,
  pages = {3231--3236},
  publisher = {{CRC Press}},
  doi = {10.1201/b16387-467},
  abstract = {Design of complex physical systems most often relies on numerical simulations that may be extremely costly. In this paper, design is formalized as the optimization of a performance index with respect to control variables. Uncertainty is modeled via a vector of environmental variables that can take any value in a known compact set and may have an adverse effect on performance. In this context, the determination of a robust design requires the continuous minimax optimization of black-box functions. An algorithm combining Kriging-based optimization with relaxation is presented, which makes it possible to find approximate solutions to such problems on a limited computational budget. The design of a vibration absorber is presented as an illustrative example.},
  file = {/home/victor/Zotero/storage/V67NAMCJ/Piet-Lahanier et al. - 2014 - Robust minimax design from costly simulations.pdf},
  isbn = {978-1-138-00086-5 978-1-315-88488-2},
  language = {en}
}

@incollection{destercke_maximum_2019,
  title = {A {{Maximum Likelihood Approach}} to {{Inference Under Coarse Data Based}} on {{Minimax Regret}}},
  booktitle = {Uncertainty {{Modelling}} in {{Data Science}}},
  author = {Guillaume, Romain and Dubois, Didier},
  editor = {Destercke, S{\'e}bastien and Denoeux, Thierry and Gil, Mar{\'i}a {\'A}ngeles and Grzegorzewski, Przemyslaw and Hryniewicz, Olgierd},
  year = {2019},
  volume = {832},
  pages = {99--106},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-97547-4_14},
  abstract = {Various methods have been proposed to express and solve maximum likelihood problems with incomplete data. In some of these approaches, the idea is that incompleteness makes the likelihood func-tion imprecise. Two proposals can be found to cope with this situation: maximize the maximal likelihood induced by precise datasets compat-ible with the incomplete observations, or maximize the minimal such likelihood. These approaches prove to be extremist, the maximax ap-proach having a tendency to disambiguate the data, while the maximin approach favors uniform distributions. In this paper we propose an al-ternative approach consisting in minimax relative regret criterion with respect to maximal likelihood solutions obtained for all precise datasets compatible with the coarse data. It uses relative likelihood and seems to achieve a trade-off between the maximax and the maximin methods.},
  file = {/home/victor/Zotero/storage/SQGQ8MX7/Guillaume et Dubois - 2019 - A Maximum Likelihood Approach to Inference Under C.pdf},
  isbn = {978-3-319-97546-7 978-3-319-97547-4},
  language = {en}
}

@article{diez_design-space_2015,
  title = {Design-Space Dimensionality Reduction in Shape Optimization by {{Karhunen}}\textendash{{Lo\`eve}} Expansion},
  author = {Diez, Matteo and Campana, Emilio F. and Stern, Frederick},
  year = {2015},
  month = jan,
  volume = {283},
  pages = {1525--1544},
  issn = {00457825},
  doi = {10.1016/j.cma.2014.10.042},
  file = {/home/victor/Zotero/storage/8B582P7U/2015-CMAME-Diez_etal.pdf},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  keywords = {Dimensionality reduction,Karhunen‚ÄìLo√®ve expansion},
  language = {en}
}

@book{dimet_variational_1988,
  title = {Variational and {{Optimization Methods}} in {{Meteorology}}: {{A Review}}},
  shorttitle = {Variational and {{Optimization Methods}} in {{Meteorology}}},
  author = {Dimet, F. X. Le and Navon, I. M.},
  year = {1988},
  abstract = {Recent advances in variational and optimization methods applied to increasingly complex numerical weather prediction models with larger numbers of degrees of freedom mandate to take a perspective view of past and recent developments in this field, and present a view of the state of art in the field. Variational methods attempt to achieve a best fit between data and model subject to some `a priori' criteria \textendash{} in view of resolving the undeterminancy problem between the size of the model and the respective number of data required for its satisfactory solution. This review paper presents in a synthesized way the combined views of the authors as to the state of the art of variational and optimization methods in meteorology. Issues discussed include topics of variational analysis, variational initialization, optimal control techniques, variational methods applied for numerical purposes and constrained adjustment, and finally how some of the variational and optimization methods discussed in the review relate to each other.},
  file = {/home/victor/Zotero/storage/W6FNUMJV/Dimet et Navon - 1988 - Variational and Optimization Methods in Meteorolog.pdf;/home/victor/Zotero/storage/TW3ZC5FN/summary.html}
}

@article{ding_model_2018,
  title = {Model {{Selection Techniques}} -- {{An Overview}}},
  author = {Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
  year = {2018},
  month = nov,
  volume = {35},
  pages = {16--34},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2018.2867638},
  abstract = {In the era of ``big data'', analysts usually explore various statistical models or machine learning methods for observed data in order to facilitate scientific discoveries or gain predictive power. Whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. Model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus central to scientific studies in fields such as ecology, economics, engineering, finance, political science, biology, and epidemiology. There has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. A considerable number of methods have been proposed, following different philosophies and exhibiting varying performances. The purpose of this article is to bring a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. We provide integrated and practically relevant discussions on theoretical properties of state-ofthe-art model selection approaches. We also share our thoughts on some controversial views on the practice of model selection.},
  archivePrefix = {arXiv},
  eprint = {1810.09583},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/XFAJYGC7/Ding et al. - 2018 - Model Selection Techniques -- An Overview.pdf},
  journal = {IEEE Signal Processing Magazine},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Economics - Econometrics,Physics - Applied Physics,Statistics - Machine Learning},
  language = {en},
  number = {6}
}

@phdthesis{dobre_analyses_2010,
  title = {{Analyses de sensibilit\'e et d'identifiabilit\'e globales. Application \`a l'estimation de param\`etres photophysiques en th\'erapie photodynamique}},
  author = {Dobre, Simona},
  year = {2010},
  month = jun,
  abstract = {La th\'erapie photodynamique (PDT) est un traitement m\'edical destin\'e \`a certains types de cancer. Elle utilise un agent photosensibilisant qui se concentre dans les tissus pathologiques est qui sera. Cet agent est ensuite activ\'e par une lumi\`ere d'une longueur d'onde pr\'ecise produisant, apr\`es une cascade de r\'eactions, des esp\`eces r\'eactives de l'oxyg\`ene qui endommagent les cellules canc\'ereuses. Cette th\`ese aborde les analyses d'identifiabilit\'e et de sensibilit\'e des param\`etres du mod\`ele dynamique non lin\'eaire retenu. Apr\`es avoir pr\'ecis\'e diff\'erents cadres d'analyse d'identifiabilit\'e, nous nous int\'eressons plus particuli\`erement \`a l'identifiabilit\'e a posteriori, pour des conditions exp\'erimentales fix\'ees, puis \`a l'identifiabilit\'e pratique, prenant en plus en compte les bruits de mesure. Pour ce dernier cadre, nous proposons une m\'ethodologie d'analyse locale autour de valeurs particuli\`eres des param\`etres. En ce qui concerne l'identifiabilit\'e des param\`etres du mod\`ele dynamique de la phase photocytotoxique de la PDT, nous montrons que parmi les dix param\`etres localement identifiables a posteriori, seulement l'un d'entre eux l'est en pratique. N\'eanmoins, ces r\'esultats locaux demeurent insuffisants en raison des larges plages de variation possibles des param\`etres du mod\`ele et n\'ecessitent d'\^etre compl\'et\'es par une analyse globale. Le manque de m\'ethode visant \`a tester l'identifiabilit\'e globale a posteriori ou pratique, nous a orient\'es vers l'analyse de sensibilit\'e globale de la sortie du mod\`ele par rapport \`a ses param\`etres. Une m\'ethode d'analyse de sensibilit\'e globale fond\'ee sur l'\'etude de la variance a permis de mettre en \'evidence trois param\`etres sensibilisants. Nous abordons ensuite les liens entre les analyses globales d'identifiabilit\'e et de sensibilit\'e des param\`etres, en employant une d\'ecomposition de Sobol'. Nous montrons alors que les liens suivants existent : une fonction de sensibilit\'e totale nulle implique un param\`etre non-identifiable; deux fonctions de sensibilit\'e colin\'eaires impliquent la non-identifiabilit\'e mutuelle des param\`etres en question ; la non-injectivit\'e de la sortie par rapport \`a un de ses param\`etres peut aussi entrainer la non-identifiabilit\'e du param\`etre en question mais ce dernier point ne peut \^etre d\'etect\'e en analysant les fonctions de sensibilit\'e uniquement. En somme, la d\'etection des param\`etres non globalement identifiables dans un cadre exp\'erimental donn\'e \`a partir de r\'esultats d'analyse de sensibilit\'e globale ne peut \^etre que partielle. Elle permet d'observer deux (sensibilit\'e nulle ou n\'egligeable et sensibilit\'es corr\'el\'ees) des trois causes de la non-identifiabilit\'e.},
  file = {/home/victor/Zotero/storage/3ZJS3E9C/Dobre - 2010 - Analyses de sensibilit√© et d'identifiabilit√© globa.pdf;/home/victor/Zotero/storage/MXQQJMVI/tel-00550527.html},
  language = {fr},
  school = {Universit\'e Henri Poincar\'e - Nancy I}
}

@article{dobrovidov_data-driven_2014,
  title = {Data-Driven Bandwidth Choice for Gamma Kernel Estimates of Density Derivatives on the Positive Semi-Axis},
  author = {Dobrovidov, A. V. and Markovich, L. A.},
  year = {2014},
  month = jan,
  abstract = {In some applications it is necessary to estimate derivatives of probability densities defined on the positive semi-axis. The quality of nonparametric estimates of the probability densities and their derivatives are strongly influenced by smoothing parameters (bandwidths). In this paper an expression for the optimal smoothing parameter of the gamma kernel estimate of the density derivative is obtained. For this parameter data-driven estimates based on methods called "rule of thumb" and "cross-validation" are constructed. The quality of the estimates is verified and demonstrated on examples of density derivatives generated by Maxwell and Weibull distributions.},
  archivePrefix = {arXiv},
  eprint = {1401.6801},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/APEW2FA4/Dobrovidov et Markovich - 2014 - Data-driven bandwidth choice for gamma kernel esti.pdf;/home/victor/Zotero/storage/SDV5GMA9/1401.html},
  journal = {arXiv:1401.6801 [math, stat]},
  keywords = {Bandwidth selection,Density estimation,Gamma kernel},
  primaryClass = {math, stat}
}

@article{doucet_marginal_2002,
  title = {Marginal Maximum a Posteriori Estimation Using {{Markov}} Chain {{Monte Carlo}}},
  author = {Doucet, Arnaud and Godsill, Simon J. and Robert, Christian P.},
  year = {2002},
  month = jan,
  volume = {12},
  pages = {77--84},
  issn = {0960-3174, 1573-1375},
  doi = {10.1023/A:1013172322619},
  abstract = {Markov chain Monte Carlo (MCMC) methods, while facilitating the solution of many complex problems in Bayesian inference, are not currently well adapted to the problem of marginal maximum a posteriori (MMAP) estimation, especially when the number of parameters is large. We present here a simple and novel MCMC strategy, called State-Augmentation for Marginal Estimation (SAME), which leads to MMAP estimates for Bayesian models. We illustrate the simplicity and utility of the approach for missing data interpolation in autoregressive time series and blind deconvolution of impulsive processes.},
  file = {/home/victor/Zotero/storage/8DPAVYIP/Doucet et al. - 2002 - Marginal maximum a posteriori estimation using Mar.pdf;/home/victor/Zotero/storage/LB5L4CSS/A1013172322619.html},
  journal = {Statistics and Computing},
  keywords = {MCMC,MMAP,SAME},
  language = {en},
  number = {1}
}

@incollection{doumpos_performance_2016,
  title = {Performance {{Analysis}} in {{Robust Optimization}}},
  booktitle = {Robustness {{Analysis}} in {{Decision Aiding}}, {{Optimization}}, and {{Analytics}}},
  author = {Chassein, Andr{\'e} and Goerigk, Marc},
  editor = {Doumpos, Michael and Zopounidis, Constantin and Grigoroudis, Evangelos},
  year = {2016},
  volume = {241},
  pages = {145--170},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-33121-8_7},
  abstract = {We discuss the problem of evaluating a robust solution. To this end, we first give a short primer on how to apply robustification approaches to uncertain optimization problems using the assignment problem and the knapsack problem as illustrative examples. As it is not immediately clear in practice which such robustness approach is suitable for the problem at hand, we present current approaches for evaluating and comparing robustness from the literature, and introduce the new concept of a scenario curve. Using the methods presented in this paper, an easy guide is given to the decision maker to find, solve and compare the best robust optimization method for his purposes.},
  file = {/home/victor/Zotero/storage/EI2VPC8N/Chassein et Goerigk - 2016 - Performance Analysis in Robust Optimization.pdf},
  isbn = {978-3-319-33119-5 978-3-319-33121-8},
  language = {en}
}

@article{du_centroidal_1999,
  title = {Centroidal {{Voronoi}} Tessellations: {{Applications}} and Algorithms},
  shorttitle = {Centroidal {{Voronoi}} Tessellations},
  author = {Du, Qiang and Faber, Vance and Gunzburger, Max},
  year = {1999},
  volume = {41},
  pages = {637--676},
  file = {/home/victor/Zotero/storage/P76KIDGT/dfg99sirv.pdf},
  journal = {SIAM review},
  keywords = {CVT,Voronoi},
  number = {4}
}

@article{dubourg_adaptive_nodate,
  title = {{Adaptive surrogate models for reliability analysis and reliability-based design optimization}},
  author = {Dubourg, Vincent},
  pages = {308},
  file = {/home/victor/Zotero/storage/ZSC6EMCA/Dubourg - Adaptive surrogate models for reliability analysis.pdf},
  language = {fr}
}

@article{dubourg_reliability-based_2011,
  title = {Reliability-Based Design Optimization Using Kriging Surrogates and Subset Simulation},
  author = {Dubourg, V. and Sudret, B. and Bourinet, J.-M.},
  year = {2011},
  month = nov,
  volume = {44},
  pages = {673--690},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-011-0653-8},
  abstract = {The aim of the present paper is to develop a strategy for solving reliability-based design optimization (RBDO) problems that remains applicable when the performance models are expensive to evaluate. Starting with the premise that simulation-based approaches are not affordable for such problems, and that the most-probablefailure-point-based approaches do not permit to quantify the error on the estimation of the failure probability, an approach based on both metamodels and advanced simulation techniques is explored. The kriging metamodeling technique is chosen in order to surrogate the performance functions because it allows one to genuinely quantify the surrogate error. The surrogate error onto the limit-state surfaces is propagated to the failure probabilities estimates in order to provide an empirical error measure. This error is then sequentially reduced by means of a populationbased adaptive refinement technique until the kriging surrogates are accurate enough for reliability analysis. This original refinement strategy makes it possible to add several observations in the design of experiments at the same time. Reliability and reliability sensitivity analyses are performed by means of the subset simulation technique for the sake of numerical efficiency. The adaptive surrogate-based strategy for reliability estimation is finally involved into a classical gradient-based optimization algorithm in order to solve the RBDO problem. The kriging surrogates are built in a so-called augmented reliability space thus making them reusable from one nested RBDO iteration to the other. The strategy is compared to other approaches available in the literature on three academic examples in the field of structural mechanics.},
  archivePrefix = {arXiv},
  eprint = {1104.3667},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/L9JA7B9K/Dubourg et al. - 2011 - Reliability-based design optimization using krigin.pdf},
  journal = {Structural and Multidisciplinary Optimization},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  language = {en},
  number = {5}
}

@article{dunlop_map_2016,
  title = {{{MAP Estimators}} for {{Piecewise Continuous Inversion}}},
  author = {Dunlop, Matthew M. and Stuart, Andrew M.},
  year = {2016},
  month = oct,
  volume = {32},
  pages = {105003},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/32/10/105003},
  abstract = {We study the inverse problem of estimating a field \$u\$ from data comprising a finite set of nonlinear functionals of \$u\$, subject to additive noise; we denote this observed data by \$y\$. Our interest is in the reconstruction of piecewise continuous fields in which the discontinuity set is described by a finite number of geometric parameters. Natural applications include groundwater flow and electrical impedance tomography. We take a Bayesian approach, placing a prior distribution on \$u\$ and determining the conditional distribution on \$u\$ given the data \$y\$. It is then natural to study maximum a posterior (MAP) estimators. Recently (Dashti et al 2013) it has been shown that MAP estimators can be characterised as minimisers of a generalised Onsager-Machlup functional, in the case where the prior measure is a Gaussian random field. We extend this theory to a more general class of prior distributions which allows for piecewise continuous fields. Specifically, the prior field is assumed to be piecewise Gaussian with random interfaces between the different Gaussians defined by a finite number of parameters. We also make connections with recent work on MAP estimators for linear problems and possibly non-Gaussian priors (Helin, Burger 2015) which employs the notion of Fomin derivative. In showing applicability of our theory we focus on the groundwater flow and EIT models, though the theory holds more generally. Numerical experiments are implemented for the groundwater flow model, demonstrating the feasibility of determining MAP estimators for these piecewise continuous models, but also that the geometric formulation can lead to multiple nearby (local) MAP estimators. We relate these MAP estimators to the behaviour of output from MCMC samples of the posterior, obtained using a state-of-the-art function space Metropolis-Hastings method.},
  archivePrefix = {arXiv},
  eprint = {1509.03136},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/IHB7KHYR/Dunlop et Stuart - 2016 - MAP Estimators for Piecewise Continuous Inversion.pdf;/home/victor/Zotero/storage/7CHTAAK8/1509.html},
  journal = {Inverse Problems},
  keywords = {MAP},
  number = {10}
}

@article{duraisamy_goal_2011,
  title = {Goal {{Oriented Uncertainty Propagation}} Using {{Stochastic Adjoints}}},
  author = {Duraisamy, Karthik and Chandrashekar, Praveen},
  year = {2011},
  month = jun,
  volume = {66},
  pages = {10--20},
  doi = {10.1016/j.compfluid.2012.05.013},
  abstract = {We propose a framework based on the use of adjoint equations to formulate an adaptive sampling strategy for uncertainty quantification for problems governed by algebraic or differential equations involving random parameters. The approach is non-intrusive and makes use of discrete sampling based on collocation on simplex elements in stochastic space. Adjoint or dual equations are introduced to estimate errors in statistical moments of random functionals resulting from the inexact reconstruction of the solution within the simplex elements. The approach is demonstrated to be accurate in estimating errors in statistical moments of interest and shown to exhibit super-convergence, in accordance with the underlying theoretical rates. Goal-oriented error indicators are then built using the adjoint solution and exploited to identify regions for adaptive sampling. The error-estimation and adaptive refinement strategy is applied to a range of problems including those governed by algebraic equations as well as scalar and systems of ordinary and partial differential equations. The strategy holds promise as a reliable method to set and achieve error tolerances for efficient aleatory uncertainty quantification in complex problems. Furthermore, the procedure can be combined with numerical error estimates in physical space so as to effectively manage a computational budget to achieve the best possible overall accuracy in the results.},
  journal = {Computers \& Fluids}
}

@article{e._randall_optimum_1981,
  title = {Optimum {{Vibration Absorbers}} for {{Linear Damped Systems}}},
  author = {E. Randall, S and M. Halsted, D and L. Taylor, D},
  year = {1981},
  month = oct,
  volume = {103},
  doi = {10.1115/1.3255005},
  abstract = {This paper presents computational graphs that determine the optimal linear vibration absorber for linear damped primary systems. Considered as independent parameters are the main system damping ratio and the mass ratio examined over the range 0 to 0. 50 and 0. 01 to 0. 40, respectively. The remaining nondimensional parameters were optimized using numerical methods based on minimum-maximum amplitude criteria. This procedure is illustrated in a design example.},
  journal = {Journal of Mechanical Design - J MECH DESIGN}
}

@article{eguchi_robustifing_2001,
  title = {Robustifing Maximum Likelihood Estimation by Psi-Divergence},
  author = {Eguchi, Shinto},
  year = {2001},
  month = jan,
  abstract = {A new class of "{$\Psi$}-divergence functionals" over the space of probability densities is defined by introduction of a generic function {$\Psi$}. A new estimator defined as a minimiser of the {$\Psi$}-divergence functional based on data is suggested and is shown to be a robustified maximum likelihood estimator under some simple conditions on the {$\Psi$}. When {$\Psi$} is an identity function, the {$\Psi$}-divergence reduces to the Kullback-Leibler divergence and the new estimator reduces to the usual maximum likelihood estimator. The "{$\Psi$}-likelihood" is defined, and minimization of the {$\Psi$}-divergence is shown to be equivalent to maximization of the {$\Psi$}-likelihood. Once a probability model and a generic function are chosen, one can automatically construct such a robustified estimator. Several informative examples are provided, and relationships with existing robust estimation methods are discussed.},
  file = {/home/victor/Zotero/storage/G3ZRZR29/Eguchi - 2001 - Robustifing maximum likelihood estimation by psi-d.pdf}
}

@phdthesis{el_amri_analyse_2019,
  title = {Analyse d'incertitudes et de Robustesse Pour Les Mod\`eles \`a Entr\'ees et Sorties Fonctionnelles},
  author = {El Amri, Mohamed},
  year = {2019},
  month = apr,
  abstract = {L'objectif de cette th\`ese est de r\'esoudre un probl\`eme d'inversion sous incertitudes de fonctions co\^uteuses \`a \'evaluer dans le cadre du param\'etrage du contr\^ole d'un syst\`eme de d\'epollution de v\'ehicules.L'effet de ces incertitudes est pris en compte au travers de l'esp\'erance de la grandeur d'int\'er\^et. Une difficult\'e r\'eside dans le fait que l'incertitude est en partie due \`a une entr\'ee fonctionnelle connue \`a travers d'un \'echantillon donn\'e. Nous proposons deux approches bas\'ees sur une approximation du code co\^uteux par processus gaussiens et une r\'eduction de dimension de la variable fonctionnelle par une m\'ethode de Karhunen-Lo\`eve.La premi\`ere approche consiste \`a appliquer une m\'ethode d'inversion de type SUR (Stepwise Uncertainty Reduction) sur l'esp\'erance de la grandeur d'int\'er\^et. En chaque point d'\'evaluation dans l'espace de contr\^ole, l'esp\'erance est estim\'ee par une m\'ethode de quantification fonctionnelle gloutonne qui fournit une repr\'esentation discr\`ete de la variable fonctionnelle et une estimation s\'equentielle efficace \`a partir de l'\'echantillon donn\'e de la variable fonctionnelle.La deuxi\`eme approche consiste \`a appliquer la m\'ethode SUR directement sur la grandeur d'int\'er\^et dans l'espace joint des variables de contr\^ole et des variables incertaines. Une strat\'egie d'enrichissement du plan d'exp\'eriences d\'edi\'ee \`a l'inversion sous incertitudes fonctionnelles et exploitant les propri\'et\'es des processus gaussiens est propos\'ee.Ces deux approches sont compar\'ees sur des fonctions jouets et sont appliqu\'ees \`a un cas industriel de post-traitement des gaz d'\'echappement d'un v\'ehicule. La probl\'ematique est de d\'eterminer les r\'eglages du contr\^ole du syst\`eme permettant le respect des normes de d\'epollution en pr\'esence d'incertitudes, sur le cycle de conduite.},
  file = {/home/victor/Zotero/storage/5UFAZ49I/El Amri - 2019 - Analyse d'incertitudes et de robustesse pour les m.pdf;/home/victor/Zotero/storage/6F4IM8BL/2019GREAM015.html},
  keywords = {Automobiles -- Dispositifs antipollution,Inversion,Math√©matiques appliqu√©es,Quantification des incertitudes,Response surfaces,Surfaces de r√©ponse (statistique),Surfaces de r√©ponses,Uncertainty Quantification},
  school = {Grenoble Alpes},
  type = {Thesis}
}

@article{el_amri_data-driven_2019,
  title = {Data-Driven Stochastic Inversion via Functional Quantization},
  author = {El Amri, Mohamed Reda and Helbert, C{\'e}line and Lepreux, Olivier and Zuniga, Miguel Munoz and Prieur, Cl{\'e}mentine and Sinoquet, Delphine},
  year = {2019},
  month = sep,
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-019-09888-8},
  journal = {Statistics and Computing},
  language = {en}
}

@phdthesis{el_mocayd_decomposition_2017,
  title = {La D\'ecomposition En Polyn\^ome Du Chaos Pour l'am\'elioration de l'assimilation de Donn\'ees Ensembliste En Hydraulique Fluviale},
  author = {El Mo{\c c}ayd, Nabil},
  year = {2017},
  month = mar,
  abstract = {Ce travail porte sur la construction d'un mod\`ele r\'eduit en hydraulique fluviale avec une m\'ethode de d\'ecomposition en polyn\^ome du chaos. Ce mod\`ele r\'eduit remplace le mod\`ele direct afin de r\'eduire le co\^ut de calcul li\'e aux m\'ethodes ensemblistes en quantification d'incertitudes et assimilation de donn\'ees. Le contexte de l'\'etude est la pr\'evision des crues et la gestion de la ressource en eau. Ce manuscrit est compos\'e de cinq parties, chacune divis\'ee en chapitres. La premi\`ere partie pr\'esente un \'etat de l'art des travaux en quantification des incertitudes et en assimilation de donn\'ees dans le domaine de l'hydraulique ainsi que les objectifs de la th\`ese. On pr\'esente le cadre de la pr\'evision des crues, ses enjeux et les outils dont on dispose pour pr\'evoir la dynamique des rivi\`eres. On pr\'esente notamment la future mission SWOT qui a pour but de mesurer les hauteurs d'eau dans les rivi\`eres avec un couverture globale \`a haute r\'esolution. On pr\'ecise notamment l'apport de ces mesures et leur compl\'ementarit\'e avec les mesures in-situ. La deuxi\`eme partie pr\'esente les \'equations de Saint-Venant, qui d\'ecrivent les \'ecoulements dans les rivi\`eres, ainsi qu'une discr\'etisation num\'erique de ces \'equations, telle qu'impl\'ement\'ee dans le logiciel Mascaret-1D. Le dernier chapitre de cette partie propose des simplifications des \'equations de Saint-Venant. La troisi\`eme partie de ce manuscrit pr\'esente les m\'ethodes de quantification et de r\'eduction des incertitudes. On pr\'esente notamment le contexte probabiliste de la quantification d'incertitudes et d'analyse de sensibilit\'e. On propose ensuite de r\'eduire la dimension d'un probl\`eme stochastique quand on traite de champs al\'eatoires. Les m\'ethodes de d\'ecomposition en polyn\^omes du chaos sont ensuite pr\'esent\'ees. Cette partie d\'edi\'ee \`a la m\'ethodologie s'ach\`eve par un chapitre consacr\'e \`a l'assimilation de donn\'ees ensemblistes et \`a l'utilisation des mod\`eles r\'eduits dans ce cadre. La quatri\`eme partie de ce manuscrit est d\'edi\'ee aux r\'esultats. On commence par identifier les sources d'incertitudes en hydraulique que l'on s'attache \`a quantifier et r\'eduire par la suite. Un article en cours de r\'evision d\'etaille la validation d'un mod\`ele r\'eduit pour les \'equations de Saint-Venant en r\'egime stationnaire lorsque l'incertitude est majoritairement port\'ee par les coefficients de frottement et le d\'ebit \`a l'amont. On montre que les moments statistiques, la densit\'e de probabilit\'e et la matrice de covariances spatiales pour la hauteur d'eau sont efficacement et pr\'ecis\'ement estim\'es \`a l'aide du mod\`ele r\'eduit dont la construction ne n\'ecessite que quelques dizaines d'int\'egrations du mod\`ele direct. On met \`a profit l'utilisation du mod\`ele r\'eduit pour r\'eduire le co\^ut de calcul du filtre de Kalman d'Ensemble dans le cadre d'un exercice d'assimilation de donn\'ees synth\'etiques de type SWOT. On s'int\'eresse pr\'ecis\'ement \`a la repr\'esentation spatiale de la donn\'ee telle que vue par SWOT: couverture globale du r\'eseau, moyennage spatial entre les pixels observ\'es. On montre notamment qu'\`a budget de calcul donn\'e les r\'esultats de l'analyse d'assimilation de donn\'ees qui repose sur l'utilisation du mod\`ele r\'eduit sont meilleurs que ceux obtenus avec le filtre classique. On s'int\'eresse enfin \`a la construction du mod\`ele r\'eduit en r\'egime instationnaire. On suppose ici que l'incertitude est li\'ee aux coefficients de frottement. Il s'agit \`a pr\'esent de juger de la n\'ecessit\'e du recalcul des coefficients polynomiaux au fil du temps et des cycles d'assimilation de donn\'ees. Pour ce travail seul des donn\'ees in-situ ont \'et\'e consid\'er\'ees. On suppose dans un deuxi\`eme temps que l'incertitude est port\'ee par le d\'ebit en amont du r\'eseau, qui est un vecteur temporel. On proc\`ede \`a une d\'ecomposition de type Karhunen-Lo\`eve pour r\'eduire la taille de l'espace incertain aux trois premiers modes. Nous sommes ainsi en mesure de mener \`a bien un exercice d'assimilation de donn\'ees. Pour finir, les conclusions et les perspectives de ce travail sont pr\'esent\'ees en cinqui\`eme partie.},
  file = {/home/victor/Zotero/storage/FGED5FTL/2017INPT0020.html},
  journal = {http://www.theses.fr},
  keywords = {Assimilation de donn√©es,Assimilation de donn√©es (g√©ophysique) -- Th√®ses et √©crits acad√©miques,Chaos polynomial expansion,Data assimilation,G√©nie fluvial -- Th√®ses et √©crits acad√©miques,Hydraulics,Hydraulique fluviale,Polyn√¥mes du chaos,Pr√©vision hydrologique -- Th√®ses et √©crits acad√©miques,Quantification des incertitudes,Surfaces Interfaces Continentales Hydrologie,Uncertainty quantification},
  school = {Toulouse, INPT},
  type = {Thesis}
}

@inproceedings{eldred_formulations_2002,
  title = {Formulations for {{Surrogate}}-{{Based Optimization Under Uncertainty}}},
  booktitle = {9th {{AIAA}}/{{ISSMO Symposium}} on {{Multidisciplinary Analysis}} and {{Optimization}}},
  author = {Eldred, Michael and Giunta, Anthony and Wojtkiewicz, Steven and Trucano, Timothy},
  year = {2002},
  month = sep,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  address = {{Atlanta, Georgia}},
  doi = {10.2514/6.2002-5585},
  abstract = {In this paper, several formulations for optimization under uncertainty are presented. In addition to the direct nesting of uncertainty quantification within optimization, formulations are presented for surrogate-based optimization under uncertainty in which the surrogate model appears at the optimization level, at the uncertainty quantification level, or at both levels. These surrogate models encompass both data fit and hierarchical surrogates. The DAKOTA software framework is used to provide the foundation for prototyping and initial benchmarking of these formulations. A critical component is the extension of algorithmic techniques for deterministic surrogate-based optimization to these surrogate-based optimization under uncertainty formulations. This involves the use of sequential trust regionbased approaches to manage the extent of the approximations and verify the approximate optima. Two analytic test problems and one engineering problem are solved using the different methodologies in order to compare their relative merits. Results show that surrogate-based optimization under uncertainty formulations show promise both in reducing the number of function evaluations required and in mitigating the effects of nonsmooth response variations.},
  file = {/home/victor/Zotero/storage/5KF3T7LA/Eldred et al. - 2002 - Formulations for Surrogate-Based Optimization Unde.pdf},
  isbn = {978-1-62410-120-5},
  language = {en}
}

@article{erickson_comparison_2017,
  title = {Comparison of {{Gaussian}} Process Modeling Software},
  author = {Erickson, Collin B. and Ankenman, Bruce E. and Sanchez, Susan M.},
  year = {2017},
  month = oct,
  abstract = {Gaussian process fitting, or kriging, is often used to create a model from a set of data. Many available software packages do this, but we show that very different results can be obtained from different packages even when using the same data and model. We describe the parameterization, features, and optimization used by eight different fitting packages that run on four different platforms. We then compare these eight packages using various data functions and data sets, revealing that there are stark differences between the packages. In addition to comparing the prediction accuracy, the predictive variance\textemdash which is important for evaluating precision of predictions and is often used in stopping criteria\textemdash is also evaluated.},
  archivePrefix = {arXiv},
  eprint = {1710.03157},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/H8N6PEJV/Erickson et al. - 2017 - Comparison of Gaussian process modeling software.pdf},
  journal = {arXiv:1710.03157 [stat]},
  keywords = {Statistics - Computation},
  language = {en},
  primaryClass = {stat}
}

@article{evans_inferences_2011,
  title = {Inferences from Prior-Based Loss Functions},
  author = {Evans, Michael and Jang, Gun Ho},
  year = {2011},
  month = apr,
  abstract = {Inferences that arise from loss functions determined by the prior are considered and it is shown that these lead to limiting Bayes rules that are closely connected with likelihood. The procedures obtained via these loss functions are invariant under reparameterizations and are Bayesian unbiased or limits of Bayesian unbiased inferences. These inferences serve as well-supported alternatives to MAP-based inferences.},
  archivePrefix = {arXiv},
  eprint = {1104.3258},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/EFUNJNSR/Evans et Jang - 2011 - Inferences from prior-based loss functions.pdf},
  journal = {arXiv:1104.3258 [math, stat]},
  keywords = {62C10,Mathematics - Statistics Theory},
  language = {en},
  primaryClass = {math, stat}
}

@article{evans_measuring_2016,
  title = {Measuring Statistical Evidence Using Relative Belief},
  author = {Evans, Michael},
  year = {2016},
  month = jan,
  volume = {14},
  pages = {91--96},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2015.12.001},
  abstract = {A fundamental concern of a theory of statistical inference is how one should measure statistical evidence. Certainly the words ``statistical evidence,'' or perhaps just ``evidence,'' are much used in statistical contexts. It is fair to say, however, that the precise characterization of this concept is somewhat elusive. Our goal here is to provide a definition of how to measure statistical evidence for any particular statistical problem. Since evidence is what causes beliefs to change, it is proposed to measure evidence by the amount beliefs change from a priori to a posteriori. As such, our definition involves prior beliefs and this raises issues of subjectivity versus objectivity in statistical analyses. This is dealt with through a principle requiring the falsifiability of any ingredients to a statistical analysis. These concerns lead to checking for prior-data conflict and measuring the a priori bias in a prior.},
  file = {/home/victor/Zotero/storage/3TSW3AQB/Evans - 2016 - Measuring statistical evidence using relative beli.pdf;/home/victor/Zotero/storage/BV3FCMIX/S2001037015000549.html},
  journal = {Computational and Structural Biotechnology Journal},
  keywords = {Checking for prior-data conflict,Principle of empirical criticism,Relative belief ratios,Statistical evidence},
  language = {en}
}

@article{feinberg_chaospy_2015,
  title = {Chaospy: {{An}} Open Source Tool for Designing Methods of Uncertainty Quantification},
  shorttitle = {Chaospy},
  author = {Feinberg, Jonathan and Langtangen, Hans Petter},
  year = {2015},
  month = nov,
  volume = {11},
  pages = {46--57},
  issn = {1877-7503},
  doi = {10.1016/j.jocs.2015.08.008},
  abstract = {The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.},
  file = {/home/victor/Zotero/storage/F5UT4FSX/Feinberg et Langtangen - 2015 - Chaospy An open source tool for designing methods.pdf;/home/victor/Zotero/storage/KZ9YB9SP/Feinberg et Langtangen - 2015 - Chaospy An open source tool for designing methods.pdf;/home/victor/Zotero/storage/CFEA9WQX/S1877750315300119.html;/home/victor/Zotero/storage/EYK3LJHA/S1877750315300119.html},
  journal = {Journal of Computational Science},
  keywords = {Monte Carlo simulation,Polynomial chaos expansions,Python package,Rosenblatt transformations,Uncertainty quantification}
}

@article{feliot_bayesian_2017,
  title = {A {{Bayesian}} Approach to Constrained Single- and Multi-Objective Optimization},
  author = {Feliot, Paul and Bect, Julien and Vazquez, Emmanuel},
  year = {2017},
  month = jan,
  volume = {67},
  pages = {97--133},
  issn = {0925-5001, 1573-2916},
  doi = {10.1007/s10898-016-0427-3},
  abstract = {This article addresses the problem of derivative-free (single- or multi-objective) optimization subject to multiple inequality constraints. Both the objective and constraint functions are assumed to be smooth, non-linear and expensive to evaluate. As a consequence, the number of evaluations that can be used to carry out the optimization is very limited, as in complex industrial design optimization problems. The method we propose to overcome this difficulty has its roots in both the Bayesian and the multi-objective optimization literatures. More specifically, an extended domination rule is used to handle objectives and constraints in a unified way, and a corresponding expected hyper-volume improvement sampling criterion is proposed. This new criterion is naturally adapted to the search of a feasible point when none is available, and reduces to existing Bayesian sampling criteria---the classical Expected Improvement (EI) criterion and some of its constrained/multi-objective extensions---as soon as at least one feasible point is available. The calculation and optimization of the criterion are performed using Sequential Monte Carlo techniques. In particular, an algorithm similar to the subset simulation method, which is well known in the field of structural reliability, is used to estimate the criterion. The method, which we call BMOO (for Bayesian Multi-Objective Optimization), is compared to state-of-the-art algorithms for single- and multi-objective constrained optimization.},
  archivePrefix = {arXiv},
  eprint = {1510.00503},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/FT8S2KHS/Feliot et al. - 2017 - A Bayesian approach to constrained single- and mul.pdf;/home/victor/Zotero/storage/ACBZ689B/1510.html},
  journal = {Journal of Global Optimization},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  number = {1-2}
}

@article{fletcher_data_2006,
  title = {A Data Assimilation Method for Log-Normally Distributed Observational Errors},
  author = {Fletcher, S. J. and Zupanski, M.},
  year = {2006},
  month = oct,
  volume = {132},
  pages = {2505--2519},
  issn = {1477-870X},
  doi = {10.1256/qj.05.222},
  abstract = {In this paper we change the standard assumption made in the Bayesian framework of variational data assimilation to allow for observational errors that are log-normally distributed. We address the question of which statistic best describes the distribution for the univariate and multivariate cases to justify our choice of the mode. From this choice we derive the associated cost function, Jacobian and Hessian with a normal background. We also find the solution to the Jacobian equal to zero in both model and observational space. Given the Hessian that we derive, we define a preconditioner to aid in the minimization of the cost function. We extend this to define a general form for the preconditioner, given a certain type of cost function. Copyright \textcopyright{} 2006 Royal Meteorological Society},
  file = {/home/victor/Zotero/storage/FD53NXLW/Fletcher et Zupanski - 2006 - A data assimilation method for log-normally distri.pdf;/home/victor/Zotero/storage/ZUS2Z2T2/abstract.html},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  keywords = {Hessian,Jacobian,Preconditioner},
  language = {en},
  number = {621}
}

@article{fletcher_hybrid_2006,
  title = {A Hybrid Multivariate {{Normal}} and Lognormal Distribution for Data Assimilation},
  author = {Fletcher, Steven J. and Zupanski, Milija},
  year = {2006},
  month = apr,
  volume = {7},
  pages = {43--46},
  issn = {1530-261X},
  doi = {10.1002/asl.128},
  abstract = {In this article, we define and prove a distribution, which is a combination of a multivariate Normal and lognormal distribution. From this distribution, we apply a Bayesian probability framework to derive a non-linear cost function similar to the one that is in current variational data assimilation (DA) applications. Copyright \textcopyright{} 2006 Royal Meteorological Society},
  file = {/home/victor/Zotero/storage/3SFBYEXR/Fletcher et Zupanski - 2006 - A hybrid multivariate Normal and lognormal distrib.pdf;/home/victor/Zotero/storage/5AFA4GSM/abstract\;jsessionid=7A1211F56DF3620C35EAF009D18E8855.html},
  journal = {Atmospheric Science Letters},
  keywords = {data assimilation,lognormal,non-Normal,probability},
  language = {en},
  number = {2}
}

@article{fletcher_mixed_2010,
  title = {Mixed {{Gaussian}}-Lognormal Four-Dimensional Data Assimilation},
  author = {Fletcher, S. J.},
  year = {2010},
  month = may,
  volume = {62},
  pages = {266--287},
  issn = {02806495, 16000870},
  doi = {10.1111/j.1600-0870.2010.00439.x},
  file = {/home/victor/Zotero/storage/6J6U69Y3/~.pdf},
  journal = {Tellus A},
  language = {en},
  number = {3}
}

@article{forouzandeh_shahraki_reliability-based_2014,
  title = {Reliability-Based Robust Design Optimization: {{A}} General Methodology Using Genetic Algorithm},
  shorttitle = {Reliability-Based Robust Design Optimization},
  author = {Forouzandeh Shahraki, Ameneh and Noorossana, Rassoul},
  year = {2014},
  month = aug,
  volume = {74},
  pages = {199--207},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2014.05.013},
  abstract = {In this paper, we present an improved general methodology including four stages to design robust and reliable products under uncertainties. First, as the formulation stage, we consider reliability and robustness simultaneously to propose the new formulation of reliability-based robust design optimization (RBRDO) problems. In order to generate reliable and robust Pareto-optimal solutions, the combination of genetic algorithm with reliability assessment loop based on the performance measure approach is applied as the second stage. Next, we develop two criteria to select a solution from obtained Pareto-optimal set to achieve the best possible implementation. Finally, the result verification is performed with Monte Carlo Simulations and also the quality improvement during manufacturing process is considered by identifying and controlling the critical variables. The effectiveness and applicability of this new proposed methodology is demonstrated through a case study.},
  file = {/home/victor/Zotero/storage/3SXRCR3K/Forouzandeh Shahraki et Noorossana - 2014 - Reliability-based robust design optimization A ge.pdf;/home/victor/Zotero/storage/PFKHCTX8/S0360835214001582.html},
  journal = {Computers \& Industrial Engineering},
  keywords = {Genetic algorithm,Most probable point,Multi-objective optimization,Process capability index,Reliability-based robust design optimization}
}

@incollection{forsyth_quick_2008,
  title = {Quick {{Shift}} and {{Kernel Methods}} for {{Mode Seeking}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2008},
  author = {Vedaldi, Andrea and Soatto, Stefano},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  year = {2008},
  volume = {5305},
  pages = {705--718},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-88693-8_52},
  abstract = {We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N 2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and overfragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces in a straightforward manner. We also show that the accelerated medoid shift can be used to initialize mean shift for increased efficiency. We illustrate our algorithms to clustering data on manifolds, image segmentation, and the automatic discovery of visual categories.},
  file = {/home/victor/Zotero/storage/54BBK8MD/Vedaldi et Soatto - 2008 - Quick Shift and Kernel Methods for Mode Seeking.pdf},
  isbn = {978-3-540-88692-1 978-3-540-88693-8},
  language = {en}
}

@article{fragoso_bayesian_2018,
  title = {Bayesian Model Averaging: {{A}} Systematic Review and Conceptual Classification},
  shorttitle = {Bayesian Model Averaging},
  author = {Fragoso, Tiago M. and Neto, Francisco Louzada},
  year = {2018},
  month = apr,
  volume = {86},
  pages = {1--28},
  issn = {03067734},
  doi = {10.1111/insr.12243},
  abstract = {Bayesian Model Averaging (BMA) is an application of Bayesian inference to the problems of model selection, combined estimation and prediction that produces a straightforward model choice criteria and less risky predictions. However, the application of BMA is not always straightforward, leading to diverse assumptions and situational choices on its different aspects. Despite the widespread application of BMA in the literature, there were not many accounts of these differences and trends besides a few landmark revisions in the late 1990s and early 2000s, therefore not taking into account any advancements made in the last 15 years. In this work, we present an account of these developments through a careful content analysis of 587 articles in BMA published between 1996 and 2014. We also develop a conceptual classification scheme to better describe this vast literature, understand its trends and future directions and provide guidance for the researcher interested in both the application and development of the methodology. The results of the classification scheme and content review are then used to discuss the present and future of the BMA literature.},
  archivePrefix = {arXiv},
  eprint = {1509.08864},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/K7NL3ZT3/Fragoso et Neto - 2018 - Bayesian model averaging A systematic review and .pdf},
  journal = {International Statistical Review},
  keywords = {Statistics - Applications,Statistics - Methodology},
  language = {en},
  number = {1}
}

@article{frangos_surrogate_2010,
  title = {Surrogate and Reduced-Order Modeling: A Comparison of Approaches for Large-Scale Statistical Inverse Problems [{{Chapter}} 7]},
  shorttitle = {Surrogate and Reduced-Order Modeling},
  author = {Frangos, M. and Marzouk, Y. and Willcox, K. and {van Bloemen Waanders}, B.},
  year = {2010},
  file = {/home/victor/Zotero/storage/RQW32K5F/Frangos et al. - 2010 - Surrogate and reduced-order modeling a comparison.pdf;/home/victor/Zotero/storage/U8J84R4V/105500.html}
}

@article{freedman_histogram_1981,
  title = {On the Histogram as a Density Estimator:{{L2}} Theory},
  shorttitle = {On the Histogram as a Density Estimator},
  author = {Freedman, David and Diaconis, Persi},
  year = {1981},
  month = dec,
  volume = {57},
  pages = {453--476},
  issn = {1432-2064},
  doi = {10.1007/BF01025868},
  file = {/home/victor/Zotero/storage/FVPC6M7L/Freedman et Diaconis - 1981 - On the histogram as a density estimatorL2 theory.pdf},
  journal = {Zeitschrift f\"ur Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  keywords = {Density Estimator,Mathematical Biology,Probability Theory,Stochastic Process},
  language = {en},
  number = {4}
}

@article{friederichs_probabilistic_2008,
  title = {A {{Probabilistic Forecast Approach}} for {{Daily Precipitation Totals}}},
  author = {Friederichs, Petra and Hense, Andreas},
  year = {2008},
  month = aug,
  volume = {23},
  pages = {659--673},
  issn = {0882-8156, 1520-0434},
  doi = {10.1175/2007WAF2007051.1},
  abstract = {Commonly, postprocessing techniques are employed to calibrate a model forecast. Here, a probabilistic postprocessor is presented that provides calibrated probability and quantile forecasts of precipitation on the local scale. The forecasts are based on large-scale circulation patterns of the 12-h forecast from the NCEP high-resolution Global Forecast System (GFS). The censored quantile regression is used to estimate selected quantiles of the precipitation amount and the probability of the occurrence of precipitation. The approach accounts for the mixed discrete-continuous character of daily precipitation totals. The forecasts are verified using a new verification score for quantile forecasts, namely the censored quantile verification (CQV) score.},
  file = {/home/victor/Zotero/storage/N6IJ5UU4/Friederichs et Hense - 2008 - A Probabilistic Forecast Approach for Daily Precip.pdf},
  journal = {Weather and Forecasting},
  language = {en},
  number = {4}
}

@article{friel_estimating_2011,
  title = {Estimating the Evidence -- a Review},
  author = {Friel, Nial and Wyse, Jason},
  year = {2011},
  month = nov,
  abstract = {The model evidence is a vital quantity in the comparison of statistical models under the Bayesian paradigm. This paper presents a review of commonly used methods. We outline some guidelines and offer some practical advice. The reviewed methods are compared for two examples; non-nested Gaussian linear regression and covariate subset selection in logistic regression.},
  archivePrefix = {arXiv},
  eprint = {1111.1957},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/W9R587RW/Friel et Wyse - 2011 - Estimating the evidence -- a review.pdf;/home/victor/Zotero/storage/ZV9QA6PC/1111.html},
  journal = {arXiv:1111.1957 [stat]},
  keywords = {Evidence estimation},
  primaryClass = {stat}
}

@article{frisch_burgulence_2000,
  title = {Burgulence},
  author = {Frisch, U. and Bec, J.},
  year = {2000},
  month = dec,
  abstract = {This is a review of selected work on the one- and multi-dimensional random Burgers equation (burgulence) with emphasis on questions generally asked for incompressible Navier\textendash Stokes turbulence, such as the law of decay of the energy and the pdf of velocity gradients. Most of the material is devoted to decaying (unforced) burgulence. For more details see the Table of Contents.},
  archivePrefix = {arXiv},
  eprint = {nlin/0012033},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/6MSAE4SI/Frisch et Bec - 2000 - Burgulence.pdf},
  journal = {arXiv:nlin/0012033},
  keywords = {Astrophysics,Condensed Matter,Nonlinear Sciences - Chaotic Dynamics},
  language = {en}
}

@article{gabrel_recent_2014,
  title = {Recent Advances in Robust Optimization: {{An}} Overview},
  shorttitle = {Recent Advances in Robust Optimization},
  author = {Gabrel, Virginie and Murat, C{\'e}cile and Thiele, Aur{\'e}lie},
  year = {2014},
  month = jun,
  volume = {235},
  pages = {471--483},
  issn = {03772217},
  doi = {10.1016/j.ejor.2013.09.036},
  abstract = {This paper provides an overview of developments in robust optimization since 2007. It seeks to give a representative picture of the research topics most explored in recent years, highlight common themes in the investigations of independent research teams and highlight the contributions of rising as well as established researchers both to the theory of robust optimization and its practice. With respect to the theory of robust optimization, this paper reviews recent results on the cases without and with recourse, i.e., the static and dynamic settings, as well as the connection with stochastic optimization and risk theory, the concept of distributionally robust optimization, and findings in robust nonlinear optimization. With respect to the practice of robust optimization, we consider a broad spectrum of applications, in particular inventory and logistics, finance, revenue management, but also queueing networks, machine learning, energy systems and the public good. Key developments in the period from 2007 to present include: (i) an extensive body of work on robust decision-making under uncertainty with uncertain distributions, i.e., ``robustifying'' stochastic optimization, (ii) a greater connection with decision sciences by linking uncertainty sets to risk theory, (iii) further results on nonlinear optimization and sequential decision-making and (iv) besides more work on established families of examples such as robust inventory and revenue management, the addition to the robust optimization literature of new application areas, especially energy systems and the public good.},
  file = {/home/victor/Zotero/storage/72QGQQ79/Gabrel et al. - 2014 - Recent advances in robust optimization An overvie.pdf},
  journal = {European Journal of Operational Research},
  language = {en},
  number = {3}
}

@misc{gassiat_statistiques_,
  title = {Statistiques {{Asymptotiques}} - {{Notes}} de {{Cours M2}}},
  author = {Gassiat, Elisabeth},
  file = {/home/victor/Zotero/storage/LCNS66GE/notes de cours- M2Stat.pdf}
}

@article{ge_stochastic_2008,
  title = {Stochastic {{Solution}} for {{Uncertainty Propagation}} in {{Nonlinear Shallow}}-{{Water Equations}}},
  author = {Ge, Liang and Cheung, Kwok Fai and Kobayashi, Marcelo H.},
  year = {2008},
  month = dec,
  volume = {134},
  pages = {1732--1743},
  issn = {0733-9429, 1943-7900},
  doi = {10.1061/(ASCE)0733-9429(2008)134:12(1732)},
  abstract = {This paper presents a stochastic approach to describe input uncertainties and their propagation through the nonlinear shallowwater equations. The formulation builds on a finite-volume model with a Godunov-type scheme for its shock capturing capabilities. Orthogonal polynomials from the Askey scheme provide expansion of the variables in terms of a finite number of modes from which the mean and higher-order moments of the distribution can be derived. The orthogonal property of the polynomials allows the use of a Galerkin projection to derive separate equations for the individual modes. Implementation of the polynomial chaos expansion and its nonintrusive counterpart determines the modal contributions from the resulting system of equations. Examples of long-wave transformation over a submerged hump illustrate the stochastic approach with uncertainties represented by Gaussian distribution. Additional results demonstrate the applicability of the approach with other distributions as well. The stochastic solution agrees well with the results from the Monte Carlo method, but at a small fraction of its computing cost.},
  file = {/home/victor/Zotero/storage/QG6NBBQ9/Ge et al. - 2008 - Stochastic Solution for Uncertainty Propagation in.pdf},
  journal = {Journal of Hydraulic Engineering},
  language = {en},
  number = {12}
}

@unpublished{gerbeau_moment-matching_2016,
  title = {A Moment-Matching Method to Study the Variability of Phenomena Described by Partial Differential Equations},
  author = {Gerbeau, Jean-Fr{\'e}d{\'e}ric and Lombardi, Damiano and Tixier, Eliott},
  year = {2016},
  month = nov,
  abstract = {Many phenomena are modeled by deterministic differential equations , whereas the observation of these phenomena, in particular in life sciences, exhibits an important variability. This paper addresses the following question: how can the model be adapted to reflect the observed variability? Given an adequate model, it is possible to account for this variability by allowing some parameters to adopt a stochastic behavior. Finding the parameters probability density function that explains the observed variability is a difficult stochastic inverse problem, especially when the computational cost of the forward problem is high. In this paper, a non-parametric and non-intrusive procedure based on offline computations of the forward model is proposed. It infers the probability density function of the uncertain parameters from the matching of the statistical moments of observable degrees of freedom (DOFs) of the model. This inverse procedure is improved by incorporating an algorithm that selects a subset of the model DOFs that both reduces its computational cost and increases its robustness. This algorithm uses the pre-computed model outputs to build an approximation of the local sensitivities. The DOFs are selected so that the maximum information on the sensitivities is conserved. The proposed approach is illustrated with elliptic and parabolic PDEs. In the Appendix, an nonlinear ODE is considered and the strategy is compared with two existing ones.},
  file = {/home/victor/Zotero/storage/M4DPXLDQ/Gerbeau et al. - 2016 - A moment-matching method to study the variability .pdf},
  keywords = {backward uncertainty quantification,maximum entropy,Moment matching,stochastic inverse problem}
}

@article{geyer_5601_nodate,
  title = {5601 {{Notes}}: {{The Sandwich Estimator}}},
  author = {Geyer, Charles J},
  pages = {28},
  file = {/home/victor/Zotero/storage/DWXES7GY/Geyer - 5601 Notes The Sandwich Estimator.pdf},
  language = {en}
}

@article{ghate_inexpensive_nodate,
  title = {Inexpensive {{Monte Carlo Uncertainty Analysis}}},
  author = {Ghate, D and Giles, M B},
  pages = {8},
  abstract = {This paper proposes a new methodology for uncertainty propagation through CFD codes using adjoint error correction[7]. The mathematical formulation is presented followed by a proof-ofconcept model example. The method is further demonstrated using artificial geometric uncertainty for a 2D inviscid airfoil code. Reduced order modelling has been used to further reduce the computational costs, and results of approximate Monte Carlo(MC) simulations have been compared with full nonlinear MC simulations.},
  file = {/home/victor/Zotero/storage/2SC6RRQI/Ghate et Giles - Inexpensive Monte Carlo Uncertainty Analysis.pdf},
  language = {en}
}

@article{gibbs_choosing_nodate,
  title = {{{ON CHOOSING AND BOUNDING PROBABILITY METRICS}}},
  author = {Gibbs, Alison L and Su, Francis Edward},
  pages = {21},
  abstract = {When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability metrics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric.},
  file = {/home/victor/Zotero/storage/SMSLWCWI/Gibbs et Su - ON CHOOSING AND BOUNDING PROBABILITY METRICS.pdf},
  language = {en}
}

@article{gilbert_numerical_1989,
  title = {Some Numerical Experiments with Variable-Storage Quasi-{{Newton}} Algorithms},
  author = {Gilbert, Jean Charles and Lemar{\'e}chal, Claude},
  year = {1989},
  volume = {45},
  pages = {407--435},
  publisher = {{Springer}},
  file = {/home/victor/Zotero/storage/VKPDTYAZ/Gilbert et Lemar√©chal - 1989 - Some numerical experiments with variable-storage q.pdf;/home/victor/Zotero/storage/FIDU8WKN/BF01589113.html},
  journal = {Mathematical programming},
  number = {1-3}
}

@phdthesis{gilquin_echantillonnages_2016,
  title = {{\'Echantillonnages Monte Carlo et quasi-Monte Carlo pour l'estimation des indices de Sobol' : application \`a un mod\`ele transport-urbanisme}},
  shorttitle = {{\'Echantillonnages Monte Carlo et quasi-Monte Carlo pour l'estimation des indices de Sobol'}},
  author = {Gilquin, Laurent},
  year = {2016},
  month = oct,
  abstract = {Le d\'eveloppement et l'utilisation de mod\`eles int\'egr\'es transport-urbanisme sont devenus une norme pour repr\'esenter les interactions entre l'usage des sols et le transport de biens et d'individus sur un territoire. Ces mod\`eles sont souvent utilis\'es comme outils d'aide \`a la d\'ecision pour des politiques de planification urbaine.Les mod\`eles transport-urbanisme, et plus g\'en\'eralement les mod\`eles math\'ematiques, sont pour la majorit\'e con\c{c}us \`a partir de codes num\'eriques complexes. Ces codes impliquent tr\`es souvent des param\`etres dont l'incertitude est peu connue et peut potentiellement avoir un impact important sur les variables de sortie du mod\`ele.Les m\'ethodes d'analyse de sensibilit\'e globales sont des outils performants permettant d'\'etudier l'influence des param\`etres d'un mod\`ele sur ses sorties. En particulier, les m\'ethodes bas\'ees sur le calcul des indices de sensibilit\'e de Sobol' fournissent la possibilit\'e de quantifier l'influence de chaque param\`etre mais \'egalement d'identifier l'existence d'interactions entre ces param\`etres.Dans cette th\`ese, nous privil\'egions la m\'ethode dite \`a base de plans d'exp\'eriences r\'epliqu\'es encore appel\'ee m\'ethode r\'epliqu\'ee. Cette m\'ethode a l'avantage de ne requ\'erir qu'un nombre relativement faible d'\'evaluations du mod\`ele pour calculer les indices de Sobol' d'ordre un et deux.Cette th\`ese se focalise sur des extensions de la m\'ethode r\'epliqu\'ee pour faire face \`a des contraintes issues de notre application sur le mod\`ele transport-urbanisme Tranus, comme la pr\'esence de corr\'elation entre param\`etres et la prise en compte de sorties multivari\'ees.Nos travaux proposent \'egalement une approche r\'ecursive pour l'estimation s\'equentielle des indices de Sobol'. L'approche r\'ecursive repose \`a la fois sur la construction it\'erative d'hypercubes latins et de tableaux orthogonaux stratifi\'es et sur la d\'efinition d'un nouveau crit\`ere d'arr\^et. Cette approche offre une meilleure pr\'ecision sur l'estimation des indices tout en permettant de recycler des premiers jeux d'\'evaluations du mod\`ele. Nous proposons aussi de combiner une telle approche avec un \'echantillonnage quasi-Monte Carlo.Nous pr\'esentons \'egalement une application de nos contributions pour le calage du mod\`ele de transport-urbanisme Tranus.},
  file = {/home/victor/Zotero/storage/W44NI63K/Gilquin - 2016 - √âchantillonnages Monte Carlo et quasi-Monte Carlo .pdf;/home/victor/Zotero/storage/RK4DE9LZ/tel-01680304.html},
  language = {fr},
  school = {Universit\'e Grenoble Alpes}
}

@article{ginsbourger_bayesian_2014,
  title = {Bayesian {{Adaptive Reconstruction}} of {{Profile Optima}} and {{Optimizers}}},
  author = {Ginsbourger, David and Baccou, Jean and Chevalier, Cl{\'e}ment and Perales, Fr{\'e}d{\'e}ric and Garland, Nicolas and Monerie, Yann},
  year = {2014},
  month = jan,
  volume = {2},
  pages = {490--510},
  issn = {2166-2525},
  doi = {10.1137/130949555},
  abstract = {Given a function depending both on decision parameters and nuisance variables, we consider the issue of estimating and quantifying uncertainty on profile optima and/or optimal points as functions of the nuisance variables. The proposed methods base on interpolations of the objective function constructed from a finite set of evaluations. Here the functions of interest are reconstructed relying on a kriging model, but also using Gaussian field conditional simulations, that allow a quantification of uncertainties in the Bayesian framework. Besides, we elaborate a variant of the Expected Improvement criterion, that proves efficient for adaptively learning the set of profile optima and optimizers. The results are illustrated on a toy example and through a physics case study on the optimal packing of polydisperse frictionless spheres.},
  file = {/home/victor/Zotero/storage/FBVA9HE4/Ginsbourger et al. - 2014 - Bayesian Adaptive Reconstruction of Profile Optima.pdf},
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  language = {en},
  number = {1}
}

@incollection{ginsbourger_kriging_2010,
  title = {Kriging Is Well-Suited to Parallelize Optimization},
  booktitle = {Computational {{Intelligence}} in {{Expensive Optimization Problems}}},
  author = {Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  editor = {Tenne, Yoel and Goh, Chi-Keong},
  year = {2010},
  pages = {131--162},
  publisher = {{springer}},
  doi = {10.1007/978-3-642-10701-6_6},
  abstract = {The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement ({$\mathsl{q}-\mathbb{E}\mathsl{I}$}q-EIq-\{\textbackslash mathbb E\}I), aimed at choosing several points at the same time. An analytical expression of the {$\mathsl{q}-\mathbb{E}\mathsl{I}$}q-EIq-\{\textbackslash mathbb E\}I is given when q\,=\,2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the {$\mathsl{q}-\mathbb{E}\mathsl{I}$}q-EIq-\{\textbackslash mathbb E\}I, and apply them to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q\,{$\in$} [1,10]).},
  file = {/home/victor/Zotero/storage/M92WKPUQ/emse-00436126.html},
  keywords = {gaussian process,global optimization,kriging,optimization},
  series = {Springer Series in {{Evolutionary Learning}} and {{Optimization}}}
}

@inproceedings{giunta_perspectives_2004,
  title = {Perspectives in {{Optimization Under Uncertainty}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Perspectives in {{Optimization Under Uncertainty}}},
  booktitle = {10th {{AIAA}}/{{ISSMO Multidisciplinary Analysis}} and {{Optimization Conference}}},
  author = {Giunta, Anthony and Eldred, Michael and Swiler, Laura and Trucano, Timothy and Wojtkiewicz, Steven},
  year = {2004},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2004-4451},
  file = {/home/victor/Zotero/storage/CJN956LS/Giunta et al. - Perspectives in Optimization Under Uncertainty Al.pdf;/home/victor/Zotero/storage/IW3DSWB8/6.html}
}

@book{glasserman_monte_2013,
  title = {Monte {{Carlo}} Methods in Financial Engineering},
  author = {Glasserman, Paul},
  year = {2013},
  volume = {53},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\Victor\\Documents\\[Paul_Glasserman]_Monte_Carlo_Methods_in_Financial_Engineering_Springer_2003.pdf},
  keywords = {Gibbs,LHS,MCMC,Sampling}
}

@article{gneiting_comparing_2011,
  title = {Comparing {{Density Forecasts Using Threshold}}- and {{Quantile}}-{{Weighted Scoring Rules}}},
  author = {Gneiting, Tilmann and Ranjan, Roopesh},
  year = {2011},
  month = jul,
  volume = {29},
  pages = {411--422},
  issn = {0735-0015, 1537-2707},
  doi = {10.1198/jbes.2010.08110},
  abstract = {We propose a method for comparing density forecasts that is based on weighted versions of the continuous ranked probability score. The weighting emphasizes regions of interest, such as the tails or the center of a variable's range, while retaining propriety, as opposed to a recently developed weighted likelihood ratio test, which can be hedged. Threshold and quantile based decompositions of the continuous ranked probability score can be illustrated graphically and prompt insights into the strengths and deficiencies of a forecasting method. We illustrate the use of the test and graphical tools in case studies on the Bank of England's density forecasts of quarterly inflation rates in the United Kingdom, and probabilistic predictions of wind resources in the Pacific Northwest.},
  file = {/home/victor/Zotero/storage/EWQYSHA7/Gneiting et Ranjan - 2011 - Comparing Density Forecasts Using Threshold- and Q.pdf},
  journal = {Journal of Business \& Economic Statistics},
  language = {en},
  number = {3}
}

@article{gneiting_making_2009,
  title = {Making and {{Evaluating Point Forecasts}}},
  author = {Gneiting, Tilmann},
  year = {2009},
  month = dec,
  abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, such as the absolute error or the squared error. The individual scores are then averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the (root) mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
  archivePrefix = {arXiv},
  eprint = {0912.0902},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/CHKBUKFR/Gneiting - 2009 - Making and Evaluating Point Forecasts.pdf;/home/victor/Zotero/storage/2YUQK24L/0912.html},
  journal = {arXiv:0912.0902 [math, stat]},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Methodology},
  primaryClass = {math, stat}
}

@article{gneiting_probabilistic_2014,
  title = {Probabilistic {{Forecasting}}},
  author = {Gneiting, Tilmann and Katzfuss, Matthias},
  year = {2014},
  month = jan,
  volume = {1},
  pages = {125--151},
  issn = {2326-8298},
  doi = {10.1146/annurev-statistics-062713-085831},
  abstract = {A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts. We emphasize methodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples.},
  file = {/home/victor/Zotero/storage/NL6TV4DL/Gneiting et Katzfuss - 2014 - Probabilistic Forecasting.pdf;/home/victor/Zotero/storage/GM53DIYU/annurev-statistics-062713-085831.html},
  journal = {Annual Review of Statistics and Its Application},
  number = {1}
}

@misc{goerigk_algorithm_nodate,
  title = {Algorithm {{Engineering}} in {{Robust Optimization}}},
  author = {Goerigk, Mark and Sch{\"o}bel, Anita},
  file = {/home/victor/Zotero/storage/6JX6UK4B/1505.04901.pdf}
}

@article{goerigk_ranking_2018,
  title = {Ranking Robustness and Its Application to Evacuation Planning},
  author = {Goerigk, Marc and Hamacher, Horst W. and Kinscherff, Anika},
  year = {2018},
  month = feb,
  volume = {264},
  pages = {837--846},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2016.05.037},
  abstract = {We present a new approach to handle uncertain combinatorial optimization problems that uses solution ranking procedures to determine the degree of robustness of a solution. Unlike classic concepts for robust optimization, our approach is not purely based on absolute quantitative performance, but also includes qualitative aspects that are of major importance for the decision maker. We discuss the two variants, solution ranking and objective ranking robustness, in more detail, presenting problem complexities and solution approaches. Using an uncertain shortest path problem as a computational example, the potential of our approach is demonstrated in the context of evacuation planning due to river flooding.},
  file = {/home/victor/Zotero/storage/M3XSTTJ7/Goerigk et al. - 2018 - Ranking robustness and its application to evacuati.pdf;/home/victor/Zotero/storage/PYHEKM95/S0377221716303745.html},
  journal = {European Journal of Operational Research},
  keywords = {Combinatorial optimization,Evacuation planning,Robust optimization,Solution ranking},
  number = {3}
}

@article{gong_adaptive_2017,
  title = {An Adaptive Surrogate Modeling-Based Sampling Strategy for Parameter Optimization and Distribution Estimation ({{ASMO}}-{{PODE}})},
  author = {Gong, Wei and Duan, Qingyun},
  year = {2017},
  month = sep,
  volume = {95},
  pages = {61--75},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2017.05.005},
  abstract = {Parameter distribution estimation has long been a hot issue for the uncertainty quantification of environmental models. Traditional approaches such as MCMC (Markov Chain Monte Carlo) are prohibitive to be applied to large complex dynamic models because of the high computational cost of computing resources. To reduce the number of model evaluations required, we proposed an adaptive surrogate modeling-based sampling strategy for parameter distribution estimation, named ASMO-PODE (Adaptive Surrogate Modeling-based Optimization \textendash{} Parameter Optimization and Distribution Estimation). The ASMO-PODE can provide an estimation of the parameter distribution using as little as one percent of the model evaluations required by a regular MCMC approach. The effectiveness and efficiency of the ASMO-PODE approach have been evaluated with 2 test problems and one land surface model, the Common Land Model. The results demonstrated that the ASMO-PODE method is an economic way for parameter optimization and distribution estimation.},
  file = {/home/victor/Zotero/storage/XLDRYL8N/Gong et Duan - 2017 - An adaptive surrogate modeling-based sampling stra.pdf;/home/victor/Zotero/storage/GLLWW75R/S1364815216310830.html},
  journal = {Environmental Modelling \& Software},
  keywords = {Adaptive sampling,Gaussian processes regression,MCMC,Surrogate model}
}

@article{gong_multi-objective_2015,
  title = {Multi-Objective Parameter Optimization of Common Land Model Using Adaptive Surrogate Modeling},
  author = {Gong, W. and Duan, Q. and Li, J. and Wang, C. and Di, Z. and Dai, Y. and Ye, A. and Miao, C.},
  year = {2015},
  month = may,
  volume = {19},
  pages = {2409--2425},
  issn = {1607-7938},
  doi = {10.5194/hess-19-2409-2015},
  abstract = {Parameter specification usually has significant influence on the performance of land surface models (LSMs). However, estimating the parameters properly is a challenging task due to the following reasons: (1) LSMs usually have too many adjustable parameters (20 to 100 or even more), leading to the curse of dimensionality in the parameter input space; (2) LSMs usually have many output variables involving water/energy/carbon cycles, so that calibrating LSMs is actually a multi-objective optimization problem; (3) Regional LSMs are expensive to run, while conventional multi-objective optimization methods need a large number of model runs (typically {$\sim$} 105\textendash 106). It makes parameter optimization computationally prohibitive. An uncertainty quantification framework was developed to meet the aforementioned challenges, which include the following steps: (1) using parameter screening to reduce the number of adjustable parameters, (2) using surrogate models to emulate the responses of dynamic models to the variation of adjustable parameters, (3) using an adaptive strategy to improve the efficiency of surrogate modeling-based optimization; (4) using a weighting function to transfer multi-objective optimization to single-objective optimization. In this study, we demonstrate the uncertainty quantification framework on a single column application of a LSM \textendash{} the Common Land Model (CoLM), and evaluate the effectiveness and efficiency of the proposed framework. The result indicate that this framework can efficiently achieve optimal parameters in a more effective way. Moreover, this result implies the possibility of calibrating other large complex dynamic models, such as regionalscale LSMs, atmospheric models and climate models.},
  file = {/home/victor/Zotero/storage/YHYSTU8F/Gong et al. - 2015 - Multi-objective parameter optimization of common l.pdf},
  journal = {Hydrology and Earth System Sciences},
  language = {en},
  number = {5}
}

@article{gong_wei_multiobjective_2015,
  title = {Multiobjective Adaptive Surrogate Modeling-based Optimization for Parameter Estimation of Large, Complex Geophysical Models},
  author = {{Gong Wei} and {Duan Qingyun} and {Li Jianduo} and {Wang Chen} and {Di Zhenhua} and {Ye Aizhong} and {Miao Chiyuan} and {Dai Yongjiu}},
  year = {2015},
  month = dec,
  volume = {52},
  pages = {1984--2008},
  issn = {0043-1397},
  doi = {10.1002/2015WR018230},
  abstract = {Abstract Parameter specification is an important source of uncertainty in large, complex geophysical models. These models generally have multiple model outputs that require multiobjective optimization algorithms. Although such algorithms have long been available, they usually require a large number of model runs and are therefore computationally expensive for large, complex dynamic models. In this paper, a multiobjective adaptive surrogate modeling?based optimization (MO?ASMO) algorithm is introduced that aims to reduce computational cost while maintaining optimization effectiveness. Geophysical dynamic models usually have a prior parameterization scheme derived from the physical processes involved, and our goal is to improve all of the objectives by parameter calibration. In this study, we developed a method for directing the search processes toward the region that can improve all of the objectives simultaneously. We tested the MO?ASMO algorithm against NSGA?II and SUMO with 13 test functions and a land surface model ? the Common Land Model (CoLM). The results demonstrated the effectiveness and efficiency of MO?ASMO.},
  file = {/home/victor/Zotero/storage/G5IIESLL/Gong Wei et al. - 2015 - Multiobjective adaptive surrogate modeling‚Äêbased o.pdf;/home/victor/Zotero/storage/RBW94KNS/2015WR018230.html},
  journal = {Water Resources Research},
  keywords = {adaptive sampling,Gaussian processes regression,multiobjective optimization,surrogate model},
  number = {3}
}

@article{gorissen_practical_2015,
  title = {A Practical Guide to Robust Optimization},
  author = {Gorissen, Bram L. and Yan{\i}ko{\u g}lu, {\.I}hsan and {den Hertog}, Dick},
  year = {2015},
  month = jun,
  volume = {53},
  pages = {124--137},
  issn = {03050483},
  doi = {10.1016/j.omega.2014.12.006},
  abstract = {Robust optimization is a young and active research field that has been mainly developed in the last 15 years. Robust optimization is very useful for practice, since it is tailored to the information at hand, and it leads to computationally tractable formulations. It is therefore remarkable that real-life applications of robust optimization are still lagging behind; there is much more potential for real-life applications than has been exploited hitherto. The aim of this paper is to help practitioners to understand robust optimization and to successfully apply it in practice. We provide a brief introduction to robust optimization, and also describe important do's and don'ts for using it in practice. We use many small examples to illustrate our discussions.},
  file = {/home/victor/Zotero/storage/7UD53J55/Gorissen et al. - 2015 - A practical guide to robust optimization.pdf},
  journal = {Omega},
  language = {en}
}

@article{gotoh_robust_2018,
  title = {Robust Empirical Optimization Is Almost the Same as Mean\textendash Variance Optimization},
  author = {Gotoh, Jun-ya and Kim, Michael Jong and Lim, Andrew E. B.},
  year = {2018},
  month = jul,
  volume = {46},
  pages = {448--452},
  issn = {0167-6377},
  doi = {10.1016/j.orl.2018.05.005},
  abstract = {We formulate a distributionally robust optimization problem where the deviation of the alternative distribution is controlled by a {$\phi$}-divergence penalty in the objective, and show that a large class of these problems are essentially equivalent to a mean\textendash variance problem. We also show that while a ``small amount of robustness'' always reduces the in-sample expected reward, the reduction in the variance, which is a measure of sensitivity to model misspecification, is an order of magnitude larger.},
  file = {/home/victor/Zotero/storage/57TMEQYI/Gotoh et al. - 2018 - Robust empirical optimization is almost the same a.pdf;/home/victor/Zotero/storage/RHUWPUAR/S016763771730514X.html},
  journal = {Operations Research Letters},
  keywords = {-divergence,Bias‚Äìvariance trade-off,Data-driven optimization,Mean‚Äìvariance optimization,Regularization,Robust empirical optimization},
  number = {4}
}

@article{gould_differentiating_2016,
  title = {On Differentiating Parameterized Argmin and Argmax Problems with Application to Bi-Level Optimization},
  author = {Gould, Stephen and Fernando, Basura and Cherian, Anoop and Anderson, Peter and Cruz, Rodrigo Santa and Guo, Edison},
  year = {2016},
  file = {/home/victor/Zotero/storage/5TLGQHHW/argmin-TR-2016.pdf},
  journal = {arXiv preprint arXiv:1607.05447}
}

@article{grodzevich_normalization_2006,
  title = {Normalization and Other Topics in Multi-Objective Optimization},
  author = {Grodzevich, Oleg and Romanko, Oleksandr},
  year = {2006},
  file = {/home/victor/Zotero/storage/FHRIYBL5/Grodzevich et Romanko - 2006 - Normalization and other topics in multi-objective .pdf}
}

@inproceedings{gu_multi-parametric_2015,
  title = {Multi-Parametric Uncertainty Quantification with a Hybrid {{Monte}}-{{Carlo}} / Polynomial Chaos Expansion {{FDTD}} Method},
  booktitle = {2015 {{IEEE MTT}}-{{S International Microwave Symposium}}},
  author = {Gu, Zixi and Sarris, C. D.},
  year = {2015},
  month = may,
  pages = {1--3},
  doi = {10.1109/MWSYM.2015.7166881},
  abstract = {The increasing complexity of microwave structures necessitates the quantification and analysis of fabrication process uncertainties that directly impact their performance, as a means of ensuring performance robustness. A particular technique that has emerged as a promising alternative to the time-consuming Monte-Carlo method for such analyses, is the polynomial chaos expansion (PCE). However, PCE-based methods suffer from a rapid increase in their computational cost with the number of random variables included in the analysis. This paper demonstrates that a hybrid Monte-Carlo/PCE technique can lead to the dramatic acceleration of its constituent methods, in the context of FDTD, effectively mitigating the ``curse of dimensionality'' and facilitating the multi-parametric uncertainty analysis of electromagnetic geometries.},
  file = {/home/victor/Zotero/storage/JWLRK5J5/7166881.html},
  keywords = {chaos,Computational electromagnetics,electromagnetic geometries,FDTD,FDTD method,finite difference time-domain analysis,finite difference time-domain method,hybrid Monte Carlo-PCE technique,hybrid Monte Carlo-polynomial chaos expansion,microwave materials,microwave structures,Monte Carlo method,Monte Carlo methods,multiparametric uncertainty analysis,multiparametric uncertainty quantification,PCE-based methods,polynomials,statistical analysis}
}

@article{guhaniyogi_divide-and-conquer_2017,
  title = {A {{Divide}}-and-{{Conquer Bayesian Approach}} to {{Large}}-{{Scale Kriging}}},
  author = {Guhaniyogi, Rajarshi and Li, Cheng and Savitsky, Terrance D. and Srivastava, Sanvesh},
  year = {2017},
  month = dec,
  abstract = {Flexible hierarchical Bayesian modeling of massive data is challenging due to poorly scaling computations in large sample size settings. This article is motivated by spatial process models for analyzing geostatistical data, which typically entail computations that become prohibitive as the number of spatial locations becomes large. We propose a three-step divide-and-conquer strategy within the Bayesian paradigm to achieve massive scalability for any spatial process model. We partition the data into a large number of subsets, apply a readily available Bayesian spatial process model on every subset in parallel, and optimally combine the posterior distributions estimated across all the subsets into a pseudo-posterior distribution that conditions on the entire data. The combined pseudo posterior distribution is used for predicting the responses at arbitrary locations and for performing posterior inference on the model parameters and the residual spatial surface. We call this approach "Distributed Kriging" (DISK). It offers significant advantages in applications where the entire data are or can be stored on multiple machines. Under the standard theoretical setup, we show that if the number of subsets is not too large, then the Bayes risk of estimating the true residual spatial surface using the DISK posterior distribution decays to zero at a nearly optimal rate. While DISK is a general approach to distributed nonparametric regression, we focus on its applications in spatial statistics and demonstrate its empirical performance using a stationary full-rank and a nonstationary low-rank model based on Gaussian process (GP) prior. A variety of simulations and a geostatistical analysis of the Pacific Ocean sea surface temperature data validate our theoretical results.},
  archivePrefix = {arXiv},
  eprint = {1712.09767},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/95I6757C/Guhaniyogi et al. - 2017 - A Divide-and-Conquer Bayesian Approach to Large-Sc.pdf;/home/victor/Zotero/storage/2H2UGTP6/1712.html},
  journal = {arXiv:1712.09767 [stat]},
  keywords = {Statistics - Methodology},
  primaryClass = {stat}
}

@article{guillaume_robust_nodate,
  title = {Robust Parameter Estimation of Density Functions under Fuzzy Interval Observations},
  author = {Guillaume, Romain and Dubois, Didier},
  pages = {12},
  abstract = {This paper deals with the derivation of a probabilistic parametric model from interval or fuzzy data using the maximum likelihood principle. In contrast with classical techniques such as the EM algorithm, that define a precise likelihood function by averaging inside each imprecise observations, our approach presupposes that each imprecise observation underlies a precise one, and that the uncertainty that pervades its observation is epistemic, rather than representing noise. We define an interval-valued likelihood function and apply robust optimisation methods to find a safe plausible estimate of the statistical parameters. The resulting density has a standard deviation that is large enough to cover the imprecision of the observations, making a pessimistic assumption on dispersion. This approach is extended to fuzzy data by optimizing the average of lower likelihoods over a collection of data sets obtained from cuts of the fuzzy intervals, as a trade off between optimistic and pessimistic interpretations of fuzzy data. The principles of this method are compared with those of other existing approaches to handle incompleteness of observations, especially the EM technique.},
  file = {/home/victor/Zotero/storage/P4R5KWNI/Guillaume et Dubois - Robust parameter estimation of density functions u.pdf},
  language = {en}
}

@article{gunantara_review_2018,
  title = {A Review of Multi-Objective Optimization: {{Methods}} and Its Applications},
  shorttitle = {A Review of Multi-Objective Optimization},
  author = {Gunantara, Nyoman},
  editor = {Ai, Qingsong},
  year = {2018},
  month = jul,
  volume = {5},
  issn = {2331-1916},
  doi = {10.1080/23311916.2018.1502242},
  abstract = {Several reviews have been made regarding the methods and application of multi-objective optimization (MOO). There are two methods of MOO that do not require complicated mathematical equations, so the problem becomes simple. These two methods are the Pareto and scalarization. In the Pareto method, there is a dominated solution and a non-dominated solution obtained by a continuously updated algorithm. Meanwhile, the scalarization method creates multi-objective functions made into a single solution using weights. There are three types of weights in scalarization which are equal weights, rank order centroid weights, and rank-sum weights. Next, the solution using the Pareto method is a performance indicators component that forms MOO a separate and produces a compromise solution and can be displayed in the form of Pareto optimal front, while the solution using the scalarization method is a performance indicators component that forms a scalar function which is incorporated in the fitness function.},
  file = {/home/victor/Zotero/storage/GDAZ9BPL/Gunantara - 2018 - A review of multi-objective optimization Methods .pdf},
  journal = {Cogent Engineering},
  language = {en},
  number = {1}
}

@article{gupta_measure_nodate,
  title = {A {{Measure Theory Tutorial}} ({{Measure Theory}} for {{Dummies}})},
  author = {Gupta, Maya R},
  pages = {7},
  abstract = {This tutorial is an informal introduction to measure theory for people who are interested in reading papers that use measure theory. The tutorial assumes one has had at least a year of college-level calculus, some graduate level exposure to random processes, and familiarity with terms like ``closed'' and ``open.'' The focus is on the terms and ideas relevant to applied probability and information theory. There are no proofs and no exercises.},
  file = {/home/victor/Zotero/storage/CJIR46KK/Gupta - A Measure Theory Tutorial (Measure Theory for Dumm.pdf},
  language = {en}
}

@incollection{gupta_unified_2016,
  title = {A {{Unified Framework}} for {{Optimization Under Uncertainty}}},
  booktitle = {Optimization {{Challenges}} in {{Complex}}, {{Networked}} and {{Risky Systems}}},
  author = {Powell, Warren B.},
  editor = {Gupta, Aparna and Capponi, Agostino and Smith, J. Cole and Greenberg, Harvey J.},
  year = {2016},
  month = oct,
  pages = {45--83},
  publisher = {{INFORMS}},
  doi = {10.1287/educ.2016.0149},
  file = {/home/victor/Zotero/storage/B72JCMMX/Powell - 2016 - A Unified Framework for Optimization Under Uncerta.pdf},
  isbn = {978-0-9843378-9-7},
  language = {en}
}

@article{habibi_exact_2011,
  title = {Exact Distribution of Argmax (Argmin)},
  author = {Habibi, Reza},
  year = {2011},
  volume = {26},
  pages = {155--162},
  file = {/home/victor/Zotero/storage/FCTZLU2W/EQC.2011.015.pdf},
  journal = {Economic Quality Control},
  keywords = {argmax distribution},
  number = {2}
}

@article{halpern_weighted_2013,
  title = {Weighted Regret-Based Likelihood: A New Approach to Describing Uncertainty},
  shorttitle = {Weighted Regret-Based Likelihood},
  author = {Halpern, Joseph Y.},
  year = {2013},
  month = sep,
  abstract = {Recently, Halpern and Leung suggested representing uncertainty by a weighted set of probability measures, and suggested a way of making decisions based on this representation of uncertainty: maximizing weighted regret. Their paper does not answer an apparently simpler question: what it means, according to this representation of uncertainty, for an event E to be more likely than an event E'. In this paper, a notion of comparative likelihood when uncertainty is represented by a weighted set of probability measures is defined. It generalizes the ordering defined by probability (and by lower probability) in a natural way; a generalization of upper probability can also be defined. A complete axiomatic characterization of this notion of regret-based likelihood is given.},
  archivePrefix = {arXiv},
  eprint = {1309.1228},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/VQQ9XGJC/Halpern - 2013 - Weighted regret-based likelihood a new approach t.pdf;/home/victor/Zotero/storage/PKZQNW3C/1309.html},
  journal = {arXiv:1309.1228 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{han_simultaneous_2009,
  title = {Simultaneous {{Determination}} of {{Tuning}} and {{Calibration Parameters}} for {{Computer Experiments}}},
  author = {Han, Gang and Santner, Thomas J. and Rawlinson, Jeremy J.},
  year = {2009},
  month = nov,
  volume = {51},
  pages = {464--474},
  issn = {0040-1706},
  doi = {10.1198/TECH.2009.08126},
  abstract = {Tuning and calibration are processes for improving the representativeness of a computer simulation code to a physical phenomenon. This article introduces a statistical methodology for simultaneously determining tuning and calibration parameters in settings where data are available from a computer code and the associated physical experiment. Tuning parameters are set by minimizing a discrepancy measure while the distribution of the calibration parameters are determined based on a hierarchical Bayesian model. The proposed Bayesian model views the output as a realization of a Gaussian stochastic process with hyperpriors. Draws from the resulting posterior distribution are obtained by the Markov chain Monte Carlo simulation. Our methodology is compared with an alternative approach in examples and is illustrated in a biomechanical engineering application. Supplemental materials, including the software and a user manual, are available online and can be requested from the first author.},
  file = {/home/victor/Zotero/storage/WHDGXHCE/Han et al. - 2009 - Simultaneous Determination of Tuning and Calibrati.pdf},
  journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
  keywords = {Calibration},
  number = {4},
  pmcid = {PMC2879656},
  pmid = {20523754}
}

@techreport{hanson_markov_2001,
  title = {Markov {{Chain Monte Carlo}} Posterior Sampling with the {{Hamiltonian Method}}},
  author = {Hanson, K.},
  year = {2001},
  month = feb,
  institution = {{Los Alamos National Lab., NM (US)}},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  file = {/home/victor/Zotero/storage/H3Y9Y8VD/Hanson - 2001 - MARKOV CHAIN MONTE CARLO POSTERIOR SAMPLING WITH T.pdf;/home/victor/Zotero/storage/AMUBRP74/775292.html},
  keywords = {Hamiltonian MCMC,MCMC},
  language = {English},
  number = {LA-UR-01-1016}
}

@misc{hartnett_what_nodate,
  title = {What {{Makes}} the {{Hardest Equations}} in {{Physics So Difficult}}?},
  author = {Hartnett, ByKevin},
  abstract = {The Navier-Stokes equations describe simple, everyday phenomena, like water flowing from a garden hose, yet they provide a million-dollar mathematical challenge},
  file = {/home/victor/Zotero/storage/XJ89CPQF/what-makes-the-hardest-equations-in-physics-so-difficult-20180116.html},
  howpublished = {https://www.quantamagazine.org/what-makes-the-hardest-equations-in-physics-so-difficult-20180116/},
  journal = {Quanta Magazine}
}

@article{hascoet_tapenade_2013,
  title = {The {{Tapenade}} Automatic Differentiation Tool: {{Principles}}, Model, and Specification},
  shorttitle = {The {{Tapenade}} Automatic Differentiation Tool},
  author = {Hascoet, Laurent and Pascual, Val{\'e}rie},
  year = {2013},
  month = apr,
  volume = {39},
  pages = {1--43},
  issn = {0098-3500, 1557-7295},
  doi = {10.1145/2450153.2450158},
  file = {/home/victor/Zotero/storage/85N3W2TF/Hascoet et Pascual - 2013 - The Tapenade automatic differentiation tool Princ.pdf},
  journal = {ACM Transactions on Mathematical Software},
  language = {en},
  number = {3}
}

@inproceedings{healy_retrospective_1991,
  title = {Retrospective Simulation Response Optimization},
  booktitle = {1991 {{Winter Simulation Conference Proceedings}}.},
  author = {Healy, K. and Schruben, L.W.},
  year = {1991},
  pages = {901--906},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}},
  doi = {10.1109/WSC.1991.185703},
  abstract = {An approach to simulation response optimization is presented where a simulation experiment is run in such a manner as to generate optimal solutions. Once the stochastic sample path of a simulation run has been generated, it is sometimes possible to retrospectively solve a deterministic optimization problem or a closely related problem. As demonstrated with some examples from communications and manufacturing, this approach can greatly simplify both the simulation experiment and the simulation model.},
  file = {/home/victor/Zotero/storage/P97JS8Y7/Healy et Schruben - 1991 - Retrospective simulation response optimization.pdf},
  isbn = {978-0-7803-0181-8},
  language = {en}
}

@article{heinrich_level_2012,
  title = {Level Sets Estimation and {{Vorob}}'ev Expectation of Random Compact Sets},
  author = {Heinrich, Philippe and Stoica, Radu S. and Tran, Viet Chi},
  year = {2012},
  month = dec,
  volume = {2},
  pages = {47--61},
  issn = {22116753},
  doi = {10.1016/j.spasta.2012.10.001},
  abstract = {The issue of a ``mean shape'' of a random set X often arises, in particular in image analysis and pattern detection. There is no canonical definition but one possible approach is the so-called Vorob'ev expectation EV (X), which is closely linked to quantile sets. In this paper, we propose a consistent and ready to use estimator of EV (X) built from independent copies of X with spatial discretization. The control of discretization errors is handled with a mild regularity assumption on the boundary of X: a not too large `box counting' dimension. Some examples are developed and an application to cosmological data is presented.},
  file = {/home/victor/Zotero/storage/2E7UAPPX/Heinrich et al. - 2012 - Level sets estimation and Vorob‚Äôev expectation of .pdf},
  journal = {Spatial Statistics},
  language = {en}
}

@article{hennig_entropy_2011,
  title = {Entropy {{Search}} for {{Information}}-{{Efficient Global Optimization}}},
  author = {Hennig, Philipp and Schuler, Christian J.},
  year = {2011},
  month = dec,
  file = {/home/victor/Zotero/storage/W9KFZ9I4/Hennig et Schuler - 2011 - Entropy Search for Information-Efficient Global Op.pdf;/home/victor/Zotero/storage/XTZ68DPE/1112.html},
  language = {en}
}

@article{hennig_fast_,
  title = {Fast {{Probabilistic Optimization}} from {{Noisy Gradients}}},
  author = {Hennig, Philipp},
  pages = {9},
  file = {/home/victor/Zotero/storage/IZHCITWD/Hennig - Fast Probabilistic Optimization from Noisy Gradien.pdf},
  language = {en}
}

@article{hernandez-lobato_predictive_2014,
  title = {Predictive {{Entropy Search}} for {{Efficient Global Optimization}} of {{Black}}-Box {{Functions}}},
  author = {{Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
  year = {2014},
  month = jun,
  abstract = {We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and realworld applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.},
  archivePrefix = {arXiv},
  eprint = {1406.2541},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/FF7YVTEY/Hern√°ndez-Lobato et al. - 2014 - Predictive Entropy Search for Efficient Global Opt.pdf},
  journal = {arXiv:1406.2541 [cs, stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hewston_quantifying_2010,
  title = {Quantifying Uncertainty in Tide, Surge and Wave Modelling during Extreme Storms},
  author = {Hewston, R and Chen, Yanxian and Pan, Shunqi and Zou, Qingping and Reeve, Dominic and Cluckie, Ian},
  year = {2010},
  month = sep,
  doi = {10.7558/bhs.2010.ic74},
  abstract = {Interactions between meteorological and hydrodynamic processes are poorly understood, and may result in large uncertainties when assessing the performance of sea defences in extreme conditions. This study integrates numerical weather prediction models with models of wave generation and propagation, and surge and tide propagation. By using an ensemble methodology, the uncertainty at each stage of the model cascade may be quantified. Subsequently, this information, either as a proxy or appropriately transformed into predictive uncertainty, will be valuable in calculating the likelihood of hydraulic and structural failure in extreme storms. This paper describes results for a domain centred on one of the locations for the proposed Severn Barrage. This barrage will be the focus for the world's largest marine renewable energy scheme and will potentially have a significant impact on the coastal flooding response of this part of the Severn Estuary. Dynamically downscaled, high resolution wind and pressure fields of historic extreme storms are generated using the Weather Research and Forecasting (WRF) modelling system. The state of the art tide and surge model, POLCOMS, in conjunction with a third generation wave model (ProWAM), utilises the meteorological data, producing hydrodynamic parameters such as surge and wave heights at a proposed location for the Severn Barrage. European Centre for Medium range Forecasting (ECMWF) Ensemble Prediction System data are used for boundary conditions in WRF, producing a 50-member ensemble. The variation in storm track and intensity between members allows the uncertainty in the model system to be quantified in terms of wave and surge heights. This work is part of the NERC funded EPIRUS consortium research but is closely allied to the interests of the EPSRC FRMRC project and the HEPEX international network focused on ensemble prediction in the context of hydrological prediction systems.},
  file = {/home/victor/Zotero/storage/TCS9SVPG/Hewston et al. - 2010 - Quantifying uncertainty in tide, surge and wave mo.pdf}
}

@article{higdon_combining_2004,
  title = {Combining Field Data and Computer Simulations for Calibration and Prediction},
  author = {Higdon, Dave and Kennedy, Marc and Cavendish, James C. and Cafeo, John A. and Ryne, Robert D.},
  year = {2004},
  volume = {26},
  pages = {448--466},
  file = {/home/victor/Zotero/storage/FTF7QNLB/Higdon et al. - 2004 - Combining field data and computer simulations for .pdf;/home/victor/Zotero/storage/SF5KN75N/S1064827503426693.html},
  journal = {SIAM Journal on Scientific Computing},
  number = {2}
}

@article{higdon_computer_2008,
  title = {Computer Model Calibration Using High-Dimensional Output},
  author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
  year = {2008},
  volume = {103},
  pages = {570--583},
  file = {/home/victor/Zotero/storage/J724KP8I/Higdon et al. - 2008 - Computer model calibration using high-dimensional .pdf;/home/victor/Zotero/storage/IWJUKBKK/016214507000000888.html},
  journal = {Journal of the American Statistical Association},
  number = {482}
}

@article{higdon_posterior_2011,
  title = {Posterior Exploration for Computationally Intensive Forward Models},
  author = {Higdon, David and Reese, C. Shane and Moulton, J. David and Vrugt, Jasper A. and Fox, Colin},
  year = {2011},
  pages = {401--418},
  file = {/home/victor/Zotero/storage/SBUQHLQH/Higdon et al. - 2011 - Posterior exploration for computationally intensiv.pdf;/home/victor/Zotero/storage/9GARKPTV/books.html},
  journal = {Handbook of Markov Chain Monte Carlo},
  keywords = {MCMC}
}

@incollection{hiot_kriging_2010,
  title = {Kriging {{Is Well}}-{{Suited}} to {{Parallelize Optimization}}},
  booktitle = {Computational {{Intelligence}} in {{Expensive Optimization Problems}}},
  author = {Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  editor = {Hiot, Lim Meng and Ong, Yew Soon and Tenne, Yoel and Goh, Chi-Keong},
  year = {2010},
  volume = {2},
  pages = {131--162},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-10701-6_6},
  abstract = {The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement (q-EI), aimed at choosing several points at the same time. An analytical expression of the q-EI is given when q = 2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the q-EI, and apply them to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q {$\in$} [1, 10]).},
  file = {/home/victor/Zotero/storage/ENBVB5QF/Ginsbourger et al. - 2010 - Kriging Is Well-Suited to Parallelize Optimization.pdf},
  isbn = {978-3-642-10700-9 978-3-642-10701-6},
  language = {en}
}

@article{hoffmann_unified_2015,
  title = {Unified Treatment of the Asymptotics of Asymmetric Kernel Density Estimators},
  author = {Hoffmann, Till and Jones, Nick S.},
  year = {2015},
  month = dec,
  abstract = {We extend balloon and sample-smoothing estimators, two types of variable-bandwidth kernel density estimators, by a shift parameter and derive their asymptotic properties. Our approach facilitates the unified study of a wide range of density estimators which are subsumed under these two general classes of kernel density estimators. We demonstrate our method by deriving the asymptotic bias, variance, and mean (integrated) squared error of density estimators with gamma, log-normal, Birnbaum-Saunders, inverse Gaussian and reciprocal inverse Gaussian kernels. We propose two new density estimators for positive random variables that yield properly-normalised density estimates. Plugin expressions for bandwidth estimation are provided to facilitate easy exploratory data analysis.},
  archivePrefix = {arXiv},
  eprint = {1512.03188},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/LJUJKCBD/Hoffmann et Jones - 2015 - Unified treatment of the asymptotics of asymmetric.pdf;/home/victor/Zotero/storage/84RUCHCQ/1512.html},
  journal = {arXiv:1512.03188 [math, stat]},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  primaryClass = {math, stat}
}

@article{hogan_point--set_1973,
  title = {Point-to-{{Set Maps}} in {{Mathematical Programming}}},
  author = {Hogan, William W.},
  year = {1973},
  volume = {15},
  pages = {591--603},
  issn = {0036-1445},
  abstract = {Properties of point-to-set maps are studied from an elementary viewpoint oriented toward applications in mathematical programming. A number of different definitions and results are compared and integrated. Conditions establishing continuity of extremal value functions and properties of maps determined by inequalities are included.},
  journal = {SIAM Review},
  number = {3}
}

@article{holmes_assigning_2017,
  title = {Assigning a Value to a Power Likelihood in a General {{Bayesian}} Model},
  author = {Holmes, Chris and Walker, Stephen},
  year = {2017},
  month = jan,
  abstract = {Bayesian approaches to data analysis and machine learning are widespread and popular as they provide intuitive yet rigorous axioms for learning from data; see Bernardo \& Smith (2004) and Bishop (2006). However, this rigour comes with a caveat, that the Bayesian model is a precise reflection of Nature. There has been a recent trend to address potential model misspecification by raising the likelihood function to a power, primarily for robustness reasons, though not exclusively. In this paper we provide a coherent specification of the power parameter once the Bayesian model has been specified in the absence of a perfect model.},
  archivePrefix = {arXiv},
  eprint = {1701.08515},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/23ETEW6R/Holmes et Walker - 2017 - Assigning a value to a power likelihood in a gener.pdf},
  journal = {arXiv:1701.08515 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{holt_prospects_2017,
  title = {Prospects for Improving the Representation of Coastal and Shelf Seas in Global Ocean Models},
  author = {Holt, Jason and Hyder, Patrick and Ashworth, Mike and Harle, James and Hewitt, Helene and Liu, Hedong and New, Adrian and Pickles, Stephen and Porter, Andrew and Popova, Ekaterina and Allen, Icarus and Siddorn, John and Wood, Richard},
  year = {2017},
  month = feb,
  volume = {10},
  pages = {499--523},
  doi = {10.5194/gmd-10-499-2017},
  abstract = {Accurately representing coastal and shelf seas in global ocean models represents one of the grand challenges of Earth system science. They are regions of immense societal importance through the goods and services they provide, hazards they pose and their role in global-scale processes and cycles, e.g. carbon fluxes and dense water formation. However, they are poorly represented in the current generation of global ocean models. In this contribution, we aim to briefly characterise the problem, and then to identify the important physical processes, and their scales, needed to address this issue in the context of the options available to resolve these scales globally and the evolving computational landscape.

We find barotropic and topographic scales are well resolved by the current state-of-the-art model resolutions, e.g. nominal 1/12\textdegree, and still reasonably well resolved at 1/4\textdegree; here, the focus is on process representation. We identify tides, vertical coordinates, river inflows and mixing schemes as four areas where modelling approaches can readily be transferred from regional to global modelling with substantial benefit. In terms of finer-scale processes, we find that a 1/12\textdegree{} global model resolves the first baroclinic Rossby radius for only {$\sim$} 8 \% of regions {$<$} 500 m deep, but this increases to {$\sim$} 70 \% for a 1/72\textdegree{} model, so resolving scales globally requires substantially finer resolution than the current state of the art.

We quantify the benefit of improved resolution and process representation using 1/12\textdegree{} global- and basin-scale northern North Atlantic nucleus for a European model of the ocean (NEMO) simulations; the latter includes tides and a k-{$\epsilon$} vertical mixing scheme. These are compared with global stratification observations and 19 models from CMIP5. In terms of correlation and basin-wide rms error, the high-resolution models outperform all these CMIP5 models. The model with tides shows improved seasonal cycles compared to the high-resolution model without tides. The benefits of resolution are particularly apparent in eastern boundary upwelling zones.

To explore the balance between the size of a globally refined model and that of multiscale modelling options (e.g. finite element, finite volume or a two-way nesting approach), we consider a simple scale analysis and a conceptual grid refining approach. We put this analysis in the context of evolving computer systems, discussing model turnaround time, scalability and resource costs. Using a simple cost model compared to a reference configuration (taken to be a 1/4\textdegree{} global model in 2011) and the increasing performance of the UK Research Councils' computer facility, we estimate an unstructured mesh multiscale approach, resolving process scales down to 1.5 km, would use a comparable share of the computer resource by 2021, the two-way nested multiscale approach by 2022, and a 1/72\textdegree{} global model by 2026. However, we also note that a 1/12\textdegree{} global model would not have a comparable computational cost to a 1\textdegree{} global model in 2017 until 2027. Hence, we conclude that for computationally expensive models (e.g. for oceanographic research or operational oceanography), resolving scales to {$\sim$} 1.5 km would be routinely practical in about a decade given substantial effort on numerical and computational development. For complex Earth system models, this extends to about 2 decades, suggesting the focus here needs to be on improved process parameterisation to meet these challenges.},
  file = {/home/victor/Zotero/storage/MPLA8FYC/Holt et al. - 2017 - Prospects for improving the representation of coas.pdf},
  journal = {Geoscientific Model Development}
}

@article{homan_output-space_nodate,
  title = {Output-{{Space Predictive Entropy Search}} for {{Flexible Global Optimization}}},
  author = {Hoffman, Matthew W and Ghahramani, Zoubin},
  pages = {5},
  file = {/home/victor/Zotero/storage/IGLU6SSE/HoÔ¨Äman et Ghahramani - Output-Space Predictive Entropy Search for Flexibl.pdf},
  language = {en}
}

@article{honnorat_identification_2010,
  title = {Identification of Equivalent Topography in an Open Channel Flow Using {{Lagrangian}} Data Assimilation},
  author = {Honnorat, Marc and Monnier, J{\'e}r{\^o}me and Rivi{\`e}re, Nicolas and Huot, {\'E}tienne and Le Dimet, Fran{\c c}ois-Xavier},
  year = {2010},
  month = mar,
  volume = {13},
  pages = {111--119},
  issn = {1432-9360, 1433-0369},
  doi = {10.1007/s00791-009-0130-8},
  abstract = {We present a Lagrangian data assimilation experiment in an open channel flow above a broad-crested weir. The observations consist of trajectories of particles transported by the flow and extracted from a video film, in addition to classical water level measurements. However, the presence of vertical recirculations on both sides of the weir actually conducts to the identification of an equivalent topography corresponding to the lower limit of a surface jet. In addition, results on the identification of the Manning coefficient may allow to detect the presence of bottom reciruclations.},
  file = {/home/victor/Zotero/storage/QBA5CTZ4/Honnorat et al. - 2010 - Identification of equivalent topography in an open.pdf},
  journal = {Computing and Visualization in Science},
  language = {en},
  number = {3}
}

@article{hourdin_art_2017,
  title = {The Art and Science of Climate Model Tuning},
  author = {Hourdin, Frederic and Mauritsen, Thorsten and Gettelman, Andrew and Golaz, Jean-Christophe and Balaji, Venkatramani and Duan, Qingyun and Folini, Doris and Ji, Duoying and Klocke, Daniel and Qian, Yun},
  year = {2017},
  volume = {98},
  pages = {589--602},
  file = {/home/victor/Zotero/storage/WM7J3PJU/Tuning2016.pdf},
  journal = {Bulletin of the American Meteorological Society},
  number = {3}
}

@incollection{huber_robust_2011,
  title = {Robust Statistics},
  booktitle = {International {{Encyclopedia}} of {{Statistical Science}}},
  author = {Huber, Peter J.},
  year = {2011},
  pages = {1248--1251},
  publisher = {{Springer}},
  file = {/home/victor/Zotero/storage/48JTUEST/Huber - 2011 - Robust statistics.pdf;/home/victor/Zotero/storage/SA65GIYR/10.html}
}

@article{hug_high-dimensional_2013,
  title = {High-Dimensional {{Bayesian}} Parameter Estimation: {{Case}} Study for a Model of {{JAK2}}/{{STAT5}} Signaling},
  shorttitle = {High-Dimensional {{Bayesian}} Parameter Estimation},
  author = {Hug, S. and Raue, A. and Hasenauer, J. and Bachmann, J. and Klingm{\"u}ller, U. and Timmer, J. and Theis, F.J.},
  year = {2013},
  month = dec,
  volume = {246},
  pages = {293--304},
  issn = {00255564},
  doi = {10.1016/j.mbs.2013.04.002},
  abstract = {In this work we present results of a detailed Bayesian parameter estimation for an analysis of ordinary differential equation models. These depend on many unknown parameters that have to be inferred from experimental data. The statistical inference in a high-dimensional parameter space is however conceptually and computationally challenging. To ensure rigorous assessment of model and prediction uncertainties we take advantage of both a profile posterior approach and Markov chain Monte Carlo sampling.},
  file = {/home/victor/Zotero/storage/IKC38469/Hug et al. - 2013 - High-dimensional Bayesian parameter estimation Ca.pdf},
  journal = {Mathematical Biosciences},
  language = {en},
  number = {2}
}

@article{hutter_bayesian_2006,
  title = {Bayesian {{Regression}} of {{Piecewise Constant Functions}}},
  author = {Hutter, Marcus},
  year = {2006},
  month = jun,
  abstract = {We derive an exact and efficient Bayesian regression algorithm for piecewise constant functions of unknown segment number, boundary location, and levels. It works for any noise and segment level prior, e.g. Cauchy which can handle outliers. We derive simple but good estimates for the in-segment variance. We also propose a Bayesian regression curve as a better way of smoothing data without blurring boundaries. The Bayesian approach also allows straightforward determination of the evidence, break probabilities and error estimates, useful for model selection and significance and robustness studies. We discuss the performance on synthetic and real-world examples. Many possible extensions will be discussed.},
  archivePrefix = {arXiv},
  eprint = {math/0606315},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/E9LCVMVB/Hutter - 2006 - Bayesian Regression of Piecewise Constant Function.pdf;/home/victor/Zotero/storage/G94863ZU/0606315.html},
  journal = {arXiv:math/0606315},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory}
}

@article{huyse_free-form_2001,
  title = {Free-Form Airfoil Shape Optimization under Uncertainty Using Maximum Expected Value and Second-Order Second-Moment Strategies},
  author = {Huyse, Luc and Bushnell, Dennis M.},
  year = {2001},
  file = {/home/victor/Zotero/storage/N7A2YLMJ/Huyse et Bushnell - 2001 - Free-form airfoil shape optimization under uncerta.pdf;/home/victor/Zotero/storage/7GVM9C4N/search.html}
}

@article{huyse_probabilistic_2002,
  title = {Probabilistic {{Approach}} to {{Free}}-{{Form Airfoil Shape Optimization Under Uncertainty}}},
  author = {Huyse, Luc and Padula, Sharon L. and Lewis, R. Michael and Li, Wu},
  year = {2002},
  volume = {40},
  pages = {1764--1772},
  issn = {0001-1452},
  doi = {10.2514/2.1881},
  file = {/home/victor/Zotero/storage/TSHAX373/2.html},
  journal = {AIAA Journal},
  number = {9}
}

@article{ibrahim_power_2015,
  title = {The {{Power Prior}}: {{Theory}} and {{Applications}}},
  shorttitle = {The {{Power Prior}}},
  author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
  year = {2015},
  month = dec,
  volume = {34},
  pages = {3724--3749},
  issn = {0277-6715},
  doi = {10.1002/sim.6728},
  abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A to Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Prequentist properties of power priors in posterior inference are established and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials.},
  file = {/home/victor/Zotero/storage/7ST5NWJ8/Ibrahim et al. - 2015 - The Power Prior Theory and Applications.pdf},
  journal = {Statistics in medicine},
  number = {28},
  pmcid = {PMC4626399},
  pmid = {26346180}
}

@article{ibrahimi_robust_2011,
  title = {Robust {{Max}}-{{Product Belief Propagation}}},
  author = {Ibrahimi, Morteza and Javanmard, Adel and Kanoria, Yashodhan and Montanari, Andrea},
  year = {2011},
  month = nov,
  abstract = {We study the problem of optimizing a graph-structured objective function under \textbackslash emph\{adversarial\} uncertainty. This problem can be modeled as a two-persons zero-sum game between an Engineer and Nature. The Engineer controls a subset of the variables (nodes in the graph), and tries to assign their values to maximize an objective function. Nature controls the complementary subset of variables and tries to minimize the same objective. This setting encompasses estimation and optimization problems under model uncertainty, and strategic problems with a graph structure. Von Neumann's minimax theorem guarantees the existence of a (minimax) pair of randomized strategies that provide optimal robustness for each player against its adversary. We prove several structural properties of this strategy pair in the case of graph-structured payoff function. In particular, the randomized minimax strategies (distributions over variable assignments) can be chosen in such a way to satisfy the Markov property with respect to the graph. This significantly reduces the problem dimensionality. Finally we introduce a message passing algorithm to solve this minimax problem. The algorithm generalizes max-product belief propagation to this new domain.},
  archivePrefix = {arXiv},
  eprint = {1111.6214},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/JXFMY8DC/Ibrahimi et al. - 2011 - Robust Max-Product Belief Propagation.pdf;/home/victor/Zotero/storage/E6QC2FXD/1111.html},
  journal = {arXiv:1111.6214 [cs, math]},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@article{ide_unified_1997,
  title = {Unified {{Notation}} for {{Data Assimilation}} : {{Operational}}, {{Sequential}} and {{Variational}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}: {{Theory}} and {{Practice}})},
  shorttitle = {Unified {{Notation}} for {{Data Assimilation}}},
  author = {Ide, Kayo and Courtier, Philippe and Ghil, Michael and Lorenc, Andrew C.},
  year = {1997},
  month = mar,
  volume = {75},
  pages = {181--189},
  issn = {0026-1165, 2186-9057},
  doi = {10.2151/jmsj1965.75.1B_181},
  abstract = {Japan's largest platform for academic e-journals: J-STAGE is a full text database for reviewed academic papers published by Japanese societies},
  file = {/home/victor/Zotero/storage/STH2ECYW/Ide et al. - 1997 - Unified Notation for Data Assimilation  Operation.pdf;/home/victor/Zotero/storage/GD2IBDM9/_article.html},
  journal = {Journal of the Meteorological Society of Japan. Ser. II},
  language = {en},
  number = {1B}
}

@article{iooss_evaluation_nodate,
  title = {{Evaluation de quantiles de codes de calcul}},
  author = {Iooss, B and Devictor, N},
  pages = {19},
  file = {/home/victor/Zotero/storage/Y8GS47EU/Iooss et Devictor - Evaluation de quantiles de codes de calcul.pdf},
  language = {fr}
}

@book{j._bichonyyk_reliability-based_2009,
  title = {Reliability-{{Based Design Optimization Using Efficient Global Reliability Analysis}}},
  author = {J. Bichonyyk, Barron and Mahadevanky, Sankaran and Eldred, Michael},
  year = {2009},
  month = may,
  doi = {10.2514/6.2009-2261},
  abstract = {Finding the optimal (lightest, least expensive, etc.) design for an engineered component that meets or exceeds a specified level of reliability is a problem of obvious interest across a wide spectrum of engineering fields. Various methods for this reliability-based design optimization problem have been proposed. Unfortunately, this problem is rarely solved in practice because, regardless of the method used, solving the problem is too expensive or the final solution is too inaccurate to ensure that the reliability constraint is actually satisfied. This is especially true for engineering applications involving expensive, implicit, and possibly nonlinear performance functions (such as large finite element models). The Efficient Global Reliability Analysis method was recently introduced to improve both the accuracy and efficiency of reliability analysis for this type of performance function. This paper explores how this new reliability analysis method can be used in a design optimization context to create a method of sufficient accuracy and efficiency to enable the use of reliability-based design optimization as a practical design tool.}
}

@article{jacquier_maximum_,
  title = {Maximum {{Expected Utility}} via {{MCMC}}},
  author = {Jacquier, Eric and Johannes, Michael and Polson, Nicholas},
  pages = {26},
  abstract = {In this paper we provide a new simulation-based approach to maximum expected utility (MEU) portfolio allocation problems. MEU requires computation of expected utility and its optimization over the decision variable. In portfolio problems, the expected utility is generally not analytically available. Traditional methods resort to gradient-based estimates of expected utility with its ensuing derivatives. This leads to computational inefficiencies which are particularly acute in portfolio problems with parameter uncertainty (a.k.a. estimation risk). Our simulation-based method avoids the calculation of derivative and also allows for functional optimization. The algorithm combines Markov Chain Monte Carlo (MCMC) with the insights of simulated annealing and evolutionary Monte Carlo. It can exploit conjugate utility functions and latent variables in the relevant predictive density for efficient simulation. We also show how slice sampling naturally allows for constraints in the portfolio weights. We illustrate our methodology with a portfolio problem with estimation risk and CARA utility.},
  file = {/home/victor/Zotero/storage/2DXK3GLL/Jacquier et al. - Maximum Expected Utility via MCMC.pdf},
  language = {en}
}

@techreport{janusevskis_simultaneous_2010,
  title = {Simultaneous Kriging-Based Sampling for Optimization and Uncertainty Propagation},
  author = {Janusevskis, Janis and Le Riche, Rodolphe},
  year = {2010},
  month = jul,
  abstract = {Robust analysis and optimization is typically based on repeated calls to a deterministic simulator that aim at propagating uncertainties and finding optimal design variables. Without loss of generality a double set of simulation parameters can be assumed: x are deterministic optimization variables, u are random parameters of known probability density function and f (x, u) is the objective function attached to the simulator. Most robust optimization methods involve two imbricated tasks, the u's uncertainty propagation (e.g., Monte Carlo simulations, reliability index calculation) which is recurcively performed inside optimization iterations on the x's. In practice, f is often calculated through a computationally expensive software. This makes the computational cost one of the principal obstacle to optimization in the presence of uncertainties. This report proposes a new efficient method for minimizing the mean objective function, min E[f(x, U)]. The efficiency stems from the simultaneous sampling of f for uncertainty propagation and optimization, i.e., the hierarchical imbrication is avoided. Y(x,u) ({$\omega$}), a kriging (Gaussian process conditioned on t past calculations of f) model of f (x, u) is built and the mean process, Z(x) ({$\omega$}) = E[Y(x,U)({$\omega$})], is analytically derived from it. The sampling criterion that yields both x and u is the one-step ahead minimum variance of the mean process Z at the maximizer of the expected improvement. The method is compared with Monte Carlo and kriging-based approaches on analytical test functions in two, four and six dimensions.},
  file = {/home/victor/Zotero/storage/B2SDTQSK/Janusevskis et Le Riche - 2010 - Simultaneous kriging-based sampling for optimizati.pdf},
  keywords = {Efficient Global Optimization,EGO,Expected improvement,Gaussian process,Kriging,Optimization under uncertainty,Robust optimization,Uncertainty Propagation}
}

@article{jewson_principles_2018,
  title = {Principles of {{Bayesian}} Inference Using General Divergence Criteria},
  author = {Jewson, Jack and Smith, Jim Q. and Holmes, Chris},
  year = {2018},
  volume = {20},
  pages = {442},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  file = {/home/victor/Zotero/storage/98E54BDF/Jewson et al. - 2018 - Principles of Bayesian inference using general div.pdf;/home/victor/Zotero/storage/ME3I52WJ/442.html},
  journal = {Entropy},
  number = {6}
}

@article{jiang_gibbs_2008,
  title = {Gibbs Posterior for Variable Selection in High-Dimensional Classification and Data Mining},
  author = {Jiang, Wenxin and Tanner, Martin A.},
  year = {2008},
  month = oct,
  volume = {36},
  pages = {2207--2231},
  issn = {0090-5364},
  doi = {10.1214/07-AOS547},
  abstract = {In the popular approach of "Bayesian variable selection" (BVS), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. A completely new direction will be considered here to study BVS with a Gibbs posterior originating in statistical mechanics. The Gibbs posterior is constructed from a risk function of practical interest (such as the classification error) and aims at minimizing a risk function without modeling the data probabilistically. This can improve the performance over the usual Bayesian approach, which depends on a probability model which may be misspecified. Conditions will be provided to achieve good risk performance, even in the presence of high dimensionality, when the number of candidate variables "\$K\$" can be much larger than the sample size "\$n\$." In addition, we develop a convenient Markov chain Monte Carlo algorithm to implement BVS with the Gibbs posterior.},
  archivePrefix = {arXiv},
  eprint = {0810.5655},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/6XQSG29X/Jiang et Tanner - 2008 - Gibbs posterior for variable selection in high-dim.pdf},
  journal = {The Annals of Statistics},
  keywords = {62F99 (Primary),82-08 (Secondary),Statistics - Machine Learning,Statistics - Methodology},
  language = {en},
  number = {5}
}

@article{jones_efficient_1998,
  title = {Efficient Global Optimization of Expensive Black-Box Functions},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  year = {1998},
  volume = {13},
  pages = {455--492},
  file = {/home/victor/Zotero/storage/8QF4EUL8/Jones et al. - 1998 - Efficient global optimization of expensive black-b.pdf;/home/victor/Zotero/storage/DSISWVYM/A1008306431147.html},
  journal = {Journal of Global optimization},
  number = {4}
}

@article{jones_kernel-type_,
  title = {Kernel-Type Density Estimation on the Unit Interval},
  author = {JONES, M C and HENDERSON, D A},
  pages = {35},
  abstract = {We consider kernel-type methods for estimation of a density on [0, 1] which eschew explicit boundary correction. Our starting point is the successful implementation of beta kernel density estimators of Chen (1999). We propose and investigate two alternatives. For the first, we reverse the roles of estimation point x and datapoint Xi in each summand of the estimator. For the second, we provide kernels that are symmetric in x and X; these kernels are conditional densities of bivariate copulas. We develop asymptotic theory for the new estimators and compare them with Chen's in a substantial simulation study. We also develop automatic bandwidth selection in the form of `rule-of-thumb' bandwidths for all three estimators. We find that our second proposal, that based on `copula kernels', seems particularly competitive with the beta kernel method of Chen in integrated squared error performance terms. Advantages include its greater range of possible values at 0 and 1, the fact that it is a bona fide density, and that the individual kernels and resulting estimator are comprehensible in terms of a direct single picture (as is ordinary kernel density estimation on the line).},
  file = {/home/victor/Zotero/storage/YSHMWFLM/JONES et HENDERSON - Kernel-type density estimation on the unit interva.pdf},
  keywords = {Beta kernels,Density estimation}
}

@article{jones_taxonomy_,
  title = {A {{Taxonomy}} of {{Global Optimization Methods Based}} on {{Response Surfaces}}},
  author = {JONES, DONALD R},
  pages = {39},
  abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
  file = {/home/victor/Zotero/storage/KNZITF9Q/JONES - A Taxonomy of Global Optimization Methods Based on.pdf},
  language = {en}
}

@inproceedings{juditsky_stochastic_2009,
  title = {Stochastic {{Approximation Approach}} to {{Stochastic Programming}}},
  booktitle = {{{ISMP}} 2009 - 20th {{International Symposium}} of {{Mathematical Programming}}},
  author = {Juditsky, Anatoli and Nemirovski, Arkadii S. and Lan, Guanghui and Shapiro, Alexander},
  year = {2009},
  month = aug,
  abstract = {A basic difficulty with solving stochastic programming problems is that it requires computation of expectations given by multidimensional integrals. One approach, based on Monte Carlo sampling techniques, is to generate a reasonably large random sample and consequently to solve the constructed so-called Sample Average Approximation (SAA) problem. The other classical approach is based on Stochastic Approximation (SA) techniques. In this talk we discuss some recent advances in development of SA type numerical algorithms for solving convex stochastic programming problems. Numerical experiments show that for some classes of problems the so-called Mirror Descent SA Method can significantly outperform the SAA approach.},
  file = {/home/victor/Zotero/storage/D3M8UC7U/Juditsky et al. - 2009 - Stochastic Approximation Approach to Stochastic Pr.pdf;/home/victor/Zotero/storage/MAKLLQFI/hal-00981931.html},
  language = {en}
}

@article{kalai_lexicographic_2010,
  title = {Lexicographic {\emph{{$\alpha$}}} -Robustness: An Application to the 1-Median Problem},
  shorttitle = {Lexicographic {\emph{{$\alpha$}}} -Robustness},
  author = {Kala{\"i}, R. and Aloulou, M. A. and Vallin, Ph. and Vanderpooten, D.},
  year = {2010},
  month = apr,
  volume = {44},
  pages = {119--138},
  issn = {0399-0559, 1290-3868},
  doi = {10.1051/ro/2010010},
  abstract = {In the last decade, several robustness approaches have been developed to deal with uncertainty. In decision problems, and particularly in location problems, the most used robustness approach rely either on maximal cost or on maximal regret criteria. However, it is well known that these criteria are too conservative. In this paper, we present a new robustness approach, called lexicographic {$\alpha$}-robustness, which compensates for the drawbacks of criteria based on the worst case. We apply this approach to the 1-median location problem under uncertainty on node weights and we give a specific algorithm to determine robust solutions in the case of a tree. We also show that this algorithm can be extended to the case of a general network.},
  file = {/home/victor/Zotero/storage/CV3RILW5/Kala√Ø et al. - 2010 - Lexicographic iŒ±i -robustness an application.pdf},
  journal = {RAIRO - Operations Research},
  language = {en},
  number = {2}
}

@book{kalbfleisch_probability_1985,
  title = {Probability and {{Statistical Inference}}},
  author = {Kalbfleisch, J. G.},
  year = {1985},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-1096-2},
  file = {/home/victor/Zotero/storage/PMS7TENJ/Kalbfleisch - 1985 - Probability and Statistical Inference.pdf},
  isbn = {978-1-4612-7009-6 978-1-4612-1096-2},
  language = {en},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@techreport{kass_bayes_1993,
  title = {Bayes Factors and Model Uncertainty},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1993},
  institution = {{DEPARTMENT OF STATISTICS, UNIVERSITY OFWASHINGTON}},
  abstract = {In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications. The points we emphasize are:- from Jeffreys's Bayesian point of view, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory;- Bayes factors offer a way of evaluating evidence in favor ofa null hypothesis;- Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis;- Bayes factors are very general, and do not require alternative models to be nested;- several techniques are available for computing Bayes factors, including asymptotic approximations which are easy to compute using the output from standard packages that maximize likelihoods;- in "non-standard " statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance},
  file = {/home/victor/Zotero/storage/SVAWKKFQ/Kass et Raftery - 1993 - Bayes factors and model uncertainty.pdf;/home/victor/Zotero/storage/M5LI2ZNZ/summary.html}
}

@article{kass_bayes_1995,
  title = {Bayes Factors},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1995},
  volume = {90},
  pages = {773--795},
  publisher = {{Taylor \& Francis}},
  file = {/home/victor/Zotero/storage/QEAUAFJ7/KassRaftery1995.pdf;/home/victor/Zotero/storage/LFW2HV6E/01621459.1995.html},
  journal = {Journal of the american statistical association},
  number = {430}
}

@article{kennedy_bayesian_2001,
  title = {Bayesian Calibration of Computer Models},
  author = {Kennedy, Marc C. and O'Hagan, Anthony},
  year = {2001},
  month = jan,
  volume = {63},
  pages = {425--464},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00294},
  abstract = {We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.},
  file = {/home/victor/Zotero/storage/ZYFTNCPQ/Kennedy et O'Hagan - 2001 - Bayesian calibration of computer models.pdf;/home/victor/Zotero/storage/NXU9WKLY/abstract.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
  language = {en},
  number = {3}
}

@article{kent_robust_1982,
  title = {Robust Properties of Likelihood Ratio Tests},
  author = {Kent, John T.},
  year = {1982},
  volume = {69},
  pages = {19--27},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/69.1.19},
  abstract = {The usual asymptotic chi-squared distribution for the likelihood ratio test statistic is based on the assumptions that the data come from the parametric model under consideration and that the parameter satisfies the null hypothesis. In this paper we examine the distribution of the likelihood ratio statistic when the data do not come from the parametric model, but when the 'nearest' member of the parametric family still satisfies the null hypothesis. In general, the likelihood ratio statistic no longer follows an asymptotic chi-squared distribution, and an alternative statistic based on the unionintersection approach is proposed.},
  file = {/home/victor/Zotero/storage/Y3RIYDF5/Kent - 1982 - Robust properties of likelihood ratio tests.pdf},
  journal = {Biometrika},
  language = {en},
  number = {1}
}

@incollection{kim_guide_2015,
  title = {A {{Guide}} to {{Sample Average Approximation}}},
  author = {Kim, Sujin and Pasupathy, Raghu and Henderson, Shane},
  year = {2015},
  month = jan,
  volume = {216},
  pages = {207--243},
  doi = {10.1007/978-1-4939-1384-8-8},
  abstract = {This chapter reviews the principles of sample average approximation (SAA) for solving simulation optimization problems. We provide an accessible overview of the area and survey interesting recent developments. We explain when one might want to use SAA and when one might expect it to provide good-quality solutions. We also review some of the key theoretical properties of the solutions obtained through SAA. We contrast SAA with stochastic approximation (SA) methods in terms of the computational effort required to obtain solutions of a given quality, explaining why SA ``wins'' asymptotically. However, an extension of SAA known as retrospective optimization can match the asymptotic convergence rate of SA, at least up to a multiplicative constant.},
  file = {/home/victor/Zotero/storage/CIX5IVAF/Kim et al. - 2015 - A Guide to Sample Average Approximation.pdf}
}

@article{kim_tractable_2015,
  title = {Tractable {{Fully Bayesian Inference}} via {{Convex Optimization}} and {{Optimal Transport Theory}}},
  author = {Kim, Sanggyun and Mesa, Diego and Ma, Rui and Coleman, Todd P.},
  year = {2015},
  month = sep,
  abstract = {We consider the problem of transforming samples from one continuous source distribution into samples from another target distribution. We demonstrate with optimal transport theory that when the source distribution can be easily sampled from and the target distribution is log-concave, this can be tractably solved with convex optimization. We show that a special case of this, when the source is the prior and the target is the posterior, is Bayesian inference. Here, we can tractably calculate the normalization constant and draw posterior i.i.d. samples. Remarkably, our Bayesian tractability criterion is simply log concavity of the prior and likelihood: the same criterion for tractable calculation of the maximum a posteriori point estimate. With simulated data, we demonstrate how we can attain the Bayes risk in simulations. With physiologic data, we demonstrate improvements over point estimation in intensive care unit outcome prediction and electroencephalography-based sleep staging.},
  archivePrefix = {arXiv},
  eprint = {1509.08582},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/UV4VLRVW/Kim et al. - 2015 - Tractable Fully Bayesian Inference via Convex Opti.pdf;/home/victor/Zotero/storage/CWFHYF8C/1509.html},
  journal = {arXiv:1509.08582 [stat]},
  keywords = {Bayesian inference,Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{kingma_adam_2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/3CTW2N4D/Kingma et Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/home/victor/Zotero/storage/LBC2YKV7/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  primaryClass = {cs}
}

@article{kisiala_conditional_nodate,
  title = {Conditional {{Value}}-at-{{Risk}}: {{Theory}} and {{Applications}}},
  author = {Kisiala, Jakob},
  pages = {96},
  file = {/home/victor/Zotero/storage/U6CSBR4I/Kisiala - Conditional Value-at-Risk Theory and Applications.pdf},
  language = {en}
}

@article{kitanidis_parameter_1986,
  title = {Parameter Uncertainty in Estimation of Spatial Functions: {{Bayesian}} Analysis},
  shorttitle = {Parameter Uncertainty in Estimation of Spatial Functions},
  author = {Kitanidis, Peter K.},
  year = {1986},
  volume = {22},
  pages = {499--507},
  file = {/home/victor/Zotero/storage/5NLQMAEG/Kitanidis - 1986 - Parameter uncertainty in estimation of spatial fun;/home/victor/Zotero/storage/U8Q6L32Z/Kitanidis - 1986 - Parameter uncertainty in estimation of spatial fun.pdf},
  journal = {Water resources research},
  number = {4}
}

@article{kiureghian_aleatory_2009,
  title = {Aleatory or Epistemic? {{Does}} It Matter?},
  shorttitle = {Aleatory or Epistemic?},
  author = {Kiureghian, Armen Der and Ditlevsen, Ove},
  year = {2009},
  month = mar,
  volume = {31},
  pages = {105--112},
  issn = {01674730},
  doi = {10.1016/j.strusafe.2008.06.020},
  abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
  file = {/home/victor/Zotero/storage/L53MFISU/Kiureghian et Ditlevsen - 2009 - Aleatory or epistemic Does it matter.pdf},
  journal = {Structural Safety},
  language = {en},
  number = {2}
}

@misc{koenker_maximum_2012,
  title = {``{{Maximum Likelihood Asymptotics}} under {{Non}}-Standard {{Conditions}}: {{A Heuristic Introduction}} to {{Sandwiches}}''},
  author = {Koenker, Roger},
  year = {2012},
  publisher = {{University of Illinois}},
  abstract = {Econ 574 Lecture notes},
  file = {/home/victor/Zotero/storage/KGIXYHHN/L10.pdf}
}

@article{koutsourelakis_multi-resolution_2009,
  title = {A Multi-Resolution, Non-Parametric, {{Bayesian}} Framework for Identification of Spatially-Varying Model Parameters},
  author = {Koutsourelakis, P. S.},
  year = {2009},
  month = sep,
  volume = {228},
  pages = {6184--6211},
  issn = {00219991},
  doi = {10.1016/j.jcp.2009.05.016},
  abstract = {This paper proposes a hierarchical, multi-resolution framework for the identification of model parameters and their spatially variability from noisy measurements of the response or output. Such parameters are frequently encountered in PDE-based models and correspond to quantities such as density or pressure fields, elasto-plastic moduli and internal variables in solid mechanics, conductivity fields in heat diffusion problems, permeability fields in fluid flow through porous media etc. The proposed model has all the advantages of traditional Bayesian formulations such as the ability to produce measures of confidence for the inferences made and providing not only predictive estimates but also quantitative measures of the predictive uncertainty. In contrast to existing approaches it utilizes a parsimonious, non-parametric formulation that favors sparse representations and whose complexity can be determined from the data. The proposed framework in non-intrusive and makes use of a sequence of forward solvers operating at various resolutions. As a result, inexpensive, coarse solvers are used to identify the most salient features of the unknown field(s) which are subsequently enriched by invoking solvers operating at finer resolutions. This leads to significant computational savings particularly in problems involving computationally demanding forward models but also improvements in accuracy. It is based on a novel, adaptive scheme based on Sequential Monte Carlo sampling which is embarrassingly parallelizable and circumvents issues with slow mixing encountered in Markov Chain Monte Carlo schemes.},
  archivePrefix = {arXiv},
  eprint = {0810.0744},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/2S4Q82F6/Koutsourelakis - 2009 - A multi-resolution, non-parametric, Bayesian frame.pdf},
  journal = {Journal of Computational Physics},
  keywords = {stochastic inverse problem},
  number = {17}
}

@article{kouvelis_algorithms_1992,
  title = {Algorithms for Robust Single and Multiple Period Layout Planning for Manufacturing Systems},
  author = {Kouvelis, Panagiotis and Kurawarwala, Abbas A. and Guti{\'e}rrez, Genaro J.},
  year = {1992},
  month = dec,
  volume = {63},
  pages = {287--303},
  issn = {0377-2217},
  doi = {10.1016/0377-2217(92)90032-5},
  abstract = {In many layout design situations, the use of `optimality' with respect to a design objective, such as the minimization of the material handling cost, is insufficiently discriminating. Robustness of the layout, in cases of demand uncertainty, is more important for the manufacturing manager. A robust layout is one that is close to the optimal solution for a wide variety of demand scenarios even though it may not be optimal under any specific demand scenario. In this paper, we develop algorithms to generate robust layout designs for manufacturing systems. Our robustness approach to the layout decision making can be applied to single and multiple period problems in the presence of considerable uncertainty, both in terms of products to be produced as well as their production volumes. Our algorithms, executed in a heuristic fashion, can be effectively used for layout design of large size manufacturing systems.},
  file = {/home/victor/Zotero/storage/8AN67IDJ/0377221792900325.html},
  journal = {European Journal of Operational Research},
  keywords = {design,integer programming,Plant layout,production},
  language = {en},
  number = {2},
  series = {Strategic {{Planning}} of {{Facilities}}}
}

@article{kouvelis_algorithms_1992-1,
  title = {Algorithms for Robust Single and Multiple Period Layout Planning for Manufacturing Systems},
  author = {Kouvelis, Panagiotis and Kurawarwala, Abbas A. and Guti{\'e}rrez, Genaro J.},
  year = {1992},
  month = dec,
  volume = {63},
  pages = {287--303},
  issn = {03772217},
  doi = {10.1016/0377-2217(92)90032-5},
  journal = {European Journal of Operational Research},
  language = {en},
  number = {2}
}

@article{kramer_feedback_2016,
  title = {Feedback {{Control}} for {{Systems}} with {{Uncertain Parameters Using Online}}-{{Adaptive Reduced Models}}},
  author = {Kramer, Boris and Peherstorfer, Benjamin and Willcox, Karen},
  year = {2016},
  file = {C\:\\Users\\Victor\\Downloads\\adaptive-model-reduction-control-kramer-peherstorfer-willcox.pdf},
  keywords = {Optim Robuste}
}

@article{krantz_primer_2016,
  title = {A {{Primer}} of {{Mathematical Writing}}, {{Second Edition}}},
  author = {Krantz, Steven G.},
  year = {2016},
  month = dec,
  abstract = {This is a tract on the art and practice of mathematical writing. Not only does the book cover basic principles of grammar, syntax, and usage, but it takes into account developments of the last twenty years that have been inspired by the Internet. There is considerable discussion of TeX and other modern writing environments. We also consider electronic journals, print-on-demand books, Open Access Journals, preprint servers, and many other aspects of modern publishing life.},
  archivePrefix = {arXiv},
  eprint = {1612.04888},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/RG9Z3FHI/Krantz - 2016 - A Primer of Mathematical Writing, Second Edition.pdf},
  journal = {arXiv:1612.04888 [math]},
  keywords = {Mathematics - History and Overview},
  language = {en},
  primaryClass = {math}
}

@article{kreutz_likelihood_2012,
  title = {Likelihood Based Observability Analysis and Confidence Intervals for Predictions of Dynamic Models},
  author = {Kreutz, Clemens and Raue, Andreas and Timmer, Jens},
  year = {2012},
  month = sep,
  volume = {6},
  pages = {120},
  issn = {1752-0509},
  doi = {10.1186/1752-0509-6-120},
  abstract = {Predicting a system's behavior based on a mathematical model is a primary task in Systems Biology. If the model parameters are estimated from experimental data, the parameter uncertainty has to be translated into confidence intervals for model predictions. For dynamic models of biochemical networks, the nonlinearity in combination with the large number of parameters hampers the calculation of prediction confidence intervals and renders classical approaches as hardly feasible.},
  file = {/home/victor/Zotero/storage/Q23QL5VX/Kreutz et al. - 2012 - Likelihood based observability analysis and confid.pdf;/home/victor/Zotero/storage/EWX9XXT3/1752-0509-6-120.html},
  journal = {BMC Systems Biology},
  number = {1}
}

@article{krige_statistical_1951,
  title = {A Statistical Approach to Some Basic Mine Valuation Problems on the {{Witwatersrand}}},
  author = {Krige, Daniel G.},
  year = {1951},
  volume = {52},
  pages = {119--139},
  file = {/home/victor/Zotero/storage/JZM426MV/Krige - 1951 - A statistical approach to some basic mine valuatio.pdf;/home/victor/Zotero/storage/ELTXPCEN/AJA0038223X_4792.html},
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  number = {6}
}

@article{krokhmal_modeling_2011,
  title = {Modeling and Optimization of Risk},
  author = {Krokhmal, Pavlo and Zabarankin, Michael and Uryasev, Stan},
  year = {2011},
  month = jul,
  volume = {16},
  pages = {49--66},
  issn = {1876-7354},
  doi = {10.1016/j.sorms.2010.08.001},
  abstract = {This paper surveys the most recent advances in the context of decision making under uncertainty, with an emphasis on the modeling of risk-averse preferences using the apparatus of axiomatically defined risk functionals, such as coherent measures of risk and deviation measures, and their connection to utility theory, stochastic dominance, and other more established methods.},
  file = {/home/victor/Zotero/storage/46S2PMER/1-s2.0-S187673541000005X-main.pdf;/home/victor/Zotero/storage/KLYHLRYI/S187673541000005X.html},
  journal = {Surveys in Operations Research and Management Science},
  number = {2}
}

@article{krzysztofowicz_bayesian_1999,
  title = {Bayesian Theory of Probabilistic Forecasting via Deterministic Hydrologic Model},
  author = {Krzysztofowicz, Roman},
  year = {1999},
  volume = {35},
  pages = {2739--2750},
  issn = {1944-7973},
  doi = {10.1029/1999WR900099},
  abstract = {Rational decision making (for flood warning, navigation, or reservoir systems) requires that the total uncertainty about a hydrologic predictand (such as river stage, discharge, or runoff volume) be quantified in terms of a probability distribution, conditional on all available information and knowledge. Hydrologic knowledge is typically embodied in a deterministic catchment model. Fundamentals are presented of a Bayesian forecasting system (BFS) for producing a probabilistic forecast of a hydrologic predictand via any deterministic catchment model. The BFS decomposes the total uncertainty into input uncertainty and hydrologic uncertainty, which are quantified independently and then integrated into a predictive (Bayes) distribution. This distribution results from a revision of a prior (climatic) distribution, is well calibrated, and has a nonnegative ex ante economic value. The BFS is compared with Monte Carlo simulation and ``ensemble forecasting'' technique, none of which can alone produce a probabilistic forecast that meets requirements of rational decision making, but each can serve as a component of the BFS.},
  file = {/home/victor/Zotero/storage/TNQG4DV4/Krzysztofowicz - 1999 - Bayesian theory of probabilistic forecasting via d.pdf;/home/victor/Zotero/storage/6L4MQFIH/1999WR900099.html},
  journal = {Water Resources Research},
  language = {en},
  number = {9}
}

@article{kuczera_there_2010,
  title = {There Are No Hydrological Monsters, Just Models and Observations with Large Uncertainties!},
  author = {Kuczera, George and Renard, Benjamin and Thyer, Mark and Kavetski, Dmitri},
  year = {2010},
  month = aug,
  volume = {55},
  pages = {980--991},
  issn = {0262-6667},
  doi = {10.1080/02626667.2010.504677},
  abstract = {Catchments that do not behave in the way the hydrologist expects, expose the frailties of hydrological science, particularly its unduly simplistic treatment of input and model uncertainty. A conceptual rainfall\textendash runoff model represents a highly simplified hypothesis of the transformation of rainfall into runoff. Sub-grid variability and mis-specification of processes introduce an irreducible model error, about which little is currently known. In addition, hydrological observation systems are far from perfect, with the principal catchment forcing (rainfall) often subject to large sampling errors. When ignored or treated simplistically, these errors develop into monsters that destroy our ability to model certain catchments. In this paper, these monsters are tackled using Bayesian Total Error Analysis, a framework that accounts for user-specified sources of error and yields quantitative insights into how prior knowledge of these uncertainties affects our ability to infer models and use them for predictive purposes. A case study involving a catchment with an apparent water balance anomaly (a hydrological monstrosity!) illustrates these concepts. It is found that, in the absence of additional information, the rainfall\textendash runoff record is insufficient to explain this anomaly \textendash{} it could be due to a large export of groundwater, systematic overestimation of catchment rainfall of the order of 40\%, or a conspiracy of these factors. There is ``no free lunch'' in hydrology. The rainfall\textendash runoff record on its own is insufficient to decompose the different sources of uncertainty affecting calibration, testing and prediction, and hydrological monstrosities will persist until additional independent knowledge of uncertainties is obtained. Citation Kuczera, G., Renard, B., Thyer, M. \& Kavetski, D. (2010) There are no hydrological monsters, just models and observations with large uncertainties! Hydrol. Sci. J. 55(6), 980\textendash 991.},
  file = {/home/victor/Zotero/storage/VDZCEVA2/Kuczera et al. - 2010 - There are no hydrological monsters, just models an.pdf;/home/victor/Zotero/storage/82J26CGU/02626667.2010.html},
  journal = {Hydrological Sciences Journal},
  keywords = {analyse Bayesienne de l'erreur totale,Bayesian total error analysis,data errors,erreur structurelle du mod√®le,erreurs d'observation,ill-posedness,inf√©rence mal pos√©e,model structural error,mod√®les pluie‚Äìd√©bit,rainfall‚Äìrunoff models},
  number = {6}
}

@article{kullback_information_1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  volume = {22},
  pages = {79--86},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/home/victor/Zotero/storage/JPHCNDHI/1177729694.html},
  journal = {The Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR39968},
  number = {1},
  zmnumber = {0042.38403}
}

@article{kumar_adjoint_,
  title = {Adjoint Based Multi-Objective Shape Optimization of a Transonic Airfoil under Uncertainties},
  author = {Kumar, D. and Miranda, J. and Raisee, M. and Lacor, C.},
  file = {/home/victor/Zotero/storage/P6MU9IAM/10873.pdf;/home/victor/Zotero/storage/Y84GAN9M/310.pdf}
}

@article{kumar_adjoint_nodate,
  title = {Adjoint Based Multi-Objective Shape Optimization of a Transonic Airfoil under Uncertainties},
  author = {Kumar, D and Miranda, J and Raisee, M and Lacor, C},
  pages = {10},
  abstract = {In the present paper, the results of ongoing work within the FP7 project UMRIDA are presented and discussed. The main idea of this work is to combine the non-intrusive polynomial chaos based uncertainty quantification methods with the gradient based methods for stochastic optimization. When introducing uncertainties in a design process, the objective is no longer deterministic and can be characterized by its mean and its variance, i.e. in a robust design the optimization becomes multiobjective. Gradient based optimization of the mean objective and of the variance of the objective therefore requires the gradient of both quantities. The gradient of the mean objective is combined with the gradient of its variance using weights. By changing the weights, the Pareto front (if any, i.e. if the 2 objectives are conflicting) or at least part of it can be recovered. The proposed method is applied to the optimal shape design of the transonic RAE2822 airfoil under uncertainties. In the current work, the flight conditions (the Mach number and the angle of attack) are considered as uniformly distributed uncertain parameters. The objectives considered are the mean drag coefficient and its standard deviation. In this work, the adjoint solver and the CFD solver of SU2 (an open source CFD solver) are coupled with the polynomial chaos methods for the optimal shape design of a transonic airfoil. Hicks-Henne functions are employed to parameterize the airfoil and to represent a new geometry in the design process. The optimization procedure is performed using the sequential least square programming (SLSQP) algorithm. Two optimal designs are obtained by considering two different points on the Pareto front.},
  file = {/home/victor/Zotero/storage/IJTQ9B7Q/Kumar et al. - Adjoint based multi-objective shape optimization o.pdf},
  language = {en}
}

@phdthesis{kumar_sequential_2008,
  title = {Sequential {{Calibration Of Computer Models}}},
  author = {Kumar, Arun},
  year = {2008},
  file = {/home/victor/Zotero/storage/RLQ6FWEZ/Kumar - 2008 - Sequential Calibration Of Computer Models.pdf;/home/victor/Zotero/storage/5YQ8FL22/pg_10.html},
  language = {en},
  school = {The Ohio State University}
}

@article{laloe_nonparametric_2013,
  title = {Nonparametric Estimation of Regression Level Sets Using Kernel Plug-in Estimator},
  author = {Lalo{\"e}, T. and Servien, R.},
  year = {2013},
  month = sep,
  volume = {42},
  pages = {301--311},
  issn = {12263192},
  doi = {10.1016/j.jkss.2012.10.001},
  abstract = {Let (X , Y ) be a random pair taking values in Rd \texttimes J, where J {$\subset$} R is supposed to be bounded. We propose a plug-in estimator of the level sets of the regression function r of Y on X , using a kernel estimator of r. We consider an error criterion defined by the volume of the symmetrical difference between the real and estimated level sets. We state the consistency of our estimator, and we get a rate of convergence equivalent to the one obtained by Cadre (2006) for the density function level sets.},
  file = {/home/victor/Zotero/storage/IIEL8EJ3/Lalo√´ et Servien - 2013 - Nonparametric estimation of regression level sets .pdf},
  journal = {Journal of the Korean Statistical Society},
  language = {en},
  number = {3}
}

@article{laloy_efficient_2013,
  title = {Efficient Posterior Exploration of a High-Dimensional Groundwater Model from Two-Stage {{Markov}} Chain {{Monte Carlo}} Simulation and Polynomial Chaos Expansion},
  author = {Laloy, Eric and Rogiers, Bart and Vrugt, Jasper A. and Mallants, Dirk and Jacques, Diederik},
  year = {2013},
  month = may,
  volume = {49},
  pages = {2664--2682},
  issn = {1944-7973},
  doi = {10.1002/wrcr.20226},
  abstract = {This study reports on two strategies for accelerating posterior inference of a highly parameterized and CPU-demanding groundwater flow model. Our method builds on previous stochastic collocation approaches, e.g., Marzouk and Xiu (2009) and Marzouk and Najm (2009), and uses generalized polynomial chaos (gPC) theory and dimensionality reduction to emulate the output of a large-scale groundwater flow model. The resulting surrogate model is CPU efficient and serves to explore the posterior distribution at a much lower computational cost using two-stage MCMC simulation. The case study reported in this paper demonstrates a two to five times speed-up in sampling efficiency.},
  file = {/home/victor/Zotero/storage/TNSMADNB/Laloy et al. - 2013 - Efficient posterior exploration of a high-dimensio.pdf;/home/victor/Zotero/storage/7282JYD5/abstract.html},
  journal = {Water Resources Research},
  keywords = {MCMC,PCE},
  language = {en},
  number = {5}
}

@article{lan_wormhole_2013,
  title = {Wormhole {{Hamiltonian Monte Carlo}}},
  author = {Lan, Shiwei and Streets, Jeffrey and Shahbaba, Babak},
  year = {2013},
  month = may,
  abstract = {In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create \textbackslash emph\{wormholes\} connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a \textbackslash emph\{residual energy\} function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function.},
  archivePrefix = {arXiv},
  eprint = {1306.0063},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/SQAV645H/Lan et al. - 2013 - Wormhole Hamiltonian Monte Carlo.pdf;/home/victor/Zotero/storage/UCNIUAYF/1306.html},
  journal = {arXiv:1306.0063 [stat]},
  keywords = {Statistics - Computation},
  primaryClass = {stat}
}

@inproceedings{landrieu_cut_2016,
  title = {Cut {{Pursuit}}: Fast Algorithms to Learn Piecewise Constant Functions},
  shorttitle = {Cut {{Pursuit}}},
  booktitle = {19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}} 2016)},
  author = {Landrieu, Loic and Obozinski, Guillaume},
  year = {2016},
  month = may,
  address = {{Cadix, Spain}},
  abstract = {We propose working-set/greedy algorithms to efficiently find the solutions to convex optimization problems penalized respectively by the total variation and the Mumford Shah boundary size. Our algorithms exploit the piecewise constant structure of the level-sets of the solutions by recursively splitting them using graph cuts. We obtain significant speed up on images that can be approximated with few level-sets compared to state-of-the-art algorithms .},
  file = {/home/victor/Zotero/storage/GPVV5DNS/Landrieu et Obozinski - 2016 - Cut Pursuit fast algorithms to learn piecewise co.pdf},
  keywords = {greedy algorithm,minimal partition,Mumford-Shah,optimization,working-set}
}

@article{laurent_overview_2019,
  title = {An {{Overview}} of {{Gradient}}-{{Enhanced Metamodels}} with {{Applications}}},
  author = {Laurent, Luc and Le Riche, Rodolphe and Soulier, Bruno and Boucard, Pierre-Alain},
  year = {2019},
  month = jan,
  volume = {26},
  pages = {61--106},
  issn = {1134-3060, 1886-1784},
  doi = {10.1007/s11831-017-9226-3},
  abstract = {Metamodeling, the science of modeling functions observed at a finite number of points, benefits from all auxiliary information it can account for. Function gradients are a common auxiliary information and are useful for predicting functions with locally changing behaviors. This article is a review of the main metamodels that use function gradients in addition to function values. The goal of the article is to give the reader both an overview of the principles involved in gradientenhanced metamodels while also providing insightful formulations. The following metamodels have gradient-enhanced versions in the literature and are reviewed here: classical, weighted and moving least squares, Shepard weighting functions, and the kernel-based methods that are radial basis functions, kriging and support vector machines. The methods are set in a common framework of linear combinations between a priori chosen functions and coefficients that depend on the observations. The characteristics common to all kernel-based approaches are underlined. A new {$\nu$}-GSVR metamodel which uses gradients is given. Numerical comparisons of the metamodels are carried out for approximating analytical test functions. The experiments are replicable, as they are performed with an opensource available toolbox. The results indicate that there is a trade-off between the better computing time of least squares methods and the larger versatility of kernelbased approaches.},
  file = {/home/victor/Zotero/storage/4C8L9TWE/Laurent et al. - 2019 - An Overview of Gradient-Enhanced Metamodels with A.pdf},
  journal = {Archives of Computational Methods in Engineering},
  language = {en},
  number = {1}
}

@incollection{le_gratiet_metamodel-based_2016,
  title = {Metamodel-Based Sensitivity Analysis: Polynomial Chaos Expansions and {{Gaussian}} Processes},
  shorttitle = {Metamodel-Based Sensitivity Analysis},
  booktitle = {Handbook of {{Uncertainty Quantification}} - {{Part III}}: {{Sensitivity}} Analysis},
  author = {Le Gratiet, Loic and Marelli, Stefano and Sudret, Bruno},
  year = {2016},
  abstract = {Global sensitivity analysis is now established as a powerful approach for determining the key random input parameters that drive the uncertainty of model output predictions. Yet the classical computation of the so-called Sobol' indices is based on Monte Carlo simulation, which is not af- fordable when computationally expensive models are used, as it is the case in most applications in engineering and applied sciences. In this respect metamodels such as polynomial chaos expansions (PCE) and Gaussian processes (GP) have received tremendous attention in the last few years, as they allow one to replace the original, taxing model by a surrogate which is built from an experimental design of limited size. Then the surrogate can be used to compute the sensitivity indices in negligible time. In this chapter an introduction to each technique is given, with an emphasis on their strengths and limitations in the context of global sensitivity analysis. In particular, Sobol' (resp. total Sobol') indices can be computed analytically from the PCE coefficients. In contrast, confidence intervals on sensitivity indices can be derived straightforwardly from the properties of GPs. The performance of the two techniques is finally compared on three well-known analytical benchmarks (Ishigami, G-Sobol and Morris functions) as well as on a realistic engineering application (deflection of a truss structure).},
  file = {/home/victor/Zotero/storage/BU5KFTVK/Le Gratiet et al. - 2016 - Metamodel-based sensitivity analysis polynomial c.pdf},
  keywords = {Error estimation,Gaussian Processes,Kriging,Polynomial Chaos Expansions,Sobol' indices}
}

@article{lecun_machine_nodate,
  title = {{{MACHINE LEARNING AND PATTERN RECOGNITION}}},
  author = {LeCun, Yann},
  pages = {24},
  file = {/home/victor/Zotero/storage/93HIVCCJ/LeCun - MACHINE LEARNING AND PATTERN RECOGNITION.pdf},
  language = {en}
}

@article{lee_robust_2010,
  title = {A Robust Structural Design Method Using the {{Kriging}} Model to Define the Probability of Design Success},
  author = {Lee, K-H},
  year = {2010},
  month = feb,
  volume = {224},
  pages = {379--388},
  issn = {0954-4062},
  doi = {10.1243/09544062JMES1736},
  abstract = {Abstract, In this study, a robust optimization method is proposed by introducing the Kriging approximation model and defining the probability of design-success. A key problem in robust optimization is that the mean and the variation of a response cannot be calculated easily. This research presents an implementation of the approximate statistical moment method based on the Kriging metamodel. Furthermore, the statistics using the second-order statistical approximation method are adopted to avoid the local robust optimum. Thus, the probability of design-success, which is defined as the probability of satisfying the imposed design requirements, is represented as a function of approximate mean and variance. The formulation for the robust optimization can be defined as the probability of design-success of each response. The mathematical problem and the design problems of a two-bar structure and microgyroscope are investigated for the validation of the proposed method.},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
  language = {en},
  number = {2}
}

@article{lehman_designing_2004,
  title = {Designing Computer Experiments to Determine Robust Control Variables},
  author = {Lehman, Jeffrey S. and Santner, Thomas J. and Notz, William I.},
  year = {2004},
  pages = {571--590},
  file = {/home/victor/Zotero/storage/SC8QPVVA/DESIGNING COMP_EXP ROBUST CONTROL VARIABLES.pdf;C\:\\Users\\Victor\\Documents\\A14n213.pdf},
  journal = {Statistica Sinica},
  keywords = {Optim Robuste}
}

@book{lehmann_theory_2006,
  title = {Theory of Point Estimation},
  author = {Lehmann, Erich L. and Casella, George},
  year = {2006},
  publisher = {{Springer Science \& Business Media}},
  file = {/home/victor/Zotero/storage/PIU47W8I/Lehmann E.L., Casella G. Theory of point estimation (2ed., Springer, 1998)(ISBN 0387985026)(O)(617s)_MVsa_.pdf;/home/victor/Zotero/storage/QTZDRT6E/books.html}
}

@article{lelievre_consideration_2016,
  title = {On the Consideration of Uncertainty in Design: Optimization-Reliability-Robustness},
  shorttitle = {On the Consideration of Uncertainty in Design},
  author = {Leli{\`e}vre, Nicolas and Beaurepaire, Pierre and Mattrand, C{\'e}cile and Gayton, Nicolas and Otsmane, Abdelkader},
  year = {2016},
  volume = {54},
  pages = {1423--1437},
  file = {/home/victor/Zotero/storage/XYKS6BH8/Leli√®vre et al. - 2016 - On the consideration of uncertainty in design opt.pdf;/home/victor/Zotero/storage/BKWV7C8V/s00158-016-1556-5.html},
  journal = {Structural and Multidisciplinary Optimization},
  number = {6}
}

@article{lempert_general_2006,
  title = {A {{General}}, {{Analytic Method}} for {{Generating Robust Strategies}} and {{Narrative Scenarios}}},
  author = {Lempert, Robert J. and Groves, David G. and Popper, Steven W. and Bankes, Steve C.},
  year = {2006},
  month = apr,
  volume = {52},
  pages = {514--528},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.1050.0472},
  file = {/home/victor/Zotero/storage/SAHRJMET/Lempert et al. - 2006 - A General, Analytic Method for Generating Robust S.pdf},
  journal = {Management Science},
  language = {en},
  number = {4}
}

@article{lewis_unified_2011,
  title = {A Unified Approach to Model Selection Using the Likelihood Ratio Test},
  author = {Lewis, Fraser and Butler, Adam and Gilbert, Lucy},
  year = {2011},
  volume = {2},
  pages = {155--162},
  issn = {2041-210X},
  doi = {10.1111/j.2041-210X.2010.00063.x},
  abstract = {1. Ecological count data typically exhibit complexities such as overdispersion and zero-inflation, and are often weakly associated with a relatively large number of correlated covariates. The use of an appropriate statistical model for inference is therefore essential. A common selection criteria for choosing between nested models is the likelihood ratio test (LRT). Widely used alternatives to the LRT are based on information-theoretic metrics such as the Akaike Information Criterion. 2. It is widely believed that the LRT can only be used to compare the performance of nested models \textendash{} i.e. in situations where one model is a special case of another. There are many situations in which it is important to compare non-nested models, so, if true, this would be a substantial drawback of using LRTs for model comparison. In reality, however, it is actually possible to use the LRT for comparing both nested and non-nested models. This fact is well-established in the statistical literature, but not widely used in ecological studies. 3. The main obstacle to the use of the LRT with non-nested models has, until relatively recently, been the fact that it is difficult to explicitly write down a formula for the distribution of the LRT statistic under the null hypothesis that one of the models is true. With modern computing power it is possible to overcome this difficulty by using a simulation-based approach. 4. To demonstrate the practical application of the LRT to both nested and non-nested model comparisons, a case study involving data on questing tick (Ixodes ricinus) abundance is presented. These data contain complexities typical in ecological analyses, such as zero-inflation and overdispersion, for which comparison between models of differing structure \textendash{} e.g. non-nested models \textendash{} is of particular importance. 5. Choosing between competing statistical models is an essential part of any applied ecological analysis. The LRT is a standard statistical test for comparing nested models. By use of simulation the LRT can also be used in an analogous fashion to compare non-nested models, thereby providing a unified approach for model comparison within the null hypothesis testing paradigm. A simple practical guide is provided in how to apply this approach to the key models required in the analyses of count data.},
  file = {/home/victor/Zotero/storage/BXCT3U68/Lewis et al. - 2011 - A unified approach to model selection using the li.pdf;/home/victor/Zotero/storage/ZPATD5T6/j.2041-210X.2010.00063.html},
  journal = {Methods in Ecology and Evolution},
  keywords = {information theoretic metrics,likelihood ratio test,model selection,non-nested models},
  language = {en},
  number = {2}
}

@article{li_evaluation_2010,
  title = {Evaluation of Failure Probability via Surrogate Models},
  author = {Li, Jing and Xiu, Dongbin},
  year = {2010},
  month = nov,
  volume = {229},
  pages = {8966--8980},
  issn = {00219991},
  doi = {10.1016/j.jcp.2010.08.022},
  abstract = {Evaluation of failure probability of a given system requires sampling of the system response and can be computationally expensive. Therefore it is desirable to construct an accurate surrogate model for the system response and subsequently to sample the surrogate model. In this paper we discuss the properties of this approach. We demonstrate that the straightforward sampling of a surrogate model can lead to erroneous results, no matter how accurate the surrogate model is. We then propose a hybrid approach by sampling both the surrogate model in a ``large'' portion of the probability space and the original system in a ``small'' portion. The resulting algorithm is significantly more efficient than the traditional sampling method, and is more accurate and robust than the straightforward surrogate model approach. Rigorous convergence proof is established for the hybrid approach, and practical implementation is discussed. Numerical examples are provided to verify the theoretical findings and demonstrate the efficiency gain of the approach.},
  file = {/home/victor/Zotero/storage/SBA5UB8Z/Li et Xiu - 2010 - Evaluation of failure probability via surrogate mo.pdf},
  journal = {Journal of Computational Physics},
  language = {en},
  number = {23}
}

@book{li_markov_2009,
  title = {Markov Random Field Modeling in Image Analysis},
  author = {Li, Stan Z.},
  year = {2009},
  publisher = {{Springer Science \& Business Media}},
  file = {/home/victor/Zotero/storage/S35XCXY8/Li - 2009 - Markov random field modeling in image analysis.pdf;/home/victor/Zotero/storage/LHKNVA3F/books.html}
}

@article{li_mingliang_calibration_2012,
  title = {Calibration of a Distributed Flood Forecasting Model with Input Uncertainty Using a {{Bayesian}} Framework},
  author = {{Li Mingliang} and {Yang Dawen} and {Chen Jinsong} and {Hubbard Susan S.}},
  year = {2012},
  month = aug,
  volume = {48},
  issn = {0043-1397},
  doi = {10.1029/2010WR010062},
  abstract = {In the process of calibrating distributed hydrological models, accounting for input uncertainty is important, yet challenging. In this study, we develop a Bayesian model to estimate parameters associated with a geomorphology?based hydrological model (GBHM). The GBHM model uses geomorphic characteristics to simplify model structure and physically based methods to represent hydrological processes. We divide the observed discharge into low? and high?flow data, and use the first?order autoregressive model to describe their temporal dependence. We consider relative errors in rainfall as spatially distributed variables and estimate them jointly with the GBHM parameters. The joint posterior probability distribution is explored using Markov chain Monte Carlo methods, which include Metropolis?Hastings, delay rejection adaptive Metropolis, and Gibbs sampling methods. We evaluate the Bayesian model using both synthetic and field data sets. The synthetic case study demonstrates that the developed method generally is effective in calibrating GBHM parameters and in estimating their associated uncertainty. The calibration ignoring input errors has lower accuracy and lower reliability compared to the calibration that includes estimation of the input errors, especially under model structure uncertainty. The field case study shows that calibration of GBHM parameters under complex field conditions remains a challenge. Although jointly estimating input errors and GBHM parameters improves the continuous ranked probability score and the consistency of the predictive distribution with the observed data, the improvement is incremental. To better calibrate parameters in a distributed model, such as GBHM here, we need to develop a more complex model and incorporate much more information.},
  file = {/home/victor/Zotero/storage/6FLWZTMQ/Li Mingliang et al. - 2012 - Calibration of a distributed flood forecasting mod.pdf;/home/victor/Zotero/storage/P4PLU5MD/2010WR010062.html},
  journal = {Water Resources Research},
  keywords = {Bayesian,calibration,distributed hydrological model,flood forecasting,input uncertainty,MCMC},
  number = {8}
}

@inproceedings{liang_single-loop_2004,
  title = {A {{Single}}-{{Loop Method}} for {{Reliability}}-{{Based Design Optimization}}},
  author = {Liang, Jinghong and Mourelatos, Zissimos P. and Tu, Jian},
  year = {2004},
  volume = {2004},
  pages = {419--430},
  publisher = {{ASME}},
  doi = {10.1115/DETC2004-57255},
  abstract = {Reliability-Based Design Optimization (RBDO) can provide optimum designs in the presence of uncertainty. It can therefore, be a powerful tool for design under uncertainty. The traditional, double-loop RBDO algorithm requires nested optimization loops, where the design optimization (outer) loop, repeatedly calls a series of reliability (inner) loops. Due to the nested optimization loops, the computational effort can be prohibitive for practical problems. A single-loop RBDO algorithm is proposed in this paper for both normal and nonnormal random variables. Its accuracy is the same with the double-loop approach and its efficiency is almost equivalent to deterministic optimization. It collapses the nested optimization loops into an equivalent single-loop optimization process by imposing the Karush-Kuhn-Tucker optimality conditions of the reliability loops as equivalent deterministic equality constraints of the design optimization loop. It therefore, converts the probabilistic optimization problem into an equivalent deterministic optimization problem, eliminating the need for calculating the Most Probable Point (MPP) in repeated reliability assessments. Several numerical applications including an automotive vehicle side impact example, demonstrate the accuracy and superior efficiency of the proposed single-loop RBDO algorithm.},
  file = {/home/victor/Zotero/storage/XNMVG9MM/Liang et al. - 2004 - A Single-Loop Method for Reliability-Based Design .pdf},
  isbn = {978-0-7918-4694-0},
  language = {en}
}

@article{lieberman_parameter_2010,
  title = {Parameter and {{State Model Reduction}} for {{Large}}-{{Scale Statistical Inverse Problems}}},
  author = {Lieberman, Chad and Willcox, Karen and Ghattas, Omar},
  year = {2010},
  month = jan,
  volume = {32},
  pages = {2523--2542},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/090775622},
  file = {C\:\\Users\\Victor\\Downloads\\60569.pdf},
  journal = {SIAM Journal on Scientific Computing},
  keywords = {MCMC,MOR methods,reduction parameter space,stochastic inverse problem},
  language = {en},
  number = {5}
}

@article{lim_likelihood_2010,
  title = {Likelihood Ratio Tests of Correlated Multivariate Samples},
  author = {Lim, Johan and Li, Erning and Lee, Shin-Jae},
  year = {2010},
  month = mar,
  volume = {101},
  pages = {541--554},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2009.10.011},
  abstract = {We develop methods to compare multiple multivariate normally distributed samples which may be correlated. The methods are new in the context that no assumption is made about the correlations among the samples. Three types of null hypotheses are considered: equality of mean vectors, homogeneity of covariance matrices, and equality of both mean vectors and covariance matrices. We demonstrate that the likelihood ratio test statistics have finite-sample distributions that are functions of two independent Wishart variables and dependent on the covariance matrix of the combined multiple populations. Asymptotic calculations show that the likelihood ratio test statistics converge in distribution to central Chi-squared distributions under the null hypotheses regardless of how the populations are correlated. Following these theoretical findings, we propose a resampling procedure for the implementation of the likelihood ratio tests in which no restrictive assumption is imposed on the structures of the covariance matrices. The empirical size and power of the test procedure are investigated for various sample sizes via simulations. Two examples are provided for illustration. The results show good performance of the methods in terms of test validity and power.},
  file = {/home/victor/Zotero/storage/XAK8H8VG/Lim et al. - 2010 - Likelihood ratio tests of correlated multivariate .pdf;/home/victor/Zotero/storage/6A7JNLPI/S0047259X09002024.html},
  journal = {Journal of Multivariate Analysis},
  keywords = {Correlated samples,Empirical rejection probability,Equality of mean vectors,Homogeneity of covariance matrices,Multivariate analysis,Resampling},
  language = {en},
  number = {3}
}

@inproceedings{lin_bayesian_2006,
  title = {Bayesian {{L1}}-{{Norm Sparse Learning}}},
  author = {Lin, Yuanqing and Lee, Daniel},
  year = {2006},
  month = jun,
  volume = {5},
  pages = {V-V},
  doi = {10.1109/ICASSP.2006.1661348},
  abstract = {We propose a Bayesian framework for learning the optimal regularization parameter in the L1-norm penalized least-mean-square (LMS) problem, also known as LASSO (R. Tibshirani, 1996) or basis pursuit (S.S. Chen et al., 1998). The setting of the regularization parameter is critical for deriving a correct solution. In most existing methods, the scalar regularization parameter is often determined in a heuristic manner; in contrast, our approach infers the optimal regularization setting under a Bayesian framework. Furthermore, Bayesian inference enables an independent regularization scheme where each coefficient (or weight) is associated with an independent regularization parameter. Simulations illustrate the improvement using our method in discovering sparse structure from noisy data},
  file = {/home/victor/Zotero/storage/UVXTNNG2/Lin et Lee - 2006 - Bayesian L1-Norm Sparse Learning.pdf}
}

@article{liu_variational_2013,
  title = {Variational Algorithms for Marginal {{MAP}}},
  author = {Liu, Qiang and Ihler, Alexander},
  year = {2013},
  volume = {14},
  pages = {3165--3200},
  file = {/home/victor/Zotero/storage/USN5RNR6/liu13b.pdf},
  journal = {The Journal of Machine Learning Research},
  keywords = {EM algorithm,MMAP},
  number = {1}
}

@article{lorenz_deterministic_1963,
  title = {Deterministic Nonperiodic Flow},
  author = {Lorenz, Edward N.},
  year = {1963},
  volume = {20},
  pages = {130--141},
  file = {/home/victor/Zotero/storage/E5EJ45IY/Lorenz - 1963 - Deterministic nonperiodic flow.pdf;/home/victor/Zotero/storage/IWFXT9YF/1520-0469(1963)0200130DNF2.0.html},
  journal = {Journal of the atmospheric sciences},
  number = {2}
}

@article{lu_limitations_2015,
  title = {Limitations of Polynomial Chaos Expansions in the {{Bayesian}} Solution of Inverse Problems},
  author = {Lu, Fei and Morzfeld, Matthias and Tu, Xuemin and Chorin, Alexandre J.},
  year = {2015},
  month = feb,
  volume = {282},
  pages = {138--147},
  issn = {00219991},
  doi = {10.1016/j.jcp.2014.11.010},
  abstract = {Polynomial chaos expansions are used to reduce the computational cost in the Bayesian solutions of inverse problems by creating a surrogate posterior that can be evaluated inexpensively. We show, by analysis and example, that when the data contain significant information beyond what is assumed in the prior, the surrogate posterior can be very different from the posterior, and the resulting estimates become inaccurate. One can improve the accuracy by adaptively increasing the order of the polynomial chaos, but the cost may increase too fast for this to be cost effective compared to Monte Carlo sampling without a surrogate posterior.},
  archivePrefix = {arXiv},
  eprint = {1404.7188},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/7UYTAW8U/Lu et al. - 2015 - Limitations of polynomial chaos expansions in the .pdf;/home/victor/Zotero/storage/IX3UP4AL/LMTC14.pdf;/home/victor/Zotero/storage/K987393G/1404.html},
  journal = {Journal of Computational Physics},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation}
}

@misc{mackay_bayesian_,
  title = {Bayesian {{Interpolation}}},
  author = {MacKay, David J.C.},
  file = {/home/victor/Zotero/storage/8VR5W2PJ/MACnc92a.pdf},
  howpublished = {https://authors.library.caltech.edu/13792/1/MACnc92a.pdf}
}

@book{mackay_information_2003,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David JC},
  year = {2003},
  publisher = {{Cambridge university press}},
  file = {/home/victor/Zotero/storage/APYP3J87/MacKay - 2003 - Information theory, inference and learning algorit.pdf;/home/victor/Zotero/storage/93KA4AD4/books.html}
}

@article{mackay_information-based_1992,
  title = {Information-{{Based Objective Functions}} for {{Active Data Selection}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = jul,
  volume = {4},
  pages = {590--604},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1992.4.4.590},
  file = {/home/victor/Zotero/storage/XHG73IPQ/MacKay - 1992 - Information-Based Objective Functions for Active D.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{maier_uncertain_2016,
  title = {An Uncertain Future, Deep Uncertainty, Scenarios, Robustness and Adaptation: {{How}} Do They Fit Together?},
  shorttitle = {An Uncertain Future, Deep Uncertainty, Scenarios, Robustness and Adaptation},
  author = {Maier, H.R. and Guillaume, J.H.A. and {van Delden}, H. and Riddell, G.A. and Haasnoot, M. and Kwakkel, J.H.},
  year = {2016},
  month = jul,
  volume = {81},
  pages = {154--164},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2016.03.014},
  abstract = {A highly uncertain future due to changes in climate, technology and socio-economics has led to the realisation that identification of ``best-guess'' future conditions might no longer be appropriate. Instead, multiple plausible futures need to be considered, which requires (i) uncertainties to be described with the aid of scenarios that represent coherent future pathways based on different sets of assumptions, (ii) system performance to be represented by metrics that measure insensitivity (i.e. robustness) to changes in future conditions, and (iii) adaptive strategies to be considered alongside their more commonly used static counterparts. However, while these factors have been considered in isolation previously, there has been a lack of discussion of the way they are connected. In order to address this shortcoming, this paper presents a multidisciplinary perspective on how the above factors fit together to facilitate the development of strategies that are best suited to dealing with a deeply uncertain future.},
  file = {/home/victor/Zotero/storage/UPRN7L7S/Maier et al. - 2016 - An uncertain future, deep uncertainty, scenarios, .pdf},
  journal = {Environmental Modelling \& Software},
  language = {en}
}

@article{malherbe_global_2017,
  title = {Global Optimization of {{Lipschitz}} Functions},
  author = {Malherbe, C{\'e}dric and Vayatis, Nicolas},
  year = {2017},
  month = mar,
  abstract = {The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H\textasciidieresis older like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.},
  archivePrefix = {arXiv},
  eprint = {1703.02628},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/R7XXF9DC/Malherbe et Vayatis - 2017 - Global optimization of Lipschitz functions.pdf},
  journal = {arXiv:1703.02628 [stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@misc{malmberg_argmax_,
  title = {Argmax over {{Continuous Indices}} of {{Random Variables}} : {{An Approach Us}}- Ing {{Random Fields}}},
  author = {Malmberg, Hannes},
  file = {/home/victor/Zotero/storage/QD8FUL8W/report.pdf},
  keywords = {argmax measure}
}

@book{manski_structural_1981,
  title = {Structural Analysis of Discrete Data with Econometric Applications},
  author = {Manski, Charles F. and McFadden, Daniel},
  year = {1981},
  publisher = {{Mit Press Cambridge, MA}}
}

@article{markowitz_portfolio_1952,
  title = {Portfolio Selection},
  author = {Markowitz, Harry},
  year = {1952},
  volume = {7},
  pages = {77--91},
  file = {/home/victor/Zotero/storage/HWSVRRWD/Markowitz - 1952 - Portfolio selection;/home/victor/Zotero/storage/T6TEETNQ/Markowitz - 1952 - Portfolio selection.pdf},
  journal = {The journal of finance},
  keywords = {Bias Variance tradeoff,Portfolio selection},
  number = {1}
}

@article{marler_weighted_2010,
  title = {The Weighted Sum Method for Multi-Objective Optimization: New Insights},
  shorttitle = {The Weighted Sum Method for Multi-Objective Optimization},
  author = {Marler, R. Timothy and Arora, Jasbir S.},
  year = {2010},
  month = jun,
  volume = {41},
  pages = {853--862},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-009-0460-7},
  abstract = {As a common concept in multi-objective optimization, minimizing a weighted sum constitutes an independent method as well as a component of other methods. Consequently, insight into characteristics of the weighted sum method has far reaching implications. However, despite the many published applications for this method and the literature addressing its pitfalls with respect to depicting the Pareto optimal set, there is little comprehensive discussion concerning the conceptual significance of the weights and techniques for maximizing the effectiveness of the method with respect to a priori articulation of preferences. Thus, in this paper, we investigate the fundamental significance of the weights in terms of preferences, the Pareto optimal set, and objective-function values. We determine the factors that dictate which solution point results from a particular set of weights. Fundamental deficiencies are identified in terms of a priori articulation of preferences, and guidelines are provided to help avoid blind use of the method.},
  file = {/home/victor/Zotero/storage/EJS8CI6X/Marler et Arora - 2010 - The weighted sum method for multi-objective optimi.pdf},
  journal = {Structural and Multidisciplinary Optimization},
  language = {en},
  number = {6}
}

@unpublished{marrel_global_2010,
  title = {Global Sensitivity Analysis for Models with Spatially Dependent Outputs},
  author = {Marrel, Amandine and Iooss, Bertrand and Jullien, Michel and Laurent, Beatrice and Volkova, Elena},
  year = {2010},
  month = feb,
  abstract = {The global sensitivity analysis of a complex numerical model often requires the estimation of variance-based importance measures, called Sobol' indices. Metamodel-based techniques have been developed in order to replace the cpu time expensive computer code with an inexpensive mathematical function, predicting the computer code output. The common metamodel-based sensitivity analysis methods are appropriate with computer codes having scalar model output. However, in the environmental domain, as in many areas of application, numerical models often give as output a spatial map, which is sometimes a spatio-temporal evolution, of some interest variables. In this paper, we introduce a novel way to obtain a spatial map of Sobol' indices with a minimal number of numerical model computations. It is based on the functional decomposition of the spatial output onto a wavelet basis and the metamodeling of the wavelet coefficients by Gaussian process. An analytical example allows us to clarify the various steps of our methodology. This technique is then applied to a real case of hydrogeological modeling: for each model input variable, a spatial map of Sobol' indices is thus obtained.},
  file = {/home/victor/Zotero/storage/Q6NIH5TD/Marrel et al. - 2010 - Global sensitivity analysis for models with spatia.pdf},
  keywords = {AS,Gaussian process,Kriging,Monte-Carlo estimation,Sobol',wavelet}
}

@article{marzat_worst-case_2013,
  title = {Worst-Case Global Optimization of Black-Box Functions through {{Kriging}} and Relaxation},
  author = {Marzat, Julien and Walter, Eric and {Piet-Lahanier}, H{\'e}l{\`e}ne},
  year = {2013},
  month = apr,
  volume = {55},
  pages = {707--727},
  issn = {0925-5001, 1573-2916},
  doi = {10.1007/s10898-012-9899-y},
  abstract = {A new algorithm is proposed to deal with the worst-case optimization of black-box functions evaluated through costly computer simulations. The input variables of these computer experiments are assumed to be of two types. Control variables must be tuned while environmental variables have an undesirable effect, to which the design of the control variables should be robust. The algorithm to be proposed searches for a minimax solution, i.e., values of the control variables that minimize the maximum of the objective function with respect to the environmental variables. The problem is particularly difficult when the control and environmental variables live in continuous spaces. Combining a relaxation procedure with Krigingbased optimization makes it possible to deal with the continuity of the variables and the fact that no analytical expression of the objective function is available in most real-case problems. Numerical experiments are conducted to assess the accuracy and efficiency of the algorithm, both on analytical test functions with known results and on an engineering application.},
  file = {/home/victor/Zotero/storage/F6CV6LGA/Marzat et al. - 2013 - Worst-case global optimization of black-box functi.pdf},
  journal = {Journal of Global Optimization},
  language = {en},
  number = {4}
}

@article{marzouk_dimensionality_2009,
  title = {Dimensionality Reduction and Polynomial Chaos Acceleration of {{Bayesian}} Inference in Inverse Problems},
  author = {Marzouk, Youssef M. and Najm, Habib N.},
  year = {2009},
  month = apr,
  volume = {228},
  pages = {1862--1902},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2008.11.024},
  abstract = {We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a spatial or temporal field, endowed with a hierarchical Gaussian process prior. Computational challenges in this construction arise from the need for repeated evaluations of the forward model (e.g., in the context of Markov chain Monte Carlo) and are compounded by high dimensionality of the posterior. We address these challenges by introducing truncated Karhunen\textendash Lo\`eve expansions, based on the prior distribution, to efficiently parameterize the unknown field and to specify a stochastic forward problem whose solution captures that of the deterministic forward model over the support of the prior. We seek a solution of this problem using Galerkin projection on a polynomial chaos basis, and use the solution to construct a reduced-dimensionality surrogate posterior density that is inexpensive to evaluate. We demonstrate the formulation on a transient diffusion equation with prescribed source terms, inferring the spatially-varying diffusivity of the medium from limited and noisy data.},
  file = {C\:\\Users\\Victor\\Downloads\\59814.pdf;/home/victor/Zotero/storage/XJ4K288U/S0021999108006062.html},
  journal = {Journal of Computational Physics},
  keywords = {Bayesian inference,Dimensionality reduction,Galerkin projection,Gaussian processes,Inverse problems,Karhunen‚ÄìLo√®ve expansion,MCMC,Polynomial chaos,RKHS},
  number = {6}
}

@book{massart_concentration_2007,
  title = {Concentration Inequalities and Model Selection},
  author = {Massart, Pascal},
  year = {2007},
  volume = {6},
  publisher = {{Springer}},
  file = {/home/victor/Zotero/storage/D6LDMJUI/flour.pdf},
  keywords = {Model selection}
}

@book{matheron_traite_1962,
  title = {Trait\'e de G\'eostatistique Appliqu\'ee. 1 (1962)},
  author = {Matheron, Georges},
  year = {1962},
  volume = {1},
  publisher = {{Editions Technip}}
}

@article{mayer_diversity_2016,
  title = {Diversity of Immune Strategies Explained by Adaptation to Pathogen Statistics},
  author = {Mayer, Andreas and Mora, Thierry and Rivoire, Olivier and Walczak, Aleksandra M.},
  year = {2016},
  month = jul,
  pages = {201600663},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1600663113},
  abstract = {Biological organisms have evolved a wide range of immune mechanisms to defend themselves against pathogens. Beyond molecular details, these mechanisms differ in how protection is acquired, processed, and passed on to subsequent generations\textemdash differences that may be essential to long-term survival. Here, we introduce a mathematical framework to compare the long-term adaptation of populations as a function of the pathogen dynamics that they experience and of the immune strategy that they adopt. We find that the two key determinants of an optimal immune strategy are the frequency and the characteristic timescale of the pathogens. Depending on these two parameters, our framework identifies distinct modes of immunity, including adaptive, innate, bet-hedging, and CRISPR-like immunities, which recapitulate the diversity of natural immune systems.},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  file = {/home/victor/Zotero/storage/STEVNHSJ/Mayer et al. - 2016 - Diversity of immune strategies explained by adapta.pdf;/home/victor/Zotero/storage/Q467KF27/1600663113.html},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {adaptive immunity,bet hedging,CRISPR immunity,evolution of immunity,immune systems},
  language = {en},
  pmid = {27432970}
}

@incollection{mayerhofer_reduced_2017,
  title = {A {{Reduced Basis Method}} for {{Parameter Functions Using Wavelet Approximations}}},
  booktitle = {Model {{Reduction}} of {{Parametrized Systems}}},
  author = {Mayerhofer, Antonia and Urban, Karsten},
  year = {2017},
  pages = {77--90},
  publisher = {{Springer}},
  file = {/home/victor/Zotero/storage/BTAFLU6L/MU2-final.pdf},
  keywords = {reduced basis,wavelet}
}

@article{mckay_comparison_2000,
  title = {A {{Comparison}} of {{Three Methods}} for {{Selecting Values}} of {{Input Variables}} in the {{Analysis}} of {{Output}} from a {{Computer Code}}},
  author = {Mckay, M. D. and Beckman, R. J. and Conover, W. J.},
  year = {2000},
  month = feb,
  volume = {42},
  pages = {55},
  issn = {00401706},
  doi = {10.2307/1271432},
  file = {C\:\\Users\\Victor\\Downloads\\McKayConoverBeckman.pdf},
  journal = {Technometrics},
  keywords = {Sampling},
  number = {1}
}

@book{mclachlan_mixture_1988,
  title = {Mixture Models: {{Inference}} and Applications to Clustering},
  shorttitle = {Mixture Models},
  author = {McLachlan, Geoffrey J. and Basford, Kaye E.},
  year = {1988},
  volume = {84},
  publisher = {{Marcel Dekker}},
  file = {/home/victor/Zotero/storage/N6ARLX48/UQ308790.html}
}

@article{mcphail_robustness_2018,
  title = {Robustness {{Metrics}}: {{How Are They Calculated}}, {{When Should They Be Used}} and {{Why Do They Give Different Results}}?},
  shorttitle = {Robustness {{Metrics}}},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  year = {2018},
  volume = {6},
  pages = {169--191},
  issn = {2328-4277},
  doi = {10.1002/2017EF000649},
  abstract = {Robustness is being used increasingly for decision analysis in relation to deep uncertainty and many metrics have been proposed for its quantification. Recent studies have shown that the application of different robustness metrics can result in different rankings of decision alternatives, but there has been little discussion of what potential causes for this might be. To shed some light on this issue, we present a unifying framework for the calculation of robustness metrics, which assists with understanding how robustness metrics work, when they should be used, and why they sometimes disagree. The framework categorizes the suitability of metrics to a decision-maker based on (1) the decision-context (i.e., the suitability of using absolute performance or regret), (2) the decision-maker's preferred level of risk aversion, and (3) the decision-maker's preference toward maximizing performance, minimizing variance, or some higher-order moment. This article also introduces a conceptual framework describing when relative robustness values of decision alternatives obtained using different metrics are likely to agree and disagree. This is used as a measure of how ``stable'' the ranking of decision alternatives is when determined using different robustness metrics. The framework is tested on three case studies, including water supply augmentation in Adelaide, Australia, the operation of a multipurpose regulated lake in Italy, and flood protection for a hypothetical river based on a reach of the river Rhine in the Netherlands. The proposed conceptual framework is confirmed by the case study results, providing insight into the reasons for disagreements between rankings obtained using different robustness metrics.},
  copyright = {\textcopyright{} 2018 The Authors.},
  file = {/home/victor/Zotero/storage/SUZY5W3T/McPhail et al. - 2018 - Robustness Metrics How Are They Calculated, When .pdf;/home/victor/Zotero/storage/J8QMG2HC/2017EF000649.html},
  journal = {Earth's Future},
  keywords = {decision analysis,deep uncertainty,ranking stability,robustness metrics,water systems},
  language = {en},
  number = {2}
}

@article{menetrier_oceanographie_2014,
  title = {{Oc\'eanographie Mar\'ees Support de cours}},
  author = {M{\'e}n{\'e}trier, Benjamin},
  year = {2014},
  pages = {38},
  file = {/home/victor/Zotero/storage/NXVGNP9L/M√©n√©trier - Oc√©anographie Mar√©es Support de cours.pdf},
  language = {fr}
}

@misc{michael_waskom_mwaskom/seaborn_2017,
  title = {Mwaskom/Seaborn: V0.8.1 ({{September}} 2017)},
  shorttitle = {Mwaskom/Seaborn},
  author = {Michael Waskom and Olga Botvinnik and Drew O'Kane and Paul Hobson and Saulius Lukauskas and David C Gemperline and Tom Augspurger and Yaroslav Halchenko and John B. Cole and Jordi Warmenhoven and {Julian de Ruiter} and Cameron Pye and Stephan Hoyer and Jake Vanderplas and Santi Villalba and Gero Kunter and Eric Quintero and Pete Bachant and Marcel Martin and Kyle Meyer and Alistair Miles and Yoav Ram and Tal Yarkoni and Mike Lee Williams and Constantine Evans and Clark Fitzgerald and Brian and Chris Fonnesbeck and Antony Lee and Adel Qalieh},
  year = {2017},
  month = sep,
  doi = {10.5281/zenodo.883859},
  abstract = {v0.8.1 (September 2017) Added a warning in FacetGrid when passing a categorical plot function without specifying order (or hue\_order when hue is used), which is likely to produce a plot that is incorrect. Improved compatibility between FacetGrid or PairGrid and interactive matplotlib backends so that the legend no longer remains inside the figure when using legend\_out=True. Changed categorical plot functions with small plot elements to use dark\_palette instead of `light\_palette`` when generating a sequential palette from a specified color. Improved robustness of kdeplot and distplot to data with fewer than two observations. Fixed a bug in clustermap when using yticklabels=False. Fixed a bug in pointplot where colors were wrong if exactly three points were being drawn. Fixed a bug inpointplot where legend entries for missing data appeared with empty markers. Fixed a bug in clustermap where an error was raised when annotating the main heatmap and showing category colors. Fixed a bug in clustermap where row labels were not being properly rotated when they overlapped. Fixed a bug in kdeplot where the maximum limit on the density axes was not being updated when multiple densities were drawn. Improved compatibility with future versions of pandas.},
  file = {/home/victor/Zotero/storage/M3DU4FR3/883859.html},
  howpublished = {Zenodo},
  keywords = {software}
}

@techreport{miller_surprised_2016,
  title = {Surprised by the {{Gambler}}'s and {{Hot Hand Fallacies}}? {{A Truth}} in the {{Law}} of {{Small Numbers}}},
  shorttitle = {Surprised by the {{Gambler}}'s and {{Hot Hand Fallacies}}?},
  author = {Miller, Joshua B. and Sanjurjo, Adam},
  year = {2016},
  month = nov,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {We prove that a subtle but substantial bias exists in a standard measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data. The magnitude of this novel form of selection bias generally decreases as the sequence gets longer, but increases in streak length, and remains substantial for a range of sequence lengths often used in empirical work.  The bias has important implications for the literature that investigates incorrect beliefs in sequential decision making---most notably the Hot Hand Fallacy and the Gambler's Fallacy. Upon correcting for the bias, the conclusions of prominent studies in the hot hand fallacy literature are reversed.   The bias also provides a novel structural explanation for how belief in the law of small numbers can persist in the face of experience.},
  file = {/home/victor/Zotero/storage/A9S8ISDT/papers.html},
  keywords = {Alternation Bias,Finite Sample Bias,Gambler's Fallacy,Hot Hand Effect,Hot Hand Fallacy,Law of Small Numbers,Negative Recency Bias,Selection Bias,Sequential Data,Sequential Decision Making,Small Sample Bias},
  language = {en},
  number = {ID 2627354},
  type = {{{SSRN Scholarly Paper}}}
}

@inproceedings{miranda_adjoint-based_2016,
  title = {Adjoint-Based {{Robust Optimization}} Using {{Polynomial Chaos Expansions}}},
  booktitle = {Proceedings of the {{VII European Congress}} on {{Computational Methods}} in {{Applied Sciences}} and {{Engineering}} ({{ECCOMAS Congress}} 2016)},
  author = {Miranda, Joao and Kumar, Dinesh and Lacor, Chris},
  year = {2016},
  pages = {8351--8364},
  publisher = {{Institute of Structural Analysis and Antiseismic Research School of Civil Engineering National Technical University of Athens (NTUA) Greece}},
  address = {{Crete Island, Greece}},
  doi = {10.7712/100016.2418.10873},
  abstract = {Adjoint methods are nowadays widely used to efficiently perform optimization for problems with a large number of design variables. However, in reality, the problem at hand might be subjected to uncertainties in the operational conditions or, in case of optimizing geometries, the design variables itself might be uncertain due to manufacturing tolerances. For such applications, the optimum obtained using deterministic methods might be very sensitive to small variations in the uncertainties, i.e. it lacks robustness. In a robust optimization, the uncertainties are taken directly into account during the optimization process by introducing, next to the mean objective, its variance as a second objective. This implies that, when using gradient based optimization methods, the gradients of both objectives (mean and variance) must be known. In this work the Polynomial Chaos Expansion (PCE) is used in combination with adjoint methods to efficiently obtain both gradients. A non intrusive, regression based PCE is used, requiring a new adjoint solution for each sampling point in order to build the PCE of the gradient. A PCE for the objective is also built (at no extra cost) in order to compute the gradient of the variance.},
  file = {/home/victor/Zotero/storage/7SRD672D/Miranda et al. - 2016 - ADJOINT-BASED ROBUST OPTIMIZATION USING POLYNOMIAL.pdf},
  isbn = {978-618-82844-0-1},
  language = {en}
}

@incollection{mockus_bayesian_1974,
  title = {On Bayesian Methods for Seeking the Extremum},
  booktitle = {Optimization {{Techniques IFIP Technical Conference Novosibirsk}}, {{July}} 1\textendash 7, 1974},
  author = {Mo{\v c}kus, J.},
  year = {1974},
  month = jul,
  pages = {400--404},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-07165-2_55},
  file = {/home/victor/Zotero/storage/UG95UKH3/Moƒçkus - 1974 - On bayesian methods for seeking the extremum.pdf;/home/victor/Zotero/storage/MCDFG96M/3-540-07165-2_55.html},
  isbn = {978-3-540-07165-5 978-3-540-37497-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{molchanov_lectures_nodate,
  title = {Lectures on Random Sets and Their Applications in Economics and Finance},
  author = {Molchanov, Ilya},
  pages = {32},
  abstract = {This course introduces main concepts from the theory of random sets with emphasis on applications in economics and finance: most importantly inference for partially identified models and transaction costs modelling.},
  file = {/home/victor/Zotero/storage/DNHLWW2L/Molchanov - Lectures on random sets and their applications in .pdf},
  language = {en}
}

@article{moritz_gohler_robustness_2016,
  title = {Robustness {{Metrics}}: {{Consolidating}} the {{Multiple Approaches}} to {{Quantify Robustness}}},
  shorttitle = {Robustness {{Metrics}}},
  author = {Moritz G{\"o}hler, Simon and Eifler, Tobias and Howard, Thomas J.},
  year = {2016},
  month = sep,
  volume = {138},
  pages = {111407},
  issn = {1050-0472},
  doi = {10.1115/1.4034112},
  abstract = {The robustness of a design has a major influence on how much the product's performance will vary and is of great concern to design, quality and production engineers. While variability is always central to the definition of robustness, the concept does contain ambiguity and although subtle, this ambiguity can have significant influence on the strategies used to combat variability, the way it is quantified and ultimately, the quality of the final design. In this contribution the literature for robustness metrics was systematically reviewed. From the 108 relevant publications found, 38 metrics were determined to be conceptually different from one another. The metrics were classified by their meaning and interpretation based on the types of information necessary to calculate the metrics. Four different classes were identified: 1) Sensitivity robustness metrics; 2) Size of feasible design space robustness metrics; 3) Functional expectancy and dispersion robustness metrics; and 4) Probability of compliance robustness metrics. The goal was to give a comprehensive overview of robustness metrics and guidance to scholars and practitioners to understand the different types of robustness metrics and to remove the ambiguities of the term robustness. By applying an exemplar metric from each class to a case study, the differences between the classes were further highlighted. These classes form the basis for the definition of four specific sub-definitions of robustness, namely the `robust concept', `robust design', `robust function' and `robust product'.},
  file = {/home/victor/Zotero/storage/BFB4RZIJ/Moritz G√∂hler et al. - 2016 - Robustness Metrics Consolidating the Multiple App.pdf},
  journal = {Journal of Mechanical Design},
  language = {en},
  number = {11}
}

@incollection{muller_simulation_2005,
  title = {Simulation {{Based Optimal Design}}},
  booktitle = {Handbook of {{Statistics}}},
  author = {M{\"u}ller, Peter},
  year = {2005},
  volume = {25},
  pages = {509--518},
  publisher = {{Elsevier}},
  doi = {10.1016/S0169-7161(05)25017-4},
  abstract = {We review simulation based methods in optimal design. Expected utility maximization, i.e., optimal design, is concerned with maximizing an integral expression representing expected utility with respect to some design parameter. Except in special cases neither the maximization nor the integration can be solved analytically and approximations and/or simulation based methods are needed. On one hand the integration problem is easier to solve than the integration appearing in posterior inference problems. This is because the expectation is with respect to the joint distribution of parameters and data, which typically allows efficient random variate generation. On the other hand, the problem is difficult because the integration is embedded in the maximization and has to possibly be evaluated many times for different design parameters.},
  file = {/home/victor/Zotero/storage/5KPF7B7I/M√ºller - 2005 - Simulation Based Optimal Design.pdf},
  isbn = {978-0-444-51539-1},
  language = {en}
}

@article{murphy_conjugate_2007,
  title = {Conjugate {{Bayesian}} Analysis of the {{Gaussian}} Distribution},
  author = {Murphy, Kevin P.},
  year = {2007},
  volume = {1},
  pages = {16},
  file = {/home/victor/Zotero/storage/JXBXPSL6/bayesGauss.pdf},
  journal = {def},
  number = {2{$\sigma$}2}
}

@inproceedings{murphy_loopy_1999,
  title = {Loopy Belief Propagation for Approximate Inference: {{An}} Empirical Study},
  shorttitle = {Loopy Belief Propagation for Approximate Inference},
  booktitle = {Proceedings of the {{Fifteenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Murphy, Kevin P. and Weiss, Yair and Jordan, Michael I.},
  year = {1999},
  pages = {467--475},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  file = {/home/victor/Zotero/storage/4YGEXIA8/qt92p8w3xb.pdf}
}

@article{nagel_bayesian_,
  title = {Bayesian Techniques for Inverse Uncertainty Quantification},
  author = {Nagel, Joseph Benjamin},
  pages = {211},
  file = {/home/victor/Zotero/storage/93SNZ7CK/Nagel - Bayesian techniques for inverse uncertainty quanti.pdf}
}

@article{nagel_spectral_2016,
  title = {Spectral Likelihood Expansions for {{Bayesian}} Inference},
  author = {Nagel, Joseph B. and Sudret, Bruno},
  year = {2016},
  month = mar,
  volume = {309},
  pages = {267--294},
  issn = {00219991},
  doi = {10.1016/j.jcp.2015.12.047},
  abstract = {A spectral approach to Bayesian inference is presented. It pursues the emulation of the posterior probability density. The starting point is a series expansion of the likelihood function in terms of orthogonal polynomials. From this spectral likelihood expansion all statistical quantities of interest can be calculated semi-analytically. The posterior is formally represented as the product of a reference density and a linear combination of polynomial basis functions. Both the model evidence and the posterior moments are related to the expansion coefficients. This formulation avoids Markov chain Monte Carlo simulation and allows one to make use of linear least squares instead. The pros and cons of spectral Bayesian inference are discussed and demonstrated on the basis of simple applications from classical statistics and inverse modeling.},
  archivePrefix = {arXiv},
  eprint = {1506.07564},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/5NHUE98M/Nagel et Sudret - 2016 - Spectral likelihood expansions for Bayesian infere.pdf;/home/victor/Zotero/storage/7L887RQ8/1506.html},
  journal = {Journal of Computational Physics},
  keywords = {Statistics - Computation}
}

@article{nagler_evading_2016,
  title = {Evading the Curse of Dimensionality in Nonparametric Density Estimation with Simplified Vine Copulas},
  author = {Nagler, Thomas and Czado, Claudia},
  year = {2016},
  month = oct,
  volume = {151},
  pages = {69--89},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2016.07.003},
  abstract = {Practical applications of nonparametric density estimators in more than three dimensions suffer a great deal from the well-known curse of dimensionality: convergence slows down as dimension increases. We show that one can evade the curse of dimensionality by assuming a simplified vine copula model for the dependence between variables. We formulate a general nonparametric estimator for such a model and show under high-level assumptions that the speed of convergence is independent of dimension. We further discuss a particular implementation for which we validate the high-level assumptions and establish its asymptotic normality. Simulation experiments illustrate a large gain in finite sample performance when the simplifying assumption is at least approximately true. But even when it is severely violated, the vine copula based approach proves advantageous as soon as more than a few variables are involved. Lastly, we give an application of the estimator to a classification problem from astrophysics.},
  archivePrefix = {arXiv},
  eprint = {1503.03305},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/T63XS699/Nagler et Czado - 2016 - Evading the curse of dimensionality in nonparametr.pdf;/home/victor/Zotero/storage/NMMKIWIW/1503.html},
  journal = {Journal of Multivariate Analysis},
  keywords = {Density estimation}
}

@article{natarajan_constructing_2009,
  title = {Constructing {{Risk Measures}} from {{Uncertainty Sets}}},
  author = {Natarajan, Karthik and Pachamanova, Dessislava and Sim, Melvyn},
  year = {2009},
  month = oct,
  volume = {57},
  pages = {1129--1141},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.1080.0683},
  abstract = {We propose a unified theory that links uncertainty sets in robust optimization to risk measures in portfolio optimization. We illustrate the correspondence between uncertainty sets and some popular risk measures in finance, and show how robust optimization can be used to generalize the concepts of these measures. We also show that by using properly defined uncertainty sets in robust optimization models, one can in fact construct coherent risk measures. Our approach to creating coherent risk measures is easy to apply in practice, and computational experiments suggest that it may lead to superior portfolio performance. Our results have implications for efficient portfolio optimization under different measures of risk.},
  file = {/home/victor/Zotero/storage/7ASYSSKY/Natarajan et al. - 2009 - Constructing Risk Measures from Uncertainty Sets.pdf},
  journal = {Operations Research},
  language = {en},
  number = {5}
}

@phdthesis{ngodock_assimilation_1996,
  title = {{Assimilation de donn\'ees et analyse de sensibilit\'e. Une application \`a la circulation oc\'eanique}},
  author = {Ngodock, Hans Emmanuel},
  year = {1996},
  month = mar,
  abstract = {Le travail men\'e dans cette th\`ese porte sur l'\'etude "\`a posteriori" de l'assimilation variationnelle de donn\'ees. Il s'agit d'une d\'emarche de faisabilit\'e pour la mise au point des outils permettant de faire une analyse diagnostique (qualitative et quantitative) du processus d'assimilation variationnelle, notamment en ce qui concerne l'influence du bruit des observations sur le processus d'assimilation ainsi que sa propagation sur les champs reconstitu\'es (nous sommes alors amen\'es \`a faire une \'etude de sensibilit\'e), et l'influence de la configuration spatio-temporelle des observations sur le processus d'assimilation. L'application usuelle des \'equations adjointes pour l'analyse de sensibilit\'e est revis\'ee, car dans le contexte de l'assimilation variationnelle, nous avons montr\'e par un exemple simple qu'il faut s'y prendre diff\'eremment. Nous proposons alors une m\'ethode pour mener correctement cette analyse de sensibilit\'e. Cette m\'ethode est bas\'ee sur l'utilisation des \'equations adjointes au second ordre, obtenues en prenant l'adjoint du syst\`eme d'optimalit\'e. La sensibilit\'e en est d\'eduite par inversion du Hessien de la fonction co\^ut via la minimisation d'une fonctionnelle quadratique. L'application est faite sur un mod\`ele de circulation g\'en\'erale oc\'eanique de type quasi-g\'eostrophique, et nous faisons aussi l'\'etude de l'existence et l'unicit\'e de la solution de l'\'equation adjointe au second ordre du mod\`ele consid\'er\'e, pour justifier l'utilisation du Hessien et l'applicabilit\'e de notre m\'ethode. Nous \'etudions aussi l'influence de la configuration spatio-temporelle des observations sur le processus d'assimilation au travers du Hessien (\`a l'optimum) dont les \'el\'ements propres varient lorsqu'on fait varier la configuration. Enfin, nous \'etudions la pr\'edicibilit\'e du syst\`eme d'optimalit\'e.},
  file = {/home/victor/Zotero/storage/JWJL94ZA/Ngodock - 1996 - Assimilation de donn√©es et analyse de sensibilit√©..pdf;/home/victor/Zotero/storage/5YDEDP49/tel-00005006.html},
  keywords = {AS,Assimilation de donn√©es,Th√®se},
  language = {fr},
  school = {Universit\'e Joseph-Fourier - Grenoble I}
}

@unpublished{nguyen_nonparametric_2018,
  title = {Nonparametric Method for Sparse Conditional Density Estimation in Moderately Large Dimensions},
  author = {Nguyen, Minh-Lien Jeanne},
  year = {2018},
  month = jan,
  abstract = {In this paper, we consider the problem of estimating a conditional density in moderately large dimensions. Much more informative than regression functions, conditional densities are of main interest in recent methods, particularly in the Bayesian framework (studying the posterior distribution, finding its modes...). Considering a recently studied family of kernel estimators, we select a pointwise multivariate bandwidth by revisiting the greedy algorithm Rodeo (Regularisation Of Derivative Expectation Operator). The method addresses several issues: being greedy and computationally efficient by an iterative procedure, avoiding the curse of high dimensionality under some suitably defined sparsity conditions by early variable selection during the procedure, converging at a quasi-optimal minimax rate.},
  file = {/home/victor/Zotero/storage/4RZ5J4QG/Nguyen - 2018 - Nonparametric method for sparse conditional densit.pdf},
  keywords = {conditional density,Density estimation,greedy algorithm,kernel density estimators,nonparametric inference}
}

@incollection{nicosia_fast_2013,
  title = {Fast {{Computation}} of the {{Multi}}-{{Points Expected Improvement}} with {{Applications}} in {{Batch Selection}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David},
  editor = {Nicosia, Giuseppe and Pardalos, Panos},
  year = {2013},
  volume = {7997},
  pages = {59--69},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-44973-4_7},
  abstract = {The Multi-points Expected Improvement criterion (or q-EI) has recently been studied in batch-sequential Bayesian Optimization. This paper deals with a new way of computing q-EI, without using Monte-Carlo simulations, through a closed-form formula. The latter allows a very fast computation of q-EI for reasonably low values of q (typically, less than 10). New parallel kriging-based optimization strategies, tested on different toy examples, show promising results.},
  file = {/home/victor/Zotero/storage/WQFDMN8K/Chevalier et Ginsbourger - 2013 - Fast Computation of the Multi-Points Expected Impr.pdf},
  isbn = {978-3-642-44972-7 978-3-642-44973-4},
  language = {en}
}

@article{nielsen_cramer-rao_2013,
  title = {Cramer-{{Rao Lower Bound}} and {{Information Geometry}}},
  author = {Nielsen, Frank},
  year = {2013},
  month = jan,
  abstract = {This article focuses on an important piece of work of the world renowned Indian statistician, Calyampudi Radhakrishna Rao. In 1945, C. R. Rao (25 years old then) published a pathbreaking paper, which had a profound impact on subsequent statistical research.},
  archivePrefix = {arXiv},
  eprint = {1301.3578},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/IBXL2CD4/Nielsen - 2013 - Cramer-Rao Lower Bound and Information Geometry.pdf},
  journal = {arXiv:1301.3578 [cs, math]},
  keywords = {Computer Science - Information Theory},
  language = {en},
  primaryClass = {cs, math}
}

@misc{noauthor_[1209.2736]_nodate,
  title = {[1209.2736] {{The Ensemble Kalman Filter}} for {{Inverse Problems}}},
  file = {/home/victor/Zotero/storage/YUSX97KI/1209.html},
  howpublished = {https://arxiv.org/abs/1209.2736}
}

@misc{noauthor_[1703.04156]_nodate,
  title = {[1703.04156] {{A}} Trust-Region Method for Derivative-Free Nonlinear Constrained Stochastic Optimization},
  file = {/home/victor/Zotero/storage/2S3MXR35/1703.html},
  howpublished = {https://arxiv.org/abs/1703.04156}
}

@misc{noauthor_airsea_nodate,
  title = {{{AIRSEA}} \textendash{} {{Mathematics}} and Computing Applied to Oceanic and Atmospheric Flows},
  file = {/home/victor/Zotero/storage/MC3GRPCQ/en.html},
  language = {en-US}
}

@misc{noauthor_association_nodate,
  title = {{Association Bernard Gregory}},
  abstract = {Evolution professionnelle et recrutement des docteurs (PhD) \textendash{} Recruitment and career development of PhD holders.},
  file = {/home/victor/Zotero/storage/FE54Z7YG/fr.html},
  howpublished = {https://www.abg.asso.fr/fr/},
  language = {fr, en}
}

@misc{noauthor_density_nodate,
  title = {Density Power Divergence - {{Recherche Google}}},
  file = {/home/victor/Zotero/storage/P3UESICT/search.html},
  howpublished = {https://www.google.com/search?client=ubuntu\&channel=fs\&q=density+power+divergence\&ie=utf-8\&oe=utf-8}
}

@misc{noauthor_learning_nodate,
  title = {Learning {{Poisson Binomial Distributions}}},
  file = {/home/victor/Zotero/storage/VDTTXQGL/stoc12pbd.html},
  howpublished = {http://www.cs.columbia.edu/\textasciitilde rocco/papers/stoc12pbd.html}
}

@article{noauthor_letat_nodate,
  title = {{L'\'etat de l'emploi scientifique en France}},
  pages = {200},
  file = {/home/victor/Zotero/storage/W5975FAL/L‚Äô√©tat de l'emploi scientifique en France.pdf},
  language = {fr}
}

@article{noauthor_loi_2018,
  title = {{Loi normale g\'en\'eralis\'ee}},
  year = {2018},
  month = jul,
  abstract = {En th\'eorie des probabilit\'es et en statistique, la loi normale g\'en\'eralis\'ee ou loi gaussienne g\'en\'eralis\'ee d\'esigne deux familles de lois de probabilit\'e \`a densit\'e dont les supports sont l'ensemble des r\'eels. Cette loi rajoute un param\`etre de forme \`a la loi normale. Pour les diff\'erencier, les deux familles seront appel\'ees \guillemotleft{} version 1 \guillemotright{} et \guillemotleft{} version 2 \guillemotright, ce ne sont cependant pas des appellations standards.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/home/victor/Zotero/storage/Q44Z4MY6/index.html},
  journal = {Wikip\'edia},
  language = {fr}
}

@misc{noauthor_modeling_nodate,
  title = {Modeling - {{Why}} Should {{I}} Be {{Bayesian}} When My Model Is Wrong?},
  file = {/home/victor/Zotero/storage/ZCPTGJZK/why-should-i-be-bayesian-when-my-model-is-wrong.html},
  howpublished = {https://stats.stackexchange.com/questions/274815/why-should-i-be-bayesian-when-my-model-is-wrong},
  journal = {Cross Validated}
}

@misc{noauthor_optimization_nodate,
  title = {Optimization under Uncertainty: State-of-the-Art and Opportunities - {{ScienceDirect}}},
  file = {/home/victor/Zotero/storage/RKSNRNZE/S0098135403002369.html},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0098135403002369}
}

@misc{noauthor_pdf_nodate,
  title = {({{PDF}}) {{Learning Feature}}-{{Parameter Mappings}} for {{Parameter Tuning}} via the {{Profile Expected Improvement}}},
  doi = {http://dx.doi.org/10.1145/2739480.2754673},
  abstract = {PDF | The majority of algorithms can be controlled or adjusted by parameters. Their values can substantially affect the algorithms' performance. Since the manual exploration of the parameter space is tedious -- even for few parameters -- several automatic procedures for parameter...},
  file = {/home/victor/Zotero/storage/WYC2QZY3/280076463_Learning_Feature-Parameter_Mappings_for_Parameter_Tuning_via_the_Profile_Expected_Imp.html},
  howpublished = {https://www.researchgate.net/publication/280076463\_Learning\_Feature-Parameter\_Mappings\_for\_Parameter\_Tuning\_via\_the\_Profile\_Expected\_Improvement},
  journal = {ResearchGate},
  language = {en}
}

@article{noauthor_post-modern_2018,
  title = {Post-Modern Portfolio Theory},
  year = {2018},
  month = sep,
  abstract = {Post-modern portfolio theory (or PMPT) is an extension of the traditional modern portfolio theory (MPT, which is an application of mean-variance analysis or MVA). Both theories propose how rational investors should use diversification to optimize their portfolios, and how a risky asset should be priced.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/home/victor/Zotero/storage/6SXFXTKC/index.html},
  journal = {Wikipedia},
  language = {en}
}

@misc{noauthor_probability_nodate,
  title = {Probability - {{Sum}} of {{Bernoulli}} Random Variables with Different Success Probabilities},
  file = {/home/victor/Zotero/storage/LG4XNV2F/sum-of-bernoulli-random-variables-with-different-success-probabilities.html},
  howpublished = {https://math.stackexchange.com/questions/392860/sum-of-bernoulli-random-variables-with-different-success-probabilities},
  journal = {Mathematics Stack Exchange}
}

@article{noauthor_profile_nodate,
  title = {On {{Profile Likelihood}}},
  pages = {18},
  file = {/home/victor/Zotero/storage/LYQFCP87/On Profile Likelihood.pdf},
  keywords = {Profile lik},
  language = {en}
}

@misc{noauthor_savage_nodate,
  title = {Savage: {{The}} Theory of Statistical Decision - {{Google Scholar}}},
  file = {/home/victor/Zotero/storage/AIZINKIG/scholar_lookup.html},
  howpublished = {https://scholar.google.com/scholar\_lookup?hl=en\&volume=46\&publication\_year=1951\&pages=55-67\&journal=Journal+of+the+American+Statistical+Association\&issue=253\&author=L.+J.+Savage\&title=The+theory+of+statistical+decision}
}

@misc{noauthor_shogun-toolbox/shogun_nodate,
  title = {Shogun-Toolbox/Shogun},
  abstract = {Sh\=ogun. Contribute to shogun-toolbox/shogun development by creating an account on GitHub.},
  file = {/home/victor/Zotero/storage/RGHC5AZS/GSoC_2016_project_large_gps.html},
  howpublished = {https://github.com/shogun-toolbox/shogun},
  journal = {GitHub},
  language = {en}
}

@misc{noauthor_structuring_nodate,
  title = {Structuring {{Your Project}} \textemdash{} {{The Hitchhiker}}'s {{Guide}} to {{Python}}},
  file = {/home/victor/Zotero/storage/QRS5EI7R/structure.html},
  howpublished = {https://docs.python-guide.org/writing/structure}
}

@misc{noauthor_time_nodate,
  title = {Time Series - {{How}} to You Measure the Accuracy of a Model That Gives Quantile Forecasts or Distributions of Forecasts?},
  file = {/home/victor/Zotero/storage/TS5CGX9R/how-to-you-measure-the-accuracy-of-a-model-that-gives-quantile-forecasts-or-dist.html},
  howpublished = {https://stats.stackexchange.com/questions/367306/how-to-you-measure-the-accuracy-of-a-model-that-gives-quantile-forecasts-or-dist?rq=1},
  journal = {Cross Validated}
}

@misc{noauthor_trappler_nodate,
  title = {{{TRAPPLER Victor}} / {{These}}},
  abstract = {Gitlab at INRIA},
  file = {/home/victor/Zotero/storage/MHBSY49A/These.html},
  howpublished = {https://gitlab.inria.fr/vtrapple/These},
  journal = {GitLab},
  language = {en}
}

@misc{noauthor_virtual_nodate,
  title = {Virtual {{Library}} of {{Simulation Experiments}}: {{Test Functions}} and {{Datasets}}},
  file = {/home/victor/Zotero/storage/EN5EEF58/index.html},
  howpublished = {https://www.sfu.ca/\textasciitilde ssurjano/index.html}
}

@misc{noauthor_wilks_nodate,
  title = {Wilks Estimate Quantile - {{Recherche Google}}},
  file = {/home/victor/Zotero/storage/3JTKPD57/search.html},
  howpublished = {https://www.google.com/search?client=ubuntu\&hs=2sf\&channel=fs\&biw=1867\&bih=982\&ei=yF3hW-f3DszMgAaf5YqABA\&q=wilks+estimate+quantile\&oq=wilks+estimate+quantile\&gs\_l=psy-ab.3..33i21k1l2.193064.194150.0.194190.9.8.0.0.0.0.165.665.6j1.7.0....0...1c.1.64.psy-ab..2.7.664...33i160k1.0.x0NctscuA9I}
}

@phdthesis{oakley_bayesian_1999,
  title = {Bayesian Uncertainty Analysis for Complex Computer Codes},
  author = {Oakley, Jeremy},
  year = {1999},
  file = {/home/victor/Zotero/storage/GDRTYMEY/jeothesis.pdf},
  keywords = {Bayesian inference},
  school = {University of Sheffield},
  type = {{{PhD Thesis}}}
}

@article{oberkampf_measures_2006,
  title = {Measures of Agreement between Computation and Experiment: {{Validation}} Metrics},
  shorttitle = {Measures of Agreement between Computation and Experiment},
  author = {Oberkampf, William L. and Barone, Matthew F.},
  year = {2006},
  month = sep,
  volume = {217},
  pages = {5--36},
  issn = {00219991},
  doi = {10.1016/j.jcp.2006.03.037},
  abstract = {With the increasing role of computational modeling in engineering design, performance estimation, and safety assessment, improved methods are needed for comparing computational results and experimental measurements. Traditional methods of graphically comparing computational and experimental results, though valuable, are essentially qualitative. Computable measures are needed that can quantitatively compare computational and experimental results over a range of input, or control, variables to sharpen assessment of computational accuracy. This type of measure has been recently referred to as a validation metric. We discuss various features that we believe should be incorporated in a validation metric, as well as features that we believe should be excluded. We develop a new validation metric that is based on the statistical concept of confidence intervals. Using this fundamental concept, we construct two specific metrics: one that requires interpolation of experimental data and one that requires regression (curve fitting) of experimental data. We apply the metrics to three example problems: thermal decomposition of a polyurethane foam, a turbulent buoyant plume of helium, and compressibility effects on the growth rate of a turbulent free-shear layer. We discuss how the present metrics are easily interpretable for assessing computational model accuracy, as well as the impact of experimental measurement uncertainty on the accuracy assessment.},
  file = {/home/victor/Zotero/storage/LM68SXBZ/Oberkampf et Barone - 2006 - Measures of agreement between computation and expe.pdf},
  journal = {Journal of Computational Physics},
  language = {en},
  number = {1}
}

@article{ogryczak_stochastic_nodate,
  title = {On {{Stochastic Dominance}} and {{Mean}}-{{Semideviation Models}}},
  author = {Ogryczak, W{\l}odzimierz and Ogryczak, Wlodzimierz},
  pages = {15},
  abstract = {We analyse relations between two methods frequently used for modeling the choice among uncertain outcomes: stochastic dominance and mean\{risk approaches. The concept of -consistency of these approaches is de ned as the consistency within a bounded range of mean\{risk trade-o s. We show that mean\{risk models using central semideviations as risk measures are -consistent with stochastic dominance relations of the corresponding degree if the trade-o coe cient for the semideviation is bounded by one.\vphantom{\}\}\}}},
  file = {/home/victor/Zotero/storage/P37ZLUIU/Ogryczak et Ogryczak - On Stochastic Dominance and Mean-Semideviation Mod.pdf},
  language = {en}
}

@article{oliveira_bayesian_2019,
  title = {Bayesian Optimisation under Uncertain Inputs},
  author = {Oliveira, Rafael and Ott, Lionel and Ramos, Fabio},
  year = {2019},
  month = feb,
  abstract = {Bayesian optimisation (BO) has been a successful approach to optimise functions which are expensive to evaluate and whose observations are noisy. Classical BO algorithms, however, do not account for errors about the location where observations are taken, which is a common issue in problems with physical components. In these cases, the estimation of the actual query location is also subject to uncertainty. In this context, we propose an upper confidence bound (UCB) algorithm for BO problems where both the outcome of a query and the true query location are uncertain. The algorithm employs a Gaussian process model that takes probability distributions as inputs. Theoretical results are provided for both the proposed algorithm and a conventional UCB approach within the uncertain-inputs setting. Finally, we evaluate each method's performance experimentally, comparing them to other input noise aware BO approaches on simulated scenarios involving synthetic and real data.},
  archivePrefix = {arXiv},
  eprint = {1902.07908},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/GMIN3HSI/Oliveira et al. - 2019 - Bayesian optimisation under uncertain inputs.pdf},
  journal = {arXiv:1902.07908 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{orbanz_nonparametric_2008,
  title = {Nonparametric {{Bayesian Image Segmentation}}},
  author = {Orbanz, Peter and Buhmann, Joachim M.},
  year = {2008},
  month = may,
  volume = {77},
  pages = {25--45},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-007-0061-0},
  abstract = {Image segmentation algorithms partition the set of pixels of an image into a specific number of different, spatially homogeneous groups. We propose a nonparametric Bayesian model for histogram clustering which automatically determines the number of segments when spatial smoothness constraints on the class assignments are enforced by a Markov Random Field. A Dirichlet process prior controls the level of resolution which corresponds to the number of clusters in data with a unique cluster structure. The resulting posterior is efficiently sampled by a variant of a conjugate-case sampling algorithm for Dirichlet process mixture models. Experimental results are provided for real-world gray value images, synthetic aperture radar images and magnetic resonance imaging data.},
  file = {/home/victor/Zotero/storage/QAEDKYMY/porbanz_OrbanzBuhmann_2008_1.pdf;/home/victor/Zotero/storage/D96J2TFF/10.html},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {1-3}
}

@article{ortega_thermodynamics_2013,
  title = {Thermodynamics as a Theory of Decision-Making with Information Processing Costs},
  author = {Ortega, Pedro A. and Braun, Daniel A.},
  year = {2013},
  month = may,
  volume = {469},
  pages = {20120683},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2012.0683},
  abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here we propose an information-theoretic formalization of bounded rational decision-making where decision-makers trade off expected utility and information processing costs. Such bounded rational decision-makers can be thought of as thermodynamic machines that undergo physical state changes when they compute. Their behavior is governed by a free energy functional that trades off changes in internal energy\textemdash as a proxy for utility\textemdash and entropic changes representing computational costs induced by changing states. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known concepts from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss the relation to satisficing decision-making procedures as well as links to existing theoretical frameworks and human decision-making experiments that describe deviations from expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to axiomatically derive and interpret the thermodynamic free energy as a model of bounded rational decision-making.},
  archivePrefix = {arXiv},
  eprint = {1204.6481},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/BITHKJGS/Ortega et Braun - 2013 - Thermodynamics as a theory of decision-making with.pdf},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  keywords = {Mathematics - Optimization and Control,Mathematics - Statistics Theory},
  language = {en},
  number = {2153}
}

@article{over_strategy_2013,
  title = {A Strategy for Improved Computational Efficiency of the Method of Anchored Distributions},
  author = {Over, Matthew William and Yang, Yarong and Chen, Xingyuan and Rubin, Yoram},
  year = {2013},
  month = jun,
  volume = {49},
  pages = {3257--3275},
  issn = {1944-7973},
  doi = {10.1002/wrcr.20182},
  abstract = {This paper proposes a strategy for improving the computational efficiency of model inversion using the method of anchored distributions (MAD) by ``bundling'' similar model parametrizations in the likelihood function. Inferring the likelihood function typically requires a large number of forward model (FM) simulations for each possible model parametrization; as a result, the process is quite expensive. To ease this prohibitive cost, we present an approximation for the likelihood function called bundling that relaxes the requirement for high quantities of FM simulations. This approximation redefines the conditional statement of the likelihood function as the probability of a set of similar model parametrizations ``bundle'' replicating field measurements, which we show is neither a model reduction nor a sampling approach to improving the computational efficiency of model inversion. To evaluate the effectiveness of these modifications, we compare the quality of predictions and computational cost of bundling relative to a baseline MAD inversion of 3-D flow and transport model parameters. Additionally, to aid understanding of the implementation we provide a tutorial for bundling in the form of a sample data set and script for the R statistical computing language. For our synthetic experiment, bundling achieved a 35\% reduction in overall computational cost and had a limited negative impact on predicted probability distributions of the model parameters. Strategies for minimizing error in the bundling approximation, for enforcing similarity among the sets of model parametrizations, and for identifying convergence of the likelihood function are also presented.},
  file = {/home/victor/Zotero/storage/68NCTQID/Over et al. - 2013 - A strategy for improved computational efficiency o.pdf;/home/victor/Zotero/storage/HXLC3EW5/abstract.html},
  journal = {Water Resources Research},
  keywords = {approximation,Bayesian,clustering,computational efficiency,inversion modeling},
  language = {en},
  number = {6}
}

@techreport{paquet_bayesian_2008,
  title = {Bayesian Inference for Latent Variable Models},
  author = {Paquet, Ulrich},
  year = {2008},
  institution = {{University of Cambridge, Computer Laboratory}},
  file = {/home/victor/Zotero/storage/9S33KVNC/UCAM-CL-TR-724.pdf}
}

@incollection{pardo-iguzquiza_corrected_2014,
  title = {Corrected {{Kriging Update Formulae}} for {{Batch}}-{{Sequential Data Assimilation}}},
  booktitle = {Mathematics of {{Planet Earth}}},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David and Emery, Xavier},
  editor = {{Pardo-Ig{\'u}zquiza}, Eulogio and {Guardiola-Albert}, Carolina and Heredia, Javier and {Moreno-Merino}, Luis and Dur{\'a}n, Juan Jos{\'e} and {Vargas-Guzm{\'a}n}, Jose Antonio},
  year = {2014},
  pages = {119--122},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32408-6_29},
  file = {/home/victor/Zotero/storage/LYXH8TW3/Chevalier et al. - 2014 - Corrected Kriging Update Formulae for Batch-Sequen.pdf},
  isbn = {978-3-642-32407-9 978-3-642-32408-6},
  language = {en}
}

@book{park_robust_2006,
  title = {Robust {{Design}}: {{An Overview}}},
  shorttitle = {Robust {{Design}}},
  author = {Park, Gyung-Jin and Hwang, Kwang-Hyeon and Lee, Tae and Lee, Kwon-Hee},
  year = {2006},
  month = jan,
  volume = {44},
  doi = {10.2514/1.13639},
  abstract = {Robust design has been developed with the expectation that an insensitive design can be obtained. That is, a product designed by robust design should be insensitive to external noises or tolerances. An insensitive design has more probability to obtain a target value, although there are uncertain noises. Theories of robust design have been developed by adopting the theories of other fields. Based on the theories, robust design can be classified into three methods: 1) the Taguchi method, 2) robust optimization, and 3) robust design with the axiomatic approach. Each method is reviewed and investigated. The methods are examined from a theoretical viewpoint and are discussed from an application viewpoint. The advantages and drawbacks of each method are discussed, and future directions for development are proposed. Copyright \textcopyright{} 2005 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.},
  keywords = {overview,Robust optimization}
}

@article{parzen_estimation_1962,
  title = {On Estimation of a Probability Density Function and Mode},
  author = {Parzen, Emanuel},
  year = {1962},
  volume = {33},
  pages = {1065--1076},
  file = {/home/victor/Zotero/storage/LPM2WCGR/Parzen - 1962 - On estimation of a probability density function an.pdf;/home/victor/Zotero/storage/JAE2F2MP/2237880.html},
  journal = {The annals of mathematical statistics},
  number = {3}
}

@phdthesis{pastel_estimation_2012,
  title = {Estimation de Probabilit\'es d'\'ev\`enements Rares et de Quantiles Extr\^emes : Applications Dans Le Domaine A\'erospatial},
  shorttitle = {Estimation de Probabilit\'es d'\'ev\`enements Rares et de Quantiles Extr\^emes},
  author = {Pastel, Rudy},
  year = {2012},
  month = jan,
  abstract = {Rare event dedicated techniques are of great interest for the aerospace industry because of the large amount money that can be lost because of risks associated with minute probabilities. This thesis is focused on the search of probability techniques able to estimate rare event probabilities and extreme quantiles associated with a black box system with static random inputs through two case studies from the industry. The first one is the estimation of the probability of collision between satellites Iridium and Cosmos. The Cross-Entropy (CE), the Non-parametric Adaptive Importance Sampling (NAIS) and an Adaptive Splitting Technique (AST) are compared. Through the comparison, an improved version of NAIS is designed. Whereas NAIS needs to be initiated with a auxiliary random variable which straight away generates rare events, the Adaptive NAIS (ANAIS) allows one to use the original input random as initial auxiliary density and therefore does not require a priori knowledge. The second case is the estimation of the safety zone with respect to the fall of a spacecraft booster. Though they can be estimated via ANAIS or AST, extreme quantiles are shown to be not adapted to spatial distribution. For that purpose, the Minimum Volume Set (MVS) is chosen from the literature. The Crude Monte Carlo (CMC) plug-in MVS estimator being not adapted to extreme level MVS estimation, both ANAIS and AST are adapted into plug-in extreme MVS estimators. These two algorithms outperform the CMC plug-in MVS estimator.},
  file = {/home/victor/Zotero/storage/BYSZJNMY/2012REN1S024.html},
  journal = {http://www.theses.fr},
  keywords = {D√©bris spatiaux,√âchantillonnage adaptatif (statistique),√âvaluation du risque,Math√©matiques et applications,Monte-Carlo; M√©thode de},
  school = {Rennes 1},
  type = {Thesis}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  month = oct,
  volume = {12},
  pages = {2825-2830},
  issn = {1533-7928},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {/home/victor/Zotero/storage/XZ34DBFU/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf},
  journal = {Journal of Machine Learning Research}
}

@phdthesis{pellerej_etude_2018,
  title = {{Etude et d\'eveloppement d'algorithmes d'assimilation de donn\'ees variationnelle adapt\'es aux mod\`eles coupl\'es oc\'ean-atmosph\`ere}},
  author = {Pellerej, R{\'e}mi},
  year = {2018},
  month = mar,
  abstract = {La qualit\'e des pr\'evisions m\'et\'eorologiques repose principalement sur la qualit\'e du mod\`ele utilis\'e et de son \'etat initial. Cet \'etat initial est reconstitu\'e en combinant les informations provenant du mod\`ele et des observations disponibles en utilisant des techniques d'assimilation de donn\'ees. Historiquement, les pr\'evisions et l'assimilation sont r\'ealis\'ees dans l'atmosph\`ere et l'oc\'ean de mani\`ere d\'ecoupl\'ee. Cependant, les centres op\'erationnels d\'eveloppent et utilisent de plus en plus des mod\`eles coupl\'es oc\'ean-atmosph\`ere. Or, assimiler des donn\'ees de mani\`ere d\'ecoupl\'ee n'est pas satisfaisant pour des syst\`emes coupl\'es. En effet, l'\'etat initial ainsi obtenu pr\'esente des inconsistances de flux \`a l'interface entre les milieux, engendrant des erreurs de pr\'evision. Il y a donc besoin d'adapter les m\'ethodes d'assimilation aux syst\`emes coupl\'es. Ces travaux de th\`ese s'inscrivent dans ce contexte et ont \'et\'e effectu\'es dans le cadre du projet FP7 ERA-Clim2, visant \`a produire une r\'eanalyse globale du syst\`eme terrestre.Dans une premi\`ere partie, nous introduisons les notions d'assimilation de donn\'ees, de couplage et les diff\'erentes m\'ethodologies existantes appliqu\'ees au probl\`eme de l'assimilation coupl\'ee. Ces m\'ethodologies n'\'etant pas satisfaisantes en terme de qualit\'e de couplage ou de co\^ut de calcul, nous proposons, dans une seconde partie, des m\'ethodes alternatives. Nous faisons le choix de m\'ethodes d'assimilation bas\'ees sur la th\'eorie du contr\^ole optimal. Ces alternatives se distinguent alors par le choix de la fonction co\^ut \`a minimiser, des variables contr\^ol\'ees et de l'algorithme de couplage utilis\'e. Une \'etude th\'eorique de ces algorithmes a permis de d\'eterminer un crit\`ere n\'ecessaire et suffisant de convergence dans un cadre lin\'eaire. Pour conclure cette seconde partie, les performances des diff\'erentes m\'ethodes introduites sont \'evalu\'ees en terme de qualit\'e de l'analyse produite et de co\^ut de calcul \`a l'aide d'un mod\`ele coupl\'e lin\'eaire 1D. Dans une troisi\`eme et derni\`ere partie, un mod\`ele coupl\'e non-lin\'eaire 1D incluant des param\'etrisations physique a \'et\'e d\'evelopp\'e et impl\'ement\'e dans OOPS (textit\{Object-Oriented Prediction System\}) qui est une surcouche logicielle permettant la mise en \oe uvre d'un ensemble d'algorithmes d'assimilation de donn\'ees. Nous avons alors pu \'evaluer la robustesse de nos algorithmes dans un cadre plus r\'ealiste, et conclure sur leurs performances vis \`a vis de m\'ethodes existantes. Le fait d'avoir d\'evelopp\'e nos m\'ethodes dans le cadre de OOPS devrait permettre \`a l'avenir de les appliquer ais\'ement \`a des mod\`eles r\'ealistes de pr\'evision. Nous exposons enfin quelques perspectives d'am\'elioration de ces algorithmes.},
  file = {/home/victor/Zotero/storage/LE54ZN4X/Pellerej - 2018 - Etude et d√©veloppement d'algorithmes d'assimilatio.pdf;/home/victor/Zotero/storage/9YS7EXAN/tel-01897347.html},
  language = {fr},
  school = {Universit\'e Grenoble Alpes}
}

@article{penny_bayesian_2014,
  title = {Bayesian {{Inference}} for the {{Multivariate Normal}}},
  author = {Penny, Will},
  year = {2014},
  file = {/home/victor/Zotero/storage/F5IYKDPX/bmn.pdf}
}

@misc{perren_gabriel-ppythonmcmc_2020,
  title = {Gabriel-p/{{pythonMCMC}}},
  author = {Perren, Gabriel},
  year = {2020},
  month = may,
  abstract = {A list of Python-based MCMC packages. Contribute to Gabriel-p/pythonMCMC development by creating an account on GitHub.},
  copyright = {GPL-3.0}
}

@misc{perrier_robust_,
  title = {Robust {{Bayesian}} Filter with Student-t Likelihood Noise},
  author = {Perrier, Regis},
  file = {/home/victor/Documents/robustBF.pdf}
}

@article{petra_computational_2013,
  title = {A Computational Framework for Infinite-Dimensional {{Bayesian}} Inverse Problems: {{Part II}}. {{Stochastic Newton MCMC}} with Application to Ice Sheet Flow Inverse Problems},
  shorttitle = {A Computational Framework for Infinite-Dimensional {{Bayesian}} Inverse Problems},
  author = {Petra, Noemi and Martin, James and Stadler, Georg and Ghattas, Omar},
  year = {2013},
  month = aug,
  abstract = {We address the numerical solution of infinite-dimensional inverse problems in the framework of Bayesian inference. In the Part I companion to this paper (arXiv.org:1308.1313), we considered the linearized infinite-dimensional inverse problem. Here in Part II, we relax the linearization assumption and consider the fully nonlinear infinite-dimensional inverse problem using a Markov chain Monte Carlo (MCMC) sampling method. To address the challenges of sampling high-dimensional pdfs arising from Bayesian inverse problems governed by PDEs, we build on the stochastic Newton MCMC method. This method exploits problem structure by taking as a proposal density a local Gaussian approximation of the posterior pdf, whose construction is made tractable by invoking a low-rank approximation of its data misfit component of the Hessian. Here we introduce an approximation of the stochastic Newton proposal in which we compute the low-rank-based Hessian at just the MAP point, and then reuse this Hessian at each MCMC step. We compare the performance of the proposed method to the original stochastic Newton MCMC method and to an independence sampler. The comparison of the three methods is conducted on a synthetic ice sheet inverse problem. For this problem, the stochastic Newton MCMC method with a MAP-based Hessian converges at least as rapidly as the original stochastic Newton MCMC method, but is far cheaper since it avoids recomputing the Hessian at each step. On the other hand, it is more expensive per sample than the independence sampler; however, its convergence is significantly more rapid, and thus overall it is much cheaper. Finally, we present extensive analysis and interpretation of the posterior distribution, and classify directions in parameter space based on the extent to which they are informed by the prior or the observations.},
  archivePrefix = {arXiv},
  eprint = {1308.6221},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/Z22783B7/Petra et al. - 2013 - A computational framework for infinite-dimensional.pdf;/home/victor/Zotero/storage/C555J7FY/1308.html},
  journal = {arXiv:1308.6221 [math, stat]},
  primaryClass = {math, stat}
}

@article{petrone_robustness_2011,
  title = {Robustness Criteria in Optimization under Uncertainty},
  author = {Petrone, Giovanni and Iaccarino, Gianluca and Quagliarella, D.},
  year = {2011},
  pages = {244--252},
  file = {/home/victor/Zotero/storage/BBSY2QRE/Petrone et al. - 2011 - Robustness criteria in optimization under uncertai.pdf},
  journal = {Evolutionary and deterministic methods for design, optimization and control (EUROGEN 2011). CIRA, Capua}
}

@article{peyre_computational_2019,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2019},
  month = apr,
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archivePrefix = {arXiv},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/326ZB4MZ/Peyr√© et Cuturi - 2019 - Computational Optimal Transport.pdf;/home/victor/Zotero/storage/RXCU72DH/1803.html},
  journal = {arXiv:1803.00567 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{peyre_computational_2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2020},
  month = mar,
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archivePrefix = {arXiv},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/DHFNKVGH/Peyr√© et Cuturi - 2020 - Computational Optimal Transport.pdf;/home/victor/Zotero/storage/XXCUZ4P9/1803.html},
  journal = {arXiv:1803.00567 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@incollection{pflug_remarks_2000,
  title = {Some Remarks on the Value-at-Risk and the Conditional Value-at-Risk},
  booktitle = {Probabilistic Constrained Optimization},
  author = {Pflug, Georg Ch},
  year = {2000},
  pages = {272--281},
  publisher = {{Springer}},
  file = {/home/victor/Zotero/storage/7K85S8BF/Pflug - 2000 - Some remarks on the value-at-risk and the conditio.pdf;/home/victor/Zotero/storage/PWB3L5GT/978-1-4757-3150-7_15.html}
}

@article{piano_tidal_2017,
  title = {Tidal Stream Resource Assessment Uncertainty Due to Flow Asymmetry and Turbine Yaw Misalignment},
  author = {Piano, M. and Neill, S. P. and Lewis, M. J. and Robins, P. E. and Hashemi, M. R. and Davies, A. G. and Ward, S. L. and Roberts, M. J.},
  year = {2017},
  month = dec,
  volume = {114},
  pages = {1363--1375},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2017.05.023},
  abstract = {The majority of tidal energy convertors (TECs) currently under development are of a non-yawing horizontal axis design. However, most energetic regions that have been identified as candidate sites for installation of TEC arrays exhibit some degree of directional and magnitude asymmetry between incident flood and ebb flow angles and velocities, particularly in nearshore environments where topographic, bathymetric and seabed frictional effects and interactions are significant. Understanding the contribution of directional and magnitude asymmetry to resource power density along with off axis rotor alignment to flow could influence site selection and help elucidate optimal turbine orientation. Here, 2D oceanographic model simulations and field data were analysed to investigate these effects at potential deployment locations in the Irish Sea; an energetic semi-enclosed shelf sea region. We find that observed sites exhibiting a high degree of asymmetry may be associated with a reduction of over 2\% in annual energy yield when deployment design optimisation is ignored. However, at the majority of sites, even in the presence of significant asymmetry, the difference is {$<$}0.3\%. Although the effects are shown to have less significance than other uncertainties in resource assessment, these impacts could be further investigated and quantified using CFD and 3D modelling.},
  file = {/home/victor/Zotero/storage/B5I6CFNP/Piano et al. - 2017 - Tidal stream resource assessment uncertainty due t.pdf;/home/victor/Zotero/storage/KJQD76MP/S0960148117304081.html},
  journal = {Renewable Energy},
  keywords = {Marine renewable energy,Telemac 2D Irish Sea hydrodynamic modelling and ADCP observations,Tidal flow asymmetry,Tidal resource assessment and optimisation,Tidal stream characterisation,Turbine yaw misalignment},
  language = {en}
}

@article{picheny_adaptive_2010,
  title = {Adaptive {{Designs}} of {{Experiments}} for {{Accurate Approximation}} of a {{Target Region}}},
  author = {Picheny, Victor and Ginsbourger, David and Roustant, Olivier and Haftka, Raphael T. and Kim, Nam-Ho},
  year = {2010},
  volume = {132},
  pages = {071008},
  issn = {10500472},
  doi = {10.1115/1.4001873},
  file = {/home/victor/Zotero/storage/YLSGBQYE/Picheny et al. - 2010 - Adaptive Designs of Experiments for Accurate Appro.pdf},
  journal = {Journal of Mechanical Design},
  language = {en},
  number = {7}
}

@article{picheny_benchmark_2013,
  title = {A Benchmark of Kriging-Based Infill Criteria for Noisy Optimization},
  author = {Picheny, Victor and Wagner, Tobias and Ginsbourger, David},
  year = {2013},
  month = sep,
  volume = {48},
  pages = {607--626},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-013-0919-4},
  abstract = {Responses of many real-world problems can only be evaluated perturbed by noise. In order to make an efficient optimization of these problems possible, intelligent optimization strategies successfully coping with noisy evaluations are required. In this article, a comprehensive comparison of existing kriging-based methods for the optimization of noisy functions is provided. Ten methods are described using a unified formalism, and compared on analytical benchmark problems with different configurations (noise level, maximum number of observations, initial number of observations). It is found that the optimal method depends on the optimization problem, even though some criteria are consistently more efficient than others.},
  file = {/home/victor/Zotero/storage/B65I8QBM/Picheny et al. - 2013 - A benchmark of kriging-based infill criteria for n.pdf},
  journal = {Structural and Multidisciplinary Optimization},
  language = {en},
  number = {3}
}

@article{pinar_mean_2014,
  title = {Mean Semi-Deviation from a Target and Robust Portfolio Choice under Distribution and Mean Return Ambiguity},
  author = {P{\i}nar, Mustafa {\c C}. and Burak Pa{\c c}, A.},
  year = {2014},
  month = mar,
  volume = {259},
  pages = {394--405},
  issn = {0377-0427},
  doi = {10.1016/j.cam.2013.06.028},
  abstract = {We consider the problem of optimal portfolio choice using the lower partial moments risk measure for a market consisting of n risky assets and a riskless asset. For when the mean return vector and variance/covariance matrix of the risky assets are specified without specifying a return distribution, we derive distributionally robust portfolio rules. We then address potential uncertainty (ambiguity) in the mean return vector as well, in addition to distribution ambiguity, and derive a closed-form portfolio rule for when the uncertainty in the return vector is modelled via an ellipsoidal uncertainty set. Our result also indicates a choice criterion for the radius of ambiguity of the ellipsoid. Using the adjustable robustness paradigm we extend the single-period results to multiple periods, and derive closed-form dynamic portfolio policies which mimic closely the single-period policy.},
  file = {/home/victor/Zotero/storage/3ELPEWAP/Pƒ±nar et Burak Pa√ß - 2014 - Mean semi-deviation from a target and robust portf.pdf;/home/victor/Zotero/storage/9JNPBK3H/S0377042713003269.html},
  journal = {Journal of Computational and Applied Mathematics},
  keywords = {Adjustable robustness,Distributional robustness,Dynamic portfolio rules,Ellipsoidal uncertainty,Lower partial moments,Portfolio choice},
  series = {Recent {{Advances}} in {{Applied}} and {{Computational Mathematics}}: {{ICACM}}-{{IAM}}-{{METU}}}
}

@article{pogany_characteristic_2010,
  title = {On the Characteristic Function of the Generalized Normal Distribution},
  author = {Pog{\'a}ny, Tibor K. and Nadarajah, Saralees},
  year = {2010},
  month = feb,
  volume = {348},
  pages = {203--206},
  issn = {1631073X},
  doi = {10.1016/j.crma.2009.12.010},
  abstract = {For the first time, an explicit closed form expression is derived for the characteristic function of the generalized normal distribution (GND). Also derived is an expression for the correlation coefficient between variate-values and their ranks in samples from the GND. The expression for the former involves the Fox-Wright generalized confluent hypergeometric 1{$\Psi$}0-function, while the latter is expressed via the Gaussian hypergeometric 2F1.},
  file = {/home/victor/Zotero/storage/D93CMEFF/Pog√°ny et Nadarajah - 2010 - On the characteristic function of the generalized .pdf},
  journal = {Comptes Rendus Mathematique},
  language = {en},
  number = {3-4}
}

@article{polavarapu_data_2005,
  title = {Data Assimilation with the {{Canadian Middle Atmosphere Model}}},
  author = {Polavarapu, Saroja and Ren, Shuzhan and Rochon, Y and Sankey, David and Ek, Nils and Koshyk, John and Tarasick, David},
  year = {2005},
  month = mar,
  volume = {43},
  pages = {77--100},
  doi = {10.3137/ao.430105},
  abstract = {A data assimilation scheme has been coupled to the Canadian Middle Atmosphere Model, providing, for the first time, the capability of assimilating data from the ground to the top of the mesosphere (about 95 km). This model is a full general circulation model with on-line fully interactive chemistry involving 127 gas-phase and heterogeneous reactions. Thus, feedback between dynamics, chemistry and radiation occurs in every model time step. In this work, validation of the system for tropospheric and lower stratospheric analyses is undertaken with the standard observation set used in operational weather forecasting. Results are found to agree reasonably well with radiosonde observations and with Met Office (UK) analyses. Although ozone is not assimilated, ozone fields match total column observations well in terms of synoptic patterns. However, due to model biases, total column values are too large at mid-latitudes and too small in the tropics. Since the assimilation scheme was designed for tropospheric weather prediction, its application to a middle atmosphere model can help to identify the challenges of assimilating data from this region of the atmosphere.},
  file = {/home/victor/Zotero/storage/2XL957FF/Polavarapu et al. - 2005 - Data assimilation with the Canadian Middle Atmosph.pdf},
  journal = {Atmosphere-ocean - ATMOS OCEAN}
}

@article{pronzato_minimax_2017,
  title = {Minimax and Maximin Space-Filling Designs: Some Properties and Methods for Construction},
  author = {Pronzato, Luc},
  year = {2017},
  pages = {31},
  abstract = {A few properties of minimax and maximin optimal designs in a compact subset of Rd are presented, and connections with other space-filling constructions are indicated. Several methods are given for the evaluation of the minimax-distance (or dispersion) criterion for a given n-point design. Various optimisation methods are proposed and their limitations, in particular in terms of dimension d, are indicated. A large majority of the results presented are not new, but their collection in a single document containing a respectable bibliography will hopefully be useful to the reader.},
  file = {/home/victor/Zotero/storage/7APNG8N5/Pronzato - 2017 - Minimax and maximin space-filling designs some pr.pdf},
  language = {en}
}

@article{pujol_minimisation_nodate,
  title = {{Minimisation de quantiles \textendash{} application en m\'ecanique}},
  author = {Pujol, Gilles and Riche, Rodolphe Le and Bay, Xavier and Roustant, Olivier},
  pages = {7},
  file = {/home/victor/Zotero/storage/GSYTV4DK/Pujol et al. - Minimisation de quantiles ‚Äì application en m√©caniq.pdf},
  language = {fr}
}

@article{qin_improving_2017,
  title = {Improving the {{Expected Improvement Algorithm}}},
  author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
  year = {2017},
  month = may,
  abstract = {The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.},
  archivePrefix = {arXiv},
  eprint = {1705.10033},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/V2QHCQYZ/Qin et al. - 2017 - Improving the Expected Improvement Algorithm.pdf;/home/victor/Zotero/storage/NBKTIDCQ/1705.html},
  journal = {arXiv:1705.10033 [cs, stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{quagliarella_optimization_2014,
  title = {Optimization {{Under Uncertainty Using}} the {{Generalized Inverse Distribution Function}}},
  author = {Quagliarella, Domenico and Petrone, Giovanni and Iaccarino, Gianluca},
  year = {2014},
  month = jul,
  abstract = {A framework for robust optimization under uncertainty based on the use of the generalized inverse distribution function (GIDF), also called quantile function, is here proposed. Compared to more classical approaches that rely on the usage of statistical moments as deterministic attributes that define the objectives of the optimization process, the inverse cumulative distribution function allows for the use of all the possible information available in the probabilistic domain. Furthermore, the use of a quantile based approach leads naturally to a multi-objective methodology which allows an a-posteriori selection of the candidate design based on risk/opportunity criteria defined by the designer. Finally, the error on the estimation of the objectives due to the resolution of the GIDF will be proven to be quantifiable},
  archivePrefix = {arXiv},
  eprint = {1407.4636},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/IK2F8YTC/Quagliarella et al. - 2014 - Optimization Under Uncertainty Using the Generaliz.pdf},
  journal = {arXiv:1407.4636 [cs, math]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  language = {en},
  primaryClass = {cs, math}
}

@book{rachdi_apprentissage_2011,
  title = {Apprentissage Statistique et Computer Experiments : Approche Quantitative Du Risque et Des Incertitudes En Mod\'elisation},
  shorttitle = {Apprentissage Statistique et Computer Experiments},
  author = {Rachdi, Nabil},
  year = {2011},
  month = jan,
  publisher = {{Toulouse 3}},
  abstract = {Cette th\`ese s'inscrit dans le domaine de l'apprentissage statistique et dans celui des exp\'eriences simul\'ees (computer experiments). Son objet est de proposer un cadre g\'en\'eral permettant d'estimer les param\`etres d'un code de simulation num\'erique de fa\c{c}on \`a reproduire au mieux certaines caract\'eristiques d'int\'er\^et extraites de donn\'ees observ\'ees. Ce travail recouvre le cadre classique de l'estimation param\'etrique dans un mod\`ele de r\'egression et \'egalement la calibration de la densit\'e de probabilit\'e des variables d'entr\'ee d'un code num\'erique afin de reproduire une loi de probabilit\'e donn\'ee en sortie. Une partie importante de ce travail consiste dans l'estimation param\'etrique d'un code num\'erique \`a partir d'observations. Nous proposons une classe de m\'ethode originale n\'ecessitant une simulation intensive du code num\'erique, que l'on remplacera par un m\'eta-mod\`ele s'il est trop co\^uteux. Nous validons th\'eoriquement les algorithmes propos\'es du point de vue non-asymptotique, en prouvant des bornes sur l'exc\`es de risque. Ces r\'esultats reposent entres autres sur des in\'egalit\'es de concentration. Un second probl\`eme que nous abordons est celui de l'\'etude d'une dualit\'e entre proc\'edure d'estimation et nature de la pr\'ediction recherch\'ee. Il s'agit ici de mieux comprendre l'effet d'une proc\'edure d'estimation des param\`etres d'un code num\'erique sur une caract\'eristique d'int\'er\^et donn\'ee. Enfin, en pratique la d\'etermination des param\`etres optimaux au sens du crit\`ere donn\'e par le risque empirique n\'ecessite la recherche du minimum d'une fonction g\'en\'eralement non convexe et poss\'edant plusieurs minima locaux. Nous proposons un algorithme stochastique consistant \`a combiner une r\'egularisation du crit\`ere par convolution avec un noyau gaussien, de variance d\'ecroissante au fil des it\'erations, avec une m\'ethode d'approximation stochastique du type Kiefer-Wolfowitz.},
  file = {/home/victor/Zotero/storage/2FJ9UWPY/2011TOU30283.pdf;/home/victor/Zotero/storage/H6T3HW7X/2011TOU30283.html}
}

@article{ranjan_sequential_2008-1,
  title = {Sequential {{Experiment Design}} for {{Contour Estimation From Complex Computer Codes}}},
  author = {Ranjan, Pritam and Bingham, Derek and Michailidis, George},
  year = {2008},
  month = nov,
  volume = {50},
  pages = {527--541},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/004017008000000541},
  file = {/home/victor/Zotero/storage/Y7A8CR4C/Ranjan et al. - 2008 - Sequential Experiment Design for Contour Estimatio.pdf},
  journal = {Technometrics},
  language = {en},
  number = {4}
}

@article{rao_robust_2015,
  title = {Robust Data Assimilation Using \${{L}}\_1\$ and {{Huber}} Norms},
  author = {Rao, Vishwas and Sandu, Adrian and Ng, Michael and {Nino-Ruiz}, Elias},
  year = {2015},
  month = nov,
  abstract = {Data assimilation is the process to fuse information from priors, observations of nature, and numerical models, in order to obtain best estimates of the parameters or state of a physical system of interest. Presence of large errors in some observational data, e.g., data collected from a faulty instrument, negatively affect the quality of the overall assimilation results. This work develops a systematic framework for robust data assimilation. The new algorithms continue to produce good analyses in the presence of observation outliers. The approach is based on replacing the traditional \$\textbackslash L\_2\$ norm formulation of data assimilation problems with formulations based on \$\textbackslash L\_1\$ and Huber norms. Numerical experiments using the Lorenz-96 and the shallow water on the sphere models illustrate how the new algorithms outperform traditional data assimilation approaches in the presence of data outliers.},
  file = {/home/victor/Zotero/storage/A7T67T3Z/Rao et al. - 2015 - Robust data assimilation using $L_1$ and Huber nor.pdf;/home/victor/Zotero/storage/ELRSCLTJ/1511.html},
  journal = {SciRate}
}

@incollection{rasmussen_gaussian_2004,
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2 - 14, 2003, {{T\"ubingen}}, {{Germany}}, {{August}} 4 - 16, 2003, {{Revised Lectures}}},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and {von Luxburg}, Ulrike and R{\"a}tsch, Gunnar},
  year = {2004},
  pages = {63--71},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28650-9_4},
  abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
  isbn = {978-3-540-28650-9},
  keywords = {Covariance Function,Gaussian Process,Joint Gaussian Distribution,Marginal Likelihood,Posterior Variance},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {/home/victor/Zotero/storage/NP79YRUY/Rasmussen et Williams - 2006 - Gaussian processes for machine learning.pdf},
  isbn = {978-0-262-18253-9},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  language = {en},
  lccn = {QA274.4 .R37 2006},
  series = {Adaptive Computation and Machine Learning}
}

@article{raue_joining_2013,
  title = {Joining Forces of {{Bayesian}} and Frequentist Methodology: A Study for Inference in the Presence of Non-Identifiability},
  shorttitle = {Joining Forces of {{Bayesian}} and Frequentist Methodology},
  author = {Raue, Andreas and Kreutz, Clemens and Theis, Fabian Joachim and Timmer, Jens},
  year = {2013},
  month = feb,
  volume = {371},
  pages = {20110544},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2011.0544},
  file = {/home/victor/Zotero/storage/YNPGQZKX/Raue et al. - 2013 - Joining forces of Bayesian and frequentist methodo.pdf},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  keywords = {profile lik},
  language = {en},
  number = {1984}
}

@article{raynaud_vacumm_2014,
  title = {{{VACUMM}} - {{A Python}} Library for Ocean Science},
  author = {Raynaud, Stephane and Charria, Guillaume and Wilkins, Jonathan and Garnier, Val{\'e}rie and Garreau, P and Theetten, S{\'e}bastien},
  year = {2014},
  month = apr,
  pages = {99},
  abstract = {VACUMM is an open-source Python library for processing data from observations and numerical models. The library is now used for several years in research and operational contexts, for instance for producing figures and reports, validating models, converting data, or making simple or advanced diagnostics. In this paper, we introduce how the library is built, and we present two applications of its use: one in an perational context and one in a research context.},
  file = {/home/victor/Zotero/storage/CNG8K37B/Raynaud et al. - 2014 - VACUMM - A Python library for ocean science.pdf},
  journal = {Mercator-Ocean Newsletter}
}

@article{reid_aspects_2013,
  title = {Aspects of Likelihood Inference},
  author = {Reid, Nancy},
  year = {2013},
  month = sep,
  volume = {19},
  pages = {1404--1418},
  issn = {1350-7265},
  doi = {10.3150/12-BEJSP03},
  abstract = {I review the classical theory of likelihood based inference and consider how it is being extended and developed for use in complex models and sampling schemes.},
  archivePrefix = {arXiv},
  eprint = {1309.7816},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/V9SSH2CL/Reid - 2013 - Aspects of likelihood inference.pdf},
  journal = {Bernoulli},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  language = {en},
  number = {4}
}

@article{reid_likelihood_2003,
  title = {Likelihood Inference in the Presence of Nuisance Parameters},
  author = {Reid, N. and Fraser, D. A. S.},
  year = {2003},
  month = dec,
  abstract = {We describe some recent approaches to likelihood based inference in the presence of nuisance parameters. Our approach is based on plotting the likelihood function and the \$p\$-value function, using recently developed third order approximations. Orthogonal parameters and adjustments to profile likelihood are also discussed. Connections to classical approaches of conditional and marginal inference are outlined.},
  archivePrefix = {arXiv},
  eprint = {physics/0312079},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/85NDWW8U/Reid et Fraser - 2003 - Likelihood inference in the presence of nuisance p.pdf},
  journal = {arXiv:physics/0312079},
  keywords = {Physics - Data Analysis; Statistics and Probability},
  language = {en}
}

@phdthesis{ribaud_krigeage_2018,
  title = {Krigeage Pour La Conception de Turbomachines : Grande Dimension et Optimisation Multi-Objectif Robuste},
  shorttitle = {Krigeage Pour La Conception de Turbomachines},
  author = {Ribaud, M{\'e}lina},
  year = {2018},
  month = oct,
  abstract = {Dans le secteur de l'automobile, les turbomachines sont des machines tournantes participant au refroidissement des moteurs des voitures. Leur performance d\'epend de multiples param\`etres g\'eom\'etriques qui d\'eterminent leur forme. Cette th\`ese s'inscrit dans le projet ANR PEPITO r\'eunissant industriels et acad\'emiques autour de l'optimisation de ces turbomachines. L'objectif du projet est de trouver la forme du ventilateur maximisant le rendement en certains points de fonctionnement. Dans ce but, les industriels ont d\'evelopp\'e des codes CFD (computational fluid dynamics) simulant le fonctionnement de la machine. Ces codes sont tr\`es co\^uteux en temps de calcul. Il est donc impossible d'utiliser directement le r\'esultat de ces simulations pour conduire une optimisation.Par ailleurs, lors de la construction des turbomachines, on observe des perturbations sur les param\`etres d'entr\'ee. Elles sont le reflet de fluctuations des machines de production. Les \'ecarts observ\'es sur la forme g\'eom\'etrique finale de la turbomachine peuvent provoquer une perte de performance cons\'equente. Il est donc n\'ecessaire de prendre en compte ces perturbations et de proc\'eder \`a une optimisation robuste \`a ces fluctuations. Dans ce travail de th\`ese, nous proposons des m\'ethodes bas\'ees sur du krigeage r\'epondant aux deux principales probl\'ematiques li\'ees \`a ce contexte de simulations co\^uteuses :\textbullet{}	Comment construire une bonne surface de r\'eponse pour le rendement lorsqu'il y a beaucoup de param\`etres g\'eom\'etriques ?\textbullet{}	Comment proc\'eder \`a une optimisation du rendement efficace tout en prenant en compte les perturbations des entr\'ees ?Nous r\'epondons \`a la premi\`ere probl\'ematique en proposant plusieurs algorithmes permettant de construire un noyau de covariance pour le krigeage adapt\'e \`a la grande dimension. Ce noyau est un produit tensoriel de noyaux isotropes o\`u chacun de ces noyaux est li\'e \`a un sous groupe de variables d'entr\'ee. Ces algorithmes sont test\'es sur des cas simul\'es et sur une fonction r\'eelle. Les r\'esultats montrent que l'utilisation de ce noyau permet d'am\'eliorer la qualit\'e de pr\'ediction en grande dimension. Concernant la seconde probl\'ematique, nous proposons plusieurs strat\'egies it\'eratives bas\'ees sur un co-krigeage avec d\'eriv\'ees pour conduire l'optimisation robuste. A chaque it\'eration, un front de Pareto est obtenu par la minimisation de deux objectifs calcul\'es \`a partir des pr\'edictions de la fonction co\^uteuse. Le premier objectif repr\'esente la fonction elle-m\^eme et le second la robustesse. Cette robustesse est quantifi\'ee par un crit\`ere estimant une variance locale et bas\'ee sur le d\'eveloppement de Taylor. Ces strat\'egies sont compar\'ees sur deux cas tests en petite et plus grande dimension. Les r\'esultats montrent que les meilleures strat\'egies permettent bien de trouver l'ensemble des solutions robustes. Enfin, les m\'ethodes propos\'ees sont appliqu\'ees sur les cas industriels propres au projet PEPITO.},
  file = {/home/victor/Zotero/storage/CKTSANXI/Ribaud - 2018 - Krigeage pour la conception de turbomachines  gra.pdf;/home/victor/Zotero/storage/R65EGT37/Ribaud - 2018 - Krigeage pour la conception de turbomachines  gra.pdf;/home/victor/Zotero/storage/7E6NAWNZ/2018LYSEC026.html;/home/victor/Zotero/storage/JRFU2DP6/2018LYSEC026.html},
  keywords = {Algorithm,Algorithme,Algorithmes,Covariance kernel,Grande dimension,High dimension,Krigeage,Kriging,Math√©matiques,Noyau de covariance,Optimisation robuste,Robust optimization,Turbomachines},
  school = {Lyon},
  type = {Thesis}
}

@unpublished{ribaud_robustness_2019,
  title = {Robustness Kriging-Based Optimization},
  author = {Ribaud, M{\'e}lina and {Blanchet-Scalliet}, Christophette and Gillot, Frederic and Helbert, C{\'e}line},
  year = {2019},
  month = feb,
  abstract = {In the context of robust shape optimization, the estimation cost of some physical models is reduced 10 by the use of a response surface. The multi objective methodology for robust optimization that requires the partitioning of the Pareto front (minimization of the function and the robustness criterion) has already been developed. However, the efficient estimation of the robustness criterion in the context of time-consuming simulation has not been much explored. We propose a robust optimization procedure based on the prediction of the function and its derivatives by a kriging. The 15 usual moment 2 is replaced by an approximated version using Taylor theorem. A Pareto front of the robust solutions is generated by a genetic algorithm named NSGA-II. This algorithm gives a Pareto front in an reasonable time of calculation. We detail seven relevant strategies and compare them for the same budget in two test functions (2D and 6D). In each case, we compare the results when the derivatives are observed and not.},
  file = {/home/victor/Zotero/storage/22TIVRVK/Ribaud et al. - 2019 - Robustness kriging-based optimization.pdf},
  keywords = {Expected Improvement 25,Gaussian process modelling,Gaussian process regression,Multi-objective optimization,Robust Optimization,robustness criterion,Tay-lor expansion}
}

@book{rios_insua_robust_2000,
  title = {Robust {{Bayesian}} Analysis},
  editor = {R{\'i}os Insua, David and Ruggeri, Fabrizio},
  year = {2000},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/home/victor/Zotero/storage/UVCAEIW7/R√≠os Insua et Ruggeri - 2000 - Robust Bayesian analysis.pdf},
  isbn = {978-0-387-98866-5},
  keywords = {Bayesian statistical decision theory},
  language = {en},
  lccn = {QA279.5 .R64 2000},
  number = {152},
  series = {Lecture Notes in Statistics}
}

@article{robbins_measure_1944,
  title = {On the {{Measure}} of a {{Random Set}}},
  author = {Robbins, H. E.},
  year = {1944},
  month = mar,
  volume = {15},
  pages = {70--74},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177731315},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/home/victor/Zotero/storage/JMFXEH7K/Robbins - 1944 - On the Measure of a Random Set.pdf;/home/victor/Zotero/storage/XU8WEAC9/1177731315.html},
  journal = {The Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR10347},
  number = {1},
  zmnumber = {0060.29406}
}

@article{robbins_stochastic_1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  month = sep,
  volume = {22},
  pages = {400--407},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  abstract = {Let M(x)M(x)M(x) denote the expected value at level xxx of the response to a certain experiment. M(x)M(x)M(x) is assumed to be a monotone function of xxx but is unknown to the experimenter, and it is desired to find the solution x=\texttheta x=\texttheta x = \textbackslash theta of the equation M(x)={$\alpha$}M(x)={$\alpha$}M(x) = \textbackslash alpha, where {$\alpha\alpha\backslash$}alpha is a given constant. We give a method for making successive experiments at levels x1,x2,{$\cdots$}x1,x2,{$\cdots$}x\_1,x\_2,\textbackslash cdots in such a way that xnxnx\_n will tend to \texttheta\texttheta\textbackslash theta in probability.},
  file = {/home/victor/Zotero/storage/B6H3EKLA/Robbins et Monro - 1951 - A Stochastic Approximation Method.pdf;/home/victor/Zotero/storage/XLRD9927/1177729586.html},
  journal = {The Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR42668},
  number = {3},
  zmnumber = {0054.05901}
}

@inproceedings{robert_marginal_1999,
  title = {Marginal {{MAP}} Estimation Using {{Markov}} Chain {{Monte Carlo}}},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, 1999. {{Proceedings}}., 1999 {{IEEE International Conference}} On},
  author = {Robert, Christian P. and Doucet, Arnaud and Godsill, Simon J.},
  year = {1999},
  volume = {3},
  pages = {1753--1756},
  publisher = {{IEEE}},
  file = {/home/victor/Zotero/storage/H5FADK4V/Robert et al. - 1999 - Marginal MAP estimation using Markov chain Monte C.pdf;/home/victor/Zotero/storage/AII4L2FS/756334.html},
  keywords = {MCMC,MMAP}
}

@article{robert_monte_nodate,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P and Casella, George},
  pages = {85},
  file = {/home/victor/Zotero/storage/TQ3MFXTJ/Robert et Casella - Monte Carlo Statistical Methods.pdf},
  language = {en}
}

@article{rockafellar_conditional_2002,
  title = {Conditional Value-at-Risk for General Loss Distributions},
  author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
  year = {2002},
  volume = {26},
  pages = {1443--1471},
  file = {/home/victor/Zotero/storage/RPR7LHY8/790b149edfb586db318363e28182a6fedc80.pdf},
  journal = {Journal of banking \& finance},
  keywords = {VaR},
  number = {7}
}

@techreport{rockafellar_deviation_2002,
  title = {Deviation {{Measures}} in {{Risk Analysis}} and {{Optimization}}},
  author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav P. and Zabarankin, Michael},
  year = {2002},
  month = dec,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {General deviation measures, which include standard deviation as a special case but need not be symmetric with respect to ups and downs, are defined and shown to correspond to risk measures in the sense of Artzner, Delbaen, Eber and Heath when those are applied to the difference between a random variable and its expectation, instead of to the random variable itself.   A property called expectation-boundedness of the risk measure is uncovered as essential for this correspondence.  It is shown to be satisfied by conditional value-at-risk and by worst-case risk, as well as various mixtures, although not by ordinary value-at-risk.},
  file = {/home/victor/Zotero/storage/3RZUKHRL/Rockafellar et al. - 2002 - Deviation Measures in Risk Analysis and Optimizati.pdf;/home/victor/Zotero/storage/AACZGBUN/papers.html},
  keywords = {coherent risk,deviation measures,risk management},
  language = {en},
  number = {ID 365640},
  type = {{{SSRN Scholarly Paper}}}
}

@book{rogers_backlund_2002,
  title = {B\"acklund and {{Darboux}} Transformations: Geometry and Modern Applications in Soliton Theory},
  shorttitle = {B\"acklund and {{Darboux}} Transformations},
  author = {Rogers, Colin and Schief, Wolfgang Karl},
  year = {2002},
  volume = {30},
  publisher = {{Cambridge University Press}},
  file = {C\:\\Users\\Victor\\Documents\\Randall_J._LeVeque_Finite_Volume_Methods_for_Hyperbolic_Problems.pdf},
  keywords = {Finite Volume}
}

@article{rolland_characterization_nodate,
  title = {Characterization of {{Space}} and {{Time}}-{{Dependence}} of 3-{{Point Shots}} in {{Basketball}}},
  author = {Rolland, Gabin and Vuillemot, Romain and Bos, Wouter and Rivi{\`e}re, Nathan},
  pages = {16},
  abstract = {Understanding characteristics of 3-point shots is paramount for modern basketball success, as in recent decades, 3-point shots have become more prevalent in the NBA. They accounted for 33,6\% of the number of total shots in 2017-2018, compared to only 3\% in 1979-1980 [1]. In this paper, we aim at better understanding the connections between the type of 3-point shooting (catch-and-shoots and pull-ups) and the timing for shooting, using two distinct space-time models of player motion. Those models allow us to identify individual behavior as a function of specific defensive settings, e.g. shot-behavior when a player is guarded closely. We assess our models using SportVU data for specific NBA players and our code is open-source to enable more players and teams explorations, as well as to support further research and application of those models, beyond basketball and sport.},
  file = {/home/victor/Zotero/storage/U6RWYATJ/Rolland et al. - Characterization of Space and Time-Dependence of 3.pdf},
  language = {en}
}

@article{rontsis_distributionally_2017,
  title = {Distributionally {{Ambiguous Optimization Techniques}} for {{Batch Bayesian Optimization}}},
  author = {Rontsis, Nikitas and Osborne, Michael A. and Goulart, Paul J.},
  year = {2017},
  month = jul,
  abstract = {We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian Expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly - even on large batch sizes - as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems.},
  archivePrefix = {arXiv},
  eprint = {1707.04191},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/VASPF8V9/Rontsis et al. - 2017 - Distributionally Ambiguous Optimization Techniques.pdf;/home/victor/Zotero/storage/RKIUBHSG/1707.html},
  journal = {arXiv:1707.04191 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{rosasco_reproducing_nodate,
  title = {Reproducing {{Kernel Hilbert Spaces}}},
  author = {Rosasco, Lorenzo and Durrett, Greg},
  pages = {8},
  file = {/home/victor/Zotero/storage/Y2LFAF22/Rosasco et Durrett - Reproducing Kernel Hilbert Spaces.pdf},
  language = {en}
}

@book{rossant_learning_2013,
  title = {Learning {{IPython}} for Interactive Computing and Data Visualization: Learn {{IPython}} for Interactive {{Python}} Programming, High-Performance Numerical Computing, and Data Visualization},
  shorttitle = {Learning {{IPython}} for Interactive Computing and Data Visualization},
  author = {Rossant, Cyrille},
  year = {2013},
  publisher = {{Packt Publ}},
  address = {{Birmingham}},
  file = {/home/victor/Zotero/storage/IAMR59A7/Learning IPython for Interactive Computing and Data Visualization [Rossant 2013-04-25].pdf},
  isbn = {978-1-78216-993-2},
  language = {eng},
  series = {Community Experience Distilled}
}

@article{rougier_ten_2014,
  title = {Ten {{Simple Rules}} for {{Better Figures}}},
  author = {Rougier, Nicolas P. and Droettboom, Michael and Bourne, Philip E.},
  year = {2014},
  month = sep,
  volume = {10},
  pages = {e1003833},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003833},
  file = {/home/victor/Zotero/storage/ILBKZ6QT/Rougier et al. - 2014 - Ten Simple Rules for Better Figures.pdf;/home/victor/Zotero/storage/YYLAVTWY/article.html},
  journal = {PLOS Computational Biology},
  keywords = {Data visualization,Eye movements,Radii,Research design,Seismic signal processing,Software design,Software tools,Vision},
  language = {en},
  number = {9}
}

@article{rubin_bayesian_2010,
  title = {A {{Bayesian}} Approach for Inverse Modeling, Data Assimilation, and Conditional Simulation of Spatial Random Fields: {{METHOD OF ANCHORED DISTRIBUTIONS}}},
  shorttitle = {A {{Bayesian}} Approach for Inverse Modeling, Data Assimilation, and Conditional Simulation of Spatial Random Fields},
  author = {Rubin, Yoram and Chen, Xingyuan and Murakami, Haruko and Hahn, Melanie},
  year = {2010},
  month = oct,
  volume = {46},
  issn = {00431397},
  doi = {10.1029/2009WR008799},
  file = {/home/victor/Zotero/storage/CNUY359C/wrcr12530.pdf},
  journal = {Water Resources Research},
  language = {en},
  number = {10}
}

@article{rue_approximate_2009,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  author = {avard Rue, H{\textbackslash}a and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  volume = {71},
  pages = {319--392},
  file = {/home/victor/Zotero/storage/AEMPKWJW/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf;/home/victor/Zotero/storage/XZ5P7BEX/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian},
  journal = {Journal of the royal statistical society: Series b (statistical methodology)},
  number = {2}
}

@article{russo_tutorial_2017,
  title = {A {{Tutorial}} on {{Thompson Sampling}}},
  author = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  year = {2017},
  month = jul,
  abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
  archivePrefix = {arXiv},
  eprint = {1707.02038},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/5YEIEJDI/Russo et al. - 2017 - A Tutorial on Thompson Sampling.pdf;/home/victor/Zotero/storage/FNS6338N/1707.html},
  journal = {arXiv:1707.02038 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@book{ruud_introduction_2000,
  title = {An {{Introduction}} to {{Classical Econometric Theory}}},
  author = {Ruud, Paul Arthur and Ruud, Professor of Economics Paul A.},
  year = {2000},
  publisher = {{Oxford University Press}},
  abstract = {In An Introduction to Classical Econometric Theory Paul A. Ruud shows the practical value of an intuitive approach to econometrics. Students learn not only why but how things work. Through geometry, seemingly distinct ideas are presented as the result of one common principle, making econometrics more than mere recipes or special tricks. In doing this, the author relies on such concepts as the linear vector space, orthogonality, and distance. Parts I and II introduce the ordinary least squares fitting method and the classical linear regression model, separately rather than simultaneously as in other texts. Part III contains generalizations of the classical linear regression model and Part IV develops the latent variable models that distinguish econometrics from statistics. To motivate formal results in a chapter, the author begins with substantive empirical examples. Main results are followed by illustrative special cases; technical proofs appear toward the end of each chapter. Intended for a graduate audience, An Introduction to Classical Econometric Theory fills the gap between introductory and more advanced texts. It is the most conceptually complete text for graduate econometrics courses and will play a vital role in graduate instruction.},
  file = {/home/victor/Zotero/storage/2GFKQMAL/Ruud.pdf},
  googlebooks = {PnVCEZOOFr0C},
  isbn = {978-0-19-511164-4},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Economics / General},
  language = {en}
}

@article{rychlik_reliability_nodate,
  title = {On {{Some Reliability Applications}} of {{Rice}}'s {{Formula}} for the {{Intensity}} of {{Level Crossings}}},
  author = {RYCHLIK, IGOR},
  pages = {19},
  abstract = {Let X be a stationary process with absolutely continuous sample paths. If E¬âjX\_¬Ö0¬Üj¬ä is \textregistered nite and if the distribution of X¬Ö0¬Ü is absolutely continuous, then, for almost all u, the crossing intensity m¬Öu¬Ü of the level u by X¬Öt¬Ü is given by the generalized Rice's formula m¬Öu¬ÜR¬à E¬âjX\_¬Ö0¬ÜjjX¬Ö0¬Ü ¬à u¬ä fX¬Ö0¬Ü¬Öu¬Ü. The classical Rice's formula for m¬Öu¬Ü, which is valid for a \textregistered xed level u, m¬Öu¬Ü ¬à jzj fX\_¬Ö0¬Ü;X¬Ö0¬Ü¬Öz; u¬Üdz, holds under more restrictive technical conditions that can be dif\textregistered cult to check in applications. In this paper it is shown that often in practice the weaker form of Rice's formula (valid for almost all u) is suf\textregistered cient. Three engineering problems are discussed; prediction of fatigue life time; computing the average stress at slams and analysis of crest height of sea waves.},
  file = {/home/victor/Zotero/storage/YEIM26HS/RYCHLIK - On Some Reliability Applications of Rice's Formula.pdf},
  language = {en}
}

@book{saad_iterative_2003,
  title = {Iterative Methods for Sparse Linear Systems},
  author = {Saad, Yousef},
  year = {2003},
  publisher = {{SIAM}},
  file = {/home/victor/Zotero/storage/4V35HM3M/IterMethBook_2ndEd.pdf},
  keywords = {Iterative methods,Krylov}
}

@article{sacks_designs_1989,
  title = {Designs for {{Computer Experiments}}},
  author = {Sacks, Jerome and Schiller, Susannah B. and Welch, William J.},
  year = {1989},
  month = feb,
  volume = {31},
  pages = {41--47},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1989.10488474},
  file = {/home/victor/Zotero/storage/J8CW6PRK/Sacks et al. - 1989 - Designs for Computer Experiments.pdf},
  journal = {Technometrics},
  language = {en},
  number = {1}
}

@article{santarelli_framework_2010,
  title = {A Framework for Reduced Order Modeling with Mixed Moment Matching and Peak Error Objectives},
  author = {Santarelli, Keith R.},
  year = {2010},
  volume = {32},
  pages = {745--773},
  file = {/home/victor/Zotero/storage/PXPVHUJV/l1_mod_reduction.pdf},
  journal = {SIAM Journal on Scientific Computing},
  keywords = {Moment matching,MOR methods},
  number = {2}
}

@article{santner_design_nodate,
  title = {The {{Design}} and {{Analysis}} of {{Computer Experiments}}},
  author = {Santner, Thomas J and Williams, Brian J and Notz, William I},
  pages = {236},
  file = {/home/victor/Zotero/storage/FM6QXR9F/Santner et al. - The Design and Analysis of Computer Experiments.pdf},
  language = {en}
}

@article{sasaki_clustering_,
  title = {Clustering via {{Mode Seeking}} by {{Direct Estimation}} of the {{Gradient}} of a {{Log}}-{{Density}}},
  author = {Sasaki, Hiroaki and Hyvarinen, Aapo and Sugiyama, Masashi},
  pages = {16},
  abstract = {Mean shift clustering finds the modes of the data probability density by identifying the zero points of the density gradient. Since it does not require to fix the number of clusters in advance, the mean shift has been a popular clustering algorithm in various application fields. A typical implementation of the mean shift is to first estimate the density by kernel density estimation and then compute its gradient. However, since a good density estimation does not necessarily imply an accurate estimation of the density gradient, such an indirect two-step approach is not reliable. In this paper, we propose a method to directly estimate the gradient of the log-density without going through density estimation. The proposed method gives the global solution analytically and thus is computationally efficient. We then develop a mean-shift-like fixed-point algorithm to find the modes of the density for clustering. As in the mean shift, one does not need to set the number of clusters in advance. We experimentally show that the proposed clustering method significantly outperforms the mean shift especially for high-dimensional data.},
  file = {/home/victor/Zotero/storage/TWV2B3NR/Sasaki et al. - Clustering via Mode Seeking by Direct Estimation o.pdf},
  language = {en}
}

@article{savage_theory_1951,
  title = {The {{Theory}} of {{Statistical Decision}}},
  author = {Savage, L. J.},
  year = {1951},
  month = mar,
  volume = {46},
  pages = {55--67},
  issn = {0162-1459},
  doi = {10.1080/01621459.1951.10500768},
  file = {/home/victor/Zotero/storage/NFKIH2DJ/Savage - 1951 - The theory of statistical decision.ps;/home/victor/Zotero/storage/FXV6BBDL/01621459.1951.html;/home/victor/Zotero/storage/J6RUE4WP/01621459.1951.html},
  journal = {Journal of the American Statistical Association},
  number = {253}
}

@article{saynisch_estimating_2018,
  title = {Estimating Ocean Tide Model Uncertainties for Electromagnetic Inversion Studies},
  author = {Saynisch, Jan and Irrgang, Christopher and Thomas, Maik},
  year = {2018},
  month = apr,
  pages = {1--9},
  doi = {10.5194/angeo-2018-27},
  abstract = {Over a decade ago the semidiurnal lunar M2 ocean tide was identified in CHAMP satellite magnetometer data. Since then and especially since the launch of the satellite magnetometer mission Swarm, electromagnetic tidal observations from satellites are used increasingly to infer electric properties of the upper lithosphere. In most of these inversions, numerical ocean tidal models are used to generate oceanic tidal electromagnetic signals via electromagnetic induction. The modelled signals are subsequently compared to the satellite observations. During the inversion, since the tidal models are considered error free, discrepancies between forward models and observations are projected only onto the induction part of the modelling, e.g., Earth's resistivity distribution. Our study analyses uncertainties in oceanic tidal models from an electromagnetic point of view. Velocities from hydrodynamic and assimilative tidal models are converted into tidal electromagnetic signals and compared. Respective uncertainties are estimated. The studies main goal is to provide errors for electromagnetic inversion studies. At satellite height, the differences between the hydrodynamic tidal models are found to reach up to 2 nT, i.e., over 100 \% of the M2 signal. Assimilative tidal models show smaller differences of up to 0.1 nT, i.e., over 30 \% of the M2 signal.},
  file = {/home/victor/Zotero/storage/694QVRSL/Saynisch et al. - 2018 - Estimating ocean tide model uncertainties for elec.pdf},
  journal = {Annales Geophysicae Discussions}
}

@article{schobi_combining_nodate,
  title = {Combining Polynomial Chaos Expansions and {{Kriging}} for Solving Structural Reliability Problems},
  author = {Schobi, Roland and Sudret, Bruno},
  pages = {11},
  abstract = {Nowadays advanced simulation models such as finite element models are used in every domain of science and engineering in order to predict the behaviour of systems and, in case of engineering applications, to optimize them and assess their performance. In parallel, engineers are all the more concerned with structural reliability and robust design, meaning that the quantification of uncertainties has become a key challenge.},
  file = {/home/victor/Zotero/storage/UWZH64MW/Schobi et Sudret - Combining polynomial chaos expansions and Kriging .pdf},
  language = {en}
}

@book{scott_multivariate_2015,
  title = {Multivariate Density Estimation: Theory, Practice, and Visualization},
  shorttitle = {Multivariate Density Estimation},
  author = {Scott, David W.},
  year = {2015},
  publisher = {{John Wiley \& Sons}},
  file = {/home/victor/Zotero/storage/MZV5PRKU/multivariate-density-estimation-theory-practice-and-visualization.html;/home/victor/Zotero/storage/UWH7X4SX/books.html}
}

@article{scott_optimal_1979,
  title = {On {{Optimal}} and {{Data}}-{{Based Histograms}}},
  author = {Scott, David W.},
  year = {1979},
  month = dec,
  volume = {66},
  pages = {605},
  issn = {00063444},
  doi = {10.2307/2335182},
  abstract = {In thispaper the formulaforthe optimalhistogrambin widthis derivedwhichasymptotically minimizesthe integratedmean squared error.Monte Carlo methodsare used to verify the usefulnessofthisformulaforsmall samples. A data-based procedureforchoosingthe bin widthparameteris proposed,whichassumes a Gaussian referencestandard and requiresonly the sample size and an estimateofthe standard deviation. The sensitivityofthe procedureis investigatedusingseveral probabilitymodelswhichviolate the Gaussian assumption.},
  file = {/home/victor/Zotero/storage/7EJWJC8N/Scott - 1979 - On Optimal and Data-Based Histograms.pdf},
  journal = {Biometrika},
  language = {en},
  number = {3}
}

@article{seijo_continuous_2011,
  title = {A Continuous Mapping Theorem for the Smallest Argmax Functional},
  author = {Seijo, Emilio and Sen, Bodhisattva},
  year = {2011},
  volume = {5},
  pages = {421--439},
  issn = {1935-7524},
  doi = {10.1214/11-EJS613},
  abstract = {This paper introduces a version of the argmax continuous mapping theorem that applies to M-estimation problems in which the objective functions converge to a limiting process with multiple maximizers. The concept of the smallest maximizer of a function in the d-dimensional Skorohod space is introduced and its main properties are studied. The resulting continuous mapping theorem is applied to three problems arising in change-point regression analysis. Some of the results proved in connection to the d-dimensional Skorohod space are also of independent interest.},
  file = {/home/victor/Zotero/storage/WZANPFUU/Seijo et Sen - 2011 - A continuous mapping theorem for the smallest argm.pdf;/home/victor/Zotero/storage/AHU5JSRH/1305034909.html},
  journal = {Electronic Journal of Statistics},
  keywords = {Change-point,compound Poisson process,Cox proportional hazards model,multiple maximizers,Skorohod spaces with multidimensional parameter},
  language = {EN},
  mrnumber = {MR2802050},
  zmnumber = {1329.60090}
}

@phdthesis{sengers_schemas_2019,
  title = {Sch\'emas Semi-Implicites et de Diffusion-Redistanciation Pour La Dynamique Des Globules Rouges},
  author = {Sengers, Arnaud},
  year = {2019},
  month = jul,
  abstract = {Dans ce travail, nous nous sommes int\'eress\'es \`a la mise en place de sch\'emas semi-implicites pour l'am\'elioration des simulations num\'eriques du d\'eplacement d'un globule rouge dans le sang. Nous consid\'erons la m\'ethode levelset, o\`u l'interface fluide-structure est repr\'esent\'ee comme la ligne de niveau 0 d'une fonction auxiliaire et le couplage est effectu\'e en ajoutant un terme source dans l'\'equation fluide.Le principe de ces sch\'emas semi-implicites est de pr\'edire la position et la forme de la structure par une \'equation de la chaleur et d'utiliser cette pr\'ediction pour obtenir un terme de force dans l'\'equation fluide plus pr\'ecis. Ce type de sch\'emas semi-implicites a d'abord \'et\'e mis en place dans le cadre d'un syst\`eme diphasique ou d'une membrane \'elastique immerg\'ee afin d'utiliser un plus grand pas de temps que pour un couplage explicite. Cela a permis d'am\'eliorer les conditions sur le pas de temps et ainsi augmenter l'efficacit\'e globale de l'algorithme complet par rapport \`a un sch\'ema explicite classique.Pour \'etendre ce raisonnement au cas d'un globule rouge, nous proposons un algorithme pour simuler le flot de Willmore en dimension 2 et 3. Notre m\'ethode s'inspire des m\'ethodes de mouvements d'interface g\'en\'er\'es par diffusion et nous arrivons \`a obtenir un flot non lin\'eaire d'ordre 4 uniquement avec des r\'esolutions d'\'equations de la chaleur. Pour assurer la conservation du volume et de l'aire d'un globule rouge, nous proposons ensuite une m\'ethode de correction qui d\'eplace l\'eg\`erement l'interface afin de recoller aux contraintes.La combinaison des deux \'etapes pr\'ec\'edentes d\'ecrit le comportement d'un globule rouge laiss\'e au repos. Nous validons cette m\'ethode en obtenant une formed'\'equilibre d'un globule rouge. Nous proposons enfin un sch\'ema semi-implicite dans le cas d'un globule rouge qui ouvre la voie vers l'utilisation de cette m\'ethode comme pr\'edicteur de l'algorithme de couplage complet.},
  file = {/home/victor/Zotero/storage/F9F4WN25/Sengers - 2019 - Sch√©mas semi-implicites et de diffusion-redistanci.pdf;/home/victor/Zotero/storage/6LWDF8A3/2019GREAM032.html},
  journal = {http://www.theses.fr},
  keywords = {Analyse fonctionnelle,Analyse num√©rique,Convolution-Redistanciation Method,√âquation de la chaleur,Finite element method,Fluides; M√©canique des,Levelset methods,Mathematical Analysis,Math√©matiques appliqu√©es,M√©thode Convolution-Redistanciation,M√©thode El√©ments finis,M√©thodes Levelset,Sch√©mas semi-Implicites,Semi-Implicit scheme},
  school = {Grenoble Alpes},
  type = {Thesis}
}

@article{seshadri_density-matching_2014,
  title = {A Density-Matching Approach for Optimization under Uncertainty},
  author = {Seshadri, Pranay and Constantine, Paul and Iaccarino, Gianluca and Parks, Geoffrey},
  year = {2014},
  month = sep,
  abstract = {Modern computers enable methods for design optimization that account for uncertainty in the system---so-called optimization under uncertainty. We propose a metric for OUU that measures the distance between a designer-specified probability density function of the system response the target and system response's density function at a given design. We study an OUU formulation that minimizes this distance metric over all designs. We discretize the objective function with numerical quadrature and approximate the response density function with a Gaussian kernel density estimate. We offer heuristics for addressing issues that arise in this formulation, and we apply the approach to a CFD-based airfoil shape optimization problem. We qualitatively compare the density-matching approach to a multi-objective robust design optimization to gain insight into the method.},
  archivePrefix = {arXiv},
  eprint = {1409.7089},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/KK6NXSQM/Seshadri et al. - 2014 - A density-matching approach for optimization under.pdf;/home/victor/Zotero/storage/XLQ37PBN/1409.html},
  journal = {arXiv:1409.7089 [math, stat]},
  keywords = {Mathematics - Optimization and Control,Statistics - Computation},
  primaryClass = {math, stat}
}

@article{shah_student-t_2014,
  title = {Student-t {{Processes}} as {{Alternatives}} to {{Gaussian Processes}}},
  author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2014},
  month = feb,
  abstract = {We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.},
  archivePrefix = {arXiv},
  eprint = {1402.4306},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/VPPMUUCH/Shah et al. - 2014 - Student-t Processes as Alternatives to Gaussian Pr.pdf;/home/victor/Zotero/storage/ZIFUTJNY/1402.html},
  journal = {arXiv:1402.4306 [cs, stat]},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@inproceedings{shankaran_robust_2011,
  title = {Robust Optimal Control Using Polynomial Chaos and Adjoints for Systems with Uncertain Inputs},
  booktitle = {20th {{AIAA Computational Fluid Dynamics Conference}}},
  author = {Shankaran, Sriram and Jameson, Antony},
  year = {2011},
  pages = {3069},
  file = {/home/victor/Zotero/storage/P4J2SVCR/Shankaran et Jameson - 2011 - Robust optimal control using polynomial chaos and .pdf;/home/victor/Zotero/storage/NLRPI8UX/6.html}
}

@article{shapiro_asymptotic_1991,
  title = {Asymptotic Analysis of Stochastic Programs},
  author = {Shapiro, Alexander},
  year = {1991},
  month = dec,
  volume = {30},
  pages = {169--186},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/BF02204815},
  abstract = {In this paper we discuss a general approach to studying asymptotic properties of statistical estimators in stochastic programming. The approach is based on an extended delta method and appears to be particularly suitable for deriving asymptotics of the optimal value of stochastic programs. Asymptotic analysis of the optimal value will be presented in detail. Asymptotic properties of the corresponding optimal solutions are briefly discussed.},
  file = {/home/victor/Zotero/storage/797X5VRH/Shapiro - 1991 - Asymptotic analysis of stochastic programs.pdf},
  journal = {Annals of Operations Research},
  language = {en},
  number = {1}
}

@book{shapiro_lectures_2009,
  title = {Lectures on {{Stochastic Programming}}: {{Modeling}} and {{Theory}}},
  shorttitle = {Lectures on {{Stochastic Programming}}},
  author = {Shapiro, Alexander and Dentcheva, Darinka and Ruszczy{\'n}ski, Andrzej},
  year = {2009},
  month = jan,
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898718751},
  file = {/home/victor/Zotero/storage/QD5HH7UZ/Lectures-on-stochastic-programming-Modeling-and-theory.pdf},
  isbn = {978-0-89871-687-0 978-0-89871-875-1},
  keywords = {Stochastic programming},
  language = {en}
}

@article{shapiro_tutorial_nodate,
  title = {A {{Tutorial}} on {{Stochastic Programming}}},
  author = {Shapiro, Alexander and Philpott, Andy},
  pages = {35},
  file = {/home/victor/Zotero/storage/RNY8UMLT/Shapiro et Philpott - A Tutorial on Stochastic Programming.pdf},
  language = {en}
}

@article{shi_nondegenerate_2015,
  title = {A Nondegenerate {{Vuong}} Test: {{A}} Nondegenerate {{Vuong}} Test},
  shorttitle = {A Nondegenerate {{Vuong}} Test},
  author = {Shi, Xiaoxia},
  year = {2015},
  month = mar,
  volume = {6},
  pages = {85--121},
  issn = {17597323},
  doi = {10.3982/QE382},
  file = {/home/victor/Zotero/storage/6U8WR7J5/Shi - 2015 - A nondegenerate Vuong test A nondegenerate Vuong .pdf},
  journal = {Quantitative Economics},
  language = {en},
  number = {1}
}

@book{silverman_density_2018,
  title = {Density Estimation: {{For}} Statistics and Data Analysis},
  shorttitle = {Density Estimation},
  author = {Silverman, B.W.},
  year = {2018},
  month = jan,
  doi = {10.1201/9781315140919},
  abstract = {Although there has been a surge of interest in density estimation in recent years, much of the published research has been concerned with purely technical matters with insufficient emphasis given to the technique's practical value. Furthermore, the subject has been rather inaccessible to the general statistician. The account presented in this book places emphasis on topics of methodological importance, in the hope that this will facilitate broader practical application of density estimation and also encourage research into relevant theoretical work. The book also provides an introduction to the subject for those with general interests in statistics. The important role of density estimation as a graphical technique is reflected by the inclusion of more than 50 graphs and figures throughout the text. Several contexts in which density estimation can be used are discussed, including the exploration and presentation of data, nonparametric discriminant analysis, cluster analysis, simulation and the bootstrap, bump hunting, projection pursuit, and the estimation of hazard rates and other quantities that depend on the density. This book includes general survey of methods available for density estimation. The Kernel method, both for univariate and multivariate data, is discussed in detail, with particular emphasis on ways of deciding how much to smooth and on computation aspects. Attention is also given to adaptive methods, which smooth to a greater degree in the tails of the distribution, and to methods based on the idea of penalized likelihood.}
}

@article{sinha_principal_1997,
  title = {The Principal Lunar Semidiurnal Tide and Its Harmonics: Baseline Solutions for {{M2}} and {{M4}} Constituents on the {{North}}-{{West European Continental Shelf}}},
  shorttitle = {The Principal Lunar Semidiurnal Tide and Its Harmonics},
  author = {Sinha, B. and Pingree, R. D.},
  year = {1997},
  month = sep,
  volume = {17},
  pages = {1321--1365},
  issn = {0278-4343},
  doi = {10.1016/S0278-4343(97)00007-1},
  abstract = {A 2D numerical model of the shelf seas around the U.K. is used to derive the M2 tide and the M4 and M6 tidal harmonics. The accuracy of the tidal harmonics is shown to be related to the form of the dissipation. In particular a quadratic form of the bottom friction results in unrealistically high amplitudes for the M6 tide. It is shown that this effect can be reduced by use of a combination of a linearised friction tensor and quadratic friction without significantly affecting the M2 constituent. The role of diffusion in controlling the amplitude of the M4 tide is demonstrated. The 2D results provide a baseline or reference solution for further development using 3D and/or multiconstituent models. Amplitude and phase diagrams derived from the model show a progression with frequency in the North Sea and the English Channel suggestive of a transition from Kelvin wave to Poincar\'e wave propagation. It is shown that both the North Sea and the English Channel can be considered from the point of view of waves propagating in a closed channel in the presence of friction. For the inviscid case, the critical frequency above which Poincar\'e waves are able to propagate is calculated to be between the M2 and M4 frequencies for the North Sea and between the M6 and M8 frequencies for the English Channel. A dispersion relation is derived for Poincar\'e and Kelvin waves in the presence of friction. Both types of wave possess decaying and propagating characteristics when friction is present with wavelength and decay scale dependent on frequency and the coefficient of friction. There is no longer a sharp critical frequency, but instead a transition region where wavelength and decay scales vary relatively rapidly with frequency. The response of a semi-infinite rectangular channel to Kelvin and Poincar\'e wave propagation is calculated providing a partial explanation of the numerical model results. \textcopyright{} 1997 Elsevier Science Ltd},
  file = {/home/victor/Zotero/storage/5AIPIYRK/Sinha et Pingree - 1997 - The principal lunar semidiurnal tide and its harmo.pdf;/home/victor/Zotero/storage/J2WHXNT7/S0278434397000071.html},
  journal = {Continental Shelf Research},
  number = {11}
}

@article{smith_optimizers_2006,
  title = {The {{Optimizer}}'s {{Curse}}: {{Skepticism}} and {{Postdecision Surprise}} in {{Decision Analysis}}},
  shorttitle = {The {{Optimizer}}'s {{Curse}}},
  author = {Smith, James E. and Winkler, Robert L.},
  year = {2006},
  month = mar,
  volume = {52},
  pages = {311--322},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.1050.0451},
  file = {/home/victor/Zotero/storage/VAK8B93Y/Smith et Winkler - 2006 - The Optimizer‚Äôs Curse Skepticism and Postdecision.pdf},
  journal = {Management Science},
  language = {en},
  number = {3}
}

@article{sniedovich_solution_1991,
  title = {Solution Strategies for Variance Minimization Problems},
  author = {Sniedovich, Moshe},
  year = {1991},
  month = jan,
  volume = {21},
  pages = {49--56},
  issn = {0898-1221},
  doi = {10.1016/0898-1221(91)90080-N},
  abstract = {We outline two solutions strategies for optimization problems requiring the minimization of an objective function with a variance or variance-like term.},
  file = {/home/victor/Zotero/storage/Y9PIPQAH/Sniedovich - 1991 - Solution strategies for variance minimization prob.pdf;/home/victor/Zotero/storage/2WMLP4CY/089812219190080N.html},
  journal = {Computers \& Mathematics with Applications},
  number = {2}
}

@article{snyder_stochastic_2004,
  title = {Stochastic P-Robust Location Problems},
  author = {Snyder, Lawrence and Daskin, Mark},
  year = {2004},
  month = aug,
  volume = {38},
  doi = {10.1080/07408170500469113},
  abstract = {Many objectives have been proposed for optimization under uncertainty. The typical stochastic programming ob-jective of minimizing expected cost may yield solutions that are inexpensive in the long run but perform poorly under certain realizations of the random data. On the other hand, the typical robust optimization objective of minimizing maximum cost or regret tends to be overly conservative, planning against a disastrous but unlikely scenario. In this paper, we present facility location models that combine the two objectives by minimizing the expected cost while bounding the relative regret in each scenario. In particular, the models seek the minimum-expected-cost solution that is p-robust; i.e., whose relative regret is no more than 100p\% in each scenario. We present p-robust models based on two classical facility location problems, the P -median problem and the uncapacitated fixed-charge location problem. We solve both problems using variable splitting (Lagrangian decomposition), in which the subproblem reduces to the multiple-choice knapsack problem. Feasible solutions are found using an upper-bounding heuristic. For many instances of the problems, finding a feasible solution, and even determining whether the instance is feasible, is difficult; we discuss a mechanism for determining infeasibility. We also show how the algorithm can be used as a heuristic to solve minimax-regret versions of the location problems.},
  file = {/home/victor/Zotero/storage/BU2CJ9WS/Snyder et Daskin - 2004 - Stochastic p-robust location problems.pdf},
  journal = {Iie Transactions}
}

@inproceedings{sohns_efficient_2006,
  title = {Efficient Parameterization of Large-Scale Dynamic Models through the Use of Activity Analysis},
  booktitle = {Proceedings of the {{ASME IMECE}}},
  author = {Sohns, B. and Allison, James and Fathy, Hosam K. and Stein, Jeffrey L.},
  year = {2006},
  pages = {5--10},
  file = {/home/victor/Zotero/storage/MX4BU3SH/Sohns et al. - 2006 - Efficient parameterization of large-scale dynamic .pdf;/home/victor/Zotero/storage/GYA5TDAS/proceeding.html}
}

@article{srinivas_gaussian_2012,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}: {{No Regret}} and {{Experimental Design}}},
  shorttitle = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
  year = {2012},
  month = may,
  volume = {58},
  pages = {3250--3265},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2011.2182033},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  archivePrefix = {arXiv},
  eprint = {0912.3995},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/5S6ZESFE/Srinivas et al. - 2012 - Gaussian Process Optimization in the Bandit Settin.pdf},
  journal = {IEEE Transactions on Information Theory},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  number = {5}
}

@article{stefanski_calculus_2002,
  title = {The Calculus of {{M}}-Estimation},
  author = {Stefanski, Leonard A. and Boos, Dennis D.},
  year = {2002},
  volume = {56},
  pages = {29--38},
  file = {/home/victor/Zotero/storage/Z5S4DBRT/Stefanski et Boos - 2002 - The calculus of M-estimation.pdf;/home/victor/Zotero/storage/V9N9YLHS/000313002753631330.html},
  journal = {The American Statistician},
  number = {1}
}

@article{steyerberg_assessing_2010,
  title = {Assessing the Performance of Prediction Models: A Framework for Some Traditional and Novel Measures},
  shorttitle = {Assessing the Performance of Prediction Models},
  author = {Steyerberg, Ewout W. and Vickers, Andrew J. and Cook, Nancy R. and Gerds, Thomas and Gonen, Mithat and Obuchowski, Nancy and Pencina, Michael J. and Kattan, Michael W.},
  year = {2010},
  month = jan,
  volume = {21},
  pages = {128--138},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e3181c30fb2},
  abstract = {The performance of prediction models can be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or area under the receiver operating characteristic (ROC) curve), and goodness-of-fit statistics for calibration., Several new measures have recently been proposed that can be seen as refinements of discrimination measures, including variants of the c statistic for survival, reclassification tables, net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Moreover, decision\textendash analytic measures have been proposed, including decision curves to plot the net benefit achieved by making decisions based on model predictions., We aimed to define the role of these relatively novel approaches in the evaluation of the performance of prediction models. For illustration we present a case study of predicting the presence of residual tumor versus benign tissue in patients with testicular cancer (n=544 for model development, n=273 for external validation)., We suggest that reporting discrimination and calibration will always be important for a prediction model. Decision-analytic measures should be reported if the predictive model is to be used for making clinical decisions. Other measures of performance may be warranted in specific applications, such as reclassification metrics to gain insight into the value of adding a novel predictor to an established model.},
  file = {/home/victor/Zotero/storage/XXH8BP7U/Steyerberg et al. - 2010 - Assessing the performance of prediction models a .pdf},
  journal = {Epidemiology (Cambridge, Mass.)},
  number = {1},
  pmcid = {PMC3575184},
  pmid = {20010215}
}

@misc{stock_notes_nodate,
  title = {Notes on Optimal Transport},
  author = {Stock, Michiel},
  abstract = {This summer, I stumbled upon the optimal transportation problem, an optimization paradigm where the goal is to transform one probability distribution into another with a minimal cost. It is so simple to understand, yet it has a mind-boggling number of applications in probability, computer vision, machine learning, computational fluid dynamics, and computational biology. I recently gave a seminar on this topic, and this post is an overview of the topic. Slides can be found on my SlideShare and some implementations can are shown in a Jupyter notebook in my Github repo. Enjoy!},
  file = {/home/victor/Zotero/storage/PA4SC2UE/OptimalTransport.html},
  howpublished = {https://michielstock.github.io/OptimalTransport/}
}

@article{stuart_inverse_2010,
  title = {Inverse Problems: {{A Bayesian}} Perspective},
  shorttitle = {Inverse Problems},
  author = {Stuart, A. M.},
  year = {2010},
  month = may,
  volume = {19},
  pages = {451--559},
  issn = {0962-4929, 1474-0508},
  doi = {10.1017/S0962492910000061},
  file = {/home/victor/Zotero/storage/EAVUSLYB/stuart15c.pdf},
  journal = {Acta Numerica},
  keywords = {Bayesian inference,Inverse problems},
  language = {en}
}

@article{sudret_global_2008,
  title = {Global Sensitivity Analysis Using Polynomial Chaos Expansion},
  author = {Sudret, Bruno},
  year = {2008},
  month = jul,
  volume = {93},
  pages = {964--979},
  doi = {10.1016/j.ress.2007.04.002},
  abstract = {Global sensitivity analysis (SA) aims at quantifying the respective effects of input random variables (or combinations thereof) onto the variance of the response of a physical or mathematical model. Among the abundant literature on sensitivity measures, the Sobol' indices have received much attention since they provide accurate information for most models. The paper introduces generalized polynomial chaos expansions (PCE) to build surrogate models that allow one to compute the Sobol' indices analytically as a post-processing of the PCE coefficients. Thus the computational cost of the sensitivity indices practically reduces to that of estimating the PCE coefficients. An original non intrusive regression-based approach is proposed, together with an experimental design of minimal size. Various application examples illustrate the approach, both from the field of global SA (i.e. well-known benchmark problems) and from the field of stochastic mechanics. The proposed method gives accurate results for various examples that involve up to eight input random variables, at a computational cost which is 2\textendash 3 orders of magnitude smaller than the traditional Monte Carlo-based evaluation of the Sobol' indices.},
  file = {/home/victor/Zotero/storage/YJFEYP88/Sudret - 2008 - Global sensitivity analysis using polynomial chaos.pdf},
  journal = {Reliability Engineering \& System Safety}
}

@incollection{sudret_polynomial_2015,
  title = {Polynomial Chaos Expansions and Stochastic Finite Element Methods},
  booktitle = {Risk and {{Reliability}} in {{Geotechnical Engineering}}},
  author = {Sudret, Bruno},
  editor = {{Kok-Kwang Phoon}, Jianye Ching},
  year = {2015},
  pages = {265--300},
  publisher = {{CRC Press}},
  abstract = {This paper is a state-of-the art review on sparse polynomial chaos expansions (PCE) for engineering applications. It contains a step-by-step presentation of PCEs and their use in moment-, sensitivity- and reliability analysis.  Error estimators and sparse expansions for addressing high-dimensional problems are discussed. Applications in geotechnical engineering showcase the efficiency of sparse PCEs.},
  file = {/home/victor/Zotero/storage/VMUHQL7E/Sudret - 2015 - Polynomial chaos expansions and stochastic finite .pdf},
  keywords = {geotechnics,global sensitivity analysis,polynomial chaos expansions,risk analysis}
}

@article{sudret_polynomial_nodate,
  title = {Polynomial Chaos Expansions and Stochastic Finite Element Methods},
  author = {Sudret, Bruno},
  pages = {43},
  file = {/home/victor/Zotero/storage/WLS85HD6/Sudret - Polynomial chaos expansions and stochastic finite .pdf},
  language = {en}
}

@article{sunar_comparative_nodate,
  title = {A {{Comparative Study}} of {{Multiobjective Optimization Methods}} in {{Structural Design}}},
  author = {Sunar, Mehmet and Kahraman, Ramazan},
  pages = {10},
  abstract = {The computational algorithms of different multiobjective optimization techniques and their applications to structural systems are presented. The weighting, -constraint, goal programming and modified game theory methods are described along with a comparative study of the results. The conflicting nature of the objective functions is studied through two multiobjective optimization problems. Specifically, the design of a 25-bar space truss and that of a satellite with flexible appendages are considered in numerical studies. The results from the multiobjective optimization methods are evaluated in terms of a supercriterion. It is concluded that the results obtained using the goal programming and modified game theory/goal programming approaches are properly balanced yielding the best compromise in the presence of conflicting objectives.},
  file = {/home/victor/Zotero/storage/CBH38ZKP/Sunar et Kahraman - A Comparative Study of Multiobjective Optimization.pdf},
  language = {en}
}

@article{syring_gibbs_nodate,
  title = {Gibbs Posterior Inference on Value-at-Risk},
  author = {Syring, Nicholas and Hong, Liang and Martin, Ryan},
  pages = {13},
  abstract = {Accurate estimation of value-at-risk (VaR) and assessment of associated uncertainty is crucial for both insurers and regulators, particularly in Europe. Existing approaches link data and VaR indirectly by first linking data to the parameter of a probability model, and then expressing VaR as a function of that parameter. This indirect approach exposes the insurer to model misspecification bias or estimation inefficiency, depending on whether the parameter is finite- or infinite-dimensional. In this paper, we link data and VaR directly via what we call a discrepancy function, and this leads naturally to a Gibbs posterior distribution for VaR that does not suffer from the aforementioned biases and inefficiencies. Asymptotic consistency and root-n concentration rate of the Gibbs posterior are established, and simulations highlight its superior finite-sample performance compared to other approaches.},
  file = {/home/victor/Zotero/storage/ZUWS6KBX/Syring et al. - Gibbs posterior inference on value-at-risk.pdf},
  language = {en}
}

@article{taddy_fast_2008,
  title = {Fast {{Bayesian Inference}} for {{Computer Simulation Inverse Problems}}},
  author = {Taddy, Matthew and Lee, Herbert KH and Sans{\'o}, Bruno},
  year = {2008},
  file = {/home/victor/Zotero/storage/BUF9L83Q/Taddy et al. - 2008 - Fast Bayesian Inference for Computer Simulation In.pdf;/home/victor/Zotero/storage/EM6S6634/Taddy et al. - 2008 - Fast Bayesian Inference for Computer Simulation In.pdf;/home/victor/Zotero/storage/UUJJQSNS/Fast Bayesian Inference for Computer Simulation Inverse Problems.pdf},
  journal = {submitted to Inverse Problems}
}

@article{taddy_fast_2009,
  title = {Fast Inference for Statistical Inverse Problems},
  author = {Taddy, Matthew A and Lee, Herbert K H and Sans{\'o}, Bruno},
  year = {2009},
  month = aug,
  volume = {25},
  pages = {085001},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/25/8/085001},
  file = {/home/victor/Zotero/storage/C7AZEST2/invprob09.pdf},
  journal = {Inverse Problems},
  keywords = {stochastic inverse problem},
  number = {8}
}

@book{tarantola_inverse_2005,
  title = {Inverse Problem Theory and Methods for Model Parameter Estimation},
  author = {Tarantola, Albert},
  year = {2005},
  publisher = {{SIAM, Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, Pa}},
  file = {/home/victor/Zotero/storage/XZX7CY5V/InverseProblemTheory.pdf},
  isbn = {978-0-89871-572-9},
  keywords = {stochastic inverse problem},
  language = {eng}
}

@techreport{terlizzese_relative_2008,
  title = {Relative {{Minimax}}},
  author = {Terlizzese, Daniele},
  year = {2008},
  month = may,
  institution = {{Einaudi Institute for Economics and Finance (EIEF)}},
  abstract = {To achieve robustness, a decision criterion that recently has been widely adopted is Wald's minimax, after Gilboa and Schmeidler (1989) showed that (one generalisation of) it can be given an axiomatic foundation with a behavioural interpretation. Yet minimax has known drawbacks. A better alternative is Savage's minimax regret, recently axiomatized by Stoye (2006). A related alternative is relative minimax, known as competitive ratio in the computer science literature, which is appealingly unit free. This paper provides an axiomatisation with behavioural content for relative minimax.},
  file = {/home/victor/Zotero/storage/PUJCMRGI/Terlizzese - 2008 - Relative Minimax.pdf;/home/victor/Zotero/storage/92U36CJ4/0804.html},
  language = {en},
  number = {0804}
}

@article{thyer_critical_2009,
  title = {Critical Evaluation of Parameter Consistency and Predictive Uncertainty in Hydrological Modeling: {{A}} Case Study Using {{Bayesian}} Total Error Analysis: {{PARAMETER CONSISTENCY AND PREDICTIVE UNCERTAINTY}}},
  shorttitle = {Critical Evaluation of Parameter Consistency and Predictive Uncertainty in Hydrological Modeling},
  author = {Thyer, Mark and Renard, Benjamin and Kavetski, Dmitri and Kuczera, George and Franks, Stewart William and Srikanthan, Sri},
  year = {2009},
  month = dec,
  volume = {45},
  issn = {00431397},
  doi = {10.1029/2008WR006825},
  file = {/home/victor/Zotero/storage/Q9Q3C5TA/Thyer et al. - 2009 - Critical evaluation of parameter consistency and p.pdf},
  journal = {Water Resources Research},
  language = {en},
  number = {12}
}

@article{tibshirani_regression_2011,
  title = {Regression Shrinkage and Selection via the Lasso: A Retrospective},
  shorttitle = {Regression Shrinkage and Selection via the Lasso},
  author = {Tibshirani, Robert},
  year = {2011},
  volume = {73},
  pages = {273--282},
  publisher = {{Wiley Online Library}},
  file = {/home/victor/Zotero/storage/KVZZZSDF/Tibshirani - 2011 - Regression shrinkage and selection via the lasso .pdf;/home/victor/Zotero/storage/T5TJGWPU/(ISSN)1467-9868.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  number = {3}
}

@book{tikhonov_solutions_1977,
  title = {Solutions of Ill-Posed Problems},
  author = {Tikhonov, Andrei and Arsenin, Vasily},
  year = {1977},
  volume = {14},
  keywords = {Regularization,Tikhonov}
}

@misc{tipping_bayesian_,
  title = {Bayesian {{Inference}}: {{An Introduction}} to {{Principles}} and {{Practice}} in {{Machine Learning}}},
  author = {Tipping, Michael E.},
  file = {/home/victor/Zotero/storage/FEGY8BPK/met-mlbayes.pdf},
  howpublished = {http://www.miketipping.com/papers/met-mlbayes.pdf}
}

@book{toro_riemann_2009,
  title = {Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction},
  shorttitle = {Riemann Solvers and Numerical Methods for Fluid Dynamics},
  author = {Toro, Eleuterio F.},
  year = {2009},
  edition = {3. ed},
  publisher = {{Springer}},
  address = {{Berlin}},
  abstract = {High resolution upwind and centered methods are today a mature generation of computational techniques applicable to a wide range of engineering and scientific disciplines, Computational Fluid Dynamics (CFD) being the most prominent up to now. This textbook gives a comprehensive, coherent and practical presentation of this class of techniques. The book is designed to provide readers with an understanding of the basic concepts, some of the underlying theory, the ability to critically use the current research papers on the subject, and, above all, with the required information for the practical implementation of the methods. Applications include: compressible, steady, unsteady, reactive, viscous, non-viscous and free surface flows. For this third edition, the book was thoroughly revised. It contains substantial more and new material both in its fundamental as well as in its applied part},
  file = {C\:\\Users\\Victor\\Documents\\Eleuterio_F._Toro_Riemann_Solvers_and_Numerical_Methods_for_Fluid_Dynamics.pdf},
  isbn = {978-3-540-25202-3 978-3-540-49834-6},
  keywords = {Finite Volume},
  language = {eng}
}

@article{tran_spectral_2016,
  title = {Spectral {{M}}-Estimation with {{Applications}} to {{Hidden Markov Models}}},
  author = {Tran, Dustin and Kim, Minjae and {Doshi-Velez}, Finale},
  year = {2016},
  month = mar,
  abstract = {Method of moment estimators exhibit appealing statistical properties, such as asymptotic unbiasedness, for nonconvex problems. However, they typically require a large number of samples and are extremely sensitive to model misspecification. In this paper, we apply the framework of M-estimation to develop both a generalized method of moments procedure and a principled method for regularization. Our proposed M-estimator obtains optimal sample efficiency rates (in the class of moment-based estimators) and the same well-known rates on prediction accuracy as other spectral estimators. It also makes it straightforward to incorporate regularization into the sample moment conditions. We demonstrate empirically the gains in sample efficiency from our approach on hidden Markov models.},
  archivePrefix = {arXiv},
  eprint = {1603.08815},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/NJYEM92J/Tran et al. - 2016 - Spectral M-estimation with Applications to Hidden .pdf;/home/victor/Zotero/storage/6MMGQQSJ/1603.html},
  journal = {arXiv:1603.08815 [cs, stat]},
  keywords = {Statistics - Computation,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@misc{trappler_robust_2020,
  title = {Robust Calibration of Numerical Models Based on Relative Regret ({{Preprint}})},
  author = {Trappler, Victor and Arnaud, {\'E}lise and Vidard, Arthur and Debreu, Laurent},
  year = {2020},
  month = feb,
  abstract = {Classical methods of parameter estimation usually imply the minimisation of an objective function, that measures the error between some observations and the results obtained by a numerical model. In the presence of random inputs, the objective function becomes a random variable, and notions of robustness have to be introduced. In this paper, we are going to present how to take into account those uncertainties by defining a family of calibration objectives based on the notion of relative-regret with respect to the best attainable performance given the uncertainties and compare it with the minimum in the mean sense, and the minimum of variance.},
  language = {en}
}

@article{trossman_impact_2013,
  title = {Impact of Parameterized Lee Wave Drag on the Energy Budget of an Eddying Global Ocean Model},
  author = {Trossman, David S. and Arbic, Brian K. and Garner, Stephen T. and Goff, John A. and Jayne, Steven R. and Metzger, E. Joseph and Wallcraft, Alan J.},
  year = {2013},
  volume = {72},
  pages = {119--142},
  file = {/home/victor/Zotero/storage/5DLGJ7RB/Trossman et al. - 2013 - Impact of parameterized lee wave drag on the energ.pdf;/home/victor/Zotero/storage/Q2GEN43H/S1463500313001595.html},
  journal = {Ocean Modelling}
}

@article{ulaganathan_high_2016,
  title = {High Dimensional {{Kriging}} Metamodelling Utilising Gradient Information},
  author = {Ulaganathan, Selvakumar and Couckuyt, Ivo and Dhaene, Tom and Degroote, Joris and Laermans, Eric},
  year = {2016},
  volume = {40},
  pages = {5256--5270},
  file = {/home/victor/Zotero/storage/Z94MSB8G/8159395.pdf},
  journal = {Applied Mathematical Modelling},
  number = {9}
}

@article{uryasev_var_nodate,
  title = {{{VaR}} vs {{CVaR}} in {{Risk Management}} and {{Optimization}}},
  author = {Uryasev, Stan},
  pages = {75},
  file = {/home/victor/Zotero/storage/3VFL725Y/Uryasev - VaR vs CVaR in Risk Management and Optimization.pdf},
  journal = {Risk Management},
  language = {en}
}

@article{valpine_monte_2004,
  title = {Monte {{Carlo State}}-{{Space Likelihoods}} by {{Weighted Posterior Lernel Density Estimation}}},
  author = {Valpine, Perry De},
  year = {2004},
  pages = {523--536},
  abstract = {Maximum likelihood estimation and likelihood ratio tests for nonlinear, non-Gaussian state-space models require numerical integration for likelihood calculations. Several methods, including Monte Carlo (MC) expectation maximization, MC likelihood ratios, direct MC integra-tion, and particle  lter likelihoods, are inef  cient for the motivating problem of stage-structured population dynamics models in experimen-tal settings. An MC kernel likelihood (MCKL) method is presented that estimates classical likelihoods up to a constant by weighted kernel density estimates of Bayesian posteriors. MCKL is derived by using Bayesian posteriors as importance sampling densities for unnormalized kernel smoothing integrals. MC error and mode bias due to kernel smoothing are discussed and two methods for reducing mode bias are proposed: ``zooming in '' on the maximum likelihood parameters using a focused prior based on an initial estimate and using a posterior cumulant-based approximation of mode bias. A simulated example shows that MCKL can be much more ef  cient than previous approaches for the population dynamics problem. The zooming-in and cumulant-based corrections are illustrated with a multivariate variance estimation problem for which accurate results are obtained even in 20 parameter dimensions.},
  file = {/home/victor/Zotero/storage/258GQKR7/Valpine - 2004 - Monte Carlo State-Space Likelihoods by Weighted Po.pdf;/home/victor/Zotero/storage/8UFFC2QL/summary.html},
  journal = {Journal of the American Statistical Association}
}

@article{van_barel_robust_2017,
  title = {Robust {{Optimization}} of {{PDEs}} with {{Random Coefficients Using}} a {{Multilevel Monte Carlo Method}}},
  author = {Van Barel, Andreas and Vandewalle, Stefan},
  year = {2017},
  month = nov,
  abstract = {This paper addresses optimization problems constrained by partial differential equations with uncertain coefficients. In particular, the robust control problem and the average control problem are considered for a tracking type cost functional with an additional penalty on the variance of the state. The expressions for the gradient and Hessian corresponding to either problem contain expected value operators. Due to the large number of uncertainties considered in our model, we suggest to evaluate these expectations using a multilevel Monte Carlo (MLMC) method. Under mild assumptions, it is shown that this results in the gradient and Hessian corresponding to the MLMC estimator of the original cost functional. Furthermore, we show that the use of certain correlated samples yields a reduction in the total number of samples required. Two optimization methods are investigated: the nonlinear conjugate gradient method and the Newton method. For both, a specific algorithm is provided that dynamically decides which and how many samples should be taken in each iteration. The cost of the optimization up to some specified tolerance \$\textbackslash tau\$ is shown to be proportional to the cost of a gradient evaluation with requested root mean square error \$\textbackslash tau\$. The algorithms are tested on a model elliptic diffusion problem with lognormal diffusion coefficient. An additional nonlinear term is also considered.},
  archivePrefix = {arXiv},
  eprint = {1711.02574},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/ZHZ5Z22V/Van Barel et Vandewalle - 2017 - Robust Optimization of PDEs with Random Coefficien.pdf;/home/victor/Zotero/storage/BDBZFZNI/1711.html},
  journal = {arXiv:1711.02574 [cs, math]},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Optimization and Control,Mathematics - Probability},
  primaryClass = {cs, math}
}

@article{van_parys_data_2017,
  title = {From {{Data}} to {{Decisions}}: {{Distributionally Robust Optimization}} Is {{Optimal}}},
  shorttitle = {From {{Data}} to {{Decisions}}},
  author = {Van Parys, Bart PG and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
  year = {2017},
  file = {/home/victor/Zotero/storage/KMNWRXK6/5961.pdf},
  journal = {arXiv preprint arXiv:1704.04118}
}

@article{vapnik_overview_1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, Vladimir N.},
  year = {1999},
  volume = {10},
  pages = {988--999},
  file = {/home/victor/Zotero/storage/G66K28RE/slt.pdf},
  journal = {IEEE transactions on neural networks},
  number = {5}
}

@inproceedings{vapnik_principles_1992,
  title = {Principles of Risk Minimization for Learning Theory},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vapnik, Vladimir},
  year = {1992},
  pages = {831--838},
  file = {/home/victor/Zotero/storage/9QIH4GBC/Vapnik - 1992 - Principles of risk minimization for learning theor.pdf}
}

@article{vazquez_3._nodate,
  title = {3. {{Some}} Tools for the Analysis of Sequential Strategies Based on a {{Gaussian}} Process Prior},
  author = {Vazquez, E},
  pages = {14},
  file = {/home/victor/Zotero/storage/FSRSMT7Y/Vazquez - 3. Some tools for the analysis of sequential strat.pdf},
  language = {en}
}

@article{vazquez_convergence_2010,
  title = {Convergence Properties of the Expected Improvement Algorithm with Fixed Mean and Covariance Functions},
  author = {Vazquez, Emmanuel and Bect, Julien},
  year = {2010},
  month = nov,
  volume = {140},
  pages = {3088--3095},
  issn = {03783758},
  doi = {10.1016/j.jspi.2010.04.018},
  abstract = {This paper deals with the convergence of the expected improvement algorithm, a popular global optimization algorithm based on a Gaussian process model of the function to be optimized. The first result is that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k. The second result states that the density property also holds for P-almost all continuous functions, where P is the (prior) probability distribution induced by the Gaussian process.},
  file = {/home/victor/Zotero/storage/HYPX6PGX/Vazquez et Bect - 2010 - Convergence properties of the expected improvement.pdf},
  journal = {Journal of Statistical Planning and Inference},
  language = {en},
  number = {11}
}

@inproceedings{vazquez_estimating_2005,
  title = {Estimating Derivatives and Integrals with {{Kriging}}},
  author = {Vazquez, E. and Walter, E.},
  year = {2005},
  pages = {8156--8161},
  publisher = {{IEEE}},
  doi = {10.1109/CDC.2005.1583482},
  abstract = {This paper formalizes a methodology based on Kriging, a technique developped by geostatisticians, for estimating derivatives and integrals of signals that are only known via possibly irregularly spaced and noisy observations. This finds direct applications, e.g., in system identification when differential algebra is used to express parameters as nonlinear functions of the inputs and outputs and their derivatives. The procedure is quite simple to implement, and allows confidence intervals on the predicted values to be derived.},
  file = {/home/victor/Zotero/storage/4BSCESRW/Vazquez et Walter - 2005 - Estimating derivatives and integrals with Kriging.pdf},
  isbn = {978-0-7803-9567-1},
  language = {en}
}

@article{vazquez_new_2014,
  title = {A New Integral Loss Function for {{Bayesian}} Optimization},
  author = {Vazquez, Emmanuel and Bect, Julien},
  year = {2014},
  month = aug,
  abstract = {We consider the problem of maximizing a real-valued continuous function f using a Bayesian approach. Since the early work of Jonas Mockus and Antanas Z\textasciicaron{} ilinskas in the 70's, the problem of optimization is usually formulated by considering the loss function max f - Mn (where Mn denotes the best function value observed after n evaluations of f ). This loss function puts emphasis on the value of the maximum, at the expense of the location of the maximizer. In the special case of a one-step Bayes-optimal strategy, it leads to the classical Expected Improvement (EI) sampling criterion. This is a special case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk associated to a certain uncertainty measure (here, the expected loss) on the quantity of interest is minimized at each step of the algorithm. In this article, assuming that f is defined over a measure space (X, {$\lambda$}), we propose to consider instead the integral loss function X( f - Mn)+ d{$\lambda$}, and we show that this leads, in the case of a Gaussian process prior, to a new numerically tractable sampling criterion that we call EI2 (for Expected Integrated Expected Improvement). A numerical experiment illustrates that a SUR strategy based on this new sampling criterion reduces the error on both the value and the location of the maximizer faster than the EI-based strategy.},
  archivePrefix = {arXiv},
  eprint = {1408.4622},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/TJXDPGCX/Vazquez et Bect - 2014 - A new integral loss function for Bayesian optimiza.pdf},
  journal = {arXiv:1408.4622 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{verdu_minimax_1984,
  title = {On Minimax Robustness: {{A}} General Approach and Applications},
  shorttitle = {On Minimax Robustness},
  author = {Verdu, Sergio and Poor, H.},
  year = {1984},
  volume = {30},
  pages = {328--340},
  file = {/home/victor/Zotero/storage/X55UAWLR/Verdu et Poor - 1984 - On minimax robustness A general approach and appl.pdf;/home/victor/Zotero/storage/XCNS6EWW/1056876.html},
  journal = {IEEE Transactions on Information Theory},
  number = {2}
}

@article{vernon_bayesian_2017,
  title = {A {{Bayesian}} Computer Model Analysis of {{Robust Bayesian}} Analyses},
  author = {Vernon, Ian and Gosling, John Paul},
  year = {2017},
  month = mar,
  abstract = {We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.},
  archivePrefix = {arXiv},
  eprint = {1703.01234},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/MUPGW78H/Vernon et Gosling - 2017 - A Bayesian computer model analysis of Robust Bayes.pdf},
  journal = {arXiv:1703.01234 [stat]},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{villemonteix_informational_2006,
  title = {An Informational Approach to the Global Optimization of Expensive-to-Evaluate Functions},
  author = {Villemonteix, Julien and Vazquez, Emmanuel and Walter, Eric},
  year = {2006},
  month = nov,
  abstract = {In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizer entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on stepwise uncertainty reduction, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the Efficient Global Optimization (EGO) algorithm. An empirical comparison is carried out between our criterion and expected improvement, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization) is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.},
  archivePrefix = {arXiv},
  eprint = {cs/0611143},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/A85EICRS/Villemonteix et al. - 2006 - An informational approach to the global optimizati.pdf},
  journal = {arXiv:cs/0611143},
  keywords = {Computer Science - Numerical Analysis,G.1.1,G.1.6},
  language = {en}
}

@phdthesis{villemonteix_optimisation_2008,
  title = {{Optimisation de fonctions co\^uteuses{$<$}br /{$>$}Mod\`eles gaussiens pour une utilisation efficace du budget d'\'evaluations : th\'eorie et pratique industrielle}},
  shorttitle = {{Optimisation de fonctions co\^uteuses{$<$}br /{$>$}Mod\`eles gaussiens pour une utilisation efficace du budget d'\'evaluations}},
  author = {Villemonteix, Julien},
  year = {2008},
  month = dec,
  abstract = {Cette th\`ese traite d'une question centrale dans de nombreux probl\`emes d'optimisation, en particulier{$<$}br /{$>$}en ing\'enierie. Comment optimiser une fonction lorsque le nombre d'\'evaluations autoris\'e est tr\`es limit\'e au regard de la dimension et de la complexit\'e du probl\`eme ? Par exemple, lorsque le budget d'\'evaluations est limit\'e par la dur\'ee des simulations num\'eriques du syst\`eme \`a optimiser, il n'est pas rare de devoir optimiser trente param\`etres avec moins{$<$}br /{$>$}de cent \'evaluations. Ce travail traite d'algorithmes d'optimisation sp\'ecifiques \`a ce contexte pour lequel la plupart des m\'ethodes classiques sont inadapt\'ees.{$<$}br /{$>$}Le principe commun aux m\'ethodes propos\'ees est d'exploiter les propri\'et\'es des processus gaussiens et du krigeage pour construire une approximation peu co\^uteuse de la fonction \`a optimiser. Cette approximation est ensuite utilis\'ee pour choisir it\'erativement les \'evaluations \`a r\'ealiser. Ce choix est dict\'e par un crit\`ere d'\'echantillonnage qui combine recherche locale, \`a proximit\'e des r\'esultats prometteurs, et recherche globale, dans les zones non explor\'ees. La plupart des crit\`eres propos\'es dans la litt\'erature, tel celui de l'algorithme EGO (pour Efficient Global Optimization), cherchent \`a \'echantillonner la fonction l\`a o\`u l'apparition d'un optimum est jug\'ee la plus probable. En comparaison, l'algorithme IAGO (pour Informational Approach to Global Optimization), principale contribution de nos travaux, cherche \`a maximiser la quantit\'e d'information apport\'ee, sur la position de l'optimum, par l'\'evaluation r\'ealis\'ee. Des probl\'ematiques industrielles ont guid\'e l'organisation de ce m\'emoire, qui se destine \`a la communaut\'e de l'optimisation{$<$}br /{$>$}tout comme aux praticiens confront\'es \`a des fonctions \`a l'\'evaluation co\^uteuse. Aussi les applications industrielles y tiennent-elles une place importante tout comme la mise en place de l'algorithme IAGO. Nous d\'etaillons non seulement le cas standard de l'optimisation d'une fonction r\'eelle, mais aussi la prise en compte de contraintes, de{$<$}br /{$>$}bruit sur les r\'esultats des \'evaluations, de r\'esultats d'\'evaluation du gradient, de probl\`emes multi-objectifs, ou encore d'incertitudes de fabrication significatives.},
  file = {/home/victor/Zotero/storage/TLCKEE4N/Villemonteix - 2008 - Optimisation de fonctions co√ªteusesbr Mod√®les g.pdf;/home/victor/Zotero/storage/9LIXMHQU/tel-00351406.html},
  language = {fr},
  school = {Universit\'e Paris Sud - Paris XI}
}

@article{volkwein_model_2011,
  title = {Model Reduction Using Proper Orthogonal Decomposition},
  author = {Volkwein, Stefan},
  year = {2011},
  file = {/home/victor/Zotero/storage/PXXNM5ZI/POD-Vorlesung.pdf},
  journal = {Lecture Notes, Institute of Mathematics and Scientific Computing, University of Graz. see http://www. uni-graz. at/imawww/volkwein/POD. pdf},
  keywords = {MOR methods,POD}
}

@article{volkwein_proper_2005,
  title = {Proper Orthogonal Decomposition ({{POD}}) for Nonlinear Dynamical Systems},
  author = {Volkwein, Stefan},
  year = {2005},
  file = {/home/victor/Zotero/storage/WA26LTST/Volkwein-1.pdf},
  journal = {Dutch Institute of Systems and Control Summerschool},
  keywords = {MOR methods,POD}
}

@article{vorobyev_new_2003,
  title = {On the {{New Notion}} of the {{Set}}-{{Expectation}} for a {{Random Set}} of {{Events}}},
  author = {Vorobyev, Oleg and Vorobyev, Alexey},
  year = {2003},
  month = jan,
  abstract = {The paper introduces new notion for the set-valued mean set of a random set. The means are defined as families of sets that minimize mean distances to the random set. The distances are determined by metrics in spaces of sets or by suitable generalizations. Some examples illustrate the use of the new definitions.},
  file = {/home/victor/Zotero/storage/L6KWNU27/Vorobyev et Vorobyev - 2003 - On the New Notion of the Set-Expectation for a Ran.pdf},
  journal = {University Library of Munich, Germany, MPRA Paper}
}

@article{vuong_likelihood_1989,
  title = {Likelihood {{Ratio Tests}} for {{Model Selection}} and {{Non}}-{{Nested Hypotheses}}},
  author = {Vuong, Quang H.},
  year = {1989},
  month = mar,
  volume = {57},
  pages = {307},
  issn = {00129682},
  doi = {10.2307/1912557},
  file = {/home/victor/Zotero/storage/TX83J45K/Vuong - 1989 - Likelihood Ratio Tests for Model Selection and Non.pdf},
  journal = {Econometrica},
  language = {en},
  number = {2}
}

@article{wald_statistical_1945,
  title = {Statistical {{Decision Functions Which Minimize}} the {{Maximum Risk}}},
  author = {Wald, Abraham},
  year = {1945},
  volume = {46},
  pages = {265--280},
  issn = {0003-486X},
  doi = {10.2307/1969022},
  journal = {Annals of Mathematics},
  number = {2}
}

@article{wald_statistical_1945-1,
  title = {Statistical {{Decision Functions Which Minimize}} the {{Maximum Risk}}},
  author = {Wald, Abraham},
  year = {1945},
  month = apr,
  volume = {46},
  pages = {265},
  issn = {0003486X},
  doi = {10.2307/1969022},
  file = {/home/victor/Zotero/storage/FSYG3D76/Wald - 1945 - Statistical Decision Functions Which Minimize the .pdf},
  journal = {The Annals of Mathematics},
  language = {en},
  number = {2}
}

@article{walker_defining_2003,
  title = {Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support},
  shorttitle = {Defining Uncertainty},
  author = {Walker, Warren E. and Harremo{\"e}s, Poul and Rotmans, Jan and {van der Sluijs}, Jeroen P. and {van Asselt}, Marjolein BA and Janssen, Peter and {Krayer von Krauss}, Martin P.},
  year = {2003},
  volume = {4},
  pages = {5--17},
  file = {/home/victor/Zotero/storage/E6Q2GRNH/79.pdf},
  journal = {Integrated assessment},
  keywords = {Uncertainty analysis,UQ},
  number = {1}
}

@article{wang_asymptotic_1999,
  title = {Asymptotic {{Properties}} of {{M}}-Estimators {{Based}} on {{Estimating Equations}} and {{Censored Data}}},
  author = {Wang, Jane-Ling},
  year = {1999},
  volume = {26},
  pages = {297--318},
  file = {/home/victor/Zotero/storage/7KCAXALE/Wang - 1999 - Asymptotic Properties of M-estimators Based on Est.pdf;/home/victor/Zotero/storage/HT2CNHF4/Wang - 1999 - Asymptotic Properties of M-estimators Based on Est},
  journal = {Scandinavian journal of statistics},
  number = {2}
}

@article{wang_evaluation_2014,
  title = {An Evaluation of Adaptive Surrogate Modeling Based Optimization with Two Benchmark Problems},
  author = {Wang, Chen and Duan, Qingyun and Gong, Wei and Ye, Aizhong and Di, Zhenhua and Miao, Chiyuan},
  year = {2014},
  month = oct,
  volume = {60},
  pages = {167--179},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2014.05.026},
  abstract = {Surrogate modeling uses cheap ``surrogates'' to represent the response surface of simulation models. It involves several steps, including initial sampling, regression and adaptive sampling. This study evaluates an adaptive surrogate modeling based optimization (ASMO) method on two benchmark problems: the Hartman function and calibration of the SAC-SMA hydrologic model. Our results show that: 1) Gaussian Processes are the best surrogate model construction method. A minimum Interpolation Surface method is the best adaptive sampling method. Low discrepancy Quasi Monte Carlo methods are the most suitable initial sampling designs. Some 15\textendash 20 times the dimension of the problem may be the proper initial sample size; 2) The ASMO method is much more efficient than the widely used Shuffled Complex Evolution global optimization method. However, ASMO can provide only approximate optimal solutions, whose precision is limited by surrogate modeling methods and problem-specific features; and 3) The identifiability of model parameters is correlated with parameter sensitivity.},
  file = {/home/victor/Zotero/storage/I4NUQW6Q/Wang et al. - 2014 - An evaluation of adaptive surrogate modeling based.pdf;/home/victor/Zotero/storage/DB6ATLC3/S1364815214001698.html},
  journal = {Environmental Modelling \& Software},
  keywords = {Adaptive sampling,Adaptive surrogate modeling based optimization,Computationally intensive computer models,Design of experiment,Global sensitivity analysis}
}

@article{wang_max-value_2017,
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2017},
  month = mar,
  abstract = {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the \$\textbackslash arg\textbackslash max\$ of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.},
  archivePrefix = {arXiv},
  eprint = {1703.01968},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/NMAKJPDF/Wang et Jegelka - 2017 - Max-value Entropy Search for Efficient Bayesian Op.pdf;/home/victor/Zotero/storage/TU5YA9D2/1703.html},
  journal = {arXiv:1703.01968 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@inproceedings{wang_new_2017,
  title = {A New Acquisition Function for {{Bayesian}} Optimization Based on the Moment-Generating Function},
  author = {Wang, Hao and {van Stein}, Bas and Emmerich, Michael and Back, Thomas},
  year = {2017},
  month = oct,
  pages = {507--512},
  publisher = {{IEEE}},
  doi = {10.1109/SMC.2017.8122656},
  abstract = {Bayesian Optimization or Efficient Global Optimization (EGO) is a global search strategy that is designed for expensive black-box functions. In this algorithm, a statistical model (usually the Gaussian process model) is constructed on some initial data samples. The global optimum is approached by iteratively maximizing a so-called acquisition function, that balances the exploration and exploitation effect of the search. The performance of such an algorithm is largely affected by the choice of the acquisition function. Inspired by the usage of higher moments from the Gaussian process model, it is proposed to construct a novel acquisition function based on the moment-generating function (MGF) of the improvement, which is the stochastic gain over the current best fitness value by sampling at an unknown point. This MGF-based acquisition function takes all the higher moments into account and introduces an additional real-valued parameter to control the trade-off between exploration and exploitation. The motivation, rationale and closed-form expression of the proposed function are discussed in detail. In addition, we also illustrate its advantage over other acquisition functions, especially the so-called generalized expected improvement.},
  file = {/home/victor/Zotero/storage/H8NR48J2/Wang et al. - 2017 - A new acquisition function for Bayesian optimizati.pdf},
  isbn = {978-1-5386-1645-1},
  language = {en}
}

@article{wang_optimization_2018,
  title = {Optimization of {{Smooth Functions}} with {{Noisy Observations}}: {{Local Minimax Rates}}},
  shorttitle = {Optimization of {{Smooth Functions}} with {{Noisy Observations}}},
  author = {Wang, Yining and Balakrishnan, Sivaraman and Singh, Aarti},
  year = {2018},
  month = mar,
  abstract = {We consider the problem of global optimization of an unknown non-convex smooth function with zeroth-order feedback. In this setup, an algorithm is allowed to adaptively query the underlying function at different locations and receives noisy evaluations of function values at the queried points (i.e. the algorithm has access to zeroth-order information). Optimization performance is evaluated by the expected difference of function values at the estimated optimum and the true optimum. In contrast to the classical optimization setup, first-order information like gradients are not directly accessible to the optimization algorithm. We show that the classical minimax framework of analysis, which roughly characterizes the worst-case query complexity of an optimization algorithm in this setting, leads to excessively pessimistic results. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations, which provides a refined picture of the intrinsic difficulty of zeroth-order optimization. We show that for functions with fast level set growth around the global minimum, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems [1, 22]. At the other end of the spectrum, for worst-case smooth functions no algorithm can converge faster than the minimax rate of estimating the entire unknown function in the {$\mathscr{l}$}8-norm. We provide an intuitive and efficient algorithm that attains the derived upper error bounds. Finally, using the local minimax framework we are able to clearly dichotomize adaptive and non-adaptive algorithms by showing that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate.},
  archivePrefix = {arXiv},
  eprint = {1803.08586},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/EBLYPEB9/Wang et al. - 2018 - Optimization of Smooth Functions with Noisy Observ.pdf},
  journal = {arXiv:1803.08586 [cs, math, stat]},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@book{wasserman_all_2006,
  title = {All of Nonparametric Statistics},
  author = {Wasserman, Larry},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/home/victor/Zotero/storage/HHDBZCPS/Wasserman - 2006 - All of nonparametric statistics.pdf},
  isbn = {978-0-387-25145-5},
  keywords = {Nonparametric statistics},
  language = {en},
  lccn = {QA278.8 .W37 2006},
  series = {Springer Texts in Statistics}
}

@article{weijs_kullbackleibler_2010,
  title = {Kullback\textendash{{Leibler Divergence}} as a {{Forecast Skill Score}} with {{Classic Reliability}}\textendash{{Resolution}}\textendash{{Uncertainty Decomposition}}},
  author = {Weijs, Steven V. and {van Nooijen}, Ronald and {van de Giesen}, Nick},
  year = {2010},
  month = sep,
  volume = {138},
  pages = {3387--3399},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/2010MWR3229.1},
  abstract = {This paper presents a score that can be used for evaluating probabilistic forecasts of multicategory events. The score is a reinterpretation of the logarithmic score or ignorance score, now formulated as the relative entropy or Kullback\textendash Leibler divergence of the forecast distribution from the observation distribution. Using the information\textendash theoretical concepts of entropy and relative entropy, a decomposition into three components is presented, analogous to the classic decomposition of the Brier score. The information\textendash theoretical twins of the components uncertainty, resolution, and reliability provide diagnostic information about the quality of forecasts. The overall score measures the information conveyed by the forecast. As was shown recently, information theory provides a sound framework for forecast verification. The new decomposition, which has proven to be very useful for the Brier score and is widely used, can help acceptance of the logarithmic score in meteorology.},
  file = {/home/victor/Zotero/storage/9YB972RM/Weijs et al. - 2010 - Kullback‚ÄìLeibler Divergence as a Forecast Skill Sc.pdf},
  journal = {Monthly Weather Review},
  language = {en},
  number = {9}
}

@article{white_maximum_1982,
  title = {Maximum {{Likelihood Estimation}} of {{Misspecified Models}}},
  author = {White, Halbert},
  year = {1982},
  volume = {50},
  pages = {1--25},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1912526},
  abstract = {This paper examines the consequences and detection of model misspecification when using maximum likelihood techniques for estimation and inference. The quasi-maximum likelihood estimator (OMLE) converges to a well defined limit, and may or may not be consistent for particular parameters of interest. Standard tests (Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in the presence of misspecification, but more general statistics are given which allow inferences to be drawn robustly. The properties of the QMLE and the information matrix are exploited to yield several useful tests for model misspecification.},
  file = {/home/victor/Zotero/storage/2LP5W55I/white_maximum_likekihood_estimation.pdf},
  journal = {Econometrica},
  number = {1}
}

@article{wiener_homogeneous_1938,
  title = {The {{Homogeneous Chaos}}},
  author = {Wiener, Norbert},
  year = {1938},
  volume = {60},
  pages = {897--936},
  issn = {0002-9327},
  doi = {10.2307/2371268},
  journal = {American Journal of Mathematics},
  number = {4}
}

@book{wilke_fundamentals_nodate,
  title = {Fundamentals of {{Data Visualization}}},
  author = {Wilke, Claus O.},
  abstract = {A guide to making visualizations that accurately reflect the data, tell a story, and look professional.},
  file = {/home/victor/Zotero/storage/DK4WPX56/dataviz.html}
}

@article{williams_sequential_2000,
  title = {Sequential Design of Computer Experiments to Minimize Integrated Response Functions},
  author = {Williams, Brian J. and Santner, Thomas J. and Notz, William I.},
  year = {2000},
  pages = {1133--1152},
  file = {/home/victor/Zotero/storage/RM4Z5QR3/Williams et al. - 2000 - Sequential design of computer experiments to minim.pdf;/home/victor/Zotero/storage/YGB5AB9Z/24306770.html},
  journal = {Statistica Sinica}
}

@article{wilson_maximizing_2018,
  title = {Maximizing Acquisition Functions for {{Bayesian}} Optimization},
  author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
  year = {2018},
  month = may,
  abstract = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide the search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, highdimensional, and intractable. We present two modern approaches for maximizing acquisition functions that exploit key properties thereof, namely the differentiability of Monte Carlo integration and the submodularity of parallel querying.},
  archivePrefix = {arXiv},
  eprint = {1805.10196},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/Z5DW775A/Wilson et al. - 2018 - Maximizing acquisition functions for Bayesian opti.pdf},
  journal = {arXiv:1805.10196 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wit_all_2012,
  title = {`{{All}} Models Are Wrong...': An Introduction to Model Uncertainty: {{{\emph{Introduction}}}}{\emph{ to Model Uncertainty}}},
  shorttitle = {`{{All}} Models Are Wrong...'},
  author = {Wit, Ernst and van den Heuvel, Edwin and Romeijn, Jan-Willem},
  year = {2012},
  month = aug,
  volume = {66},
  pages = {217--236},
  issn = {00390402},
  doi = {10.1111/j.1467-9574.2012.00530.x},
  file = {/home/victor/Zotero/storage/E2B7ZAJV/2012_wit_et_al_-_all_models_are_wrong.pdf},
  journal = {Statistica Neerlandica},
  keywords = {Model inadequacy,Model selection},
  language = {en},
  number = {3}
}

@article{wolpert_no_1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  volume = {1},
  pages = {67--82},
  issn = {1089778X},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between e ective optimization algorithms and the problems they are solving. A number of \textbackslash no free lunch" (NFL) theorems are presented that establish that for any algorithm, any elevated performance over one class of problems is exactly paid for in performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed are time-varying optimization problems and a priori \textbackslash head-to-head" minimax distinctions between optimization algorithms, distinctions that can obtain despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  file = {/home/victor/Zotero/storage/3WVWVEGC/Wolpert et Macready - 1997 - No free lunch theorems for optimization.pdf},
  journal = {IEEE Transactions on Evolutionary Computation},
  language = {en},
  number = {1}
}

@article{wong_frequentist_2014,
  title = {A {{Frequentist Approach}} to {{Computer Model Calibration}}},
  author = {Wong, Raymond K. W. and Storlie, Curtis B. and Lee, Thomas C. M.},
  year = {2014},
  month = nov,
  abstract = {This paper considers the computer model calibration problem and provides a general frequentist solution. Under the proposed framework, the data model is semi-parametric with a nonparametric discrepancy function which accounts for any discrepancy between the physical reality and the computer model. In an attempt to solve a fundamentally important (but often ignored) identifiability issue between the computer model parameters and the discrepancy function, this paper proposes a new and identifiable parametrization of the calibration problem. It also develops a two-step procedure for estimating all the relevant quantities under the new parameterization. This estimation procedure is shown to enjoy excellent rates of convergence and can be straightforwardly implemented with existing software. For uncertainty quantification, bootstrapping is adopted to construct confidence regions for the quantities of interest. The practical performance of the proposed methodology is illustrated through simulation examples and an application to a computational fluid dynamics model.},
  archivePrefix = {arXiv},
  eprint = {1411.4723},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/H2NR9V7X/Wong et al. - 2014 - A Frequentist Approach to Computer Model Calibrati.pdf;/home/victor/Zotero/storage/J3MIEC85/1411.html},
  journal = {arXiv:1411.4723 [stat]},
  keywords = {Calibration,Frequentist},
  primaryClass = {stat}
}

@article{wu_bayesian_nodate,
  title = {Bayesian {{Optimization}} with {{Gradients}}},
  author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter},
  pages = {12},
  abstract = {Bayesian optimization has been successful at global optimization of expensiveto-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledgegradient (d-KG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. d-KG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
  file = {/home/victor/Zotero/storage/IW7GFGC8/Wu et al. - Bayesian Optimization with Gradients.pdf},
  language = {en}
}

@article{wu_inverse_2018,
  title = {Inverse {{Uncertainty Quantification}} Using the {{Modular Bayesian Approach}} Based on {{Gaussian Process}}, {{Part}} 1: {{Theory}}},
  shorttitle = {Inverse {{Uncertainty Quantification}} Using the {{Modular Bayesian Approach}} Based on {{Gaussian Process}}, {{Part}} 1},
  author = {Wu, Xu and Kozlowski, Tomasz and Meidani, Hadi and Shirvan, Koroush},
  year = {2018},
  month = jan,
  abstract = {In nuclear reactor system design and safety analysis, the Best Estimate plus Uncertainty (BEPU) methodology requires that computer model output uncertainties must be quantified in order to prove that the investigated design stays within acceptance criteria. "Expert opinion" and "user self-evaluation" have been widely used to specify computer model input uncertainties in previous uncertainty, sensitivity and validation studies. Inverse Uncertainty Quantification (UQ) is the process to inversely quantify input uncertainties based on experimental data in order to more precisely quantify such ad-hoc specifications of the input uncertainty information. In this paper, we used Bayesian analysis to establish the inverse UQ formulation, with systematic and rigorously derived metamodels constructed by Gaussian Process (GP). Due to incomplete or inaccurate underlying physics, as well as numerical approximation errors, computer models always have discrepancy/bias in representing the realities, which can cause over-fitting if neglected in the inverse UQ process. The model discrepancy term is accounted for in our formulation through the "model updating equation". We provided a detailed introduction and comparison of the full and modular Bayesian approaches for inverse UQ, as well as pointed out their limitations when extrapolated to the validation/prediction domain. Finally, we proposed an improved modular Bayesian approach that can avoid extrapolating the model discrepancy that is learnt from the inverse UQ domain to the validation/prediction domain.},
  archivePrefix = {arXiv},
  eprint = {1801.01782},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/SSSI56TQ/Wu et al. - 2018 - Inverse Uncertainty Quantification using the Modul.pdf;/home/victor/Zotero/storage/ZKX8968Q/1801.html},
  journal = {arXiv:1801.01782 [stat]},
  keywords = {Inverse UQ,inversion modeling,Statistics - Computation,UQ},
  primaryClass = {stat}
}

@article{wu_parallel_2016,
  title = {The {{Parallel Knowledge Gradient Method}} for {{Batch Bayesian Optimization}}},
  author = {Wu, Jian and Frazier, Peter I.},
  year = {2016},
  month = jun,
  abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.},
  archivePrefix = {arXiv},
  eprint = {1606.04414},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/7MDQJAV2/Wu et Frazier - 2016 - The Parallel Knowledge Gradient Method for Batch B.pdf;/home/victor/Zotero/storage/4C3GXJWK/1606.html},
  journal = {arXiv:1606.04414 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xiu_wiener--askey_2002,
  title = {The {{Wiener}}--{{Askey Polynomial Chaos}} for {{Stochastic Differential Equations}}},
  author = {Xiu, D. and Karniadakis, G.},
  year = {2002},
  month = jan,
  volume = {24},
  pages = {619--644},
  issn = {1064-8275},
  doi = {10.1137/S1064827501387826},
  abstract = {We present a new method for solving stochastic differential equations based on Galerkin projections and extensions of Wiener's polynomial chaos. Specifically, we represent the stochastic processes with an optimum trial basis from the Askey family of orthogonal polynomials that reduces the dimensionality of the system and leads to exponential convergence of the error. Several continuous and discrete processes are treated, and numerical examples show substantial speed-up compared to Monte Carlo simulations for low dimensional stochastic inputs.},
  file = {/home/victor/Zotero/storage/HGFIPU5K/S1064827501387826.html},
  journal = {SIAM Journal on Scientific Computing},
  number = {2}
}

@article{xiu_wiener-askey_nodate,
  title = {{{THE WIENER}}-{{ASKEY POLYNOMIAL CHAOS FOR STOCHASTIC DIFFERENTIAL EQUATIONS}}},
  author = {Xiu, Dongbin and Karniadakis, George Em},
  pages = {26},
  abstract = {We present a new method for solving stochastic differential equations based on Galerkin projections and extensions of Wiener's polynomial chaos. Specifically, we represent the stochastic processes with an optimum trial basis from the Askey family of orthogonal polynomials that reduces the dimensionality of the system and leads to exponential convergence of the error. Several continuous and discrete processes are treated, and numerical examples show substantial speed-up compared to Monte-Carlo simulations for low dimensional stochastic inputs.},
  file = {/home/victor/Zotero/storage/B2ENDBXJ/Xiu et Karniadakis - THE WIENER-ASKEY POLYNOMIAL CHAOS FOR STOCHASTIC D.pdf},
  language = {en}
}

@article{yizong_cheng_mean_1995,
  title = {Mean Shift, Mode Seeking, and Clustering},
  author = {{Yizong Cheng}},
  year = {Aug./1995},
  volume = {17},
  pages = {790--799},
  issn = {01628828},
  doi = {10.1109/34.400568},
  abstract = {Mean shift, a simple iterative procedure that shifts each data point to the average of data points in its neighborhood, is generalized and analyzed in this paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on a surface constructed with a ``shadow'' kernel. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis is treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.},
  file = {/home/victor/Zotero/storage/A2Z9TT4S/Yizong Cheng - 1995 - Mean shift, mode seeking, and clustering.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {8}
}

@unpublished{zahm_certified_2018,
  title = {Certified Dimension Reduction in Nonlinear {{Bayesian}} Inverse Problems},
  author = {Zahm, Olivier and Cui, Tiangang and Law, Kody and Spantini, Alessio and Marzouk, Youssef},
  year = {2018},
  month = jul,
  abstract = {We propose a dimension reduction technique for Bayesian inverse problems with nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation noise. The likelihood function is approximated by a ridge function, i.e., a map which depends non-trivially only on a few linear combinations of the parameters. We build this ridge approximation by minimizing an upper bound on the Kullback-Leibler divergence between the posterior distribution and its approximation. This bound, obtained via logarithmic Sobolev inequalities, allows one to certify the error of the posterior approximation. Computing the bound requires computing the second moment matrix of the gradient of the log-likelihood function. In practice, a sample-based approximation of the upper bound is then required. We provide an analysis that enables control of the posterior approximation error due to this sampling. Numerical and theoretical comparisons with existing methods illustrate the benefits of the proposed methodology.},
  file = {/home/victor/Zotero/storage/PLPEVGG7/Zahm et al. - 2018 - Certified dimension reduction in nonlinear Bayesia.pdf}
}

@phdthesis{zahm_model_2015,
  title = {Model Order Reduction Methods for Parameter-Dependent Equations\textendash{{Applications}} in {{Uncertainty Quantification}}.},
  author = {Zahm, Olivier},
  year = {2015},
  file = {C\:\\Users\\Victor\\Downloads\\Zahm_thesis.pdf},
  keywords = {MOR methods,Th√®se},
  school = {\'Ecole Centrale Nantes}
}

@article{zaman_robustness-based_2011,
  title = {Robustness-Based Design Optimization under Data Uncertainty},
  author = {Zaman, Kais and McDonald, Mark and Mahadevan, Sankaran and Green, Lawrence},
  year = {2011},
  month = aug,
  volume = {44},
  pages = {183--197},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-011-0622-2},
  abstract = {This paper proposes formulations and algorithms for design optimization under both aleatory (i.e., natural or physical variability) and epistemic uncertainty (i.e., imprecise probabilistic information), from the perspective of system robustness. The proposed formulations deal with epistemic uncertainty arising from both sparse and interval data without any assumption about the probability distributions of the random variables. A decoupled approach is proposed in this paper to un-nest the robustness-based design from the analysis of non-design epistemic variables to achieve computational efficiency. The proposed methods are illustrated for the upper stage design problem of a two-stage-toorbit (TSTO) vehicle, where the information on the random design inputs are only available as sparse point and/or interval data. As collecting more data reduces uncertainty but increases cost, the effect of sample size on the optimality and robustness of the solution is also studied. A method is developed to determine the optimal sample size for sparse point data that leads to the solutions of the design problem that are least sensitive to variations in the input random variables.},
  file = {/home/victor/Zotero/storage/TDFMEENU/Zaman et al. - 2011 - Robustness-based design optimization under data un.pdf},
  journal = {Structural and Multidisciplinary Optimization},
  language = {en},
  number = {2}
}

@article{zanette_robust_2018,
  title = {Robust {{Super}}-{{Level Set Estimation}} Using {{Gaussian Processes}}},
  author = {Zanette, Andrea and Zhang, Junzi and Kochenderfer, Mykel J.},
  year = {2018},
  month = nov,
  abstract = {This paper focuses on the problem of determining as large a region as possible where a function exceeds a given threshold with high probability. We assume that we only have access to a noise-corrupted version of the function and that function evaluations are costly. To select the next query point, we propose maximizing the expected volume of the domain identified as above the threshold as predicted by a Gaussian process, robustified by a variance term. We also give asymptotic guarantees on the exploration effect of the algorithm, regardless of the prior misspecification. We show by various numerical examples that our approach also outperforms existing techniques in the literature in practice.},
  archivePrefix = {arXiv},
  eprint = {1811.09977},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/TJ484U2M/Zanette et al. - 2018 - Robust Super-Level Set Estimation using Gaussian P.pdf},
  journal = {arXiv:1811.09977 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhao_same_2014,
  title = {{{SAME}} but {{Different}}: {{Fast}} and {{High}}-{{Quality Gibbs Parameter Estimation}}},
  shorttitle = {{{SAME}} but {{Different}}},
  author = {Zhao, Huasha and Jiang, Biye and Canny, John},
  year = {2014},
  month = sep,
  abstract = {Gibbs sampling is a workhorse for Bayesian inference but has several limitations when used for parameter estimation, and is often much slower than non-sampling inference methods. SAME (State Augmentation for Marginal Estimation) \textbackslash cite\{Doucet99,Doucet02\} is an approach to MAP parameter estimation which gives improved parameter estimates over direct Gibbs sampling. SAME can be viewed as cooling the posterior parameter distribution and allows annealed search for the MAP parameters, often yielding very high quality (lower loss) estimates. But it does so at the expense of additional samples per iteration and generally slower performance. On the other hand, SAME dramatically increases the parallelism in the sampling schedule, and is an excellent match for modern (SIMD) hardware. In this paper we explore the application of SAME to graphical model inference on modern hardware. We show that combining SAME with factored sample representation (or approximation) gives throughput competitive with the fastest symbolic methods, but with potentially better quality. We describe experiments on Latent Dirichlet Allocation, achieving speeds similar to the fastest reported methods (online Variational Bayes) and lower cross-validated loss than other LDA implementations. The method is simple to implement and should be applicable to many other models.},
  archivePrefix = {arXiv},
  eprint = {1409.5402},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/3FZ6AFKT/Zhao et al. - 2014 - SAME but Different Fast and High-Quality Gibbs Pa.pdf;/home/victor/Zotero/storage/7W43GP2H/Zhao et al. - 2014 - SAME but Different Fast and High-Quality Gibbs Pa.pdf;/home/victor/Zotero/storage/ZPDSAHMX/1409.html},
  journal = {arXiv:1409.5402 [cs, stat]},
  keywords = {D.1.3,K.3.2,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zheng_model_2017,
  title = {Model {{Selection Confidence Sets}} by {{Likelihood Ratio Testing}}},
  author = {Zheng, Chao and Ferrari, Davide and Yang, Yuhong},
  year = {2017},
  month = sep,
  abstract = {The traditional activity of model selection aims at discovering a single model superior to other candidate models. In the presence of pronounced noise, however, multiple models are often found to explain the same data equally well. To resolve this model selection ambiguity, we introduce the general approach of model selection confidence sets (MSCSs) based on likelihood ratio testing. A MSCS is defined as a list of models statistically indistinguishable from the true model at a user-specified level of confidence, which extends the familiar notion of confidence intervals to the model-selection framework. Our approach guarantees asymptotically correct coverage probability of the true model when both sample size and model dimension increase. We derive conditions under which the MSCS contains all the relevant information about the true model structure. In addition, we propose natural statistics based on the MSCS to measure importance of variables in a principled way that accounts for the overall model uncertainty. When the space of feasible models is large, MSCS is implemented by an adaptive stochastic search algorithm which samples MSCS models with high probability. The MSCS methodology is illustrated through numerical experiments on synthetic data and real data examples.},
  archivePrefix = {arXiv},
  eprint = {1709.04342},
  eprinttype = {arxiv},
  file = {/home/victor/Zotero/storage/EY6SRTRF/Zheng et al. - 2017 - Model Selection Confidence Sets by Likelihood Rati.pdf},
  journal = {arXiv:1709.04342 [math, stat]},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  language = {en},
  primaryClass = {math, stat}
}

@article{zhuang_enhancing_2015,
  title = {Enhancing Product Robustness in Reliability-Based Design Optimization},
  author = {Zhuang, Xiaotian and Pan, Rong and Du, Xiaoping},
  year = {2015},
  month = jun,
  volume = {138},
  pages = {145--153},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2015.01.026},
  abstract = {Different types of uncertainties need to be addressed in a product design optimization process. In this paper, the uncertainties in both product design variables and environmental noise variables are considered. The reliability-based design optimization (RBDO) is integrated with robust product design (RPD) to concurrently reduce the production cost and the long-term operation cost, including quality loss, in the process of product design. This problem leads to a multi-objective optimization with probabilistic constraints. In addition, the model uncertainties associated with a surrogate model that is derived from numerical computation methods, such as finite element analysis, is addressed. A hierarchical experimental design approach, augmented by a sequential sampling strategy, is proposed to construct the response surface of product performance function for finding optimal design solutions. The proposed method is demonstrated through an engineering example.},
  file = {/home/victor/Zotero/storage/VZ5TD4AD/Zhuang et al. - 2015 - Enhancing product robustness in reliability-based .pdf;/home/victor/Zotero/storage/NUX5FFBN/S0951832015000368.html},
  journal = {Reliability Engineering \& System Safety},
  keywords = {Kriging metamodel,Robust design optimization,Sequential optimization and reliability assessment,Sequential sampling}
}


