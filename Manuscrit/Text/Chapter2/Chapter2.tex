\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter2/#1}
}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter3/build/Chapter3}
\externaldocument{../../Text/Chapter4/build/Chapter4}
\externaldocument{../../Text/Chapter5/build/Chapter5}
\externaldocument{../../Text/Conclusion/build/Conclusion}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\dominitoc
\faketableofcontents
% Rappel du précédent style, pour qu'il aille jusqu'à la dernière page (?? latex...)
% \pagestyle{introStyle}
% \TitleBtwLines
\showthe\columnwidth
%% ---- C'est le vrai ch 1 en numérotation arabe
% \subfileLocal{\setcounter{chapter}{0}}t
% \renewcommand{\thechapter}{\arabic{chapter}}%
\chapter{Inverse Problem Framework}
\label{chap:inverse_problem}

\minitoc
% \listoftodos
\newpage
\subfileLocal{\pagestyle{contentStyle}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Forward, inverse problems and probability theory}
Model calibration or parameter estimation has been widely treated in the literature, either from a statistical and probabilistic point of view using Bayesian infernce, or from a variational point of view (see~\cite{das_estimation_1991,das_variational_1992} for parameter estimation using adjoint methods on hydraulical models).
 \subsection{Model space data space and forward problem}
\label{sec:model_space_data_space}
In order to describe accurately a physical system, we have to define the notion of a model, and will be following~\cite{tarantola_inverse_2005} approach to define inverse problems.
 A model represents the link between some parameters and some observable quantities. A simple example is a model that takes the form of a system of ODEs or PDEs, maybe discretized, while the parameters are the initial conditions and the output is one or several time series, describing the time evolution of a quantity at one or several spatial points. An important point to make is that a model is not only the \emph{forward operator}, but must also include the parameter space

\begin{definition}[Model]
  A model $\mathfrak{M}$ is defined as a pair composed of a \emph{forward operator} $\mathcal{M}$, and a \emph{parameter space} $\Theta$
  \begin{equation}
    \mathfrak{M} = (\mathcal{M}, \Theta)
  \end{equation}
The forward operator is the mathematical representation of the physical system, while the parameter space is chosen here to be a subset of a finite dimensional space, so usually $\Theta$ will be a subset of $\mathbb{R}^n$.
\end{definition}
As we will usually consider $\Theta$ as a subset of $\mathbb{R}^n$, for $n\geq 1$, we can define a kind of dimensionality of the model, based on the number of \emph{degrees of freedom} available for the parameters to vary freely.
\begin{remark}
  The dimension of a model $\mathfrak{M}=(\mathcal{M},\Theta)$ is the number of parameters not reduced to a singleton, so if $\Theta \subset \mathbb{R}^n$, the dimension of $\mathfrak{M}$ is $d \leq n$. The dimension of a model $\mathfrak{M}$ is sometimes called the degrees of freedom of $\mathfrak{M}$.
  \end{remark}
  
\begin{example}
  A model with parameter space $\Theta = \mathbb{R}^2\times [0, 1]$ has dimension $3$, while $\Theta = \mathbb{R}^2 \times \{1\}$ has dimension $2$.
\end{example}
 Now that we have introduced the forward operator and the parameter space, we will focus on the output of the model.
The data space consists in all the physically acceptable results of the physical experiment. This set is noted $\Yspace$.
Then, the forward operator $\mathcal{M}$ maps the parameter space $\Theta \subset \mathbb{R}^{d}$ to the data space $\Yspace$, as one can expect that all models provide physically acceptable outputs.

\subsection{Forward problem}
Given a model $(\mathcal{M}, \Theta)$, the \emph{forward problem} consists in applying the forward operator to a given $\theta \in \Theta$, in order to get the \emph{model prediction}. The forward problem is then to obtain information on the result of the experiment based on the parameters we chose as input, so deriving a satisfying forward operator $\mathcal{M}$.
\begin{equation}
  \begin{array}{cccc}
    \mathcal{M}:& \Theta &\longrightarrow & \Yspace \\
                & \theta &\longmapsto     & \mathcal{M}(\theta)
  \end{array}
\end{equation}
As said earlier, the forward operator can be a set of ODEs or PDEs, discretized or not. The forward problem is then the attempt to link the causes (i.e.\ the parameters) to the consequence, i.e.\ the output in the data space.

\subsection{Inverse Problem}
The inverse problem is the natural counterpart of the forward problem, and consists in trying to gather more information on the parameters, based on the result of the experiment or the physical process, and the knowledge of the forward operator. This kind of circular procedure: adding complexity by updating the forward operator and the parameter space by choosing a model with higher complexity for the forward problem, and reducing this complexity by comparing some observations with the output of the model, and reducing the parameter space.\unsure{General approaches for inverse} 

However, a purely deterministic approach for the inverse problem is doomed to fail: as most physical processes are not perfectly known, some uncertainties remain in the whole modelling process. Those uncertainties are ubiquitous: the observations available may be corrupted by a random noise coming from the measurement devices and the model may not represent perfectly the reality, thus introducing a systematic bias for instance. Taking into account those uncertainties is crucial to solve the inverse problem.


In that perspective we are going to introduce briefly the usual probabilistic framework, along with common notations that we will use throughout this manuscript. Those notions are well established in the scientific literature, and one can read~\cite{billingsley_probability_2008} for a more thorough description.
\subsection{Notions of probability theory}
\subsubsection{Probability measure, and random variables}
\label{sec:notion_prob_theory}

We are first going through some usual notions of probability theory. 
Let us consider the usual probabilistic space $(\Omega, \mathcal{F}, \Prob)$.
\begin{definition}[Event probability and conditioning]
  \label{def:prob_event}
   We call an event an element of the $\sigma$-algebra $\mathcal{F}$, and the probability of an event $A\in \mathcal{F}$ is defined as the Lebesgue integral
  \begin{equation}
    \Prob[A] = \int_{A} \,\mathrm{d}\Prob(\omega)
  \end{equation}
Observing an event $B \in \mathcal{F}$ can bring information upon another event $A\in \mathcal{F}$. In that sense, we introduce the conditional probability of $A$ given $B$.
\label{def:cond_proba}
  Let $A$, $B \in \mathcal{F}$.
  The event $A$ given $B$ is written $A \mid B$ and its probability is
  \begin{equation}
    \Prob[A \mid B] = \frac{\Prob[A \cap B]}{\Prob[B]}
  \end{equation}
\end{definition}
Formally, an event can be seen as an outcome of some uncertain experiment, and its probability is ``how likely'' this event will happen.

Let us now introduce a measurable state (or sample) space $(S, \mathcal{B}(S))$.
\begin{definition}[Random Variable, Expectation]
  \label{def:random_variable}
  A random variable (abbreviated as r.v.) $X$ is a measurable function from $\Omega \longrightarrow S$. A random variable will usually be written with an upper case letter. A realisation or observation $x$ of the r.v. $X$ is the actual image of $\omega\in\Omega$ under $X$: $x = X(\omega)$. If $S$ is countable, the random variable is said to be \emph{discrete}. When $S\subseteq \mathbb{R}^p$ for $p\geq 1$, $X$ is sometimes called a random vector
  
  \label{def:expectation}
  The expectation of a r.v. $X:\Omega \rightarrow S$ is defined as
  \begin{equation}
    \Ex[X] = \int_{\Omega} X(\omega) \,\mathrm{d}\Prob(\omega)
  \end{equation}
\end{definition}

\begin{remark}
  Using the \cref{def:expectation}, the probability of an event $A$ can be seen as the expectation of a well chosen random variable:
  \begin{equation}
    \begin{array}{cccc}
      \mathbbm{1}_{A}:& \Omega& \longrightarrow& \{0,1\} \\
                      & \omega& \longmapsto & \begin{cases}
                        1\text{ if } \omega \in A \\
                        0 \text{ if } \omega \notin A
                                              \end{cases}
    \end{array}
  \end{equation}
  and
  \begin{align*}
    \Ex[\mathbbm{1}_A] &= \int_{\Omega} \mathbbm{1}_A \, \mathrm{d}\Prob(\omega) \\
                       &= \int_{A} \, \mathrm{d}\Prob(\omega) = \Prob[A]
  \end{align*}
  $\mathbbm{1}_A$ is called the indicator function of the event $A$.
\end{remark}
\begin{definition}[Image (Pushforward) measure]
  \label{def:image_measure}
  Let $X:\Omega \rightarrow S$ be a random variable, and $A \subseteq S$. The image measure (also called pushforward measure) of $\Prob$ through $X$ is denoted by $\Prob_X = \Prob \circ X^{-1}$. This notation can differ slightly depending on the community, so one can find also $ \Prob_X = \Prob \circ X^{-1} = X_{\sharp}\Prob$, the latter notation being used in transport theory. The probability, for the r.v. $X$ to be in $A$ is equal to
  \begin{equation}
    \Prob[X \in A] = \Prob_X[A] = \int_{A}\,\mathrm{d}\Prob_X(\omega) =  \int_{X^{-1}(A)}\,\mathrm{d}\Prob(\omega) = \Prob[X^{-1}(A)] = \Prob[\{\omega\,;\,X(\omega) \in A\}]
  \end{equation}

  
Similarly, the for any measurable function $h$, the expectation taken with respect to a specific random variable $X$ is 
\begin{equation}
  \Ex_{X}[h(X)] = \int_{\Omega} h(X(\omega)) \,\mathrm{d}\Prob_{X}(\omega)
\end{equation}
\end{definition}
Generally speaking, the sample space will be $S\subseteq \mathbb{R}^p$ for $p\geq 1$, so we are going to introduce useful tools and notations to caracterize these particular r.v.
\subsubsection{Real-valued random variables}
We are now going to focus on
real-valued random variables, so measurable function from $\Omega$ to
the sample space $(S,\mathcal{B}(S)) =
(\mathbb{R},\mathcal{B}(\mathbb{R}))$.
\begin{definition}[Distribution of a real-valued r.v.]
  \label{def:distribution}
  The distribution of a r.v. can be characterized by a few functions:
  \begin{itemize}
  \item The \emph{cumulative distribution function} (further
abbreviated as cdf) of a real-valued r.v. $X$ is defined as the
probability of the right closed intervals that generate the Borel
$\sigma$-algebra $\mathcal{B}(\mathbb{R})$ of the real line.
  \begin{equation} F_{X}(x) = \Prob\left[X \leq x\right] =
\Prob_X\big[\,]-\infty; x]\, \big]
  \end{equation} and $\lim_{-\infty}F_X = 0$ and $\lim_{+\infty} F_X
= 1$
If the cdf of a random variable is continuous, the r.v. is said to be \emph{continuous} as well.
  
\item The \emph{quantile function} $Q_X$ is the generalized inverse function
of the cdf:
  \begin{equation} Q_X(p) = \inf\{q:\, F_X(q)\geq p\}
  \end{equation}
\item If there exists a function $f: S\rightarrow \mathbb{R}^{+}$ such that
  for all measurable sets $A$
  \begin{equation} \Prob[X \in A] = \int_A \,\mathrm{d}\Prob_X(\omega) = \int_A f(x)\,\mathrm{d}x
\end{equation}
then $f$ is called the \emph{probability density function} (abbreviated pdf) of $X$ and is denoted $p_X$.
As $\Prob[X \in S] = 1$, it follows trivially that $\int_{S}f(x)\,\mathrm{d}x=1$.


  % If the pushforward measure $\Prob_X$ is absolutely continuous
% with respect to the Lebesgue measure $\lambda$ defined as
% $\lambda\left(]a, b]\right) = b-a$, then according to Radon-Nikodym
% theorem, there exists a function $p_X$, such that for all measurable
% set $A$,
%   \begin{equation} \Prob_X[A] = \Prob[X \in A] = \int_A
% \,\mathrm{d}\Prob_X(\omega) = \int_A p_X(y)\,\mathrm{d}y
%   \end{equation} This function $p_X: S\subseteq\mathbb{R} \rightarrow
% \mathbb{R}$, is called the \emph{probability density function}
% (abbreviated pdf) of $X$, called the Radon-Nikodym derivative of
% $\Prob_X$ wrt $\lambda$: $p_X=\frac{\mathrm{d}\Prob_X}{\mathrm{d}
% \lambda} = \frac{\mathrm{d} F_X}{\mathrm{d} y}$.  As $X$ is
% real-valued, the probability for $X$ to be in an interval is
%   \begin{equation} \Prob_X\big[]a;\,b]\big]=\Prob[a \leq X < b] =
% \int_{a}^b p_X(y)\,\mathrm{d}y = F_X(b) - F_X(a)
%   \end{equation} and $\Prob_X[\mathbb{R}] =
% \int_{\mathbb{R}}p_X(y)\,\mathrm{d}y=1$. 
  \end{itemize}
\end{definition}
\begin{remark}
  When restricting this search to ``classical'' functions, $p_X$ may not exist. However, allowing generalized functions such as the \emph{dirac delta function}, provides a way to consider simultaneously all types of real-valued random variables (continous, discrete, and mixture of both). Dirac's delta function can (in)formally be defined as
  \begin{equation}
    \label{eq:def_dirac_delta}
    \delta_{x_0}(x) = 
    \begin{cases}
      +\infty \text{ if } x=x_0 \\
      0 \text{ elsewhere}
    \end{cases} \quad \text{ and }
    \int_S \delta_{x_0}(x)\,\mathrm{d}x = 1
  \end{equation}
\end{remark}
\begin{example}
  \label{ex:X_rv}
  Let us consider the random variable $X$ that takes the value $1$ with probability $0.5$, and follows a uniform distribution with probability $0.5$ over $[2;4]$. Its cdf can be expressed as
  \begin{equation}
    F_X(x) =
    \begin{cases}
      0 \text{ if } x < 1 \\
      0.5 \text{ if } 1 \leq x < 2 \\
      0.5 + \frac{x-2}{8} \text{ if } 2 \leq x < 4 \\
      1 \text{ if } 4 \leq x
    \end{cases}
  \end{equation}
  and its pdf (as a generalized function)
  \begin{equation}
    p_X(x) = \frac{1}{2}\delta_{1}(x) + \frac{1}{4}\mathbbm{1}_{\{2\leq x < 4\}}(x) 
  \end{equation}
\end{example}
\begin{figure}[!h]
  \centering
  \input{/home/victor/acadwriting/Manuscrit/Text/Chapter2/img/example_cdf_pdf.pgf}
  \caption{Cdf and Pdf of $X$ defined in \cref{ex:X_rv}. The arrow indicates a dirac delta function}
  \label{fig:example_pdf_cdf}
\end{figure}

\begin{definition}[Moments of a r.v. and $L^s$ spaces]
  Let $X$ be a random variable.
  The moment of order $s$ is defined as $\Ex\left[X^s\right]$, and the centered moment of order $s$ is defined as
  \begin{equation}
    \Ex[(X-\Ex[X])^s]=\int \left(X(\omega) - \Ex[X]\right)^s \,\mathrm{d}\Prob(\omega) = \int (x-\Ex[X])^s\cdot p_X(x)\,\mathrm{d}x
  \end{equation}
  To ensure that those moments exists, let us define $L^s(\Prob)$ as the space of random variables $X$ such that $\Ex\left[\lvert X \rvert^s\right] < +\infty$.
  If $X\in L^2(\Prob)$, the centered moment of order $2$ is called the variance:
  \begin{equation}
    \label{eq:variance_def}
    \Ex\left[(X-\Ex[X])^2 \right] = \Var[X] \geq 0
  \end{equation}
\end{definition}


Extending those definitions from real-valued random variables to real-valued random vectors is pretty straightforward
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsubsection{Real-valued random vectors}
 Most of the definitions for a random variable extends component-wise to the random vectors:

\begin{definition}[Joint, marginal and conditional densities]
 \label{def:joint_marginal_cond_densities}
 Let $X=[X_1,\cdots,X_p]$ be a random vector from $\Omega \rightarrow S\subseteq\mathbb{R}^p$
 The expected value of a random vector is the expectation of the components
 \begin{equation}
\Ex[X] = \left[\Ex[X_1],\dots,\Ex[X_p]\right]
\end{equation}

The cdf of $X$ at the point $x=[x_1,\dots x_p]$ is
   \begin{align}
     \label{eq:def_cdf_multi}
     F_{X}(x) = F_{X_1,\dots, X_p}(x_1,\dots, x_p) &= \Prob\left[X_1 \leq x_1, \cdots, X_p\leq x_p\right]\\
                                                   &= \Prob\left[\bigcap_{i=1}^p \{\omega;\,X_i(\omega) \leq x_i\}\right] \nonumber
  \end{align}
 Similarly as in the real-valued case, we can define the pdf of the random vector, or \emph{joint pdf} by derivating with respect to the variables:
  \begin{equation}
    p_{X}(x)= p_{X_1,\dots, X_p}(x_1,\dots, x_p) =\frac{\partial^p F_X}{\partial x_1 \cdots \partial x_p}(x)
  \end{equation}
  
  and $\int_{S}p_{X_1,\dots, X_p}(x_1,\dots, x_p)\,\mathrm{d}(x_1,\dots, x_p)=1$

Extending the variance to vectors brings the covariance matrix $\Sigma \in \mathbb{R}^{p\times p}$ of $X$, defined as
  \begin{equation}
    \Sigma = \Cov(X)=\Cov[X,X]= \Ex\left[\left(X - \Ex[X]\right)\left(X-\Ex[X]\right)^T\right] = \Ex[XX^T] - \Ex[X]\Ex[X]^T
  \end{equation}
 More generally, the (cross-)covariance matrix of two random vector $X$ and $Y$ is defined as
  \begin{align}
    \Cov\left[X,Y\right] &= \Ex\left[(X-\Ex[X])(Y - \Ex[Y])^T\right] = \Ex[XY^T] - \Ex[X]\Ex[Y]^T
  \end{align}

  We can now define the \emph{marginal densities}
  For notation clarity, we are going to set $X = [Y,Z]$

  \begin{equation}
    \label{eq:marginals_def}
    p_{Y}(y) = \int_{\mathbb{R}}p_{Y,Z}(y,z) \,\mathrm{d}z \quad \text{ and } \quad p_{Z}(z) = \int_{\mathbb{R}}p_{Y,Z}(y,z) \,\mathrm{d}y
  \end{equation}
  The random variable $Y$ given $Z$, denoted by $Y \mid Z$ has the conditional density
  \begin{equation}
    p_{Y \mid Z}(y \mid z) = \frac{p_{Y,Z}(y,z)}{p_Z(z)}
  \end{equation}
  allowing us to rewrite the marginals as
  \begin{align}
    \label{eq:marginal_conditioned}
        p_{Y}(y) = \int_{\mathbb{R}}p_{Y\mid Z}(y\mid z)p_Z(z) \,\mathrm{d}z=\Ex_Z\left[p_{Y\mid Z}(y\mid z)\right] \\ p_{Z}(z) = \int_{\mathbb{R}}p_{Z\mid Y}(z\mid y)p_Y(y) \,\mathrm{d}y = \Ex_{Y}\left[p_{Z\mid Y}(z\mid y)\right]
  \end{align}

\end{definition}


The influence of one (or a set of) random variable(s) over another can be measured with the conditional probabilities. Indeed, if the state of information on a random variable does not change when observing another one, this leads to think that the observed one provides no information on the other. This leads to the definition of independence.
\begin{definition}[Independence]
  Let $A,B\in \mathcal{F}$. Those two events are deemed independent if $\Prob[A \cap B] = \Prob[A]\Prob[B]$.
  Quite similarly, two real-valued random variables $Y$ and $Z$ are said to be independent if $F_{Y,Z}(y,z) = F_Y(y) F_Z(z)$ or equivalently, $p_{Y,Z}(y,z) = p_Y(y) p_Z(z)$
  Speaking in terms of conditional probabilities, this can be written as $p_{Y|Z}(y,z) = p_{Y}(y)$.
  If $Y$ and $Z$ are independent, $\Cov[Y, Z] = 0$. The converse if false in general.
\end{definition}
One central multivariate distribution is the normal distribution (or Gaussian).
\begin{example}[The Normal Distribution]
  \label{ex:gaussian_distribution}
  One central example is the normal (or Gaussian) distribution. Let $X$ be a r.v.\ from $\Omega$ to $\mathbb{R}$.
  $X$ follows the normal distribution of mean $\mu \in \mathbb{R}$ and variance $\sigma^2>0$ when
  \begin{equation}
    p_X(x) = \phi(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}\right)
  \end{equation}
and we write $X \sim \mathcal{N}(\mu,\sigma^2)$
For the multidimensional case, so when $X$ is a r.v.\ from $\Omega$ to $\mathbb{R}^p$,
$X$ follows a normal distribution of mean $\mu \in \mathbb{R}^p$ and covariance matrix $\Sigma \in \mathbb{R}^{p\times p}$, where $\Sigma$ is semi-definite positive.
In that case, $X\sim \mathcal{N}(\mu, \Sigma)$ the density of the random vector $X$ can be written as
\begin{equation}
    p_X(x) = (2\pi)^{-\frac{d}{2}}\lvert\Sigma\rvert^{-1}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
  \end{equation}
  where $|\Sigma|$ is the determinant of the matrix $\Sigma$, and $(\cdot)^T$ is the transposition operator.
  As the covariance matrix appears through its inverse, another encountered parametrization is to use the precision matrix $\Sigma^{-1}$.
  Examples of pdf of Gaussian normal distribution are displayed on \cref{fig:example_normal}.
\end{example}
\begin{figure}[!h]
  \centering
  \input{/home/victor/acadwriting/Manuscrit/Text/Chapter2/img/example_normal.pgf}
  \caption{Densities of 1D Gaussian distributed r.v. (left), and density of a 2D Gaussian r.v.}
  \label{fig:example_normal}
\end{figure}

\subsubsection{Bayes' Theorem}
\label{ssec:bayes_theorem}

The classical Bayes' theorem is directly a consequence of the definition of the conditional probabilities in \cref{def:cond_proba}, or by considering the pdf of r.v.\ in~\cref{def:joint_marginal_cond_densities}.

\begin{theorem}[Bayes' theorem]
  Let $A, B\in\mathcal{F}$. Bayes' theorem states that
  \begin{align*}
    \Prob[A\mid B]\cdot \Prob[B] = \Prob[B \mid A]\cdot\Prob[A] \\
    \Prob[A \mid B] = \frac{\Prob[B \mid A]\cdot\Prob[A]}{\Prob[B]} \text{ if } \Prob[B] \neq 0
  \end{align*}
 In terms of densities, the formulation is sensibly the same.
  Let $Y$ and $Z$ be two random variables. The conditional density of $Y$ given $Z$ can be expressed using the conditional density of $Z$ given $Y$.
  \begin{equation}
    p_{Y\mid Z}(y \mid z) = \frac{p_{Z\mid Y}(z\mid y) p_Y(y)}{p_Z(z)} = \frac{p_{Z\mid Y}(z\mid y) p_Y(y)}{\int p_{Z,Y}(z,y) \,\mathrm{d}y}  \propto p_{Z\mid Y}(z\mid y) p_Y(y)
  \end{equation}
\end{theorem}
Bayes' theorem is central as it links in a simple way conditional densities. In the inverse problem framework, if $Y$ represents the state of information on the parameter space, while $Z$ represents the information on the data space, $Z\mid Y$ can be seen as the forward problem. Bayes' theorem allow us to ``swap'' the conditioning, and get information on $Y\mid Z$, that can be seen as the inverse problem.

\section{Parameter inference}
\subsection{From the physical experiment to the model}
\label{ssec:inv_problem}
The physical system (the reality) that is observed can formally be represented by a model, so by an operator $\mathscr{M}$, applied to a set of parameters $\vartheta \in \Theta_{\mathrm{real}}$ that is unknown:
\begin{equation}
  \begin{array}{llll}
    \mathscr{M} :& \Theta_{\mathrm{real}} &\longrightarrow& \Yspace \\
                 & \vartheta & \longmapsto& \mathscr{M}(\vartheta)
  \end{array}
\end{equation}
The physical reality yields some observations $\mathscr{M}(\vartheta)$, shortened as $\mathscr{M}(\vartheta) = y\in\Yspace$.
\cite{kennedy_bayesian_2001,higdon_combining_2004}
The main objective is to find an appropriate model $(\mathcal{M},\Theta)$, that represents as accurately as possible the given reality.
% \begin{equation}
%     \mathscr{M}(x, \vartheta) = \mathcal{M}(x, \theta) + \delta(x,\theta)
% \end{equation}
% When evaluated on a fixed vector $x_{\mathrm{grid}} = (x_1,\dots,x_n)$, we will omit the input, and define
Let us assume that $\Theta = \Theta_{\mathrm{real}}$, we can rewrite the link between the reality and the model as
\begin{equation}
    % \mathscr{M}(x_{\mathrm{grid}}, \vartheta) = 
    \mathscr{M}(\vartheta) = % \mathcal{M}(x_{\mathrm{grid}}, \theta) + \epsilon(x_{\mathrm{grid}},\theta) =
    \mathcal{M}(\vartheta) + \epsilon \in \Yspace \subseteq \mathbb{R}^p
  \end{equation}
  
The difference $\epsilon = \mathscr{M}(\vartheta) - \mathcal{M}(\vartheta)$ is the error between the physical model and the model, called sometimes the misfit, or the residuals error. However, this writing 

\subsection{Frequentist inference, MLE}
\label{sec:frequentist_inference_MLE}

For a given choice of parameter $\theta\in\Theta$, one common assumption is that those residuals are normally distributed $\epsilon(\theta) \sim \mathcal{N}(0, \Sigma)$, with a given covariance matrix $\Sigma$, so the observations $\mathscr{M}(\vartheta) - \epsilon(\theta)=Y$ form a random variable, and as we assume that $\Yspace \subseteq \mathbb{R}^p$, $Y$ is a random vector with the following distribution:
\begin{equation}
  \label{eq:lik_gaussian}
  Y  \sim \mathcal{N}(\mathcal{M}(\theta), \Sigma)
\end{equation}
Its pdf will be denoted of this random variable will be  $y \mapsto p_Y(y;\theta)$ to show the depedency with respect to $\theta$. Instead of looking at this function as a pdf, we may look at it instead as a function of $\theta$, as the observations $y\in\Yspace$ do not vary
\begin{definition}[Likelihood function, MLE]
  \label{def:mle}
  The probability density function of the observations for a set of parameters is called the likelihood of those parameters given the observations, and is written $\mathcal{L}$:
  \begin{align}
    \label{eq:likelihood_definition}
    \mathcal{L}(\cdot ;y): \theta \mapsto p_{Y}(y;\theta) &= \mathcal{L}(\theta;y) \\
    &=(2\pi)^{-n/2}\lvert \Sigma \rvert^{-1/2}\exp\left(-\frac{1}{2}(\mathcal{M}(\theta) - y)^T\Sigma^{-1}(\mathcal{M}(\theta) - y)\right)
  \end{align}
  If $\Sigma = \mathrm{diag}(\sigma^2_1,\dots, \sigma^2_p)$, the likelihood can be written as the product ot 1D Gaussians:
  \begin{equation}
    \mathcal{L}(\theta;y) = \left(\prod_{i=1}^p\frac{1}{\sqrt{2\pi}\sigma_i}\right)\exp\left(\sum_{i=1}^p -\frac{(\mathcal{M}(\theta)_i - y_i)^2}{2\sigma^2_i}\right)
  \end{equation}
  Based on the likelihood function, we can define the \emph{Maximum Likelihood Estimator}, or \emph{MLE}, that maximizes the likelihood defined above:
  \begin{equation}
    \label{eq:def_MLE}
    \estimtxt{\theta}{MLE} = \argmax_{\theta\in\Theta}\mathcal{L}(\theta;y) = \argmin_{\theta \in \Theta} -\log \mathcal{L}(\theta;y)
  \end{equation}

\end{definition}
  In practice, instead of maximizing the likelihood, one looks for minimizing the negative log-likelihood. Given the~\cref{def:mle}
  \begin{equation}
    \estimtxt{\theta}{MLE} = \argmin_{\theta \in \Theta} -\log \mathcal{L}(\theta;y)
  \end{equation}
  where
  \begin{equation}
    -\log\mathcal{L}(\theta;y) = \frac{1}{2}(\mathcal{M}(\theta) - y)^T\Sigma^{-1}(\mathcal{M}(\theta) - y)+  \frac{n}{2}\log(2\pi) + \frac{1}{2}\log\lvert \Sigma \rvert
  \end{equation}
  By optimizing with respect to $\theta$, we can omit the constant terms, and
  \begin{align*}
    \estimtxt{\theta}{MLE} &= \argmin_{\theta \in \Theta}\frac{1}{2}(\mathcal{M}(\theta) - y)^T\Sigma^{-1}(\mathcal{M}(\theta) - y)\\ &= \argmin_{\theta \in \Theta}\frac12 \| \mathcal{M}(\theta) - y \|^2_{\Sigma^{-1}}
  \end{align*}

  Frequentist inference and Maximum Likelihood estimation boils down to Generalized non-linear least-square regression, that minimizes the squared Mahalabonis distance between $\mathcal{M}({\theta})$ and $y$. This is only true as we assumed a Gaussian form of the errors in \cref{eq:lik_gaussian}. Other choices of likelihood will bring different forms of objective functions. 
  \todol{insert examples / elliptical distributions ?}
Most of the time, an estimator will be the minimizer of an objective function
\subsection{Bayesian Inference}
\label{sec:bayesian_inference_MAP}
In Bayesian inference, the uncertainty present on $\theta$ is modelled by considering it as a random variable. Instead of having a precise value for $\theta$, albeit unknown, we assume that we have a \emph{prior distribution} on $\theta$, denoted $p_{\theta}$, that represents the initial state of belief upon the parameter, prior to any experiment and observations.The choice of this prior distribution will be discussed later.
After the experiment, whose sampling distribution is given by the likelihood, the prior distribution is updated to reflect the new state of belief upon the parameter. 
The Gaussian likelihood in \cref{eq:lik_gaussian} for the frequentist approach can be almost be rewritten as is in the Bayesian setting, just by conditioning $Y$ with $\theta$.
\cref{eq:lik_gaussian} becomes
\begin{equation}
  Y \mid  \theta \sim \mathcal{N}(\mathcal{M}(\theta), \Sigma)
\end{equation}
and the likelihood is the pdf $\mathcal{L}(\theta;y) = p_{Y\mid \theta}(y \mid  \theta)$.
Using Bayes' theorem, the \emph{posterior distribution} of the parameters given the observed data is
\begin{equation}
  \label{eq:bayes_posterior}
  p_{\theta \mid Y}(\theta \mid y) = \frac{p_{Y\mid \theta}(y \mid  \theta)p_{\theta}(\theta)}{p_Y(y)} = \frac{\mathcal{L}(\theta;y)p_{\theta}(\theta)}{p_Y(y)} \propto \mathcal{L}(\theta;y)p_{\theta}(\theta)
\end{equation}
As the data is usually fixed, the denominator of~\eqref{eq:bayes_posterior} is constant, and the posterior is usually evaluated up to a multiplicative constant.


\begin{definition}[Model Evidence]  
  The model evidence, (or marginal likelihood, integrated likelihood) is defined as the distribution of the data marginalised over the parameters.
  \begin{equation}
    \label{eq:model_evidence}
    p_Y(y) = \int_{\Theta}p_{Y,\theta}(y,\theta)\,\mathrm{d}\theta = \int_{\Theta}p_{Y \mid \theta}(y \mid \theta)p_{\theta}(\theta)\,\mathrm{d}\theta
  \end{equation}
  This quantity depends implicitly on the underlying mathematical model $\mathcal{M}$. Comparing evidence of different models allows for the comparison of those different models. However, computing the model evidence requires the expensive evaluation of an integral over the whole parameter space, and no analytical form is available except for trivial cases. Specific techniques for this evaluation are reviewed in~\cite{friel_estimating_2011}.
\end{definition}

\subsubsection{Posterior inference}
\label{sec:posterior_inference}
This posterior distribution is central in Bayesian analysis, as it gathers all the information we have on the parameter, given the observed data. Given~\eqref{eq:bayes_posterior}, evaluating the posterior density at a point requires the evaluation of the model evidence. To circumvent the expensive integral, we choose to evaluate the posterior up to a multiplicative constant.

But more than the posterior density values, we want to be able to sample from the posterior. Plethora of methods can be found in the literature, as this is one the main topic of Bayesian analysis. One of the most popular techniques is MCMC: Markov Chain Monte Carlo.


\subsubsection{Bayesian Point estimates}
\label{sec:bayes_point_estimates}
Bayesian point estimation refer to point estimation of the parameter $\theta$, using the posterior distribution $p_{\theta \mid Y}$.\cite{lehmann_theory_2006} provides a more complete overview of this subject.
This can be done by defining the \emph{Bayesian risk}, that is the expectation of a Bayesian loss functions $L: \Theta \times \Theta \rightarrow \mathbb{R}^+$ under the posterior distribution. A Bayesian point estimate is then a minimizer of this Bayesian risk.
\begin{equation}
  \theta_{L} = \argmin_{\theta^{\prime} \in \Theta} \Ex_{\theta\mid Y}\left[L(\theta^{\prime}, \theta) \mid y\right]
\end{equation}

\paragraph{Posterior mean}
By taking a loss function as the squared error $L(\theta^{\prime}, \theta) = (\theta^{\prime} - \theta)^2$, we can define the Mean Squared Error (MSE) as $\mathrm{MSE}: \theta^{\prime}\mapsto\Ex_{\theta\mid Y}\left[(\theta^{\prime} - \theta)^2\right]$. Finally, the value corresponding to the Minimum Mean Squared Error is
\begin{equation}
  \estimtxt{\theta}{MMSE} = \argmin_{\theta^{\prime}\in\Theta}\Ex_{\theta\mid Y}\left[(\theta^{\prime} - \theta)^2 \mid  y\right]
\end{equation}
Simple algebraic manipulations show that the minimizer is in fact the posterior mean:
\begin{equation}
  \label{eq:def_MMSE}
  \estimtxt{\theta}{MMSE} = \Ex_{\theta\mid Y}[\theta \mid  y] = \int_{\Theta}\theta\cdot p_{\theta\mid Y}(\theta \mid  y)\,\mathrm{d}\theta
\end{equation}


\paragraph{Posterior Median}
In a 1 dimensional case, instead of a squared error, one can define $L(\theta^{\prime}, \theta) = \lvert\theta^{\prime} - \theta\rvert$, and the the bayesian risk associated is called the mean absolute error. Again, one can show that the Minimum Mean Absolute Error (MMAE) is in fact the median of the posterior distribution.
\begin{equation}
  \label{eq:def_MMAE}
  \estimtxt{\theta}{MMAE} = \argmin_{\theta^{\prime}\in\Theta}\Ex_{\theta\mid Y}\left[\lvert \theta^{\prime} - \theta \rvert \mid y\right] = \mathop{\mathrm{Median}}(\theta \mid y)
\end{equation}

\paragraph{Posterior Mode: the MAP}
Taking $L(\theta^{\prime},\theta) = \delta_{\theta}(\theta^{\prime})$, the dirac delta function defined in~\cref{eq:def_dirac_delta}, one can show that the minimizer of $\Ex_{\theta\mid Y}\left[L(\theta^{\prime},\theta)\right]$ is the mode of the posterior distribution, and is called the \emph{Maximum A Posteriori} (MAP):
\begin{align}
  \label{eq:def_MAP}
  \estimtxt{\theta}{MAP} &= \argmin_{\theta^{\prime} \in \Theta}\Ex_{\theta\mid Y}\left[\delta_{\theta}(\theta^{\prime})\mid y\right] = \argmin_{\theta^{\prime} \in \Theta} -p_{\theta\mid Y}(\theta^{\prime}\mid y) \\
                         &= \argmax_{\theta^{\prime} \in \Theta} p_{\theta\mid Y}(\theta^{\prime} \mid y) = \argmax_{\theta^{\prime} \in \Theta} \mathcal{L}(\theta^{\prime} ;y)p_{\theta}(\theta^{\prime})
                           \nonumber
\end{align}
We chose to use a generalized function as a tool to link the MAP to Bayesian point estimate, but it is sometimes introduced as the limit of $0-1$ loss functions. In~\cite{bassett_maximum_2019}, the authors shows that this claim does not always hold, unless some conditions are met.
One interesting fact about the MAP, is that its evaluation does not require the full knowledge of the posterior distribution, nor samples to evaluate the integral of~\cref{eq:def_MMSE}. We can resort to classical optimization techniques for this evaluation. Similarly to the likelihood, taking the negative logarithm leads to the following minimization problem.
\begin{equation}
  \label{eq:minimisation_MAP_log}
  \estimtxt{\theta}{MAP} = \argmin_{\theta^{\prime}\in \Theta} -\ell(\theta^{\prime};y) - \log p_{\theta}(\theta^{\prime})
\end{equation}

\subsubsection{Choice of a prior distribution}
\label{sec:choice_prior}
As seen in the application of Bayes' theorem in~\cref{eq:bayes_posterior}, the prior has a preponderant role in the formulation of the posterior distribution. Indeed, this prior distribution represents the current state of knowledge on the value of the parameter, before any experiment. This comes usually from an expert opinion, or some reasonable assumptions about the nature of $\theta$.

Let us assume for instance that we have a Gaussian prior for $p_\theta$, such that $\theta \sim \mathcal{N}(\theta_{b},B)$ where $B$ is called the background covariance error matrix, and a Gaussian model for the errors as well, the MAP can be written as
\begin{equation}
  \estimtxt{\theta}{MAP} = \argmin_{\theta \in \Theta} \frac{1}{2}\| \mathcal{M}(\theta) - y\|^2_{\Sigma^{-1}} + \frac{1}{2}\|\theta - \theta_b\|^2_{B^{-1}}
\end{equation}
Adding a Gaussian prior for the parameter comes down to adding a $L^2$ regularization term to the optimization problem, also called Tikhonov regularization~\cite{tikhonov_solutions_1977}. This expression is very analoguous to the state estimation in the 3D-Var method in Data assimilation 
Other choices of priors leads to other regularizations, such as the lasso regularization~\cite{tibshirani_regression_2011} that is a consequence for choosing $\theta$ that follows a priori a Laplace distribution of mean $0$.


The choice of a prior distribution has an influence on the inference of the parameter and its point estimation. Where there is no knowledge on the parameter beforehand, one can try to choose a non-informative prior in order to try to mitigate its effect. One can for instance choose a ``flat'' prior over the parameter space, but this can lead to \emph{improper prior}, in the sense that they do not integrate to $1$. However, improper priors, though mathematically questionable, do not necessarily lead to improper posterior, allowing for the usual Bayesian analysis of the quantity.

If $\Theta = \mathbb{R}^p$, an improper non-informative prior is $p_{\theta}(\theta) \propto 1$, as it should be invariant by translation. In this case, the MAP estimation is equivalent to the MLE, as the prior in~\cref{eq:minimisation_MAP_log} is constant with respect to $\theta^{\prime}$.
When $\Theta = \mathbb{R}_+$, the prior distribution should be invariant by multiplication by a positive constant, so $p_{\theta}(\theta) \propto \frac{1}{\theta}\mathbbm{1}_{\theta > 0}$ (i.e.\ flat in the log scale), and that leads to a regularization term of the form $\log(\theta)$.


All in all, when looking for the MAP or the MLE, parameter estimation boils down to the minimization of a well chosen objective function, that measures the misfit between the output of the numerical model and the observations. This cost function will be written $J$ in the following, to match the notation of data assimilation. As mentioned before, the MAP does not require the full knowledge of the posterior distribution $p_{\theta \mid Y}$, as ``only'' an optimization is required.

In this context of calibration, we can then summarize the estimation as a minimization problem, where $J$ represents some kind of distance between $\mathcal{M}(\theta)$ and the observations.
\begin{equation}
  \estimtxt{\theta}{} = \argmin_{\theta\in \Theta} J(\theta)
\end{equation}
% If one is looking for the whole posterior distribution, 

\section{Calibration using adjoint-based optimization}

\subsection{The adjoint method}
This devolves into the following constrained optimisation problem :
\begin{align}
  \min_{\theta \in \Theta} J(\theta) &= J(\mathcal{M}(\theta), \theta) \label{eq:def_J_adjoint}\\
  \text{such that } &\mathcal{F}(\mathcal{M}(\theta), \theta) = 0 \label{eq:constraint_adjoint}
\end{align}
Derivating the~\cref{eq:def_J_adjoint,eq:constraint_adjoint} gives
\begin{align}
  \frac{\mathrm{d}J}{\mathrm{d}\theta} &= \pfrac{J}{\mathcal{M}} \pfrac{\mathcal{M}}{\theta} + \pfrac{J}{\theta}\\
  \frac{\mathrm{d}\mathcal{F}}{\mathrm{d}\theta} &= \pfrac{\mathcal{F}}{\mathcal{M}} \pfrac{\mathcal{M}}{\theta} + \pfrac{\mathcal{F}}{\theta}
\end{align}

Defining an appropriate scalar product $\langle \cdot, \cdot \rangle_{\Yspace}$, we can write the Lagrangian $\mathcal{L}$ using a Lagrange multiplier $\lambda \in \Yspace^*$ (the dual of $\Yspace$):
\begin{equation}
  \mathcal{L}(\theta, y, \lambda) = J(y,\theta) - \langle \lambda , \mathcal{F}(y, \theta)\rangle_{\Yspace}
\end{equation}
The condition of optimality $\pfrac{\mathcal{L}}{\theta} = \pfrac{\mathcal{L}}{y} = \pfrac{\mathcal{L}}{\lambda} = 0$ translate into the optimality condition, adjoint equation and state equation:
\begin{align}
  \pfrac{\mathcal{L}}{\lambda} &= - \mathcal{F}(y,\theta) = 0 \tag{State equation}\\
  \pfrac{\mathcal{L}}{y} &= \pfrac{J}{y} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{y}} \tag{Adjoint equation} = 0 \\
    \pfrac{\mathcal{L}}{\theta} &= \pfrac{J}{\theta}(y,\theta) - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\theta}(y,\theta)} \tag{Optimality condition}\\
                               &=\pfrac{J}{\mathcal{M}}\pfrac{\mathcal{M}}{\theta}+ \pfrac{J}{\theta} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\mathcal{M}} \pfrac{\mathcal{M}}{\theta} + \pfrac{\mathcal{F}}{\theta}} \\  
                               &= \left(\pfrac{J}{\mathcal{M}} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\mathcal{M}}} \right) \pfrac{\mathcal{M}}{\theta} + \left( \pfrac{J}{\theta} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\theta}} \right) \\
                               &= \pfrac{J}{\theta} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\theta}} = 0
\end{align}





\section{Parametric model misspecification}
We introduced earlier the mathematical model $(\mathcal{M},\Theta)$, and based our analysis on the fact that the ``target model'', i.e.\ the reality is $(\mathscr{M},\Theta_{\mathrm{real}} = \Theta)$, so the parameter spaces are the same.
In practice, the parameter space $\Theta$ does not contain all the parameters needed to run the forward model, but represents the space of the parameters of interest, or calibration parameters. In addition to them, some other parameters are at play, that we are going to call the \emph{environmental parameters}, or \emph{uncertain parameters} written $u\in \Uspace$. These parameters come from instance from the physical forcings. 

The environmental parameters introduced before, are in fact a bit different from the calibration parameters generically introduced as $\theta$. Bayesian framework and more specifically Bayesian update of the prior by the likehihood puts the emphasis on the update of the information on the parameter of interest. However the environmental parameters have a inherent variability. In that sense, it may not be worth spending time and resources to infer these parameter values. Moreover, we can only get information on the environmental conditions used to generate the observations.
We aim at letting this parameter stay free, and at finding a good value of the calibration parameter $\theta$, without doing any inference on $\uu$.


\cite{andreassian_all_2012} \cite{kuczera_there_2010}.

\subsection{Model misspecification}

We have then a family of models, indexed by $u$: $\{(\mathcal{M}(\cdot,u) , \Theta);\, \uu \in \Uspace\}$ that have to be compared with the reality: $(\mathscr{M}, \Theta_{\mathrm{real}})$, that has been ``evaluated'' at a value $\vartheta \notin \Theta$, so for each $\uu$, we have an inverse problem.
The main issue that arises is that every choice of $\uu$ for $\mathcal{M}$ will lead to another inverse problem. When looking at an extremum estimator, $\estimtxt{\theta}{}(\uu) = \argmin_{\theta \in \Theta} J(\theta,\uu)$, it becomes clear that the estimation of the parameter $\theta$ depends on $\uu$.


Robustness under parametric model misspecification


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
}
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t %"../../Main_ManuscritThese"
%%% End:
