\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter2/#1}
}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter3/build/Chapter3}
\externaldocument{../../Text/Chapter4/build/Chapter4}
\externaldocument{../../Text/Chapter5/build/Chapter5}
\externaldocument{../../Text/Conclusion/build/Conclusion}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \dominitoc
% \faketableofcontents
% Rappel du précédent style, pour qu'il aille jusqu'à la dernière page (?? latex...)
% \pagestyle{introStyle}
% \TitleBtwLines
\showthe\columnwidth
%% ---- C'est le vrai ch 1 en numérotation arabe
% \subfileLocal{\setcounter{chapter}{0}}
% \renewcommand{\thechapter}{\arabic{chapter}}%
\chapter{Inverse Problem and calibration}
\label{chap:inverse_problem}
\mtcsetdepth{minitoc}{4} 

\minitoc
\listoftodos
\newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro_chap2}
In this chapter we will first lay the ground for developing the general ideas behind calibration, by introducing the notions of models and forward and inverse problems in~\cref{sec:forward_inverse_problem_proba_theory}. This implies also a short review of notions of probability theory. Calibration will be defined in \cref{sec:parameter_inference} as the optimization of a certain objective function: Maximum likelihood estimation in a frequentist setting, or posterior maximization using Bayes' theorem. In practice, for large-scale applications, the optimization is performed using gradient-descent, and the computational cost of gradient computation can be overcome by adjoint method, as described in~\cref{sec:calibration_adjoint_optimization}. Finally, we are going to discuss two aspects related to calibration, namely model selection in \cref{sec:model_selection} and the influence of nuisance parameters and model misspecification in calibration in \cref{sec:model_misspecification}.

\section{Forward, inverse problems and probability theory}
\label{sec:forward_inverse_problem_proba_theory}
Running the model\todo{definir model} using numerical tools is useful to grasp a better understanding of the physical phenomena, or to forecast them. On the other hand, observing and comparing the physical phenomena and the output of the model brings information on our \todo{enlever possessif} modelling. More specifically, models are designed with a set of parameters, that aim at representing specific physical quantities. Those quantities have to be properly known, in order have a meaningful output when evaluating the model. 
Model calibration or parameter estimation has been widely treated in the literature, either from a statistical and probabilistic point of view using Bayesian inference, or from a variational point of view.
 \subsection{Model space data space and forward problem}
\label{sec:model_space_data_space}
In order to describe accurately a physical system, we have to define the notion of a model, and will be following~\cite{tarantola_inverse_2005} approach to define inverse problems.
 A model represents the link between some parameters and some observable quantities. A simple example is a model that takes the form of a system of ODEs or PDEs, maybe discretized, while the parameters are the initial conditions and the output is one or several time series, describing the time evolution of a quantity at one or several spatial points. An important point is that a model is not only the \emph{forward operator}, but must also include the parameter space

\begin{definition}[Model]
  A model $\mathfrak{M}$ is defined as a pair composed of a \emph{forward operator} $\mathcal{M}$, and a \emph{parameter space} $\Theta$
  \begin{equation}
    \mathfrak{M} = (\mathcal{M}, \Theta)
  \end{equation}
The forward operator is the mathematical representation of the physical system, while the parameter space is chosen here to be a subset of a finite dimensional space, so usually $\Theta$ will be a subset of $\mathbb{R}^n$.
\end{definition}
As we will usually choose $\Theta$ as a subset of $\mathbb{R}^n$, for $n\geq 1$, we can define a kind of dimensionality of the model, based on the number of \emph{degrees of freedom} available for the parameters to vary freely.
\begin{remark}
  \label{rmk:model_dimension}
  The dimension of a model $\mathfrak{M}=(\mathcal{M},\Theta)$ is the number of parameters not reduced to a singleton, so if $\Theta \subset \mathbb{R}^n$, the dimension of $\mathfrak{M}$ is $d \leq n$. The dimension of a model $\mathfrak{M}$ is sometimes called the degrees of freedom of $\mathfrak{M}$.
  \end{remark}
  
\begin{example}
  A model with parameter space $\Theta = \mathbb{R}^2\times \interval{0}{1}$ has dimension $3$, while $\Theta = \mathbb{R}^2 \times \{1\}$ has dimension $2$.
\end{example}
 Now that we have introduced the forward operator and the parameter space, we will focus on the output of the model.
The data space consists in all the physically acceptable results of the physical experiment. This set is noted $\Yspace$.
Then, the forward operator $\mathcal{M}$ maps the parameter space $\Theta \subset \mathbb{R}^{d}$ to the data space $\Yspace$, as one can expect that all models provide physically acceptable outputs.

\subsection{Forward problem}
Given a model $(\mathcal{M}, \Theta)$, the \emph{forward problem} consists in applying the forward operator to a given $\theta \in \Theta$, in order to get the \emph{model prediction}. The forward problem is then to obtain information on the result of the experiment based on the parameters we chose as input, so deriving a satisfying forward operator $\mathcal{M}$.
\begin{equation}
  \begin{array}{cccc}
    \mathcal{M}:& \Theta &\longrightarrow & \Yspace \\
                & \theta &\longmapsto     & \mathcal{M}(\theta)
  \end{array}
\end{equation}
As said earlier, the forward operator can be a set of ODEs or PDEs, discretized or not. The forward problem is then the attempt to link the causes, i.e.\ the parameters, to the consequences, i.e.\ the output in the data space.

\subsection{Inverse Problem}
The inverse problem is the natural counterpart of the forward problem, and consists in trying to gather more information on the parameters, based on the result of the experiment or the physical process, and the knowledge of the forward operator. This kind of circular procedure: adding complexity by updating the forward operator and the parameter space by choosing a model with higher complexity for the forward problem, and reducing this complexity by comparing some observations with the output of the model, and reducing the parameter space.\unsure{General approaches for inverse} 
\todo{reformuler, enlever ref circulaire}
However, a purely deterministic approach for the inverse problem is doomed to underperform: as most physical processes are not perfectly known, some uncertainties remain in the whole modelling process. Those uncertainties are ubiquitous: the observations available may be corrupted by a random noise coming from the measurement devices and the model may not represent perfectly the reality, thus introducing a systematic bias for instance. Taking into account those uncertainties is crucial to solve the inverse problem.

\todol{ajouter schema}

In that perspective we are going to introduce briefly the usual probabilistic framework, along with common notations that we will use throughout this manuscript. Those notions are well established in the scientific literature, and one can read~\cite{billingsley_probability_2008} for a more thorough description.

\subsection{Notions of probability theory}
\unsure{join subsections ? more brief ?}
\subsubsection{Probability measure, and random variables}
\label{sec:notion_prob_theory}

We are first going through some usual notions of probability theory. 
Let us consider the usual probabilistic space $(\Omega, \mathcal{F}, \Prob)$.
\begin{definition}[Event probability and conditioning]
  \label{def:prob_event}
   We call an event an element of the $\sigma$-algebra $\mathcal{F}$, and the probability of an event $A\in \mathcal{F}$ is defined as the Lebesgue integral
  \begin{equation}
    \Prob[A] = \int_{A} \,\mathrm{d}\Prob(\omega) = \Prob[\{\omega ; \omega \in A\}]
  \end{equation}
Observing an event $B \in \mathcal{F}$ can bring information upon another event $A\in \mathcal{F}$. In that sense, we introduce the conditional probability of $A$ given $B$.
\label{def:cond_proba}
  Let $A$, $B \in \mathcal{F}$.
  The event $A$ given $B$ is written $A \mid B$ and its probability is
  \begin{equation}
    \Prob[A \mid B] = \frac{\Prob[A \cap B]}{\Prob[B]}
  \end{equation}
\end{definition}
Formally, an event can be seen as an outcome of some uncertain experiment, and its probability is ``how likely'' this event will happen.

Let us now introduce a measurable state (or sample) space $(S, \mathcal{B}(S))$\todo{definir ?}.
\begin{definition}[Random Variable, Expectation]
  \label{def:random_variable}
  A random variable (abbreviated as r.v.) $X$ is a measurable function from $\Omega \longrightarrow S$. A random variable will usually be written with an upper case letter. A realisation or observation $x$ of the r.v. $X$ is the actual image of $\omega\in\Omega$ under $X$: $x = X(\omega)$. If $S$ is countable, the random variable is said to be \emph{discrete}. When $S\subseteq \mathbb{R}^p$ for $p\geq 1$, $X$ is sometimes called a random vector
  
  \label{def:expectation}
  The expectation of a r.v. $X:\Omega \rightarrow S$ is defined as
  \begin{equation}
    \Ex[X] = \int_{\Omega} X(\omega) \,\mathrm{d}\Prob(\omega)
  \end{equation}
\end{definition}

  Using the \cref{def:expectation}, the probability of an event $A$ can be seen as the expectation of the indicator function of $A$:
  \begin{equation}
    \begin{array}{cccc}
      \mathbbm{1}_{A}:& \Omega& \longrightarrow& \{0,1\} \\
                      & \omega& \longmapsto & \begin{cases}
                        1\text{ if } \omega \in A \\
                        0 \text{ if } \omega \notin A
                                              \end{cases}
    \end{array}
  \end{equation}
  and it follows that
  \begin{equation}
    \Ex[\mathbbm{1}_A] = \int_{\Omega} \mathbbm{1}_A \, \mathrm{d}\Prob(\omega)= \int_{A} \, \mathrm{d}\Prob(\omega) = \Prob[A]
  \end{equation}

  
As we defined the notion of a r.v.\ in \cref{def:random_variable} as a measurable function from $\Omega \to S$, we can now focus on the measurable sets through $X$, by using in a sense the change of variable $x = X(\omega)$. 
\begin{definition}[Image (Pushforward) measure]
  \label{def:image_measure}
  Let $X:\Omega \rightarrow S$ be a random variable, and $A \subseteq S$. The image measure (also called pushforward measure) of $\Prob$ through $X$ is denoted by $\Prob_X = \Prob \circ X^{-1}$. This notation can differ slightly depending on the community, so one can find also $ \Prob_X = \Prob \circ X^{-1} = X_{\sharp}\Prob$, the latter notation being used in transport theory. The probability, for the r.v. $X$ to be in $A$ is equal to
  \begin{equation}
    \Prob[X \in A] = \Prob_X[A] = \int_{A}\,\mathrm{d}\Prob_X(\omega) =  \int_{X^{-1}(A)}\,\mathrm{d}\Prob(\omega) = \Prob[X^{-1}(A)] = \Prob\left[\{\omega\,;\,X(\omega) \in A\}\right]
  \end{equation}
  
Similarly, for any measurable function $h$, the expectation taken with respect to a specific random variable $X$ is 
\begin{equation}
  \Ex_{X}[h(X)] = \int_{\Omega} h(X(\omega)) \,\mathrm{d}\Prob_{X}(\omega)
\end{equation}
\end{definition}



Generally speaking, the sample space will be $S\subseteq \mathbb{R}^p$ for $p\geq 1$, so we are going to introduce useful tools and notations to caracterize these particular r.v.
\subsubsection{Real-valued random variables}
We are now going to focus on
real-valued random variables, so measurable function from $\Omega$ to
the sample space $(S,\mathcal{B}(S)) =
(\mathbb{R},\mathcal{B}(\mathbb{R}))$.
\begin{definition}[Distribution of a real-valued r.v.]
  \label{def:distribution}
  The distribution of a r.v.\ can be characterized by a few functions:
  \begin{itemize}
  \item The \emph{cumulative distribution function} (further
abbreviated as cdf) of a real-valued r.v. $X$ is defined as:
  \begin{equation} F_{X}(x) = \Prob\left[X \leq x\right] =
\Prob_X\big[\,\interval[open left]{-\infty}{x}\, \big]
  \end{equation} and $\lim_{-\infty}F_X = 0$ and $\lim_{+\infty} F_X
= 1$
If the cdf of a random variable is continuous, the r.v. is said to be \emph{continuous} as well.
  
\item The \emph{quantile function} $Q_X$ is the generalized inverse function
of the cdf:
  \begin{equation} Q_X(p) = \inf\{q:\, F_X(q)\geq p\}
  \end{equation}
\item If there exists a function $f: S\rightarrow \mathbb{R}^{+}$ such that
  for all measurable sets $A$
  \begin{equation} \Prob[X \in A] = \int_A \,\mathrm{d}\Prob_X(\omega) = \int_A f(x)\,\mathrm{d}x
\end{equation}
then $f$ is called the \emph{probability density function} (abbreviated pdf), or \emph{density} of $X$ and is denoted $p_X$.
As $\Prob[X \in S] = 1$, it follows trivially that $\int_{S}f(x)\,\mathrm{d}x=1$.
One can verify that if $F_X$ is derivable, then its derivative is the density the r.v.\ :
\begin{equation}
  \frac{\mathrm{d}F_X}{\mathrm{d}x}(x) = p_X(x)
\end{equation}


  % If the pushforward measure $\Prob_X$ is absolutely continuous
% with respect to the Lebesgue measure $\lambda$ defined as
% $\lambda\left(]a, b]\right) = b-a$, then according to Radon-Nikodym
% theorem, there exists a function $p_X$, such that for all measurable
% set $A$,
%   \begin{equation} \Prob_X[A] = \Prob[X \in A] = \int_A
% \,\mathrm{d}\Prob_X(\omega) = \int_A p_X(y)\,\mathrm{d}y
%   \end{equation} This function $p_X: S\subseteq\mathbb{R} \rightarrow
%   \mathbb{R}$, is called the \emph{probability density function}
% (abbreviated pdf) of $X$, called the Radon-Nikodym derivative of
% $\Prob_X$ wrt $\lambda$: $p_X=\frac{\mathrm{d}\Prob_X}{\mathrm{d}
% \lambda} = \frac{\mathrm{d} F_X}{\mathrm{d} y}$.  As $X$ is
% real-valued, the probability for $X$ to be in an interval is
%   \begin{equation} \Prob_X\big[]a;\,b]\big]=\Prob[a \leq X < b] =
% \int_{a}^b p_X(y)\,\mathrm{d}y = F_X(b) - F_X(a)
%   \end{equation} and $\Prob_X[\mathbb{R}] =
% \int_{\mathbb{R}}p_X(y)\,\mathrm{d}y=1$. 
  \end{itemize}
\end{definition}
\begin{remark}
  When restricting this search to ``classical'' functions, $p_X$ may not exist. However, allowing generalized functions such as the \emph{dirac delta function}, provides a way to consider simultaneously all types of real-valued random variables (continous, discrete, and mixture of both). Dirac's delta function can (in)formally be defined as
  \begin{equation}
    \label{eq:def_dirac_delta}
    \delta_{x_0}(x) = 
    \begin{cases}
      +\infty \text{ if } x=x_0 \\
      0 \text{ elsewhere}
    \end{cases} \quad \text{ and }
    \int_S \delta_{x_0}(x)\,\mathrm{d}x = 1
  \end{equation}
\end{remark}
\begin{example}
  \label{ex:X_rv}
  Let us consider the random variable $X$ that takes the value $1$ with probability $0.5$, and follows a uniform distribution with probability $0.5$ over $[2;4]$. Its cdf can be expressed as
  \begin{equation}
    F_X(x) =
    \begin{cases}
      0 \text{ if } x < 1 \\
      0.5 \text{ if } 1 \leq x < 2 \\
      0.5 + \frac{x-2}{8} \text{ if } 2 \leq x < 4 \\
      1 \text{ if } 4 \leq x
    \end{cases}
  \end{equation}
  and its pdf (as a generalized function)
  \begin{equation}
    p_X(x) = \frac{1}{2}\delta_{1}(x) + \frac{1}{4}\mathbbm{1}_{\{2\leq x < 4\}}(x) 
  \end{equation}
\end{example}
\begin{figure}[!h]
  \centering
  \input{/home/victor/acadwriting/Manuscrit/Text/Chapter2/img/cdf_pdf_example.pgf}
  \caption{Cdf and Pdf of $X$ defined in \cref{ex:X_rv}. The arrow indicates Dirac's delta function}
  \label{fig:example_pdf_cdf}
\end{figure}

\begin{definition}[Moments of a r.v. and $L^s$ spaces]
  Let $X$ be a random variable.
  The moment of order $s$ is defined as $\Ex\left[X^s\right]$, and the centered moment of order $s$ is defined as
  \begin{equation}
    \Ex[(X-\Ex[X])^s]=\int \left(X(\omega) - \Ex[X]\right)^s \,\mathrm{d}\Prob(\omega) = \int (x-\Ex[X])^s\cdot p_X(x)\,\mathrm{d}x
  \end{equation}
  To ensure that those moments exists, let us define $L^s(\Prob)$ as the space of random variables $X$ such that $\Ex\left[\lvert X \rvert^s\right] < +\infty$.
  If $X\in L^2(\Prob)$, the centered moment of order $2$ is called the variance:
  \begin{equation}
    \label{eq:variance_def}
    \Ex\left[(X-\Ex[X])^2 \right] = \Var[X] \geq 0
  \end{equation}
\end{definition}


Extending those definitions from real-valued random variables to real-valued random vectors is pretty straightforward
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsubsection{Real-valued random vectors}
 Most of the definitions for a random variable extends component-wise to the random vectors:

\begin{definition}[Joint, marginal and conditional densities]
 \label{def:joint_marginal_cond_densities}
 Let $X=[X_1,\cdots,X_p]$ be a random vector from $\Omega \rightarrow S\subseteq\mathbb{R}^p$
 The expected value of a random vector is the expectation of the components
 \begin{equation}
\Ex[X] = \left[\Ex[X_1],\dots,\Ex[X_p]\right]
\end{equation}

The cdf of $X$ at the point $x=[x_1,\dots x_p]$ is
   \begin{align}
     \label{eq:def_cdf_multi}
     F_{X}(x) = F_{X_1,\dots, X_p}(x_1,\dots, x_p) &= \Prob\left[X_1 \leq x_1, \cdots, X_p\leq x_p\right]\\
                                                   &= \Prob\left[\bigcap_{i=1}^p \{\omega;\,X_i(\omega) \leq x_i\}\right] \nonumber
  \end{align}
 Similarly as in the real-valued case, we can define the pdf of the random vector, or \emph{joint pdf} by derivating with respect to the variables:
  \begin{equation}
    p_{X}(x)= p_{X_1,\dots, X_p}(x_1,\dots, x_p) =\frac{\partial^p F_X}{\partial x_1 \cdots \partial x_p}(x)
  \end{equation}
  
  and $\int_{S}p_{X_1,\dots, X_p}(x_1,\dots, x_p)\,\mathrm{d}(x_1,\dots, x_p)=1$

For two random vectors $X$ and $Y$, the (cross-)covariance matrix of $X$ and $Y$ is defined as  
  \begin{equation}
    \Cov\left[X,Y\right] = \Ex\left[(X-\Ex[X])(Y - \Ex[Y])^T\right] = \Ex[XY^T] - \Ex[X]\Ex[Y]^T
  \end{equation}
and based on this definition, we can extend the notion of variance to vectors. The covariance matrix $\Sigma \in \mathbb{R}^{p\times p}$ of $X$, is defined to be
  \begin{equation}
    \Sigma = \Cov(X)=\Cov[X,X]= \Ex\left[\left(X - \Ex[X]\right)\left(X-\Ex[X]\right)^T\right] = \Ex[XX^T] - \Ex[X]\Ex[X]^T
  \end{equation}


  We can now define the \emph{marginal densities}
  For notation clarity, we are going to set $X = [Y,Z]$

  \begin{equation}
    \label{eq:marginals_def}
    p_{Y}(y) = \int_{\mathbb{R}}p_{Y,Z}(y,z) \,\mathrm{d}z \quad \text{ and } \quad p_{Z}(z) = \int_{\mathbb{R}}p_{Y,Z}(y,z) \,\mathrm{d}y
  \end{equation}
  The random variable $Y$ given $Z$, denoted by $Y \mid Z$ has the conditional density
  \begin{equation}
    p_{Y \mid Z}(y \mid z) = \frac{p_{Y,Z}(y,z)}{p_Z(z)}
  \end{equation}
  allowing us to rewrite the marginals as
  \begin{align}
    \label{eq:marginal_conditioned}
        p_{Y}(y) = \int_{\mathbb{R}}p_{Y\mid Z}(y\mid z)p_Z(z) \,\mathrm{d}z=\Ex_Z\left[p_{Y\mid Z}(y\mid z)\right] \\ p_{Z}(z) = \int_{\mathbb{R}}p_{Z\mid Y}(z\mid y)p_Y(y) \,\mathrm{d}y = \Ex_{Y}\left[p_{Z\mid Y}(z\mid y)\right]
  \end{align}

\end{definition}


The influence of one (or a set of) random variable(s) over another can be measured with the conditional probabilities. Indeed, if the state of information on a random variable does not change when observing another one, this leads to think that the observed one provides no information on the other. This leads to the definition of independence.
\begin{definition}[Independence]
  Let $A,B\in \mathcal{F}$. Those two events are deemed independent if $\Prob[A \cap B] = \Prob[A]\Prob[B]$.
  Quite similarly, two real-valued random variables $Y$ and $Z$ are said to be independent if $F_{Y,Z}(y,z) = F_Y(y) F_Z(z)$ or equivalently, $p_{Y,Z}(y,z) = p_Y(y) p_Z(z)$
  Speaking in terms of conditional probabilities, this can be written as $p_{Y|Z}(y,z) = p_{Y}(y)$.
  If $Y$ and $Z$ are independent, $\Cov[Y, Z] = 0$. The converse if false in general.
\end{definition}
One of the most well known distribution is the normal distribution (or Gaussian).
\begin{example}[The Normal Distribution]
  \label{ex:gaussian_distribution}
  One central example is the normal (or Gaussian) distribution. Let $X$ be a r.v.\ from $\Omega$ to $\mathbb{R}$.
  $X$ follows the normal distribution of mean $\mu \in \mathbb{R}$ and variance $\sigma^2>0$ when
  \begin{equation}
    p_X(x) = \phi(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}\right)
  \end{equation}
and we write $X \sim \mathcal{N}(\mu,\sigma^2)$
For the multidimensional case, so when $X$ is a r.v.\ from $\Omega$ to $\mathbb{R}^p$,
$X$ follows a normal distribution of mean $\mu \in \mathbb{R}^p$ and covariance matrix $\Sigma \in \mathbb{R}^{p\times p}$, where $\Sigma$ is semi-definite positive.
In that case, $X\sim \mathcal{N}(\mu, \Sigma)$ the density of the random vector $X$ can be written as
\begin{equation}
    p_X(x) = (2\pi)^{-\frac{p}{2}}\lvert\Sigma\rvert^{-1}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
  \end{equation}
  where $|\Sigma|$ is the determinant of the matrix $\Sigma$, and $(\cdot)^T$ is the transposition operator.
  As the covariance matrix appears through its inverse, another encountered parametrization is to use the precision matrix $\Sigma^{-1}$.
  Examples of pdf of Gaussian normal distribution are displayed on \cref{fig:example_normal}.
\end{example}
\begin{figure}[!h]
  \centering
  \input{/home/victor/acadwriting/Manuscrit/Text/Chapter2/img/gaussian_distribution_examples.pgf}
  \caption{Densities of 1D Gaussian distributed r.v.\ (left), and density of a 2D Gaussian r.v.\ (right)}
  \label{fig:example_normal}
\end{figure}

We discussed so far the different quantities associated with random variables. Various ways exist to compare random variable in the same sample space, 

\begin{definition}[KL--divergence]
  
  The Kullback-Leibler divergence, introduced in~\cite{kullback_information_1951} is a measure of dissimilarity between two distributions, based on information-theoretic considerations.
  Let $X$, $X'$ be r.v.\ with the same sample space $\mathbb{X}$, and $p_X$ and $p_{X^{\prime}}$ their densities, such that $\forall x$, $p_X(x) = 0 \implies p_{X^{\prime}}(x) =0$. The KL-divergence is defined as
  \begin{equation}
    % \DKL{p_X}{p_Y} = \int_{\Omega} p_X(X(\omega)) \log \frac{p_X(X(\omega)}{p_Y(X(\omega)} \,\mathrm{d}\Prob_X(\omega)
    \DKL{p_X}{p_{X^\prime}} = \int_{\mathbb{X}} p_X(x) \log \frac{p_X(x)}{p_{X^\prime}(x)} \,\mathrm{d}x = \Ex_X\left[\log p_X(X)\right] - \Ex_X\left[\log p_{X^\prime}(X)\right]
  \end{equation}
\end{definition}


\subsubsection{Bayes' Theorem}
\label{ssec:bayes_theorem}

The classical Bayes' theorem is directly a consequence of the definition of the conditional probabilities in \cref{def:cond_proba}, or by considering the pdf of r.v.\ in~\cref{def:joint_marginal_cond_densities}.

\begin{theorem}[Bayes' theorem]
  Let $A, B\in\mathcal{F}$. Bayes' theorem states that
  \begin{align*}
    \Prob[A\mid B]\cdot \Prob[B] = \Prob[B \mid A]\cdot\Prob[A] \\
    \Prob[A \mid B] = \frac{\Prob[B \mid A]\cdot\Prob[A]}{\Prob[B]} \text{ if } \Prob[B] \neq 0
  \end{align*}
 In terms of densities, the formulation is sensibly the same.
  Let $Y$ and $Z$ be two random variables. The conditional density of $Y$ given $Z$ can be expressed using the conditional density of $Z$ given $Y$.
  \begin{equation}
    p_{Y\mid Z}(y \mid z) = \frac{p_{Z\mid Y}(z\mid y) p_Y(y)}{p_Z(z)} = \frac{p_{Z\mid Y}(z\mid y) p_Y(y)}{\int p_{Z,Y}(z,y) \,\mathrm{d}y}  \propto p_{Z\mid Y}(z\mid y) p_Y(y)
  \end{equation}
\end{theorem}
Bayes' theorem is central as it links in a simple way conditional densities. In the inverse problem framework, if $Y$ represents the state of information on the parameter space, while $Z$ represents the information on the data space, $Z\mid Y$ can be seen as the forward problem. Bayes' theorem allow us to ``swap'' the conditioning, and get information on $Y\mid Z$, that can be seen as the inverse problem.

\section{Parameter inference}
\label{sec:parameter_inference}
\subsection{From the physical experiment to the model}
\label{ssec:inv_problem}
The physical system (the reality) that is observed can formally be represented by a model $\mathfrak{M}=\left(\mathscr{M},\Theta_{\mathrm{real}}\right)$, so by an operator $\mathscr{M}$, applied to a set of parameters $\vartheta \in \Theta_{\mathrm{real}}$ that is unknown:\todo{mieux détailler ce qu'il se passe}
\begin{equation}
  \begin{array}{llll}
    \mathscr{M} :& \Theta_{\mathrm{real}} &\longrightarrow& \Yspace \\
                 & \vartheta & \longmapsto& \mathscr{M}(\vartheta)
  \end{array}
\end{equation}
The physical reality yields some observations $\mathscr{M}(\vartheta)$ that lie in $\Yspace$, a subset of $\mathbb{R}^p$.
\cite{kennedy_bayesian_2001,higdon_combining_2004}
Given a model $(\mathcal{M},\Theta)$, the main objective of calibration is to find $\hat{\theta}$ such that the numerical model $(\mathcal{M}, \{\hat{\theta}\})$ represents as accurately as possible the physical system, and thus matches as closely the data $\mathscr{M}(\vartheta)$.
% \begin{equation}
%     \mathscr{M}(x, \vartheta) = \mathcal{M}(x, \theta) + \delta(x,\theta)
% \end{equation}
% When evaluated on a fixed vector $x_{\mathrm{grid}} = (x_1,\dots,x_n)$, we will omit the input, and define
Let us assume that $\vartheta \in \Theta$, so we can rewrite the link between the reality and the model as
\begin{equation}
  \label{eq:vartheta_models}
    % \mathscr{M}(x_{\mathrm{grid}}, \vartheta) = 
    \mathscr{M}(\vartheta) = % \mathcal{M}(x_{\mathrm{grid}}, \theta) + \epsilon(x_{\mathrm{grid}},\theta) =
    \mathcal{M}(\vartheta) + \epsilon(\vartheta) \in \Yspace \subseteq \mathbb{R}^p
  \end{equation}
 
  The difference $\epsilon(\vartheta) = \mathscr{M}(\vartheta) - \mathcal{M}(\vartheta)$ is the error between the physical model and the model, called sometimes the misfit, or the residual error.\todo{ajouter schéma}
  This error is unknown and encompasses different sources of uncertainties, such as measurement errors, or model bias (with respect to the reality). To deal with this unknown, we are going to model it as a sample of a random variable, leading us to treat the data obtained as a random sample as well

  From the diverse assumptions we can make upon this sampled random variable, we can then treat the calibration procedure as a problem of estimation of parameters of a random variable.
  In this thesis, we are looking for an extremum estimator, estimator defined as the optimizer of a given objective function\aciter{extremum estimator}.
\subsection{Frequentist inference, MLE}
\label{sec:frequentist_inference_MLE}
\subsubsection{Formulation of the MLE}
As mentioned before, we can model the observations as a random variable, say $Y$, and assume that those were generated using a parametric family of distributions, whose densities are
\begin{equation}
  \label{eq:family_pdf}
  \left\{y\mapsto p_{Y}(y; \theta) ; \theta\in\Theta \right\}
\end{equation}
The choice has been made to keep explicit the dependency on $\theta$. For instance, we can use the hypothesis that the residual are normally distributed with a given covariance matrix $\Sigma$. As we assume that $\Yspace \subseteq \mathbb{R}^p$, $Y$ is a random vector distributed as
\begin{equation}
  \label{eq:lik_gaussian}
  Y  \sim \mathcal{N}(\mathcal{M}(\theta), \Sigma)
\end{equation}
whose one sample is $y=\mathscr{M}(\vartheta)$.
Now, instead of looking at the densities of~\cref{eq:family_pdf} as functions mapping the sample space $\Yspace$ to $\mathbb{R}$, we may look at it instead as a function of $\theta$, as the observations $y\in\Yspace$ do not vary. We can then define the likelihood function and its associated extremum estimator.
\begin{definition}[Likelihood function, MLE]
  \label{def:mle}
  The probability density function of the observations for a set of parameters is called the likelihood of those parameters given the observations, and is written $\mathcal{L}$:
  \begin{align}
    \label{eq:likelihood_definition}
    \mathcal{L}(\cdot ;y): \theta \mapsto p_{Y}(y;\theta) &= \mathcal{L}(\theta;y) \\
    &=(2\pi)^{-n/2}\lvert \Sigma \rvert^{-1/2}\exp\left(-\frac{1}{2}(\mathcal{M}(\theta) - y)^T\Sigma^{-1}(\mathcal{M}(\theta) - y)\right)
  \end{align}
  If $\Sigma = \mathrm{diag}(\sigma^2_1,\dots, \sigma^2_p)$, the likelihood can be written as the product of 1D Gaussians: 
  \begin{align}
    \mathcal{L}(\theta;y) &= \left(\prod_{i=1}^p\frac{1}{\sqrt{2\pi}\sigma_i}\right)\exp\left(\sum_{i=1}^p -\frac{(\mathcal{M}(\theta)_i - y_i)^2}{2\sigma^2_i}\right) \\
                          &= \prod_{i=1}^p\frac{1}{\sqrt{2\pi}\sigma_i}\exp\left(-\frac{(\mathcal{M}(\theta)_i - y_i)^2}{2\sigma^2_i}\right)
  \end{align}
  with $y = [y_1, \dots, y_p]$ and $\mathcal{M}(\theta) = [\mathcal{M}(\theta)_1,\dots \mathcal{M}(\theta)_p]$.
  Based on the likelihood function, we can define the \emph{Maximum Likelihood Estimator}, or \emph{MLE}, that maximizes the likelihood defined above:
  \begin{equation}
    \label{eq:def_MLE}
    \estimtxt{\theta}{MLE} = \argmax_{\theta\in\Theta}\mathcal{L}(\theta;y)
  \end{equation}

\end{definition}
  Equivalently, for practical and numerical reasons, the maximization of the likelihood is replaced by the minimization the negative log-likelihood:
  \begin{equation}
    \label{eq:MLE_iid}
    \estimtxt{\theta}{MLE} = \argmin_{\theta \in \Theta} -\log \mathcal{L}(\theta;y)= \argmin_{\theta \in \Theta} -\sum_{i=1}^p\log p_{Y_i\mid \theta}(y_i \mid \theta) 
  \end{equation} 
  where
  \begin{equation}
    -\log\mathcal{L}(\theta;y) = \frac{1}{2}(\mathcal{M}(\theta) - y)^T\Sigma^{-1}(\mathcal{M}(\theta) - y)+  \frac{n}{2}\log(2\pi) + \frac{1}{2}\log\lvert \Sigma \rvert
  \end{equation}
  % More generally, we can show that the MLE is also the minimizer of the KL-divergence between the sampling distribution:

As the optimization is performed on $\theta$, we can remove the constant terms of the objective, and rewrite the cost function as a $L^2$ norm in~\cref{eq:MLE_L2norm}.  % By optimizing with respect to $\theta$, we can omit the constant terms, and
  \begin{align}
    \estimtxt{\theta}{MLE} &= \argmin_{\theta \in \Theta}\frac{1}{2}(\mathcal{M}(\theta) - y)^T\Sigma^{-1}(\mathcal{M}(\theta) - y) \nonumber\\
                           &= \argmin_{\theta \in \Theta}\frac12 \| \mathcal{M}(\theta) - y \|^2_{\Sigma^{-1}} \label{eq:MLE_L2norm}
  \end{align}

  Frequentist inference and Maximum Likelihood estimation boils down to Generalized non-linear least-square regression, that minimizes the squared Mahalabonis distance between $\mathcal{M}({\theta})$ and $y$. This is only true as we assumed a Gaussian form of the errors in~\cref{eq:lik_gaussian}. Other choices of the sampling distribution~\cref{eq:lik_gaussian} will result in different objective functions. To reduce the sensitivity on outliers, some authors such as~\cite{rao_robust_2015} introduce Student or Laplace distributed errors, or specifically designed norm such as the Huber norm~\cite{huber_robust_2011}.

  If the covariance matrix is diagonal, the residual errors are then uncorrelated, thus indepedent due to their Gaussian nature as defined in~\cref{eq:lik_gaussian}. The likelihood can be rewritten as the product of densities evaluated at the different samples $y_i$, obtained from their true distribution $Y$. By rewriting the KL-divergence between the true density $p_Y$ and the parametric sampling distribution $p_Y(\cdot;\theta)$:
  \begin{equation}
  \DKL{p_Y}{p_Y(\cdot;\theta)} = \Ex_Y\left[\log p_Y(Y)\right]-\Ex_Y\left[\log p_Y(Y;\theta)\right]  
\end{equation}
As the first term does not depend on $\theta$, minimizing the KL--divergence is equivalent to minimizing the second part of the equation, so
\begin{align}
 \argmin_{\theta \in\Theta} \DKL{p_Y}{p_Y\left(\cdot;\theta \right)} = \argmin_{\theta \in \Theta} -\Ex_{Y}\left[\log p_{Y}(Y ; \theta)\right]
\end{align}
The true distribution of the observation is unknown, but samples $y_i$ are available, so by replacing the theoretical expectation with the empirical one, we have

\begin{equation}
  \argmin_{\theta \in\Theta} D^{\mathrm{empirical}}_{\mathrm{KL}}(p_Y \| p_Y\left(\cdot;\theta \right)) = \argmin_{\theta \in\Theta} \frac{1}{p} \sum_{i=1}^p - \log p_Y(y_i;\theta) = \estimtxt{\theta}{MLE}
\end{equation}
The MLE minimizes the empirical KL-divergence between the true distribution of the observations, and the sampling distribution of the observation (that depends on $\theta$).

\subsubsection{(Asymptotic) Properties of the MLE \todo{à garder/écrire ?}}
\todo{y faire référence, pas de détails}
In the following, the log-likelihood will be denoted $\ell(\theta) = \log \mathcal{L}(\theta;y)$, and the derivative of the log-likelihood with respect to $\theta$ is the score vector $s(\theta) = \pfrac{\ell(\theta)}{\theta}$.
Implicitly, $s$ is a function of the observation $Y$ modelled by a random variable of density $p_Y(y;\vartheta)$, so by taking the expected value of the score wrt to this r.v.\, we can see that the score function at the true value $\vartheta$ has mean $0$.
\begin{align}
  \Ex\left[s(\vartheta)\right] &= \int_{\Yspace} p_{Y}(y ; \vartheta) s(\vartheta) \,\mathrm{d}y = \int_{\Yspace} p_{Y}(y ; \vartheta) \pfrac{\log p_Y(y ; \vartheta)}{\vartheta} \,\mathrm{d}y \\
                            & = \int_{\Yspace} \pfrac{p_Y(y ; \vartheta)}{\vartheta} \,\mathrm{d}y = \frac{\partial}{\partial \vartheta} \int_{\Yspace} p_Y(y ; \vartheta) \,\mathrm{d}y \\
                            &= 0
\end{align}

The covariance matrix of the score function is the \emph{Fisher Information Matrix}:
\begin{equation}
  \mathcal{I}(\theta) = \Cov\left[s(\theta)\right]=  \Ex\left[s(\theta) s(\theta)^T\right] = \Ex\left[\left(\pfrac{}{\theta}\log \mathcal{L}(\theta;y)\right) \left(\pfrac{}{\theta}\log \mathcal{L}(\theta;y)\right)^T\right]
\end{equation}
\begin{equation}
  \mathcal{J}(\theta) = \Ex\left[-\nabla^2 \ell(\theta)\right]
\end{equation}

  Frequentist approaches do not take into account any information we could have on the possible values taken on $\theta$, except for its parameter space $\Theta$.
\subsection{Bayesian Inference}
\label{sec:bayesian_inference_MAP}
In Bayesian inference, the uncertainty present on $\theta$ is modelled by considering it as a random variable. Instead of having a precise value for $\theta$, albeit unknown, we assume that we have a \emph{prior distribution} on $\theta$, denoted $p_{\theta}$, that represents the initial state of belief upon the parameter, prior to any experiment and observations.The choice of this prior distribution will be discussed later.
After the experiment, whose sampling distribution is given by the likelihood, the prior distribution is updated to reflect the new state of belief upon the parameter. 
The Gaussian likelihood in \cref{eq:lik_gaussian} for the frequentist approach can be almost be rewritten as is in the Bayesian setting, just by conditioning $Y$ with $\theta$.
\cref{eq:lik_gaussian} becomes
\begin{equation}
  Y \mid  \theta \sim \mathcal{N}(\mathcal{M}(\theta), \Sigma)
\end{equation}
and the likelihood is the pdf $\mathcal{L}(\theta;y) = p_{Y\mid \theta}(y \mid  \theta)$.
Using Bayes' theorem, the \emph{posterior distribution} of the parameters given the observed data is
\begin{equation}
  \label{eq:bayes_posterior}
  p_{\theta \mid Y}(\theta \mid y) = \frac{p_{Y\mid \theta}(y \mid  \theta)p_{\theta}(\theta)}{p_Y(y)} = \frac{\mathcal{L}(\theta;y)p_{\theta}(\theta)}{p_Y(y)}
\end{equation}
The denominator can be seen as normalizing constant, ensuring that $\int_{\Theta} p_{\theta \mid Y} = 1$. But it can also be seen as a measure of how well does the model explain the data obtained.
\begin{definition}[Model Evidence]
\label{def:model_evidence}
  The model evidence, (or marginal likelihood, integrated likelihood) is defined as the distribution of the data marginalised over the parameters.
  \begin{equation}
    \label{eq:model_evidence}
    p_Y(y) = \int_{\Theta}p_{Y,\theta}(y,\theta)\,\mathrm{d}\theta = \int_{\Theta}p_{Y \mid \theta}(y \mid \theta)p_{\theta}(\theta)\,\mathrm{d}\theta
  \end{equation}
  This quantity depends implicitly on the underlying mathematical model $\mathfrak{M} = (\mathcal{M},\Theta)$. Comparing evidence of different models allows for the comparison of those different models. However, computing the model evidence requires the expensive evaluation of an integral over the whole parameter space, and no analytical form is available except for trivial cases. Specific techniques for this evaluation are reviewed in~\cite{friel_estimating_2011}.
\end{definition}
When the $(\mathcal{M},\Theta)$ and the data $y$ is fixed, the model evidence is constant with respect to the calibration parameter $\theta$. The posterior distribution is thus often written, and evaluated up to a multiplicative constant.
\begin{equation}
p_{\theta \mid Y}(\theta \mid y) \propto \mathcal{L}(\theta;y)p_{\theta}(\theta)
\end{equation}

From the posterior distribution (known up to a multiplicative constant), getting samples from the posterior is the main challenge \todo{finir}

\subsubsection{Posterior inference}
\label{sec:posterior_inference}
This posterior distribution is central in Bayesian analysis, as it gathers all the information we have on the parameter, given the observed data. Given~\cref{eq:bayes_posterior}, evaluating the posterior density at a point requires the evaluation of the model evidence, that is an expensive integral. To bypass this evaluation, several techniques have been developed to get samples from a unnormalized arbitrary function. One of the most well-known method is based on the construction of a Markov-chain whose stationary state is the searched posterior. The classical MCMC algorithm is require the use of a proposal density
A lot of refinement of this methods are available in the literature in order to better tackle the high-dimensionality of the parameter space, or to improve the mixing of the sampled MC chain. One refinement important to mention is Hamiltonian Monte-Carlo~\cite{hanson_markov_2001,betancourt_conceptual_2017}, that improves the performance of the chain by using the value of the gradient of the log-posterior distribution. Obtaining this gradient (although for a different purpose) is discussed in~\cref{sec:calibration_adjoint_optimization}.
\todo{évoquer mcmc sequentielle avec filtre à particules}


\subsubsection{Bayesian Point estimates}
\label{sec:bayes_point_estimates}
\todo{partir du choix de L}
Instead of tracting the whole posterior distribution, one can choose and find a value of $\theta$ that summarizes this distribution. Let us define a function $L$ that measures the distance between two values: $L:\Theta\times \Theta$.
Bayesian point estimation refer to point estimation of the parameter $\theta$, using the posterior distribution $p_{\theta \mid Y}$.\cite{lehmann_theory_2006} provides a more complete overview of this subject. Let us
This can be done by defining the \emph{Bayesian risk}, that is the expectation of a Bayesian loss functions $L: \Theta \times \Theta \rightarrow \mathbb{R}^+$ under the posterior distribution. A Bayesian point estimate is then a minimizer of this Bayesian risk.
\begin{equation}
  \theta_{L} = \argmin_{\theta^{\prime} \in \Theta} \Ex_{\theta\mid Y}\left[L(\theta^{\prime}, \theta) \mid y\right]
\end{equation}

\paragraph{Posterior mean}
By taking a loss function as the squared error $L(\theta^{\prime}, \theta) = (\theta^{\prime} - \theta)^2$, we can define the Mean Squared Error (MSE) as $\mathrm{MSE}: \theta^{\prime}\mapsto\Ex_{\theta\mid Y}\left[(\theta^{\prime} - \theta)^2\right]$. Finally, the value corresponding to the Minimum Mean Squared Error is
\begin{equation}
  \label{eq:MMSE_mini}
  \estimtxt{\theta}{MMSE} = \argmin_{\theta^{\prime}\in\Theta}\Ex_{\theta\mid Y}\left[(\theta^{\prime} - \theta)^2 \mid  y\right]
\end{equation}
Simple algebraic manipulations show that the minimizer is in fact the posterior mean:
\begin{equation}
  \label{eq:def_MMSE}
  \estimtxt{\theta}{MMSE} = \Ex_{\theta\mid Y}[\theta \mid  y] = \int_{\Theta}\theta\cdot p_{\theta\mid Y}(\theta \mid  y)\,\mathrm{d}\theta
\end{equation}
In order to compute $\estimtxt{\theta}{MMSE}$, it is easier to compute directly the mean of the posterior samples obtained via posterior inference, than to solve the minimization problem in~\cref{eq:MMSE_mini}.

\paragraph{Posterior Mode: the MAP}
Taking $L(\theta^{\prime},\theta) = -\delta_{\theta}(\theta^{\prime})$, the dirac delta function defined in~\cref{eq:def_dirac_delta}, one can show that the minimizer of $\Ex_{\theta\mid Y}\left[L(\theta^{\prime},\theta)\right]$ is the mode of the posterior distribution, and is called the \emph{Maximum A Posteriori} (MAP):\todo{corriger!}
\begin{align}
  \label{eq:def_MAP}
  \estimtxt{\theta}{MAP} &= \argmin_{\theta^{\prime} \in \Theta}\Ex_{\theta\mid Y}\left[\delta_{\theta}(\theta^{\prime})\mid y\right] = \argmin_{\theta^{\prime} \in \Theta} -p_{\theta\mid Y}(\theta^{\prime}\mid y) \\
                         &= \argmax_{\theta^{\prime} \in \Theta} p_{\theta\mid Y}(\theta^{\prime} \mid y) = \argmax_{\theta^{\prime} \in \Theta} \mathcal{L}(\theta^{\prime} ;y)p_{\theta}(\theta^{\prime})
                           \nonumber
\end{align}
% We chose to use a generalized function as a tool to link the MAP to Bayesian point estimate, but it is sometimes introduced as the limit of $0-1$ loss functions. In~\cite{bassett_maximum_2019}, the authors shows that this claim does not always hold, unless some conditions are met. \todo{à enlever}
One interesting fact about the MAP, is that its evaluation does not require the full knowledge of the posterior distribution, nor samples to evaluate the integral of~\cref{eq:def_MMSE}. We can resort to classical optimization techniques for this evaluation. Similarly to the likelihood, taking the negative logarithm leads to the following minimization problem.
\begin{equation}
  \label{eq:minimisation_MAP_log}
  \estimtxt{\theta}{MAP} = \argmin_{\theta^{\prime}\in \Theta} -\ell(\theta^{\prime};y) - \log p_{\theta}(\theta^{\prime})
\end{equation}

\subsubsection{Choice of a prior distribution}
\label{sec:choice_prior}
As seen in the application of Bayes' theorem in~\cref{eq:bayes_posterior}, the prior has a preponderant role in the formulation of the posterior distribution. Indeed, this prior distribution represents the current state of knowledge on the value of the parameter, before any experiment. This comes usually from an expert opinion, or some reasonable assumptions about the nature of $\theta$.

Let us assume for instance that we have a Gaussian prior for $\theta$: $\theta \sim \mathcal{N}(\theta_{b},B)$ where $B$ is called the background covariance error matrix and $\theta_b$ is called the \emph{background value} that act as a plausible reference value. Assuming a Gaussian form for the errors as well with covariance matrix $\Sigma$, the MAP can be written as
\begin{equation}
  \estimtxt{\theta}{MAP} = \argmin_{\theta \in \Theta} \frac{1}{2}\| \mathcal{M}(\theta) - y\|^2_{\Sigma^{-1}} + \frac{1}{2}\|\theta - \theta_b\|^2_{B^{-1}}
\end{equation}
Adding a Gaussian prior for the parameter comes down to adding a $L^2$ regularization term to the optimization problem, also called Tikhonov regularization~\cite{tikhonov_solutions_1977}. This expression is very analoguous to the state estimation in the 3D-Var method in Data assimilation 
Other choices of priors leads to other regularizations, such as the lasso regularization~\cite{tibshirani_regression_2011} that is a consequence for choosing $\theta$ that follows a priori a Laplace distribution of mean $0$.

\todo{reformuler}
The choice of a prior distribution has an influence on the inference of the parameter and its point estimation. Where there is no knowledge on the parameter beforehand, one can try to choose a non-informative prior in order to try to mitigate its effect. One can for instance choose a ``flat'' prior over the parameter space, but this can lead to \emph{improper prior}, in the sense that they do not integrate to $1$. However, improper priors, though mathematically questionable, do not necessarily lead to improper posterior, allowing for the usual Bayesian analysis of the quantity.

If $\Theta = \mathbb{R}^p$, an improper non-informative prior is $p_{\theta}(\theta) \propto 1$, as it should be invariant by translation. In this case, the MAP estimation is equivalent to the MLE, as the prior in~\cref{eq:minimisation_MAP_log} is constant with respect to $\theta^{\prime}$.
When $\Theta = \mathbb{R}_+$, the prior distribution should be invariant by multiplication by a positive constant, so $p_{\theta}(\theta) \propto \frac{1}{\theta}\mathbbm{1}_{\theta > 0}$ (i.e.\ flat in the log scale), and that leads to a regularization term of the form $\log(\theta)$.


All in all, when looking for the MAP or the MLE, parameter estimation boils down to the minimization of a well chosen objective function, that measures the misfit between the output of the numerical model and the observations. This cost function will be written $J$ in the following, to match the notation of data assimilation. As mentioned before, the MAP does not require the full knowledge of the posterior distribution $p_{\theta \mid Y}$, as ``only'' an optimization is required.

In this context of calibration, we can then summarize the estimation as a minimization problem, where $J$ represents some kind of distance between $\mathcal{M}(\theta)$ and the observations.
\begin{equation}
  \estimtxt{\theta}{} = \argmin_{\theta\in \Theta} J(\theta)
\end{equation}
% If one is looking for the whole posterior distribution,

\section{Calibration using adjoint-based optimization}
\label{sec:calibration_adjoint_optimization}
Point estimates in this context take the form of extremum estimators, that is an extremum of some given objective function $J$. This function takes the form of the log-likelihood, or the log-posterior for the MLE and MAP, but other misfits can be considered, such as optimal transport based metrics. The formulation is then quite simple, but the problem of efficient optimization remains. For differentiable problems, most of minimization instances are solved using gradient based methods, such as gradient descent, quasi-newton methods.
However, this implies to be able to compute efficiently the gradient of the cost function $J$ with respect to the parameter: $\frac{\mathrm{d}J}{\mathrm{d}\theta}$. The straightforward way, is to compute the gradient using finite differences. Let us suppose that $\theta = (\theta_1,\cdots \theta_n)$, and $e_i$ is 0 for all its component except the $i$th one which is $1$. The gradient can be approximated by the usual 1st order forward finite-difference scheme, as displayed in \cref{eq:finite_diff}.
\begin{equation}
  \label{eq:finite_diff}
  \nabla_{\theta} J  \approx \left[\frac{J(\theta + \epsilon e_1) - J(\theta)}{\epsilon}, \frac{J(\theta + \epsilon e_2) - J(\theta)}{ \epsilon},\dots, \frac{J(\theta + \epsilon e_n)- J(\theta)}{\epsilon} \right] \quad \text{ for } \epsilon \ll 1
\end{equation}
In addition to the run of the model at $\theta$, we have to evaluate the model $n$ times, for each one of the coordinate of $\theta$. If this is feasible in practice for low dimensional problems, this is impossible for large problems that cumulate more than hundreds of parameters. Nevertheless, different methods can be used to compute the gradient, atleast approximately for optimization purpose: for instance, \cite{boutet_estimation_2015} uses Simultaneous Perturbation Stochastic Approximation to approximate the gradient using only one additional run, indepedently on the number of parameters.

In geophysical applications, parameter estimation and the subsequent optimization is usually performed by deriving the adjoint equation in order to get the exact gradient for a relatively reasonable cost. This gradient is used afterward in optimization methods such as conjuguate gradient, or BFGS for instance. Adjoint methods are thus very popular in large-scale optimization of Computational Fluid Dynamics codes, as the additional cost of implementation is often worth the gain in the short term. This situation is common in data assimilation, as shown in~\cite{das_estimation_1991,das_variational_1992,honnorat_identification_2010,couderc_dassfow-shallow_2013}, or in shape optimization of airfoils in~\cite{huyse_free-form_2001}.

To derive the adjoint equations, we will first rewrite the cost function as a function of the forward operator and the parameter: $J(\theta) = J(\mathcal{M}(\theta),\theta)$:
The estimation of the parameter can be written as the following constrained optimisation problem:
\begin{equation}
  \begin{aligned}
  \min_{\theta \in \Theta} J(\theta) &= J(y, \theta) \label{eq:def_J_adjoint} \\
  \text{such that } &\mathcal{F}(y, \theta) = 0 % \label{eq:contraint_adjoint}
  \end{aligned}
\end{equation}
where the constraint on $\mathcal{F}$ signifies that the model is admissible, i.e.\ that $y = \mathcal{M}(\theta)\in \Yspace$.

 Differentiating the~\cref{eq:def_J_adjoint} with respect to $\theta$ using the chain rule gives
\begin{equation}
  \begin{aligned}
  \nabla_\theta J &= \pfrac{J}{y} \pfrac{y}{\theta} + \pfrac{J}{\theta}\\
    \nabla_\theta \mathcal{F} &= \pfrac{\mathcal{F}}{y} \pfrac{y}{\theta} + \pfrac{\mathcal{F}}{\theta}
  \end{aligned} \label{eq:aze}
\end{equation}
In those equations, the partial derivatives with respect to $\theta$ are quite easily attainable, while the real challenge is to obtain the derivative with respect to the state variable.

Introducing the Lagrange multiplier  $\lambda \in \Yspace$, we can write the Lagrangian $\mathscr{L}$
\begin{equation}
  \mathscr{L}(\theta, y, \lambda) = J(y,\theta) - \innerprod{\lambda}{\mathcal{F}(y, \theta)}
\end{equation}
The equivalent unconstrained minimization problem~\cref{eq:def_J_adjoint} is then
\begin{equation}
  \min_{\theta,y,\lambda} \mathscr{L}(\theta,y,\lambda)
\end{equation}
The first-order condition of optimality for the Lagrangian: $\pfrac{\mathscr{L}}{\theta} = \pfrac{\mathscr{L}}{y} = \pfrac{\mathscr{L}}{\lambda} = 0$ translates into the optimality condition, adjoint equation and the state equation:
When differentiating with respect to the adjoint variable, we retrieve the state equation:
\begin{equation}
  \pfrac{\mathscr{L}}{\lambda} = - \mathcal{F}(y,\theta) = 0 \tag{State equation}
\end{equation}
When differentiating with respect to the state variable, the equation that verifies the adjoint variable is called the adjoint equation
\begin{equation}
  \pfrac{\mathscr{L}}{y} = \pfrac{J}{y} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{y}}= 0  \tag{Adjoint equation}
\end{equation}
Finally, when $\lambda$ verifies the adjoint equation: $ \left(\pfrac{\mathcal{F}}{y}\right)^T\lambda= \left(\pfrac{J}{y}\right)^T$, the gradient of the cost function can be expressed using the partial derivative \emph{with respect to $\theta$} of the cost function and of the forward model, and the adjoint variable:

\begin{equation}
    \pfrac{\mathscr{L}}{\theta} = \nabla_\theta J %  &= \pfrac{J}{\theta}(y,\theta) - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\theta}(y,\theta)} \\
                               % &=\pfrac{J}{\mathcal{M}}\pfrac{\mathcal{M}}{\theta}+ \pfrac{J}{\theta} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\mathcal{M}} \pfrac{\mathcal{M}}{\theta} + \pfrac{\mathcal{F}}{\theta}} \\  
                               % &= \left(\pfrac{J}{\mathcal{M}} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\mathcal{M}}} \right) \pfrac{\mathcal{M}}{\theta} + \left( \pfrac{J}{\theta} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\theta}} \right) \\
                               = \pfrac{J}{\theta} - \innerprod{\lambda}{\pfrac{\mathcal{F}}{\theta}} = 0 \tag{Optimality condition}
\end{equation}




\section{Model selection}
\label{sec:model_selection}
So far, we have discussed the calibration of a specific model $(\mathcal{M}, \Theta)$ given some observations, thus solving an inverse problem and finding $\estimtxt{\theta}{}$ as an extremum of a specified objective function. In terms of models, it can be described as reducing the model $(\mathcal{M}, \Theta)$ to $(\mathcal{M}, \{\estimtxt{\theta}{}\})$, thus reducing the dimension of the original model.
\todol{Occam's razor}

\subsection{Likelihood ratio test}
\label{sec:likelihood_ratio_test}

If this specific calibrated model is efficient, it can be also quite complex, and it can be interesting to test if a ``simpler'' model would give similar performances, or at least show a decrease in performances not statistically significant.
One of the well-known test is the Likelihood-ratio test, that test if two \emph{nested models} are equivalent:
Let us consider two nested models: $\mathfrak{M}_1 = (\mathcal{M}_1, \Theta_1)$, $\mathfrak{M}_2= (\mathcal{M}_2,\Theta_2)$, such that $\mathcal{M}_1=\mathcal{M}_2=\mathcal{M}$ and $\Theta_2 \subsetneq \Theta_1$. In this case, $\mathfrak{M}_2$ represents the simpler model, with a reduced parameter space, while $\mathfrak{M}_1$ is the more general model. Recalling the notion of model dimension in~\cref{rmk:model_dimension},  $\mathfrak{M}_1$ has dimension $r$, and $\mathfrak{M}_2$ has dimension $d$ with $r>d$.

\todo{famille de nested models/concurrents}
Under the null hypothesis, the two models are equivalent, that is the ``smaller'' parameter space is enough to represent the inverse problem: $\theta \in \Theta_2$. The alternative hypothesis is that $\theta \in \Theta_1$.
The likelihood ratio is defined as the ratio of the largest values taken by the likelihood on their respective parameter space, value that is assumed to be attained as $\hat{\theta}_1$ and $\hat{\theta}_2$. \todo{mieux détailler hyp}
\begin{equation}
  \label{eq:def_lik_ratio}
  \Lambda(y) = \frac{\sup_{\theta \in \Theta_2} \mathcal{L}(\theta ; y)}{\sup_{\theta \in \Theta_1} \mathcal{L}(\theta ; y)} = \frac{\mathcal{L}(\hat{\theta}_2 ; y)}{\mathcal{L}(\hat{\theta}_1 ; y)}
\end{equation}
Under the null hypothesis (that the models are equivalent), $-2 \log \Lambda$, (sometimes called the deviance) follows asymptotically (as the number of observations becomes large) a $\chi^2$ distribution, whose degrees of freedom is given by the difference of dimensionality between the two models:
\begin{equation}
  - 2 \log \Lambda(y) \xrightarrow[]{\mathrm{d}} \chi^2_{r-d}
\end{equation}
By denoting $\chi^2_{r-d}(1-\alpha)$ the quantile of order $1-\alpha$ of the $\chi^2$ distribution of $r-d$ degrees of freedom, the asymptotic rejection region of level $\alpha$ is:
\begin{align}
  \mathrm{RejReg}_{\alpha} &= \left\{y \mid -2 \log \Lambda(y) > \chi^2_{r-d}(1-\alpha) \right\} \label{eq:LRT_rej_reg}\\
                           &= \left\{ y \mid (\sup_{\theta\in\Theta_1} l(\theta;y) - \sup_{\theta\in\Theta_2} l(\theta;y)) > \frac12 \chi^2_{r-d}(1-\alpha) \right\} \\
                             &= \left\{ y \mid J(\hat{\theta}_2) - J(\hat{\theta}_1) >  \frac12 \chi^2_{r-d}(1-\alpha) \right\}
\end{align}
As a basis for comparison, when $\Theta \subset \mathbb{R}$, $r-d=1$ and $\chi^2_1(1-0.05) =3.84$ 
\subsection{Relative Likelihood}
\label{sec:relative_likelihood}
Relative Likelihood, as defined in~\cite{kalbfleisch_probability_1985}, is the ratio of the likelihood evaluated at a point $\theta$ to the maximal value of the likelihood:
\begin{equation}
  R(\theta) = \frac{\mathcal{L}(\theta;y)}{\mathcal{L}(\estimtxt{\theta}{MLE};y)} = \frac{\mathcal{L}(\theta;y)}{\sup_{\theta^{\prime} \in \Theta}\mathcal{L}(\theta^{\prime};y)}
\end{equation}
This function allows for comparing the plausibility of the value $\theta$, compared to the MLE.
Extending a bit the relative likelihood, we can define in the same vein the likelihood interval of level $p\in ]0,1]$, defined as
\begin{equation}
  \mathcal{I}_{\mathrm{Lik}}(p) = \left\{\theta \mid R(\theta)=\frac{\mathcal{L}(\theta;y)}{\mathcal{L}(\estimtxt{\theta}{MLE};y)} \geq p\right\}
\end{equation}
This interval can be understood as a kind of confidence interval for the MLE. $p$ can be arbitrarily set to arbitrary threshold, but it can also be chosen specifically in order to avoid the rejection region of a likelihood ratio test with certain confidence. When comparing the models $(\mathcal{M}, \{\theta\})$ and $(\mathcal{M},\Theta)$, $R(\theta)$ is their likelihood ratio, the complement of the rejection region of~\cref{eq:LRT_rej_reg} written as a likelihood interval becomes
\begin{equation}
 \mathcal{I}_{\mathrm{Lik}}\left(\exp\left(-\frac{1}{2}\chi^2_{\mathrm{dim}(\Theta)}(1-\alpha)\right)\right) = \left\{\theta \mid R(\theta) \geq \exp\left(-\frac{1}{2}\chi^2_{\mathrm{dim}(\Theta)}(1-\alpha)\right) \right\}
\end{equation}
The values in this set generate models that are statistically equivalent to the model comprising the MLE as its calibrated parameter. Again, for $1$ dimensional models, and the confidence level of $.05$, $\exp\left(-\frac{1}{2}\chi^2_{\mathrm{dim}(\Theta)}(1-\alpha)\right) = \exp\left(-\frac{1}{2}\chi^2_{1}(.95)\right) \approx 0.15$

\subsection{AIC and non-nested models}
\label{sec:AIC_crit}
The AIC: Akaike's Criterion Information is defined as
\begin{equation}
  \mathop{\mathrm{AIC}}(\mathfrak{M}) = -2\left(\sup_{\Theta} \log \mathcal{L}\right) + 2 \mathop{\mathrm{dim}} \Theta = -2\log \mathcal{L}(\estimtxt{\theta}{MLE}) + 2 \mathop{\mathrm{dim}} \Theta
\end{equation}
where $\mathcal{L}$ is the likelihood function for the model $\mathfrak{M} = (\mathcal{M},\Theta)$.
 % of the AIC are the deviance of a modely the number of parameters needed to describe this model.
Small values of the AIC tend towards a better representation of the reality, whilst avoiding overfitting due to the penalization term.
Contrary to the Likelihood ratio test that compares two nested models, the AIC let us compare non-nested models: the difference between the AIC can be considered as an indication toward one or the other model~\cite{burnham_multimodel_2004}.
When both models have the same number of dimension, the difference of AIC is equal to the deviance.
\subsection{Bayesian model comparison and model averaging}
For a model $\mathfrak{M}$, the model evidence as introduced in~\cref{def:model_evidence}, is the likelihood marginalized over the parameter space, and will be written $p_{Y\mid \mathfrak{M}}$. This evidence represents how likely have the data $y$ been generated using the statistical model $\mathfrak{M}$.

The Bayes' factor is defined as the ratio of the evidence of the two models.
\begin{equation}
  \label{eq:bayes_factor}
  \mathop{\mathrm{BF}}(\mathfrak{M}_1,\mathfrak{M}_2)= \frac{p_{Y\mid \mathfrak{M}_1}(y \mid \mathfrak{M}_1) }{p_{Y\mid \mathfrak{M}_1}(y \mid \mathfrak{M}_2)}
\end{equation}
% This factor, as described in~\cite{fragoso_bayesian_2018}, represents how much more evidence does the model $\mathfrak{M}_1$ provides of being ``true'' than $\mathfrak{M}_2$.
Quite similarly as the AIC introduced in \cref{sec:AIC_crit}, the Bayes' factor is usually compared to specific values, allowing us to conclude roughly on how strong does the data favors $\mathfrak{M}_1$: for~\cite{kass_bayes_1995}, if $\log\mathop{\mathrm{BF}}(\mathfrak{M}_1,\mathfrak{M}_2)>2$, there is ``decisive'' evidence for $\mathfrak{M}_1$ against $\mathfrak{M}_2$, while lower values will indicate ``strong'', ``substantial'' and then ``not worth mentioning'' difference of evidence.

\unsure{parler de model averaging ? AIC weights et Bayesian factor as weights}


\section{Parametric model misspecification and nuisance parameters}
\label{sec:model_misspecification}
We introduced earlier the mathematical model $(\mathcal{M},\Theta)$, and based our analysis on the fact that the ``target model'', i.e.\ the reality is $(\mathscr{M},\Theta_{\mathrm{real}} = \Theta)$, so the parameter spaces are the same.
In practice, the parameter space $\Theta$ does not contain all the parameters needed to run the forward model, but represents the space of the parameters of interest, or calibration parameters. In addition to them, some other parameters are at play, that we are going to call the \emph{environmental parameters}, or \emph{uncertain parameters} written $u\in \Uspace$. These parameters come from instance from the physical forcings. 

The environmental parameters introduced before, are in fact a bit different from the calibration parameters generically introduced as $\theta$. Bayesian framework and more specifically Bayesian update of the prior by the likehihood puts the emphasis on the update of the information on the \emph{parameter of interest}. However the environmental parameters have a inherent variability. In that sense, it may not be worth spending time and resources to infer these parameter values. Moreover, we can only get information on the environmental conditions used to generate the observations.
We aim at letting this parameter stay free, and at finding a good value of the calibration parameter $\theta$, without doing any inference on $\uu$.

We have then a family of models, indexed by $u$: $\{(\mathcal{M}(\cdot,u) , \Theta);\, \uu \in \Uspace\}$ that have to be compared with the reality: $(\mathscr{M}, \Theta_{\mathrm{real}})$, that has been ``evaluated'' at a value $\vartheta \notin \Theta$.
The main issue that arises is that every choice of $\uu$ for $\mathcal{M}$ will lead to a specific inverse problem, misspecified in the sense that $\vartheta \notin \Theta$.  When looking at an extremum estimator, that is an estimator that minimizes an objective function $J$ $\estimtxt{\theta}{}(\uu) = \argmin_{\theta \in \Theta} J(\theta,\uu)$, it becomes clear that the estimation of the parameter $\theta$ depends on $\uu$.

Similarly to the characterization of the MLE inf~\cref{sec:frequentist_inference_MLE}, for each $u$, $\estimtxt{\theta}{MLE}(u)$ minimizes the empirical KL-divergence between the true distribution of the observations and the misspecified sampling model, and can be seen as the ``best'' value given $u$. However, the asymptotic properties of the MLE are slightly different as described in~\cite{white_maximum_1982}.


In the following, we assume that $\uu$ is sampled from a random variable $\UU$, with density $p_U$, and that $p_{\theta,\UU}(\theta,\uu) = p_{\theta}(\theta) p_{\UU}(\uu)$, that is independence through the prior distributions.
All the equations written above still hold, and can be derived by doing the substitution $\theta \gets (\theta,u)$

\subsection{Nuisance parameters}
\label{sec:nuisance_parameters}
\cite{berger_integrated_1999}
\subsubsection{Profile and integrated Likelihood}

The likelihood is now $\mathcal{L}(\theta, u ;y)$. One common way to deal with the ``corrupted'' likelihood, in the sense that it depends on the uncertain variable, is to define the \emph{profile} likelihood:
\begin{equation}
  \label{eq:eq:def_profile_lik}
  \mathcal{L}_{\mathrm{profile}}(\theta;y) = \max_{\uu \in \Uspace} \mathcal{L}(\theta,u;y)
\end{equation}
Another alternative is to define the \emph{integrated} likelihood as
\begin{align}
  \mathcal{L}_{\mathrm{integrated}}(\theta;y) &= \int_{\Uspace} \mathcal{L}(\theta,\uu;y) p_{\UU}(\uu) \,\mathrm{d}\uu \\
                                              &=\int_{\Uspace} p_{Y|\theta,\UU}(y \mid \theta,u) p_{\UU}(\uu) \,\mathrm{d}\uu \\
                                              &=\int_{\Uspace} p_{Y,\UU|\theta}(y,u \mid \theta)\,\mathrm{d}\uu \\
\end{align}



\subsection{Model misspecification}


% Robustness under parametric model misspecification



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
  % \listoftodos
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
}
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
