\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter3/#1}
}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter2/build/Chapter2}
\externaldocument{../../Text/Chapter4/build/Chapter4}
\externaldocument{../../Text/Chapter5/build/Chapter5}
\externaldocument{../../Text/Conclusion/build/Conclusion}
}
\newcommand\imgpath{/home/victor/acadwriting/Manuscrit/Text/Chapter3/img/} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \dominitoc
% \faketableofcontents
% \subfileLocal{\setcounter{chapter}{1}}
\chapter{Robust estimators in the presence of uncertainties} 
\label{chap:robust_estimators}

\minitoc
% \newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the previous chapter, we raised the problem of misspecification of the numerical model with respect to the reality. Moreover, this misspecification is random by nature, and we wish that the calibrated model shows relatively good performances when the environmental variables varies.

\section{Defining robustness}
\label{sec:def_robustness}
\subsection{Classifying the uncertainties}
In the Bayesian formulation of the problem, the uncertainty on the calibration parameter is modelled through the prior distribution, while the uncertain parameter, $u$ has its own distribution. While mathematically similar, those two representations actually encompasses a significant difference: we are actively trying to reduce the uncertainty of the calibration parameter by Bayesian update, while the uncertainty on the environmental parameter is seen as a nuisance.

In that context, the very notion of uncertainty can be roughly split in two, as described in~\cite{walker_defining_2003}:
\begin{itemize}
\item Aleatoric uncertainties, coming from the inherent variability of a phenomenon, \emph{e.g.} intrinsic randomness of some environmental variables
\item Epistemic uncertainties coming from a lack of knowledge about the properties and conditions of the phenomenon underlying the behaviour of the system under study
\end{itemize}
According to this distinction,  the epistemic uncertainty can be reduced by investigating the effect of the calibration parameter $\kk$ upon the physical system, and choose it accordingly to an objective function.
The uncertain variable on the other hand is uncertain in the aleatoric sense, and cannot be controlled directly, as its value is doomed to change. This distinction illustrated \cref{fig:sources_uncertainties} is a bit rough, as~\cite{kiureghian_aleatory_2009} point out that deciding the type of uncertainties is up to the modeller, who decide on which parameters inference is worth doing.

\begin{figure}[ht]
  \begin{center}
  \resizebox{\linewidth}{!}
  {
      \input{/home/victor/acadwriting/Manuscrit/Text/Chapter3/img/modelling_uncertainties}
    }
    \end{center}
  \caption{\label{fig:sources_uncertainties} Sources of uncertainties and errors in the modelling. The natural variability of the physical system can be seen as aleatoric uncertainties, and the errors on the parameters as epistemic uncertainties}
\end{figure}

\subsection{Robustness or reliability ?}


The notion of \emph{Robustness} is dependent on the field in which it is used. Worse, in the same community, robustness may carry a lot of different meanings. Robust is often used to describe something that behaves still nicely under uncertainties, or to put it in an other way, that is insensitive up to certain extent to some perturbations.

For instance, Bayesian approaches are sometimes criticized for their use of subjective probabilities that represent the state of beliefs, especially on the choice of prior distributions. In that sense, robust Bayesian analysis aims at quantifying the sensitivity of the choice of the prior distribution on the resulting inference and relative Bayesian quantities derived. In the statistical community, robustness is often implied as the non-sensitivity on the outliers in the sample set.

Moreover, robustness is often linked and sometimes confused to the semantically close notion of \emph{reliability}. In~\cite{lelievre_consideration_2016} we can find summarized in~\cref{tab:lelievre} the difference between these notions,  by defining optimality as the deterministic counterpart of robustness, and admissibility as the counterpart of reliability.

\begin{table}[htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clllll@{}}
\multicolumn{1}{l}{}         & \multicolumn{5}{c}{Robustness}                                                                                                \\ \cmidrule(l){2-6} 
\multirow{5}{*}{\rotatebox{90}{Reliability}} & \multicolumn{2}{l}{\multirow{2}{*}{}}        & \multirow{2}{*}{no objective} & \multicolumn{2}{c}{objective}                  \\ \cmidrule(l){5-6} 
                             & \multicolumn{2}{l}{}                         &                               & \multicolumn{1}{c}{deterministic inputs}  & uncertain inputs      \\ \cmidrule(l){2-6} 
                             & \multicolumn{2}{c}{Unconstrained}            & No problem                    & Optimal                & \cellcolor{brewlight} Robust \\ % \cmidrule(l){2-6} 
                             & \multirow{2}{*}{Constrained} & \multicolumn{1}{r}{deterministic constraints} & Admissible                    &Optimal and admissible & \cellcolor{brewlight} Robust and admissible \\
                             &                              & \multicolumn{1}{r}{uncertain constraints}     & \cellcolor{brewlight} Reliable &\cellcolor{brewlight} Optimal and reliable   & \cellcolor{brewlight} Robust and reliable   \\ \cmidrule(l){2-6} 
\end{tabular}%
}
\caption{Types of problems, depending on their deterministic nature for the constraints or the objective. Shaded cells correspond to problems comprising an uncertain part. Reproduced from~\cite{lelievre_consideration_2016}}
\label{tab:lelievre}
\end{table}



\subsection{Robustness under parameteric misspecification}

Given a family of models $\left\{\left(\mathcal{M}(\cdot, \uu), \Theta\right), \uu\in\Uspace\right\}$, we can derive a problem of parameter estimation for each $\uu$. As detailed in~\cref{chap:inverse_problem}, we can look for the MLE or the MAP, and formulate the likelihood and the posterior distribution accordingly.

% More generally, we can assume the existence of an objective function $J$ that takes two distinct inputs where $\kk\in \Kspace$ is the calibration parameter, and $\uu \in \Uspace$ is the uncertain parameter.
% \begin{equation}
%   \label{eq:def_J}
%   \kk, \uu \longmapsto J(\kk,\uu)
% \end{equation}
% This uncertain parameter is modelled by a random variable $\UU$.


Not taking into account the uncertainty on $\uu$ may be an issue in the modelling, especially if the influence of this variable is non-negligible.
Choosing a specific $\uu \in \Uspace$ leads to \emph{localized optimization} \citep{huyse_free-form_2001} and \emph{overcalibration}, that is choosing a value $\hat{\kk}$ that is optimal for the given situation (which is induced by $\uu$). This value does not carry the optimality to other situations, or in layman's term according to~\cite{andreassian_all_2012}, being lured by ``fool's gold''.
In geophysics and especially in hydrological models, this overcalibration may lead to the appearance of abberations in the predictions as those uncertainties become prevalent sources of errors. In hydrology, uncertainties are the principal culprit of the existence of  ``Hydrological monsters''~\citep{kuczera_there_2010}, that are calibrated models that perform really badly.

There are various ways to treat those environmental parameters. We are first going to see how to get rid of this dependence from a frequentist (likelihood-based) and Bayesian point of view.


\section{Nuisance parameters}
\label{sec:nuisance_parameters}
The environmental parameters are sometimes called \emph{nuisance} parameters.
Dealing with nuisance parameters usually means to eliminate them from the estimation. We will first detail likelihood-based methods of eliminating and their extension to Bayesian framework. 
\subsection{Profile and integrated Likelihood}
From a frequentist approach, we define the joint likelihood $\mathcal{L}(\theta, u ;y) = p_{Y\mid \KK,\UU}(y \mid \kk, \uu)$. There are two common ways to get rid of the nuisance parameters: one by \emph{profiling}, one by \emph{marginalization}.
Profiling implies to perform first a maximization of the likelihood with respect to the nuisance parameters:
\begin{equation}
  \label{eq:eq:def_profile_lik}
  \mathcal{L}_{\mathrm{profile}}(\theta;y) = \max_{\uu \in \Uspace} \mathcal{L}(\theta,u;y)
\end{equation}
and
\begin{equation}
  \estimtxt{\kk}{prMLE} = \argmax_{\kk\in\Kspace} \mathcal{L}_{\mathrm{profile}}(\theta;y)
\end{equation}
In other words, considering the most favorable case of the likelihood given the nuisance parameters.
Comparing the MLE over $\Kspace \times \Uspace$ for the original joint likelihood and the profile MLE on $\Kspace$ for the profile likelihood, it is straightforward to verify that their components on $\Kspace$ coincide as
\begin{equation}
  \max_{(\kk ,\uu) \in \Kspace\times \Uspace} \mathcal{L}(\theta,\uu;y) = \max_{\kk\in \Kspace} \mathcal{L}_{\mathrm{profile}}(\theta;y)
\end{equation}

The resulting estimator does not take into account the uncertainty upon $\uu$, and can perform quite badly when the likelihood presents sharp ridges~\cite{berger_integrated_1999}.

Another alternative is to define the \emph{integrated}, or \emph{marginalized} likelihood as
\begin{align}
  \mathcal{L}_{\mathrm{integrated}}(\theta;y) &= \int_{\Uspace} \mathcal{L}(\theta,\uu;y) p_{\UU}(\uu) \,\mathrm{d}\uu \label{eq:def_int_lik}\\
                                              &=\int_{\Uspace} p_{Y|\theta,\UU}(y \mid \theta,u) p_{\UU}(\uu) \,\mathrm{d}\uu \\
                                              &=\int_{\Uspace} p_{Y,\UU|\theta}(y,u \mid \theta)\,\mathrm{d}\uu \\
  &= p_{Y \mid \kk}(y \mid \kk)
\end{align}
and by maximizing this function,
\begin{equation}
  \estimtxt{\kk}{intMLE} = \argmax_{\kk\in\Kspace}   \mathcal{L}_{\mathrm{integrated}}(\theta;y)
\end{equation}
The profile and integrated likelihood have been computed for the following likelihood:
\begin{align}
  Y \mid \KK, \UU &\sim \mathcal{N}(\kk + \uu^2, 2^2)
\end{align}
and the observations $y = [y_1,\cdots, y_{10}]$ have been generated using $\theta+\uu^2=1$. We set $\Kspace = [-5,5]$ and $\Uspace = [-2, 2]$. The likelihood evaluated on $\Kspace \times \Uspace$ is displayed~\cref{fig:profile_integrated_lik}, with the integrated and profile likelihood.
\begin{figure}[ht]
  \centering
  \input{\imgpath profile_integrated_lik.pgf}
  \caption{\label{fig:profile_integrated_lik} Profile and integrated likelihood for an uniform nuisance parameter}
\end{figure}

We can see that there is not unicity of the maximizer of the profile likelihood: $\mathcal{L}_{\mathrm{profile}}(\kk;y)$ is constant for $\kk \in [-3, 1]$. For the integrated likelihood, there is a unique maximum, attained for $\estimtxt{\kk}{intMLE} \approx 0.8$.

\subsection{Joint posterior distribution}
Again, by introducing a prior distribution on $\kk$: $p_{\kk}$, we can derive the posterior distribution. We assume that $\UU$ and $\kk$ are independent: $p_{\KK, \UU} = p_{\KK}\cdot p_{\UU}$.
The likelihood of the data given $\kk$ and $\uu$ is
\begin{equation}
  \mathcal{L}(\kk,\uu;y) = p_{Y \mid \KK, \UU}(y\mid \kk,\uu)
\end{equation}
The joint posterior distribution can be written as:
\begin{align}
  p_{\KK,\UU \mid Y}(\kk,\uu \mid y) &= \mathcal{L}(\kk,\uu;y)p_{\KK}(\kk)p_{\UU}(\uu) \frac{1}{p_Y(y)} \\
  &\propto \mathcal{L}(\kk,\uu;y)p_{\KK}(\kk)p_{\UU}(\uu)
\end{align}
Here, the posterior is used to do inference on $\kk$ and $\uu$ jointly. In order to suppress the dependency in $u$, we integrate with respect to $\UU$ and get the marginalized posterior $p_{\theta \mid Y}$:
\begin{align}
  p_{\KK \mid Y}(\kk \mid y) &= \int_{\Uspace} p_{\KK,\UU\mid Y}(\kk,\uu\mid y)\,\mathrm{d}u \label{eq:marg_MMAP}\\
                             &= \int_{\Uspace}p_{\KK \mid Y, \UU,}(\kk \mid y, \uu) p_{\UU\mid Y}(\uu \mid y)\,\mathrm{d}u
\end{align}

We can then define the marginalized maximum as posteriori (MMAP)~\cite{doucet_marginal_2002} as the  maximizer of this marginalized posterior, written by omitting the model evidence:
\begin{equation}
  \label{eq:def_MMAP}
  \estimtxt{\kk}{MMAP} = \argmax_{\kk\in\Kspace} p_{\KK \mid Y}(\kk\mid y)
\end{equation}
or, by taking the negative logarithm to get a minimization problem:
\begin{equation}
\estimtxt{\kk}{MMAP} = \argmin_{\kk\in\Kspace} -\log \left(\int_{\Uspace} p_{\KK\mid Y,\UU}(\kk\mid y, u) p_{\UU \mid Y}(\uu \mid y)\,\mathrm{d}\uu \right)
\end{equation}
Unfortunately, neither the integration with respect to the nuisance parameter in~\eqref{eq:marg_MMAP} nor the subsequent optimization is analytically easy. \cite{doucet_marginal_2002} proposes a MCMC method in order to estimate iteratively the MMAP, through sampling of the joint posterior.

\section{The cost function as a random variable}
\label{sec:J_rv}
We discussed so far the calibration problem with nuisance parameters in the formulation of the likelihood or the posterior distribution. We are now going to focus on a ``variational'' approach, using a cost function
\begin{equation}
  J: \Kspace \times \Uspace \rightarrow \mathbb{R}^+
\end{equation} 
This function in a context of calibration is measuring the misfit between the data $y$, and the forward operator. However, the methods introduced in the following are not specific to this context, and $J$ can represent other undesirable properties, such as instability or drag in airfoil design optimization for instance. This work is largely based on~\cite{trappler_robust_2020}, which was submitted for review in February 2020.


All in all, $J(\kk, \uu)$ represent the cost of taking the decision $\kk\in\Kspace$ when the environmental variable is equal to $\uu$.
We are going to make several assumptions on this function:
\begin{itemize}
\item $\Kspace$ is convex and bounded 
\item For all $\kk \in \Kspace$ and $\uu \in \Uspace$, $J(\kk, \uu)>0$
\item For all $\kk \in \Kspace$, $J(\kk, \cdot)$ is measurable
\item For all $\kk \in \Kspace$, $J(\kk, \UU) \in L^p(\Prob_{\UU})$ and $p \geq 2$. So for each $\kk$, mean and variance exist and are finite.
\end{itemize}

Most of existing methods require first to remove the dependency on the uncertain variable, by defining a \emph{robust} (and deterministic) counterpart of the minimization problem under uncertainty, that we can solve using classical methods of optimization.

\subsection{Decision under uncertainty}
We will first detail some estimators that can arguably be said robust, even though the random nature of $\UU$ is not taken into account. This uncertainty is modelled through the principle of indifference, stating that no information whatsoever is available on $\uu$, or the distribution of $\UU$.


\subsubsection{Global optimization}
A global optimization criterion, as its name suggests, advocates for minimizing the cost function over the whole space $\Kspace \times \Uspace$, giving this optimization problem:
\begin{equation}
  \min_{(\kk,\uu) \in \Kspace \times \Uspace} J(\kk, \uu)
\end{equation}
Rearranging slightly this problem, the $\theta$-component of the minimizer can be written as
\begin{equation}
  \label{eq:kkglobal}
  \estimtxt{\kk}{global} = \argmin_{\kk \in \Kspace} \min_{\uu \in \Uspace} J(\kk, \uu)
\end{equation}
 This strategy is optimistic in the sense that we minimize over only the \emph{most favourable} cases. 
The global minimum is the equivalent of profile likelihood maximization, hence inherits its flaws: there is no guarantee on the behaviour of $J$ outside of the optimistic situation.

\subsubsection{Worst-case optimization}
\label{sec:saddle_point}
As global optimization is inhenrently optimistic, we can easily derive a criterion which is pessimistic in the sense that we want to minimize over the \emph{least favourable} cases, thus minimizing the loss in the worst-case scenarios. The optimization problem in this case becomes
\begin{equation}
  \min_{\kk\in\Kspace} \max_{\uu \in \Uspace} J(\kk,\uu)
\end{equation}
This criterion is sometimes called Wald's Minimax criterion~\cite{wald_statistical_1945}, and the associated estimator is
\begin{equation}
  \label{eq:kkwc}
  \estimtxt{\kk}{WC} =  \argmin_{\kk \in \Kspace} \max_{\uu \in \Uspace} J(\kk, \uu)
\end{equation}

Minimizing in the worst-case sense also possesses some flaws, especially from a computational point of view.
First, the maximum on $\Uspace$ may not exist, especially if $\Uspace$ is unbounded: we could make the model perform as badly as possible by taking extreme values of $\uu$. 
Additionally, if it exists, the resulting estimator is most likely very conservative as only the worse cases are considered.

\subsubsection{Regret maximin}
\label{ssec:regret_savage}
One other approach, called Savage's maximin regret~\cite{savage_theory_1951} is to compare the current loss to the best performance given the uncertain variable $\uu$:
The translated loss is called the \emph{regret} and is defined as
\begin{equation}
  r(\kk, \uu) = J(\kk, \uu) - \min_{\kk \in \Kspace} J(\kk, \uu)
\end{equation}
Using the regret as the new loss function, we can optimize it in the worst-case sense, as introduced in~\cref{sec:saddle_point}:
\begin{equation}
  \min_{\kk \in \Kspace} \max_{\uu \in \Uspace} r(\kk,\uu)
\end{equation}
The minimum is attained at $\estimtxt{\kk}{rWC}$:
\begin{equation}
  \estimtxt{\kk}{rWC} = \argmin_{\kk\in\Kspace} \max_{\uu \in \Uspace} r(\kk, \uu)
\end{equation}

\Cref{fig:decision_under_uncertainty} shows global, worst-case and regret optimization for an analytical cost function:
\begin{equation}
  \label{eq:decision_under_unc}
  J(\kk, \uu) = \left(1 + \uu(\kk + 0.1)^2\right)\left(1 + (\kk - \uu)^2\right)
\end{equation}


\begin{figure}[ht]
  \centering
  \input{\imgpath decision_under_uncertainty.pgf}
  \caption{\label{fig:decision_under_uncertainty} Illustration of global optimization, worst-case, and regret worst-case. The red points on the contour of the cost function are the location of the minimizers, while the blue points, the maximizers, are all located on the line $\kk = 1$}
\end{figure}

\subsection{Robustness based on the moments of an objective function}
In the presence of uncertainties, choosing a parameter value $\kk$ can also be seen as making as making a choice under risk. Let $J:\Kspace \times \Uspace\rightarrow \mathbb{R}^+$ be an objective function, and assume that for all $\kk \in \Kspace$, $J(\kk, \cdot)$ is a measurable function.
$J$ can be seen as the opposite of the \emph{utility} function, often encountered in game theory or econometrics.
Because of the random nature of $\UU$, we can define a family of real random variables $\{J(\kk,\UU) \mid \kk \in \Kspace\}$, indexed by $\kk \in \Kspace$. \cite{beyer_robust_2007} proposes an \emph{aggregation approach}, using integral criteria. This integration is usually done with respect to $\UU$.

Moreover, by considering to integrate the powers of the cost function, we can account for dispersion through the centered moments of the random variable $J(\kk, \UU)$, as we are going to see next.
\subsubsection{Expected loss minimization, central tendency}
\label{sec:exp_loss_minimization}

Quite intuitively, as we want to minimize some kind of typical value of a realization of these random variables, we can optimize a central tendency of those random variables. The mean value being an obvious candidate, we define the expected loss as
\begin{equation}
  \label{eq:mean_loss}
  \mu(\kk) = \Ex_\UU\left[J(\kk,\UU)\right] =\int_{\Uspace} J(\kk,\uu)p_\UU(\uu)\,\mathrm{d}\uu
\end{equation}
The expected loss $\mu(\kk)$ is sometimes called the conditional mean given $\kk$. Taking the average of the loss function is very common in many problems of classification and regression~\cite{bishop_pattern_2006}.


The conditional mean is minimized, giving $\estimtxt{\kk}{mean}$. Assuming that $J(\kk, \uu) \propto - \log \mathcal{L}(\kk,\uu;y)$, we have
\begin{align}
  \estimtxt{\kk}{mean} = \argmin_{\kk\in\Kspace} \mu(\theta)&= \argmin_{\kk\in\Kspace}\int_\Uspace J(\kk,\uu) p_{\UU}(\uu)\,\mathrm{d}\uu \\
                                                            &= \argmin_{\kk\in\Kspace} - \int_\Uspace \log \mathcal{L}(\kk,\uu;y) p_{\UU}(\uu)\,\mathrm{d}\uu \\
                                                            &= \argmin_{\kk\in\Kspace} - \int_\Uspace\log\left(p_{Y \mid \KK,\UU}(y \mid \kk,\uu)\right)p_{\UU}(\uu)\,\mathrm{d}\uu 
\end{align}

Taking the average of a loss function is the basis of \emph{stochastic programming}.
However, the integral \cref{eq:mean_loss} is intractable analytically, so instead of computing it exactly, one usually resort to minimizing the empirical mean risk. For $1\leq i \leq n_U$, let $\uu_i$ be i.i.d.\ samples from $\UU$. We can then use those samples to approximate $\mu$: the empirical mean is
\begin{equation}
  \label{eq:emp_mean_loss}
  \mu^{\mathrm{emp}}(\kk) = \frac{1}{n_U}\sum_{i=1}^{n_U} J(\kk, \uu_i)
\end{equation}
and the minimization problem
\begin{equation}
  \min_{\kk\in\Kspace} \frac{1}{n_U}\sum_{i=1}^{n_U} J(\kk, \uu_i)
\end{equation}
is called the \emph{sample average problem}~\cite{juditsky_stochastic_2009}, or \emph{empirical risk minimization} problem in Machine Learning (see e.g.~\cite{vapnik_principles_1992})

Other indicator of central tendency can be considered for optimization, such as the mode or the median of the cost function. 
This expression shares some similarities with the integrated likelihood introduced \cref{eq:def_int_lik}. However, quite unsurprisingly, $\estimtxt{\kk}{mean}$ and $\estimtxt{\kk}{intMLE}$ are not equal in general, as shown \cref{fig:difference_arithmetic_geometric_mean}.
\begin{figure}[ht]
  \centering
  \input{\imgpath integrated_lik_average_costfunction.pgf}
  \caption{\label{fig:difference_arithmetic_geometric_mean} Difference between the negative logarithm of the integrated likelihood, and the mean loss of $J = -\log \mathcal{L}$ and the subsequent difference in estimators}
\end{figure}



A low expected value is to be taken with caution, as it refers to a behaviour \emph{in the long run}. Indeed, the mean values is equivalent to averaging over all the outcomes, but there can be a compensation effect, where ``good surprises'' balance the ``bad surprises''.
An example is the following problem:
\begin{align}
  J(\kk_1, \UU) &\sim \mathcal{N}(2, 2^2) \\
  J(\kk_2, \UU) & \sim \mathcal{N}(3, 1^2)
\end{align}
It is clear that $\Ex_{\UU}[J(\kk_1, \UU)] < \Ex_{\UU}[J(\kk_2, \UU)]$. However, choosing $\estimtxt{\kk}{} = \kk_2$ leads to less extreme values:
\begin{align}
  \Prob_{U}[J(\kk_1, \UU) > 5] = 0.06681> \Prob_{U}[J(\kk_2, \UU) > 5] = 0.02275
\end{align}
Depending on the application, such a behaviour could be prohibitive.
This difference in probability is explained by the difference in the variance of the random variable $J(\kk, \UU)$.
Accounting for the variance is discussed in~\cref{sec:multiobjective_optimization}.

\subsubsection{Variance, dispersion, Multiobjective optimization}
\label{sec:multiobjective_optimization}
In \cref{sec:exp_loss_minimization}, we used the mean as a measure of the central tendency that we want to minimize. Jointly with the central tendency, information about the dispersion of the random variable may also be relevant, in order to predict how much deviation should be expected around the mean.
Let us define the variance of the cost function:
\begin{equation}
  \sigma^2(\kk) = \Var\left[J(\kk,\UU)\right]
\end{equation}
and minimizing this variance yields
\begin{equation}
  \estimtxt{\kk}{var} = \min_{\kk} \sigma^2(\kk)
\end{equation}
As the exact variance computation require the evaluation of an expensive integral, the problem is tackled using sample averaging, and the minimization problem becomes
\begin{equation}
  \min_{\kk\in \Kspace} \frac{1}{n_{\UU}-1} \sum_{i=1}^{n_{\UU}} \left(J(\kk, \uu_i) - \mu^{\mathrm{emp}}(\kk)\right)^2
\end{equation}

\Cref{fig:mean_std} shows the conditional mean and conditional standard deviation for a the cost function $J$ defined~\cref{eq:decision_under_unc}.
\begin{figure}[ht]
  \centering
  \input{\imgpath mean_std_wc.pgf}
  \caption{Illustration of conditional mean and conditional standard deviation, as a function of $\kk$. Those quantities have been rescaled to share the same range on the bottom plot.}
  \label{fig:mean_std} 
\end{figure}

Minimizing the variance alone is often irrelevant, as it could just point toward really high values of the cost function, but steady with respect to $\kk$. Taking both objectives: low mean value and low variance together to the following multiobjective optimization problem:
\begin{align}
  \label{eq:multiobj_e_var}
  \min_{\kk\in\Kspace} \left[\mu(\kk),\sigma(\kk)\right]^T
\end{align}
The multiobjective problem can be tackled in different ways: the literature is rich in methods to approach or even find the Pareto frontier. To compare $\kk_1$ and $\kk_2$, we can compare component wise the objective vectors $[\mu(\kk_i),\sigma(\kk_i)]$ for $i=1,2$. If $\mu(\kk_1) \leq \mu(\kk_2)$ and $\sigma(\kk_1) \leq \sigma(\kk_2)$, $\kk_2$ is said to be \emph{dominated} by $\kk_1$. The Pareto frontier is defined as the set of points in $\Kspace$ that cannot be dominated by any other points.
On~\cref{fig:pareto} is illustrated the Pareto frontier for a multiobjective problem \cref{eq:multiobj_e_var}. The red point corresponding to $\kk_1$ is dominated by the green point $\kk_0$ on the frontier, but not by the green point of $\kk_2$.

\begin{figure}[ht]
  \label{fig:pareto} 
  \centering
  \input{\imgpath pareto_frontier.pgf}
  \caption{Illustration of the Pareto frontier for the multiobjective problem of~\cref{eq:multiobj_e_var}. The shaded regions corresponds to the domain dominated by each points}
\end{figure}

Instead of trying to find the Pareto frontier, the multiobjective problem is often ``scalarized'' by adding the weighted objectives \cite{marler_weighted_2010}, provided that such an operation makes sense in regards to the units of the quantities, justifying the use of the standard deviation instead of the variance.
\begin{equation}
  \min_{\kk\in\Kspace} \lambda \mu(\kk) + (1- \lambda)\sigma(\kk) =   \min_{\kk\in\Kspace} \lambda \Ex_{\UU}[J(\kk,\UU)] + (1- \lambda)\sqrt{\Var[J(\kk,\UU)]}
\end{equation}
where $\lambda \in [0,1]$ is chosen to reflect the preference toward one or another objective.

\subsubsection{Higher moments in optimization}

\paragraph{Skewness}
The skewness coefficient measures the asymmetry in the distribution, and is the (normalized) centered moment of order $3$:
\begin{equation}
  \mathrm{sk}\left[X\right] = \Ex\left[\left(\frac{X - \mu}{\sigma}\right)^3\right]
\end{equation}
where $\mu = \Ex[X]$ and $\sigma = \sqrt{\Var[X]}$.

Adding the skewness in the optimization translates to a preference toward a risk-averse or a risk-seeking approach. Indeed, as the main goal is the optimization of a cost function, deviations of the value of the random variable toward lower values is more desirable than deviations toward larger values.

This is illustrated \cref{fig:skewness_example}: all three of the random variables whose pdf and cdf are displayed have the same mean and variance.
If the skewness coefficient is negative, the distribution presents a heavier left tail than right. In other words, a sample taken from this distribution has a higher probability of being a ``good surprise''. On the other hand, if a big deviation occurs for a sample from a right-skewed distribution, it is more probable to be a large deviation toward large values of the sample space, hence the term ``bad surprise''.

\begin{figure}[ht]
  \centering
  \input{\imgpath skewness_examples.pgf}
  \caption{\label{fig:skewness_example} Pdf and cdf of random variables with same mean, variance but different skewness}
\end{figure}
\aciter{Horsetail matching?}


\begin{table}[ht]
  \centering
  \begin{tabular}{ll}
    \toprule
    Objective name & Objective to minimize wrt $\kk$  \\ \midrule
    Profile Likelihood & $-\log \max_{\uu \in \Uspace} p_{Y \mid \KK, \UU}(y \mid \kk, \uu)$  \\
    Integrated Likelihood & $-\log \int_{\Uspace} p_{Y \mid \KK, \UU}(y \mid \kk, \uu) \,\mathrm{d}\uu=-\log p_{Y\mid \KK}(y\mid \kk)$ \\
    Marginal maximum a posteriori & $-\log p_{\KK\mid Y}(\kk \mid y)$  \\ \midrule
    Global Optimum & $\min_{\uu\in\Uspace} J(\kk, \uu)$ \\
    Worst-case & $\max_{\uu \in \Uspace} J(\kk, \uu)$ \\
    Regret worst-case & $\max_{\uu\in\Uspace}\left\{J(\kk, \uu) - \min_{\kk^{\prime}\in\Kspace} J(\kk^{\prime}, \uu)\right\}$ \\ \midrule
    Mean & $\Ex_{\UU}[J(\kk, \UU)]$  \\
    Mean and variance & $ \lambda \Ex_{\UU}[J(\kk, \UU)] + (1-\lambda) \sqrt{\Var_{\UU}[J(\kk, \UU)]}$  \\ \bottomrule
  \end{tabular}
  \caption{Summary of single objective robust estimators}
\end{table}

\section{Regret-based estimators family}
\label{sec:rr_family}
All the methods introduced above required first to eliminate in some sense the dependency on the environmental parameter, in order to transform the random variable $J(\kk, \UU)$ into an objective that depends solely on $\kk$, and to optimize this deterministic counterpart.

For a given $\kk\in\Kspace$, this elimination is done by aggregating all the possible outcomes $J(\kk, \uu)$ where $\uu$ is a sample from $\UU$.


We propose now to reverse these steps, by first optimizing with respect to $\kk$ the cost function, and from the set of minimizer that depend on $\uu$ obtained, derive an estimator. The rationale behind this idea is that every situation induced by the realisation $\uu$ is to be taken separately, quite similarly as Savage's regret introduced \cref{ssec:regret_savage}. In turn, this avoid aggregation (and in a sense compensation) between the different $\uu$.
\begin{figure}[ht]
  \centering
  \input{\imgpath reversing.pgf}
  \caption{\label{fig:reversing_steps} Principle of regret based estimators}
\end{figure}



\subsection{Conditional minimimum and minimizer}
\label{sec:MPE}
\begin{definition}[Conditional minimum, minimizer]
  Let $J: \Kspace \times \Uspace$ be a cost function, and let us assume that for each $\uu \in \Uspace$, $\min_{\kk \in \Kspace} J(\kk,\uu)$ exists and is attained at a unique point.
  We denote
  \begin{equation}
    J^*(\uu) = \min_{\kk \in \Kspace} J(\kk,\uu)
  \end{equation}
  the \emph{conditional minimum} of $J$ given $u$, and
  \begin{equation}
    \kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk, \uu)
  \end{equation}
 is defined as the \emph{conditional minimizer}
\end{definition}
As $\uu$ is thought to be a realization of a random variable $\UU$, we can consider the two random variables $\kk^*(\UU)$ and $J^*(\UU)$.
The conditional minimum $J^*(\UU)$ is then a random variable describing the best performances for the calibration, if we could optimize the cost function for each realization of $\UU$. % As we look for a single value $\hat{\kk}$, $J^*(\UU)$ can be seen as an unattainable target:
% \begin{equation}
%   \forall (\kk,\uu) \in \Kspace \times \Uspace, \quad J^*(\uu) \leq J(\kk, \uu)
% \end{equation}


If $J$ behaves nicely enough, the mapping
\begin{equation}
  \label{eq:def_kstar}
  \begin{array}{rcl}
    \kk^*: \Uspace &\longrightarrow& \Kspace \\
    \uu &\longmapsto& \kk^*(\uu) = \argmin_{\kk \in \Kspace} J(\kk, \uu)
  \end{array}
\end{equation}

is well defined for all $\uu \in \Uspace$, and we assume that it is measurable. We can study the image random variable through this mapping, that we will denote $\KK^* = \kk^*(\UU)$.
We assume that $\UU$ is a continuous random variable, with a compact support.

Let us define $\Kspace = \Uspace= [0, 1]$, and $\UU \sim \mathrm{Unif}(\Uspace)$, and the following cost functions:
\begin{align}
  J_1(\kk, \uu) &= (1+\uu)+(\kk - 0.5)^2\\
  J_2(\kk, \uu) &= (\kk-\uu)^2 + 1
\end{align}
We have
\begin{align}
  \kk^*_1(\uu) &= \argmin_{\kk \in \Kspace} J_1(\kk, \uu) = 0.5 \\
  \kk^*_2(\uu) &= \argmin_{\kk\in\Kspace} J_2(\kk, \uu) = \uu
\end{align}

In the first case, $\kk_0 = \argmin_{\kk} J(\kk, \UU)$, so $\kk_1^*(\UU)$ is a degenerate random variable equals to  $\kk_0$. In other words, the minimizer is not dependent on the value taken by the environmental parameter. The minimum value $J^*$ might be dependent though. On the other hand, for $J_2$, as $\Kspace=\Uspace$ and $\UU \sim \mathrm{Unif}(\Uspace)$, $\KK_2^*(\UU)$ is uniformly distributed on $\Kspace$, no value shows a better affinity of being a minimizer than the other.


In the more general case, let us consider the random variable $\kk^*(\UU)$. In general, this random variable cannot be classified as continuous or discrete beforehand. However, in the following, we are going to assume that it is a \emph{continuous random variable}, and thus admits a pdf $p_{\kk^*}$ \citep{hennig_entropy_2011}:
\begin{equation}
  p_{\KK^*}(\kk) = \int_{\Uspace} p_{\UU}(\uu)\prod_{\substack{\tilde{\kk}\in \Kspace\\\tilde{\kk}\neq \kk}} \mathbbm{1}_{\{J(\tilde{\kk},\uu) > J(\kk, \uu)\}}\,\mathrm{d}\uu
\end{equation}
Except for simple analytical problems, this pdf cannot be obtained analytically, and needs to be estimated.
Let $\{\uu_i\}_{1\leq i \leq n_{\mathrm{samples}}}$ be $n_{\mathrm{samples}}$ i.i.d.\ samples of $\UU$, and
$\{\kk^*(\uu_i)\}_{1\leq i \leq n_{\mathrm{samples}}}$ the corresponding minimizers, as defined \cref{eq:def_kstar}.
The isotropic \emph{Kernel density estimation}, or KDE is defined as
\begin{equation}
  \hat{p}_{\KK^*}(\kk^*) = \frac{1}{n_{\mathrm{samples}} h^{\dim \Kspace}} \sum_{i=1}^{n_{\mathrm{samples}}} \mathcal{K}\left(\frac{\kk^* - \kk^*(\uu_i)}{h}\right)
\end{equation}
where $h>0$ is the bandwidth (that measure the influence of each sample), and $\mathcal{K}$ is a kernel of dimension $\dim \Kspace$, usually defined as the product of one-dimensional kernels $\mathcal{K}_1$. Several choices of 1D kernels are available, and one of the most common one is the Gaussian Kernel: $\mathcal{K}_1(\kk) = (2\pi)^{-1/2}\exp(-\kk^2 /2)$

More generally, the entropy of the random variable $\kk^*(\UU)$ can be seen as a measure of the sensitivity of the calibration when the environmental variable varies.
Assuming that the distribution of the minimizers is continuous, this entropy can be estimated by various methods (see for instance~\cite{beirlant_nonparametric_1997}).

Finally, when we have an estimation of the density of $\KK^*$, and if it exists, we can compute its mode, that we are going to call the \emph{Most Probable estimator}:
\begin{equation}
  \estimtxt{\kk}{MPE} = \argmax_{\kk \in \Kspace} p_{\KK^*}(\kk)
\end{equation}

Choosing $\estimtxt{\kk}{MPE}$ means to select the value that is ``most often'' the minimiser of the cost function. However, we have no indications on its performances when it is \emph{not} optimal. In \cref{sec:regret}, we are going to introduce the notions of regret (additive and relative), in order to try to be ``near optimal'' with high probability.


\subsection{Regret and model selection}
\label{sec:regret}
In this section, we will first focus on cost functions defined as the negative log-likelihood in order to link the additive regret and the Likelihood ratio test.
\subsubsection{Cost as the negative log-likelihood}

Let us define the cost function as the negative log-likelihood:
\begin{equation}
  J(\kk, \uu) = - \log p_{Y\mid \KK, \UU}(y \mid \kk, \uu) = -\log \mathcal{L}(\kk,\uu)
\end{equation}

As established in the first chapter, for a fixed $\uu_0\in\Uspace$, the model $\left\{\left(\mathcal{M}(\cdot, \uu_0), \Theta \right)\right\}$ is misspecified.
  The notion of Wald's regret introduced earlier is directly linked to the likelihood ratio test:
  \begin{align}
    r(\kk,\uu_0) &= J(\kk, \uu_0) - \min_{\kk^{\prime}\in\Kspace}J(\kk^{\prime}, \uu_0) = J(\kk,\uu_0) - J^*(\uu_0)  \\
                 &= \max_{\kk^{\prime}\in\Kspace} \log \mathcal{L}(\kk^{\prime}, \uu_0) - \log \mathcal{L}(\kk, \uu_0) \\
                 &= -\log \frac{\mathcal{L}(\kk, \uu_0)}{\max_{\kk^{\prime}\in\Kspace}\mathcal{L}(\kk^{\prime}, \uu_0)} = -\log \frac{\max_{\kk^{\prime}\in\{\kk\}}\mathcal{L}(\kk^{\prime}, \uu_0)}{\max_{\kk^{\prime}\in\Kspace}\mathcal{L}(\kk^{\prime}, \uu_0)}
  \end{align}

  Recalling the misspecified likelihood ratio test, the regret $r$ is to be compared with a quantile of the r.v.\ $X(\uu_0)$ defined as
  \begin{equation}
    \label{eq:sumchi2}
X(\uu_0)=\sum_{i=1}^{\dim \Kspace} c_i(\uu_0) \Xi_i \quad \text{ with } \Xi_i \sim \chi^2_1 \text{ i.i.d.}
\end{equation}
 where $\{c_i\}_{1\leq i \leq \dim \Kspace}$ are coefficients linked to the eigenvalues of the Fisher information matrix as evoked in~\label{sec:model_misspecification}.

  
 However, by making the assumption that for all $1 \leq i \leq \dim \Kspace$, $c_i(\uu_0)=1$, the sum of \cref{eq:sumchi2} can be simplified, and $X(\uu) = X \sim \chi^2_{\dim \Kspace}$. This assumption leads us to the likelihood ratio test for well-specified models:

 For $\kk\in \Kspace$ we can test for the following hypotheses:
 \begin{itemize}
 \item $\mathcal{H}_0$: the model $\left(\mathcal{M}(\cdot, \uu_0), \{\kk\}\right)$ is statistically equivalent to $\left\{\mathcal{M}(\cdot, \uu_0), \Kspace\right\}$
 \item $\mathcal{H}_1$: the models are statistically different
 \end{itemize}
 $\mathcal{H}_0$ is rejected at a level $\nu$ if
  \begin{align}
  r(\kk, \uu_0)=J(\kk, \uu_0) - J^*(\uu_0) > \frac{1}{2}\chi^2_{\dim \Kspace}(1-\nu)
  \end{align}
  Under this condition, we can construct a likelihood interval (as defined~\cref{eq:def_lik_interval}): that depends on $\uu_0$:
  \begin{equation}
    \label{eq:lik_interval_add}
    \mathcal{I}_{\mathrm{Lik}}(\uu_0; \beta) = \left\{\kk\in \Kspace \mid  J(\kk, \uu_0) - J^*(\uu_0) \leq \beta=\frac{1}{2}\chi^2_{\dim \Kspace}(1-\nu)\right\}
  \end{equation}
   In the more general case, we can choose $\beta>0$ in a more arbitrary way, in order to avoid the computations of the coefficients $\{c_i(\uu)\}_i$, as we are going to see~\cref{ssec:general_cost_prob}.
     \subsubsection{General cost function: interval and probability of acceptability}
     \label{ssec:general_cost_prob}
 We assumed before that $J$ was the negative log-likelihood. In the more general case, $J$ represents a loss function, that we want to minimize. The generalization of \cref{eq:lik_interval_add} is what we are calling the \emph{acceptibility interval}.
  \begin{definition}[Acceptable interval]
  By analogy with the likelihood interval defined~\cref{eq:def_lik_interval} and \cref{eq:lik_interval_add}, we can construct a set for an arbitrary threshold $\beta \geq 0$ such that
  \begin{equation}
    \mathcal{I}_{\beta}(\uu) = \left\{\kk \in \KK \mid J(\kk,\uu) \leq J^*(\uu) + \beta \right\}
  \end{equation}
As $J$ may not stem from a likelihood, we call $\mathcal{I}_{\beta}(\uu)$ the \emph{acceptable interval}. In other word, we say that $\kk\in \Kspace$ is $\beta$-acceptable for $\UU=\uu$ if $\kk \in \mathcal{I}_{\beta}(\uu)$
\end{definition}

\Cref{fig:lik_interval_threshold} shows a cost function evaluated for different fixed $u_i$. The $\beta$-acceptable intervals for those environmental variables are plotted below the curves.
\begin{figure}[ht]
  \centering
  \input{\imgpath lik_interval_threshold.pgf}
  \caption{\label{fig:lik_interval_threshold} Different acceptable regions corresponding to different $\uu \in \Uspace$}
\end{figure}

  Now, for a given $\kk$, we can define the set of $\uu\in \Uspace$ such that $\kk \in  \mathcal{I}_{\beta}(\uu)$, i.e.\
  \begin{align}
    R_{\beta}(\kk) &= \left\{\uu\in \Uspace \mid \kk \in \mathcal{I}_{\beta}(\uu) \right\} \\
           &= \left\{\uu\in \Uspace \mid J(\kk, \uu)  \leq J^*(\uu) + \beta \right\}
  \end{align}

  This set can be measured with respect to the distribution of $\UU$:
  \begin{equation}
    \label{eq:def_gamma_additive}
    \Gamma_\beta(\kk) = \Prob_U\left[\UU \in R_{\beta}(\kk)\right]
  \end{equation}
  Loosely speaking, for a given $\kk$, $\Gamma_{\beta}(\kk)$ is the probability that the model $(\mathcal{M}(\cdot, \UU), \{\kk\})$ is ``statistically equivalent'' to the ``full model'' $\left(\mathcal{M}(\cdot, \UU), \Kspace)\right)$, at a certain level linked to the value of $\beta$.

  \begin{definition}[Additive-regret family of estimators]
    For $\beta \geq 0$, we define the family of robust estimators as the maximizers of \cref{eq:def_gamma_additive}:
    \begin{equation}
      \left\{\estimtxt{\kk}{add,\beta} = \max_{\kk\in\Kspace} \Gamma_{\beta}= \Prob_U\left[\UU \in R_{\beta}(\kk)\right]\mid \beta \geq 0\right\}
    \end{equation}
  \end{definition}

  In other words, we can find the value $\kk\in\Kspace$ that gives a model $(\mathcal{M}(\cdot, \UU), \{\estimtxt{\kk}{\beta}\})$ that is statistically equivalent with the highest probability.


  
  \subsection{Relative regret}
\subsubsection{Shortcomings of the additive regret}
We examined before regret that can be qualified as \emph{additive} as this is the difference between $J$ and $J^*$ that is compared to fixed thresholds.
However, we can argue that the relative magnitude of the cost function has an importance in the comparison. For illustration purposes, let us consider the situation described \cref{tab:hyp_situation}.

\begin{table}[ht]
  \centering
  \begin{tabular}{rll|l}
    \toprule
    $J$     & $\uu_1$ & $\uu_2$ & $\Ex[J(\cdot, \UU)]$  \\ \midrule
    $\kk_1$ & 10000 & 110 & 5055 \\
    $\kk_2$ & 10100 & 10 & 5055 \\ \bottomrule
  \end{tabular}
  \quad
  \begin{tabular}{rll}
    \toprule
    $J-J^*$     & $\uu_1$ & $\uu_2$  \\ \midrule
    $\kk_1$ & 0 & 100 \\
    $\kk_2$ & 100 & 0  \\ \bottomrule
  \end{tabular}
    \quad
  \begin{tabular}{rll}
    \toprule
    $\frac{J-J^*}{J^*}$     & $\uu_1$ & $\uu_2$  \\ \midrule
    $\kk_1$ & 0 & 10 \\
    $\kk_2$ & 0.01 & 0  \\ \bottomrule
  \end{tabular}  
  \caption{\label{tab:hyp_situation} Illustration of a cost function, expected loss, additive regret and relative error}
\end{table}
In this situation, for both $\uu_1$ and $\uu_2$, the maximal additive regret is $\max_{\kk} J(\kk, \uu) - J^*(\uu) = 100$, so no clear preference could be infered toward one or another value. % The situation $\UU=\uu_1$ seems less favorable than $\UU=\uu_2$, as the values of the cost function $J(\cdot, \uu_1)$ are larger than for all $\kk$.
However, choosing $\kk_1$ over $\kk_2$ means to choose to improve the performance of an already pretty bad situation ($10000$ instead of $10100$), while increasing tenfold the loss for the situation $\UU=\uu_2$. 


From the example developed~\cref{tab:hyp_situation}, considering the additive regret may be ill-advised, as the difference in magnitude of the cost function between $\UU = \uu_1$ and $\UU = \uu_2$ is probably due to the effect of a misspecification. To apply the likelihood ratio test, one should choose the quantiles of two different random variables: $X(\uu_1)$ and $ X(\uu_2)$.

In that sense, we are going now to focus on the \emph{relative regret} $J/J^*$, instead of the \emph{absolute regret} $J - J^*$.

\subsubsection{$\alpha$-acceptability}
Again, instead of 
  \begin{definition}[$\alpha$-acceptable interval]
  For an arbitrary threshold $\alpha \geq 1$, we define the $\alpha$-acceptable interval as
  \begin{equation}
    \mathcal{I}_{\alpha}(\uu) = \left\{\kk \in \KK \mid J(\kk,\uu) \leq \alpha J^*(\uu) \right\}
  \end{equation}
\end{definition}

Then, for a given $\alpha$ and $\kk$, we can define the set of $\uu\in \Uspace$ such that $\kk$ is $\alpha$-acceptable:
\begin{equation}
  R_{\alpha}(\kk) = \left\{\uu \in \Uspace \mid \kk \in \mathcal{I}_{\alpha}(\uu) \right\} = \left\{\uu \in \Uspace \mid  J(\kk, \uu) \leq \alpha J^*(\uu) \right\}
\end{equation}

We define the family of relative regret estimators as
\begin{equation}
      \left\{\estimtxt{\kk}{rel,\alpha} = \max_{\kk\in\Kspace} \Gamma_{\alpha}= \Prob_U\left[\UU \in R_{\alpha}(\kk)\right]\mid \alpha \geq 1\right\}
    \end{equation}


We can also notice that setting $\alpha=1$ gives
\begin{align}
  \mathcal{I}_{\alpha}(\uu) &= \{\kk^*(\uu)\} \\
  R_1(\kk) &= \left\{\uu\in\Uspace \mid J(\kk, \uu) = J^*(\uu)\right\} = \left\{\uu\in\Uspace \mid \kk = \kk^*(\uu)\right\}
\end{align}

If $\kk^*(\UU)$ is a continuous random variable, for all $\kk\in\Kspace$, $\Prob_{\UU}[\UU \in R_1(\kk)] = \Prob_{\UU}[\kk^*(\UU) = \kk] = 0$.


\subsection{The choice of thresholds}
\label{sec:choice_threshold}
  
Finding a relevant threshold, either for the additive or for the relative regret,  can be a thorny issue, especially with no further information than $J$. Setting it too large will lead to large $\beta$-acceptable (resp. $\alpha$-acceptable) intervals, and $\Gamma_\beta$ (resp. $\Gamma_\alpha$) may reach $1$ for several different values. On the other hand, choosing a threshold too small may give a maximal probability too low.


In order to have an insight on the potential robustness of an estimator $\estimtxt{\kk}{}$, both values can be considered together: the threshold and the maximal probability reached.




% \subsection{Physical analogy, energy, temperature and Gibbs distribution}
% Simulated annealing analogy
% We can also interpret the cost function as an energy. We introduce an analog to the temperature $T = \frac{1}{\beta}$.

% The probability of the ``system'' being in the state corresponding to $\kk$ and $\UU=\uu$ is
% \begin{equation}
% \pi(\kk, \uu)=\frac{1}{Z(T)}\exp\left(-\frac{1}{T}J(\kk,\uu)\right)
% \end{equation}
% As the temperature grows, $\pi$ flattens.

% Let us consider $u$ fixed:
% the lower state of energy that the system can attain is $J^*(\uu)$, reached with probability
% $\pi(\kk^*(\uu), \uu) = \frac{1}{Z(\beta)}\exp(-\beta J^*(\uu))$ at t
% \begin{align}
%   \text{probability of being optimal}\frac{1}{Z(\kk, \uu, 1)}\exp(-J(\kk,\uu)) \\
%   \frac{1}{Z(\kk^*(\uu), \uu, \alpha)}\exp(-\alpha J^*(\uu))
% \end{align}
% Where $Z$ is a function of $\kk,\uu$ that represents the integration constant.
% By interpreting $\alpha$ as the reciprocical of a temperature, 


\section{Partial Conclusion}
\label{sec:ch3_partial_ccl}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
      }
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
