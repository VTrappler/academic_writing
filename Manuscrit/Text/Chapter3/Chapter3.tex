\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter3/#1}
}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter2/build/Chapter2}
\externaldocument{../../Text/Chapter4/build/Chapter4}
\externaldocument{../../Text/Chapter5/build/Chapter5}
\externaldocument{../../Text/Conclusion/build/Conclusion}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \dominitoc
% \faketableofcontents
% \subfileLocal{\setcounter{chapter}{1}}
\chapter{Robust estimators in the presence of uncertainties} 
\label{chap:robust_estimators}
\minitoc
\newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Defining robustness}
\subsection{Robustness or reliability ?}
The notion of \emph{Robustness} is dependent on the field in which it is used. Worse, in the same community, robustness may carry a lot of different meanings. Robust is often used to describe something that behaves still nicely under uncertainties, or to put it in an other way, that is insensitive up to certain extent to some perturbations.

For instance, Bayesian approaches are sometimes criticized for their use of subjective probabilities that represent the state of beliefs, especially on the choice of prior distributions. In that sense, robust Bayesian analysis aims at quantifying the sensitivity of the choice of the prior distribution on the resulting inference and relative Bayesian quantities derived. In the statistical community, robustness is often implied as the non-sensitivity on the outliers in the sample set.

Moreover, robustness is often linked and sometimes confused to the semantically close notion of \emph{reliability}. In~\cite{lelievre_consideration_2016} we can find summarized in~\cref{tab:lelievre} the difference between these notions,  by defining optimality as the deterministic counterpart of robustness, and admissibility as the counterpart of reliability.

\begin{table}[htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clllll@{}}
\multicolumn{1}{l}{}         & \multicolumn{5}{c}{Robustness}                                                                                                \\ \cmidrule(l){2-6} 
\multirow{5}{*}{\rotatebox{90}{Reliability}} & \multicolumn{2}{l}{\multirow{2}{*}{}}        & \multirow{2}{*}{no objective} & \multicolumn{2}{c}{objective}                  \\ \cmidrule(l){5-6} 
                             & \multicolumn{2}{l}{}                         &                               & \multicolumn{1}{c}{deterministic inputs}  & uncertain inputs      \\ \cmidrule(l){2-6} 
                             & \multicolumn{2}{c}{Unconstrained}            & No problem                    & Optimal                & Robust                \\ % \cmidrule(l){2-6} 
                             & \multirow{2}{*}{Constrained} & \multicolumn{1}{r}{deterministic constraints} & Admissible                    & Optimal and admissible & Robust and admissible \\
                             &                              & \multicolumn{1}{r}{uncertain constraints}     & Reliable                      & Optimal and reliable   & Robust and reliable   \\ \cmidrule(l){2-6} 
\end{tabular}%
}
\caption{Types of problems, depending on their deterministic nature for the constraints or the objective. Reproduced from~\cite{lelievre_consideration_2016}}
\label{tab:lelievre}
\end{table}
\subsection{Classifying the uncertainties}
In the Bayesian formulation of the problem, the uncertainty on the calibration parameter is modelled through the prior distribution, while the uncertain parameter, $u$ has its own distribution. While mathematically similar, those two representations actually encompasses a significant difference: we are actively trying to reduce the uncertainty of the calibration parameter by Bayesian update, while the uncertainty on the environmental parameter is seen as a nuisance.

In that context, the very notion of uncertainty can be roughly split in two, as described in~\cite{walker_defining_2003}:
\begin{itemize}
\item Aleatoric uncertainties, coming from the inherent variability of a phenomenon, \emph{e.g.} intrinsic randomness of some environmental variables
\item Epistemic uncertainties coming from a lack of knowledge about the properties and conditions of the phenomenon underlying the behaviour of the system under study
\end{itemize}
According to this division,  the epistemic uncertainty can be reduced by investigating the effect of the calibration parameter $\kk$ upon the physical system, and choose it accordingly to the objective function defined in~\cref{eq:def_J}.
The uncertain variable on the other hand is uncertain in the aleatoric sense, and cannot be controlled directly, as its value is doomed to change. This distinction is not as sharp as it may seem, as~\cite{kiureghian_aleatory_2009} conclude that the choice of the distinction is up to the modeller.



\subsection{Robustness under parameteric misspecification}
In this thesis we are interested in a slightly different notion of robustness, that we can qualify as \emph{robustness under parametric model misspecification}:

As established before, we have an objective function that takes two distinct inputs:
\begin{equation}
  \label{eq:def_J}
  \kk, \uu \longmapsto J(\kk,\uu)
\end{equation}
where $\kk\in \Kspace$ is the calibration parameter, and $\uu \in \Uspace$ is the uncertain parameter. This uncertain parameter is modelled as a realisation of a random variable $\UU$.
Not taking into account this uncertainty may be an issue in the modelling, especially if the influence of this variable is non-negligible.
Choosing a specific $\uu \in \Uspace$ leads to \emph{localized optimization} \citep{huyse_free-form_2001} and \emph{overcalibration}, that is an value that is optimal for a given situation, but usually does not carry the optimality to other situations, or in layman's term according to~\cite{andreassian_all_2012}: being lured by fool's gold.
In geophysics and especially in hydrological models, this overcalibration may lead to the appearance of abberations in predictions, as those uncertainties become a prevalent source of errors and feed ``Hydrological monsters'' as~\cite{kuczera_there_2010} puts it.



\subsection{Joint posterior distribution}
The likelihood of the data given $\kk$ and $\uu$ is
\begin{equation}
  \mathcal{L}(\kk,\uu;y) = p_{Y \mid \KK, \UU}(y\mid \kk,\uu)
\end{equation}
The ``full'' posterior distribution can be written as:
\begin{equation}
  p_{\KK,\UU \mid Y}(\kk,\uu \mid y) = \mathcal{L}(\kk,\uu;y)p_{\KK}(\kk)p_{\UU}(\uu)
\end{equation}
Here, the posterior is used to do inference on $\kk$ and $\uu$ jointly, so in order to suppress the dependenc√®y in $u$, we can marginalize directly, and get the marginalized posterior:
\begin{align}
  p_{\KK \mid Y}(\kk \mid y) &= \int_{\Uspace} p_{\KK,\UU\mid Y}(\kk,\uu\mid y)\,\mathrm{d}u\\
                             &=\int_{\Uspace} \mathcal{L}(\kk,\uu;y)p_{\KK}(\kk)p_{\UU}(\uu)\,\mathrm{d}u \\
                             &=\int_{\Uspace}p_{\KK\mid Y,\UU}(\kk,\mid y,\uu)p_{\UU}(\uu)\,\mathrm{d}u \\
                             &= \Ex_{\UU}\left[p_{\KK\mid Y,\UU}(\kk,\mid y,\uu)\right]
\end{align}
We can then define the marginalized maximum as posteriori (MMAP)~\cite{doucet_marginal_2002} as the following maximizer:
\begin{align}
  \label{eq:def_MMAP}
  \estimtxt{\kk}{MMAP} &= \argmax_{\kk\in\Kspace} p_{\KK \mid Y}(\kk\mid y) \\
                        &= \argmin_{\kk\in\Kspace} -\log \left(\int_{\Uspace} p_{\KK\mid Y,\UU}(\kk\mid y, u) p_{\UU}(\uu)\,\mathrm{d}\uu \right)
\end{align}

\section{Robustness based on the moments of an objective funciton}
\label{sec:rob_moments}
Let us define $J:\Kspace \times \Uspace\rightarrow \mathbb{R}^+$ as an objective function, and assume that for all $\kk \in \Kspace$, $J(\kk, \cdot)$ is a measurable function. 
Because of the random nature of $\UU$, we in fact can define a family of real random variables, indexed by $\kk in \Kspace$: $\{J(\kk,\UU) \mid \kk \in \Kspace\}$. Quite intuitively, we want to minimize some kind of central tendency of those random variables.
\begin{equation}
  \mu(\kk) = \Ex_\UU\left[J(\kk,\UU)\right] =\int_{\Uspace} J(\kk,\uu)p_\UU(\uu)\,\mathrm{d}\uu
\end{equation}


by the assumption that $J(\kk,\uu) \propto -\log p_{Y \mid \KK,\UU}(y \mid \kk,\uu)- \log p_{\KK}(\kk)$,

$\mu$ can be seen as a ``natural'' robust objective for the cost function $J$
\begin{align}
  \estimtxt{k}{mean} &= \argmin_{\kk\in\Kspace}-\int_\Uspace\log\left(p_{Y \mid \KK,\UU}(y \mid \kk,\uu)p_{\KK}(\kk)\right)p_{\UU}(\uu)\,\mathrm{d}\uu \\
                     &= \argmin_{\kk\in\Kspace}-\int_\Uspace\log\left(p_{\KK \mid Y,\UU}(\kk \mid y,\uu)\right)p_{\UU}(\uu)\,\mathrm{d}\uu 
\end{align}


\section{Relative-regret estimators family}
\label{sec:rr_family}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
      }
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
