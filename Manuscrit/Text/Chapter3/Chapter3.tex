\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter3/#1}
}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter2/build/Chapter2}
\externaldocument{../../Text/Chapter4/build/Chapter4}
\externaldocument{../../Text/Chapter5/build/Chapter5}
\externaldocument{../../Text/Conclusion/build/Conclusion}
}
\newcommand\imgpath{/home/victor/acadwriting/Manuscrit/Text/Chapter3/img/} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \dominitoc
% \faketableofcontents
% \subfileLocal{\setcounter{chapter}{1}}
\chapter{Robust estimators in the presence of uncertainties} 
\label{chap:robust_estimators}
\minitoc
\newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the previous chapter, we raised the problem of misspecification of the numerical model with respect to the reality. Moreover, this misspecification is random by nature, and we wish that the calibrated model shows relatively good performances when the environmental variables varies.
\begin{figure}[ht]
  \begin{center}
  \resizebox{\linewidth}{!}
  {
      \input{/home/victor/acadwriting/Manuscrit/Text/Chapter3/img/modelling_uncertainties}
    }
    \end{center}
  \caption{\label{fig:label} }
\end{figure}



\section{Defining robustness}
\subsection{Robustness or reliability ?}


The notion of \emph{Robustness} is dependent on the field in which it is used. Worse, in the same community, robustness may carry a lot of different meanings. Robust is often used to describe something that behaves still nicely under uncertainties, or to put it in an other way, that is insensitive up to certain extent to some perturbations.

For instance, Bayesian approaches are sometimes criticized for their use of subjective probabilities that represent the state of beliefs, especially on the choice of prior distributions. In that sense, robust Bayesian analysis aims at quantifying the sensitivity of the choice of the prior distribution on the resulting inference and relative Bayesian quantities derived. In the statistical community, robustness is often implied as the non-sensitivity on the outliers in the sample set.

Moreover, robustness is often linked and sometimes confused to the semantically close notion of \emph{reliability}. In~\cite{lelievre_consideration_2016} we can find summarized in~\cref{tab:lelievre} the difference between these notions,  by defining optimality as the deterministic counterpart of robustness, and admissibility as the counterpart of reliability.

\begin{table}[htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clllll@{}}
\multicolumn{1}{l}{}         & \multicolumn{5}{c}{Robustness}                                                                                                \\ \cmidrule(l){2-6} 
\multirow{5}{*}{\rotatebox{90}{Reliability}} & \multicolumn{2}{l}{\multirow{2}{*}{}}        & \multirow{2}{*}{no objective} & \multicolumn{2}{c}{objective}                  \\ \cmidrule(l){5-6} 
                             & \multicolumn{2}{l}{}                         &                               & \multicolumn{1}{c}{deterministic inputs}  & uncertain inputs      \\ \cmidrule(l){2-6} 
                             & \multicolumn{2}{c}{Unconstrained}            & No problem                    & Optimal                & Robust                \\ % \cmidrule(l){2-6} 
                             & \multirow{2}{*}{Constrained} & \multicolumn{1}{r}{deterministic constraints} & Admissible                    & Optimal and admissible & Robust and admissible \\
                             &                              & \multicolumn{1}{r}{uncertain constraints}     & Reliable                      & Optimal and reliable   & Robust and reliable   \\ \cmidrule(l){2-6} 
\end{tabular}%
}
\caption{Types of problems, depending on their deterministic nature for the constraints or the objective. Reproduced from~\cite{lelievre_consideration_2016}}
\label{tab:lelievre}
\end{table}
\subsection{Classifying the uncertainties}
In the Bayesian formulation of the problem, the uncertainty on the calibration parameter is modelled through the prior distribution, while the uncertain parameter, $u$ has its own distribution. While mathematically similar, those two representations actually encompasses a significant difference: we are actively trying to reduce the uncertainty of the calibration parameter by Bayesian update, while the uncertainty on the environmental parameter is seen as a nuisance.

In that context, the very notion of uncertainty can be roughly split in two, as described in~\cite{walker_defining_2003}:
\begin{itemize}
\item Aleatoric uncertainties, coming from the inherent variability of a phenomenon, \emph{e.g.} intrinsic randomness of some environmental variables
\item Epistemic uncertainties coming from a lack of knowledge about the properties and conditions of the phenomenon underlying the behaviour of the system under study
\end{itemize}
According to this division,  the epistemic uncertainty can be reduced by investigating the effect of the calibration parameter $\kk$ upon the physical system, and choose it accordingly to the objective function defined in~\cref{eq:def_J}.
The uncertain variable on the other hand is uncertain in the aleatoric sense, and cannot be controlled directly, as its value is doomed to change. This distinction is not as sharp as it may seem, as~\cite{kiureghian_aleatory_2009} conclude that the choice of the distinction is up to the modeller.



\subsection{Robustness under parameteric misspecification}
In this thesis we are interested in a slightly different notion of robustness, that we can qualify as \emph{robustness under parametric model misspecification}:

As established before, we have an objective function that takes two distinct inputs:
\begin{equation}
  \label{eq:def_J}
  \kk, \uu \longmapsto J(\kk,\uu)
\end{equation}
where $\kk\in \Kspace$ is the calibration parameter, and $\uu \in \Uspace$ is the uncertain parameter. This uncertain parameter is modelled as a realisation of a random variable $\UU$.
Not taking into account this uncertainty may be an issue in the modelling, especially if the influence of this variable is non-negligible.
Choosing a specific $\uu \in \Uspace$ leads to \emph{localized optimization} \citep{huyse_free-form_2001} and \emph{overcalibration}, that is an value that is optimal for a given situation, but usually does not carry the optimality to other situations, or in layman's term according to~\cite{andreassian_all_2012}: being lured by fool's gold.
In geophysics and especially in hydrological models, this overcalibration may lead to the appearance of abberations in predictions, as those uncertainties become a prevalent source of errors and feed ``Hydrological monsters'' as~\cite{kuczera_there_2010} puts it.



\subsection{Nuisance parameters}
\label{sec:nuisance_parameters}
The environmental parameters are sometimes called \emph{nuisance} parameters.

\subsubsection{Profile and integrated Likelihood}
From a frequentist approach, we define the joint likelihood $\mathcal{L}(\theta, u ;y) = p_{Y\mid \KK,\UU}(y \mid \kk, \uu)$. One common way to deal with the ``corrupted'' likelihood (in the sense that it depends on the uncertain variable) is to define the \emph{profile} likelihood:
\begin{equation}
  \label{eq:eq:def_profile_lik}
  \mathcal{L}_{\mathrm{profile}}(\theta;y) = \max_{\uu \in \Uspace} \mathcal{L}(\theta,u;y)
\end{equation}
When doing parameter inference, it is straightforward to verify that
\begin{equation}
  \max_{(\kk ,\uu) \in \Kspace\times \Uspace} \mathcal{L}(\theta,\uu;y) = \max_{\kk\in \Kspace} \mathcal{L}_{\mathrm{profile}}(\theta;y)
\end{equation},

Another alternative is to define the \emph{integrated} likelihood as
\begin{align}
  \mathcal{L}_{\mathrm{integrated}}(\theta;y) &= \int_{\Uspace} \mathcal{L}(\theta,\uu;y) p_{\UU}(\uu) \,\mathrm{d}\uu \\
                                              &=\int_{\Uspace} p_{Y|\theta,\UU}(y \mid \theta,u) p_{\UU}(\uu) \,\mathrm{d}\uu \\
                                              &=\int_{\Uspace} p_{Y,\UU|\theta}(y,u \mid \theta)\,\mathrm{d}\uu \\
\end{align}

In~\cite{berger_integrated_1999}, the authors report on the difference between those two approaches.


\subsection{Joint posterior distribution}
The likelihood of the data given $\kk$ and $\uu$ is
\begin{equation}
  \mathcal{L}(\kk,\uu;y) = p_{Y \mid \KK, \UU}(y\mid \kk,\uu)
\end{equation}
The joint posterior distribution can be written as:
\begin{align}
  p_{\KK,\UU \mid Y}(\kk,\uu \mid y) &= \mathcal{L}(\kk,\uu;y)p_{\KK}(\kk)p_{\UU}(\uu) \frac{1}{p_Y(y)} \\
  &\propto \mathcal{L}(\kk,\uu;y)p_{\KK}(\kk)p_{\UU}(\uu)
\end{align}
Here, the posterior is used to do inference on $\kk$ and $\uu$ jointly, so in order to suppress the dependenc√®y in $u$, we can marginalize and get the marginalized posterior $p_{\theta \mid Y}$:
\begin{align}
  p_{\KK \mid Y}(\kk \mid y) &= \int_{\Uspace} p_{\KK,\UU\mid Y}(\kk,\uu\mid y)\,\mathrm{d}u \label{eq:marg_MMAP}\\
                             &= \int_{\Uspace}p_{\KK \mid Y, \UU,}(\kk \mid y, \uu) p_{\UU\mid Y}(\uu \mid y)\,\mathrm{d}u
\end{align}

We can then define the marginalized maximum as posteriori (MMAP)~\cite{doucet_marginal_2002} as the  maximizer of this marginalized posterior, written by omitting the model evidence:
\begin{equation}
  \label{eq:def_MMAP}
  \estimtxt{\kk}{MMAP} = \argmax_{\kk\in\Kspace} p_{\KK \mid Y}(\kk\mid y)
\end{equation}
or, by taking the negative logarithm to get a minimization problem:
\begin{equation}
\estimtxt{\kk}{MMAP}    = \argmin_{\kk\in\Kspace} -\log \left(\int_{\Uspace} p_{\KK\mid Y,\UU}(\kk\mid y, u) p_{\UU}(\uu)\,\mathrm{d}\uu \right)
\end{equation}
Unfortunately, neither the integration with respect to the nuisance parameter in~\eqref{eq:marg_MMAP} nor the subsequent optimization is analytically easy. \cite{doucet_marginal_2002} proposes a MCMC method in order to estimate iteratively the MMAP, through sampling of the joint posterior

\section{The cost function as a random variable}
\label{sec:J_rv}


\subsection{Saddle-point optimization}
\label{sec:saddle_point}



\subsection{Robustness based on the moments of an objective funciton}

\subsubsection{Expected loss minimization}
\label{sec:exp_loss_minimization}


In the presence of uncertainties, choosing a parameter value $\theta$ can also be seen as making as making a choice under risk. Let $J:\Kspace \times \Uspace\rightarrow \mathbb{R}^+$ be an objective function, and assume that for all $\kk \in \Kspace$, $J(\kk, \cdot)$ is a measurable function. $J$ can be seen as the opposite of the \emph{utility} function, often encountered in game or econometrics.
Because of the random nature of $\UU$, we can define a family of real random variables, indexed by $\kk \in \Kspace$: $\{J(\kk,\UU) \mid \kk \in \Kspace\}$. Quite intuitively, we want to minimize some kind of central tendency of those random variables, the most prominent one being the mean:
\begin{equation}
  \mu(\kk) = \Ex_\UU\left[J(\kk,\UU)\right] =\int_{\Uspace} J(\kk,\uu)p_\UU(\uu)\,\mathrm{d}\uu
\end{equation}
In other term, $\mu(\theta)$ is the expected loss for the decision $\theta$, that we can seek to minimize, giving:
\begin{align}
  \estimtxt{\kk}{mean} = \argmin_{\kk\in\Kspace} \mu(\theta)&= \argmin_{\kk\in\Kspace}-\int_\Uspace\log\left(p_{Y \mid \KK,\UU}(y \mid \kk,\uu)p_{\KK}(\kk)\right)p_{\UU}(\uu)\,\mathrm{d}\uu \\
                     &= \argmin_{\kk\in\Kspace}-\int_\Uspace\log\left(p_{\KK \mid Y,\UU}(\kk \mid y,\uu)\right)p_{\UU}(\uu)\,\mathrm{d}\uu 
\end{align}


under the assumption that $p_{\KK \mid Y,\UU}(\kk\mid y,\uu) \propto e^{-J(\kk,\uu)}$,  $J(\kk,\uu) \propto -\log p_{Y \mid \KK,\UU}(y \mid \kk,\uu)- \log p_{\KK}(\kk)$,

\subsubsection{Multiobjective optimization}
\label{sec:multiobjective_optimization}
In \cref{sec:exp_loss_minimization}, we used the mean as a measure of the central tendency that we want to minimize. Jointly with the central tendency,  information about of the dispersion of the random variable may also be relevant, in order to predict how much 
\subsubsection{Higher moments in optimization}

\paragraph{Skewness}
The skewness coefficient measures the asymmetry in the distribution:
\begin{equation}
  \mathrm{sk}\left[X\right] = \Ex\left[\left(\frac{X - \mu}{\sigma}\right)^3\right]
\end{equation}
where $\mu = \Ex[X]$ and $\sigma = \sqrt{\Var[X]}$.

Adding the skewness in the optimization translates to a preference toward a risk-averse or a risk-seeking approach. Indeed, as the main goal is the optimization of a cost function, so deviation of the value of the random variable toward lower values is more desirable than deviation toward larger values.

\begin{figure}[ht]
  \centering
  \input{\imgpath skewness_examples.pgf}
  \caption{\label{fig:skewness_example} Pdf and cdf of random variables with same mean, variance but different skewness}
\end{figure}

\paragraph{Kurtosis}



\section{Relative-regret estimators family}
\label{sec:rr_family}
All the methods introduced above required first to eliminate in some sense the dependency on the environmental parameter, in order to transform the random variable $J(\kk, \UU)$ into an objective that depends solely on $\kk$, and to optimize this deterministic function.
We propose now to reverse these steps.
\subsection{Most Probable Estimate}
\label{sec:MPE}
\begin{definition}[Conditional minimum, minimizer]
  Let $J: \Kspace \times \Uspace$ be a cost function, and let us assume that for each $\uu \in \Uspace$, $\min_{\kk \in \Kspace} J(\kk,\uu)$ exists and is attained at a unique point.
  We denote as
  \begin{equation}
    J^*(\uu) = \min_{\kk \in \Kspace} J(\kk,\uu)
  \end{equation}
  the \emph{conditional minimum} of $J$ given $u$, and
  \begin{equation}
    \kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk, \uu)
  \end{equation}
  the \emph{conditional minimizer}
\end{definition}
\cite{trappler_robust_2020}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
      }
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
