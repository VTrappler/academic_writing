\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Resumes/#1}
}

\newcommand{\frchap}[1]{\hyperref[#1]{Chapitre}~\ref{#1}}

% \subfileLocal{
% \externaldocument{../../Text/Chapter2/build/Chapter2}
% \externaldocument{../../Text/Chapter3/build/Chapter3}
% \externaldocument{../../Text/Chapter4/build/Chapter4}
% \externaldocument{../../Text/Chapter5/build/Chapter5}
% \externaldocument{../../Text/Conclusion/build/Conclusion}
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\chapter*{Résumé Français}
\TitleBtwLines

\phantomsection
\addstarredchapter{Résumé Français}
\label{chap:resume_fr}
%\newpage
%\minitoc
\pagestyle{resumeStyle}

                                % \subfileLocal{\pagestyle{contentStyle}}
\subsection*{Présentation générale et calibration de modèles}
De nombreux phénomènes naturels sont modélisés afin de mieux connaître
leurs comportements et de pouvoir les prévoir.  Cependant, lors du
processus de modélisation, de nombreuses sources d'erreurs sont
introduites. Elles proviennent par exemple des paramétrisations qui
rendent compte des phénomènes sous-mailles, ou bien de l'ignorance des
conditions environnementales réelles dans lesquelles le phénomène est
observé.

De manière plus formelle, on peut distinguer deux types d'incertitudes
dans ces modèles, comme évoqué dans~\cite{walker_defining_2003}.
\begin{itemize}
\item les incertitudes dites \emph{épistémiques}, qui proviennent d'un
  manque de connaissance sur des charactéristiques du phénomène
  étudié, mais qui pourraient être réduites
\item les incertitudes dites \emph{aléatoires}, qui proviennent
  directement de la variabilité intrinsèque du phénomène étudié.
\end{itemize}

Dans le cadre de cette thèse, les incertitudes épistémiques prennent
la forme de la méconnaissance de la valeur d'un paramètre
$\kk \in \Kspace$, que l'on va chercher à calibrer.  Un exemple de ce
genre de problème est l'estimation de la friction dans les modèles
océaniques.  En effet, la friction de fond est dûe à la rugosité du
plancher océanique, provoquant de la dissipation d'énergie à cause des
turbulences engendrées. L'estimation de la friction de fond est un
problème à fort enjeu, notamment dans les régions côtières, du fait de
son influence sur les courants et de son interaction avec la
marée~\cite{sinha_principal_1997,boutet_estimation_2015}.

Cette estimation qui a déjà été traité dans un cadre
d'assimilation de données avec des méthodes variationelles comme
dans~\cite{das_estimation_1991,das_variational_1992} sur un cas
simplifié, ou dans un cas plus réaliste
dans~\cite{boutet_estimation_2015}, avec une méthode de gradient
stochastique, permettant de se passer du calcul du gradient.


Les incertitudes aléatoires, quant à elles, représentent des
conditions environnementales, comme le forçage d'un modèle ou les
conditions aux bords. Ces conditions ne sont pas directement
contrôlées par le modèle, donc l'on subit leurs fluctuations, ou leur
imprécision.

Ces variables environnementales vont être modélisées par une variable
aléatoire $\UU$, de réalisation $\uu\in \Uspace$.

Comme le modèle que l'on cherche à calibrer vise à représenter la
réalité, il est souhaitable que les predictions du modèle soient le
plus similaire possible aux observations dont on dispose. Cette notion
de distance est retranscrite en définissant une fonction $J$, dite
fonction coût ou fonction objectif qui mesure l'écart entre la sortie
du modèle et les observations disponible. Cette fonction prendra donc
en entrée le paramètre à estimer $\kk$, que l'on nommera paramètre de
contrôle, ainsi que $\uu$, le paramètre environnemental:

\begin{equation}
  \label{eq:def_J}
  \begin{array}{rccc}
   J: & \Kspace\times\mathbb{U}& \rightarrow& \mathbb{R}_+ \\
   &(\kk,\uu)& \mapsto& J(\kk,\uu)
  \end{array}
\end{equation}

La définition de la fonction coût dans un problème de calibration sera abordé dans le~\frchap{chap:inverse_problem}.

\subsection*{Notions de robustesse}

Ne pas prendre en compte les incertitudes aléatoires dans l'estimation
de $\kk$ peut amener à compenser de manière artificielle l'erreur
aléatoire, et donc amener à un comportement analogue au
\emph{sur-apprentissage} (overfitting), ou \emph{optimisation
  localisée}~\cite{huyse_probabilistic_2002}. Cela peut amener à des
situations où le paramètre estimé n'est optimal que pour la valeur de
$\uu$ supposée, et pour une autre réalisation de la variable aléatoire
sous-jacente, le modèle ainsi calibré donne des prédictions
potentiellement aberrantes~\cite{kuczera_there_2010}.

On cherche donc à définir une valeur de $\kk$, notée $\hat{\kk}$ de
manière à ce que $J(\hat{\kk}, \uu)$ reste \emph{acceptable}, lorsque
l'on prend en compte la variabilité intrinsèque de $\uu$.
En prenant en compte le caractère aléatoire de la variable
environnementale, pour un $\kk$ donné, la fonction coût peut être vue
comme une variable aléatoire: $J(\kk,\UU)$, que l'on va chercher
intuitivement à ``minimiser'' dans un certain sens qui reste à
définir.  Cette problématique porte différents noms, comme
l'optimisation robuste, où robuste doit être compris comme
l'insensibilité aux variations de $\UU$, optimisation sous
incertitudes (\emph{Optimisation under Uncertainty} ou \emph{OUU}), ou encore
d'optimisation stochastique. Une nomenclature prenant en compte les
différences notamment sur les contraintes potentiellement présentes
peut être trouvé dans~\cite{lelievre_consideration_2016}


L'objectif de la thèse est d'établir différents critères de
robustesse, et d'appliquer des méthodes adaptées permettant d'estimer
un paramètre en présence d'incertitudes. Cette estimation se réalise
dans un premier temps dans des cas simples (fonctions analytiques,
problèmes simplifiés de faibles dimensions), puis sur des problèmes
plus complexes d'estimation de la friction de fond (modèles réalistes
coûteux en temps de calcul, dimension élevée).

\subsection*{Critères basés sur le regret additif et relatif}
Dans le \frchap{chap:robust_estimators}, nous abordons le problème de
calibration 
Un certain nombre des méthodes
d'optimisation sous incertitudes se basent sur la minimisation des
moments de la variable aléatoire $J(\mathbf{\cdot}, \UU)$ comme
dans~\cite{lehman_designing_2004,janusevskis_simultaneous_2010}, ou
bien se basent sur la résolution d'un problème
multiobjectifs~\cite{baudoui_optimisation_2012,ribaud_krigeage_2018}.

Dans le cadre de cette thèse, nous proposons une approche basée sur le
regret, qui consiste à comparer les valeurs de la fonction $J$ avec le
\emph{minimum conditionnel}, qui est le minimum de la fonction
$J(\cdot, \uu)$, où $\uu$ est une réalisation de la variable aléatoire
$\UU$. Le minimum conditionnel est donc défini par
\begin{equation}
  \label{eq:Jstar}
  J^*(\uu) = \min_{\kk\in\Kspace} J(\kk,\uu)
\end{equation}
et le \emph{minimiseur conditionnel} associé est
\begin{equation}
  \label{eq:Kstar}
  \kk^*(\uu) = \argmin_{\kk\in \Kspace}J(\kk,\uu)
\end{equation}
Ceci permet de définir le regret additif: $J - J^*$. Étant donné la
strict positivité de $J$, nous pouvons définir aussi le regret relatif
$J/J^*$. Ceci nous permet d'introduire une notion
d'\emph{acceptabilité}, à entendre dans le sens d'écart par rapport au
minimum conditionnel.

Pour un $\uu\in\Uspace$ donné, $\kk\in\Kspace$ est dit
$\beta$-acceptable si $J(\kk, \uu) \leq J^*(\uu) + \beta$, pour $\beta \geq 0$. La notion
de $\beta$-acceptabilité est donc associée au regret additif: 
$J(\kk, \uu) - J^*(\uu)$.  Similairement, on définit la notion de
$\alpha$-acceptabilité: $\kk$ est dit $\alpha$-acceptable si
$J(\kk, \uu) \leq \alpha J^*(\uu)$, pour $\alpha > 1$

Nous nous intéresserons plus particulièrement au regret relatif, qui
permet de mieux prendre en compte les variations de magnitude de la
fonction objectif, mais les définitions suivantes peuvent être adaptée
au regret additif. En prenant en compte le caractère aléatoire de
$\UU$, nous pouvons donc étudier la probabilité pour un point $\kk$, d'être acceptable:
\begin{equation*}
\Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk,\UU) \leq \alpha J^*(\UU) \right]
\end{equation*}
Cette probabilité peut donc être optimisée, pour donner
\begin{equation}
  \kk_{\mathrm{RR},\alpha} = \argmax_{\kk \in\Kspace} \Gamma_\alpha(\kk)
\end{equation}
L'optimum atteint est donc la probabilité maximale avec laquelle le
regret-relatif est borné par $\alpha$.


  De manière opposée\todo{bof}, nous pouvons définir la
fonction quantile du regret relatif comme
\begin{equation}
  q_p(\kk) = Q_{\UU}\left(\frac{J(\kk,\UU)}{J^*(\UU)};p\right)
\end{equation}
où $Q_{\UU}(\cdot;p)$ est la fonction quantile à l'ordre $p$ de la
variable aléatoire en argument. $q_p(\kk)$ qui représente donc la valeur qui
borne le regret au point $\kk$ avec une probabilité donnée $p$.

Nous pouvons donc chercher à maximiser la probabilité
$\Gamma_{\alpha}$ pour $\alpha > 1$ bien choisi, ou à minimiser le
quantile $q_p$, au niveau de confiance $p$.

Ce travail a mené à la publication d'un
article~\cite{trappler_robust_2020}. Ce type de critères peuvent être
rapproché de la Value-at-Risk~\cite{rockafellar_deviation_2002}, qui
est utilisé notamment dans le domaine de la finance.

\subsection*{Optimisation robuste et processus Gaussiens}
D'un point de vue pratique, ces notions de minimiseur conditionnel et
de minimum conditionnel peuvent s'avérer difficiles et chères à
calculer, car nécessitant une procédure d'optimisation. De plus, la
connaissance de la fonction objectif doit être suffisante afin de
calculer assez précisemment les quantités $\Gamma_{\alpha}$ et $q_p$.
Dans le \frchap{chap:adaptative_design_gp}, nous proposons d'utiliser
des processus Gaussiens (GP), afin de créer un modèle de substitution
permettant de se passer d'une connaissance exhaustive de la fonction
$J$.

Les propriétés des GP nous permettrons aussi d'établir des stratégies
d'enrichissement. En effet, des méthodes existantes dites
\emph{adaptatives} permettent d'améliorer l'estimation de diverses
quantités, comme la probabilité de défaillance
\cite{razaaly_rare_2019,moustapha_quantile-based_2016,bect_sequential_2012},
ou les minimiseurs et minimums conditionnels
dans~\cite{ginsbourger_bayesian_2014}. Ces méthodes, parfois appelées
méthodes SUR (\emph{Stepwise Uncertainty reduction}, réduction
d'incertitude séquentielle) sont basées sur la définition d'un
\emph{critère}, dont le minimiseur va être évalué par la fonction
originale. Ce critère va donc représenter une mesure de l'incertitude
sur l'estimation, que l'on va chercher à réduire. Nous allons ainsi
proposer plusieurs méthodes permettant d'améliorer l'estimation de
$\Gamma_{\alpha}$ et de $q_p$.


\subsection*{Application au code de calcul CROCO}
Dans le \frchap{chap:croco}, nous nous intéressons à la calibration
robuste d'un modèle réaliste d'océan, basé sur le code de calcul CROCO.

\todo{écrire}


\vfill
\etoile
\vfill

% Pour le futur et la fin de la thèse, plusieurs points sont à considérer:
% \begin{itemize}
% \item Des méthodes permettant un contrôle plus fin sur les propriétés recherchées du paramètre (aversion/recherche du risque) sont à explorer, comme le ``horsetail matching''~\cite{cook_horsetail_2018}, en prenant notamment pour cible la distribution des minimums.
% \item L'application à l'estimation de la friction de fond dans le cadre du modèle CROCO est à envisager. CROCO\footnote{\url{https://www.croco-ocean.org/}} (Coastal and Regional Ocean COmmunity model) est un modèle régional d'océan, notamment conçu pour des applications de modélisation côtières. La région étudiée dans le cadre de la thèse est la façade atlantique de la France. 
%   \begin{itemize}
%     \item Premièrement les incertitudes du modèle sont à spécifier. Une première piste envisagée est introduire des incertitudes sur les composantes de marée à ajouter dans le forçage.
%   \item La friction de fond, que l'on cherche à estimer, est un paramètre variant dans l'espace, et donc peut potentiellement être défini en chaque point du maillage d'un modèle. Ceci force à prendre en compte la possibilité d'une grande dimension de $\Kspace$. On peut donc s'interroger sur la façon dont les méthodes et les critères décrits vont se mettre à l'échelle, et sur la possibilité d'appliquer des procédures de réduction de dimension.
%   \item En plus de la possible grande dimension du problème, les modèles réalistes sont souvent très coûteux en terme de temps de calcul. Il serait donc intéressant de pouvoir réduire au plus les évaluations du code, en adoptant par exemples des méthodes basées sur l'utilisation de méta-modèles. 
%   \end{itemize}
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{../../bibzotero}
}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
