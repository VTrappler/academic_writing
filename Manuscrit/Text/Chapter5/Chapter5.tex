\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter5/#1}
}
\newcommand{\CROCO}{CROCO}
\newcommand{\zob}{z_b}


\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter2/build/Chapter2}
\externaldocument{../../Text/Chapter3/build/Chapter3}
\externaldocument{../../Text/Chapter4/build/Chapter4}
\externaldocument{../../Text/Conclusion/build/Conclusion}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\imgpath{/home/victor/acadwriting/Manuscrit/Text/Chapter5/img/} 


\begin{document}
% \subfileLocal{\dominitoc}
% \subfileLocal{\setcounter{chapter}{4}}
% \subfileLocal{\chapter{Application to the numerical coastal model CROCO}}
\chapter{Application to the numerical coastal model \CROCO}
\label{chap:croco}
\minitoc
\newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Introduction}
\label{sec:intro_croco}

% \todo{\cite{mcwilliams_irreducible_2007,zanna_ocean_2011}}

In this chapter, we will study the problem of calibration under
uncertainties of the bottom friction, in a realistic numerical based
on \CROCO{} of the Atlantic French coast.

Since the bottom friction depends directly on the size of the
asperities on the ocean bed, it is a subgrid phenomenon, meaning that
the program does not solve the equations of motions of the fluid
around those asperities. Instead, the dissipation coming from this
rugosity is parametrized at every point of the mesh.

The bottom friction has been identified as a crucial parameter that,
if ill-specified, limits the accuracy of the
predictions~\citep{sinha_principal_1997,kreitmair_effect_2019},
especially in shallow water regions; consequently, there has been an
effort to control this parameter in various
studies, for instance in~\cite{das_variational_1992,das_estimation_1991,boutet_estimation_2015}.
We will detail how bottom friction affects the oceanic circulation
in~\cref{ssec:modelling_bottom}, in order to get a first insight on
the regions that may influence the most the calibration.

The deterministic problem of calibration will then be addressed in
\cref{sec:deterministic_calibration_bott}, by first defining the cost
function and the input space. We will then calibrate the model without
external uncertainties using adjoint-based method, in high-dimension
($\sim$\num{15000}).

However for such problems, as the parameter may be spatially
distributed and thus high-dimensional, any procedure may become
quickly expensive.  In consequence, instead of considering each grid
point individually, we will segment the geographical input space in
different independent regions, which are based on the type of
sediments listed at the bottom of the water. In this problem of
calibration we will assume that the uncertainties take the form of an
environmental parameter which perturbates the amplitude of some tidal
constituents.  In order to quantify the influence of each of the
sediments-based regions and the influence of each of the components of the
environmental variable, we will carry a global sensitivity analysis,
in~\cref{sec:sensitivity-analysis}.

Finally using this study, we will reduce significantly the input
space, and then apply some of the methods proposed in the previous
chapter, in order to get a robust estimation of the bottom friction
using Gaussian processes in~\cref{sec:robust_calibration}.


\section{\CROCO\ and bottom friction modelling}
\label{sec:croco_bottom_fr}
\CROCO{}\footnote{\CROCO\ and CROCO\_TOOLS are provided by
  \url{https://www.croco-ocean.org}} (Coastal and Regional Ocean
COmmunity model) is a program written in Fortran that describes the
motion of the ocean by solving the \emph{primitive equations}, which
are simplified versions of the Navier-Stokes equations, in order to
take into account the particular scales at play at the surface of the
Earth. \CROCO{} has been developed upon
ROMS\_AGRIF%\footnote{\url{https://www.myroms.org/},\url{http://www-ljk.imag.fr/MOISE/AGRIF/}}
(Regional Ocean Modeling System, Adaptive Grid Refinement in
Fortran~\cite{debreu_two-way_2012}), and is designed to be coupled with
other modelling systems, such as atmospheric, biological or ecosystem
models.

% gradually including algorithms from MARS3D (sediments) and
% HYCOM (vertical coordinates). An important objective for \CROCO\ is to
% resolve very fine scales (especially in the coastal area), and their
% interactions with larger scales. It is the oceanic component of a
% complex coupled system including various components, e.g., atmosphere,
% surface waves, marine sediments, biogeochemistry and ecosystems.


\subsection{Parameters and configuration of the model}
\label{sec:geographical_setting}

The configuration used in this thesis is based on the one used
in~\cite{boutet_estimation_2015}. The spatial domain ranges from
\ang{9}W to \ang{1}E and from \ang{43}N to \ang{51}N, and spans most
of the Bay of Biscay, the English Channel and the eastern part of the
Celtic Sea.  The resolution is \SI{1/14}{\degree}, which leads to a
mesh size between \SI{5}{\kilo\metre} and \SI{6}{\kilo\metre}. The
bathymetry map and the spatial domain is
shown~\cref{fig:depth_maps}. The ocean can be split roughly in two
regions, based on its depth : the region near the coasts which
corresponds to the continental shelf, where the water depth is less
than \SI{200}{\meter}, and the offshore region of the Bay of Biscay,
where the depth is closer to \SI{5000}{\meter}.
\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath depth_maps_log.png}
  \caption[Bathymetry chart of the domain
  modelled]{\label{fig:depth_maps} Bathymetry used in \CROCO, and
    geographical landmarks. The continental shelf corresponds roughly
    to the area with depth less than \SI{200}{\meter} (green hue),
    while the abyssal plain has a depth larger than \SI{4000}{\meter}
    (blue hue)}
\end{figure}


\CROCO{} can solve the fluid motion equations in 3D, but in this
configuration, solves the shallow water equations instead, which are
obtained by vertically integrating the primitive equations:
% \begin{align}
    %     \left\{
    %     \begin{array}{rcl}
    %     \frac{\partial u}{\partial t} + \nabla \cdot \left(\vec{v}u\right) - fv &=& -\frac{\partial \phi}{\partial x} + \mathcal{F}_{u} + \mathcal{D}_u \\
    %     \frac{\partial v}{\partial t} + \nabla \cdot \left(\vec{v}v\right) + fv &=& -\frac{\partial \phi}{\partial y} + \mathcal{F}_{v} + \mathcal{D}_v
                                                                                      %   \end{array}
                                                                                      %                                                                                       \right.
                                                                                      %   \end{align}
\begin{align}
  \left\{
  \begin{array}{lll}
    \pfrac{\mathbf{v}}{t} + (\mathbf{v} \cdot \nabla )\mathbf{v} + 2 \bm{\Omega} \wedge \mathbf{v} & = & -g\nabla H + \frac{\bm{\tau}_b}{\rho H} + F \\
    \pfrac{H}{t} + \nabla (H \cdot \mathbf{v})                                                     & = & 0
  \end{array}
                                                   \right.
\end{align}
where $\mathbf{v} = (v_x,v_y)$ is the velocity field of the fluid,
$\bm{\Omega}$ is the rotational angular vector of Earth, $H$ is the
water column height, $g$ is the gravitational constant, $\rho$ is the
fluid density, and $\bm{\tau}_b$ is the shear stress at the
bottom. Finally, $F$ represents the forcing of the model, in this case
the influence of the tides.  The bottom friction affects the
circulation through $\bm{\tau}_b$, and different parameterizations of
this stress can be derived.

\subsection{Modelling of the bottom friction}
\label{ssec:modelling_bottom}
In \CROCO, the bottom friction is modelled using a quadratic drag
coefficient $C_d$:
\begin{equation}
  \label{eq:bottom_stress_tau}
  \bm{\tau}_b= -C_d \|\mathbf{v}_b\|\mathbf{v}_b 
\end{equation}
where $\mathbf{v}_b$ is the velocity at the bottom, so in the case
of the Shallow Water equations, $\mathbf{v}_b = \mathbf{v}$.  The
drag coefficient can also be formulated as a function of the water
column height and the \emph{bottom roughness} $\zob$ by assuming a
logarithmic profile of the velocity at bottom (a derivation can be found in~\cite{le_bars_amandes_2010} for instance)
\begin{equation}
  \label{eq:quadratic_friction_vonkarman}
  C_d = \left(\frac{\kappa}{\log\left(\frac{H}{\zob}\right) - 1}\right)^2% \text{for } C_d \in [C_d^{\min}, C_d^{\max}]
\end{equation}
where $\kappa$ is the Von K\'arm\'an constant, usually taken equal to
$0.41$.  The bottom roughness $\zob$, or \emph{rugosity} in this
document, can be interpreted as the size of the turbulent layer at the
bottom, induced by the asperities of the sediments.
\cite{boutet_estimation_2015} shows that in a calibration context,
controlling the rugosity $\zob$ yields better result than controlling
the drag coefficient $C_d$ due to the influence of the water column
height $H$.
On~\cref{fig:cd_zob} is shown the drag coefficient $C_d$ as a function
of the roughness $\zob$ of the ocean floor, for different heights of
the water column $H$.
\begin{figure}[ht]
  \centering \input{\imgpath cd_zob.pgf}
  \caption[Drag coefficient $C_d$ as a function of the height and the
  roughness]{\label{fig:cd_zob} Drag coefficient $C_d$ as a function
    of the column water height and the roughness at the bottom}
\end{figure}


We can see that the higher the water column height, the less variation
appears when adjusting the bottom roughness $\zob$.  Considering the
physical properties of the bottom friction and the types of sediments,
it can be expected that the English Channel, and at a lesser extent
the rest of the continental shelf are the areas which are the most
influential for the calibration.


We are going to make the assumption that the size of the turbulent
layer at the bottom is equal to the size of the sediments there, so
the rugosity is directly linked to the type of sediment found on the
ocean bed. \Cref{tab:size_sediments} presents a coarse classification,
along with the typical size of the sediments that can be found, that
will serve as a reference value, or \emph{truth value}
$\zob^{\mathrm{truth}}$.  % We can see that the rugosity spans several
% order of magnitude, hence it may be worth considering controlling the
% logarithm of the rugosity instead of the rugosity itself.

\begin{table}[!ht]
  \centering
  \begin{tabular}{rrrl} \toprule Code & Description & Size of the
    majority of particles             & $\zob^{\mathrm{truth}}$                                                                                  \\ \midrule
    R                                 & Rock        & Larger                                                         & \SI{50}{\milli\meter}     \\
    C                                 & Pebble      & $>$\SI{20}{\milli\metre}                                       & \SI{25}{\milli\meter}     \\
    G                                 & Gravel      & $\interval{\SI{20}{\milli\metre}}{\SI{2}{\milli\metre}}$       & \SI{7}{\milli\meter}      \\
    S                                 & Sand        & $ \interval{\SI{2}{\milli\metre}}{\SI{0.5}{\milli\metre}}$     & \SI{1}{\milli\meter}      \\
    SF                                & Fine Sand   & $ \interval{\SI{0.5}{\milli\metre}}{\SI{0.05}{\milli\metre}}$  & \SI{1.5e-1}{\milli\meter} \\
    Si                                & Silt        & $ \interval{\SI{0.05}{\milli\metre}}{\SI{0.01}{\milli\metre}}$ & \SI{2e-2}{\milli\meter}   \\
    V                                 & Muds        & $< \SI{0.05}{\milli\metre}$                                    & \SI{2e-2}{\milli\meter}
                                                                                                                                                 \\ \bottomrule
            %             A           & Clay        & $< \SI{0.01}{\milli\metre}$                                    & \bottomrule
  \end{tabular}
  \caption[Types and sizes of each sediment class]{\label{tab:size_sediments} Type of sediments and size of the majority of particles for each type of sediment. Data source: SHOM, used under \href{https://creativecommons.org/licenses/by-sa/4.0/}{CC BY-SA 4.0} license}
\end{table}

Based on the documentation of the SHOM\footnote{Service hydrographique
  et océanographique de la Marine, \url{https://www.shom.fr/fr}},
\cref{fig:sediments_reduced} shows a map of the repartition of the
different types of sediments introduced there. A more complete chart
which comprises more types of sediments can be found in the
appendix~\cref{fig:sediments_full}.
\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath sediments_reduced.png}
  \caption[Repartition of the sediments
    on the ocean floor]{\label{fig:sediments_reduced} Repartition of the sediments
    on the ocean floor. Data source: SHOM, used under \href{https://creativecommons.org/licenses/by-sa/4.0/}{CC BY-SA 4.0} license}
\end{figure}


Based on this classification, we can see that most of the ocean floor
of the studied domain is composed of sand. Even though siltic soil is listed,
it is only scarcely present. The figure also shows that the largest
sediments are rocks but are mostly located in the Bay of Biscay, near
the boundary of the continental shelf. Pebbles however are mostly
located in the shallow region in the English Channel, thus it may be
expected that controlling the roughness in the regions listed as
pebbles will affect significantly the water circulation, and thus the sea
surface height. Incidentally, we can notice the inverse correlation
between the size of the sediments, and the depth at which they are
found.

According to~\cref{tab:size_sediments}, the rugosity $\zob$ spans
multiple orders of magnitude, so we are then going to define the control
variable $\kk$ as
\begin{equation}
  \kk = \log \zob \in \Kspace = \interval{\kk_{\min}}{\kk_{\max}}^p
\end{equation}
where $\kk_{\min} = \log(10^{-5}) \approx -11.5$, and
$\kk_{\max} = \log(5\cdot 10^{-2}) \approx -3$.  The dimension of
$\Kspace$ will be written $p$, and will be specified later depending on the
chosen segmentation.

\subsection{Tidal modelling and uncertainties}
\label{ssec:tidal_modelling}
The ocean, especially near the English Channel is driven by tidal
forces that produce currents at the surface. As a periodic signal, the
tidal forcing is usually analysed harmonically, in order to separate
its influence by frequency. In \CROCO, this forcing comes from the
TPXO model of tides~\citep{egbert_efficient_2002}, and the
configuration uses the 5 primary harmonic constituents as described
\cref{tab:tides_components}.
\begin{table}[!h]
  \centering
  % % Chose order from the rank in the TPXO file :
                                                                                                               %                                                                                                                "M2 S2 N2 K2 K1 O1 P1 Q1 Mf Mm"
                                                                                                               %                                                                                                                " 1  2  3  4  5  6  7  8  9 10"
  \begin{tabular}{rrr}\toprule
    Darwin Symbol & Period (h)   & Species                           \\ \midrule
    $M_2$         & 12.4206      & Principal Lunar Semidiurnal       \\
    $S_2$         & 12           & Principal Solar Semidiurnal       \\
    $N_2$         & 12.65834751  & Larger Lunar Elliptic Semidiurnal \\
    $K_2$         & 11.96723606  & Lunisolar Semidiurnal             \\
    $K_1$         & 23.93447213  & Lunar Diurnal                     \\
            %             \midrule
            %             $O_1$         & 25.81933871  & Lunar Diurnal                     \\ 
            %             $P_1$         & 24.06588766  & Solar Diurnal                     \\
            %             $Q_1$         & 26.868350    & Larger Lunar Elliptic Diurnal     \\
            %             $M_f$         & 13.660830779 & Lunisolar Fortnightly             \\
            %             $M_m$         & 27.554631896 & Lunar Monthly                     \\
    \bottomrule
  \end{tabular}
  \caption{Harmonic constituents used in the configuration}
  \label{tab:tides_components}
\end{table}

By perturbating some properties of those tide components, we can
introduce some error in the numerical model, \emph{i.e.} a parametric
misspecification that we will define as the environmental variable. In
this work, this takes the form of a small multiplicative error on the
amplitude of the different components of the tide. Let
$\uu = (\uu_1,\dots,\uu_5)\in \Uspace = \interval{0}{1}^5$ be the
environmental variable, that will be considered as a random variable
later, and let $A_k$ be the amplitude of the $k$th component of the
tide.  The perturbated amplitude $\tilde{A}_k$ is defined as
\begin{equation}
  \label{eq:tide_error}
  \tilde{A}_k(\uu_k) = A_k (1 + 0.01(2\uu_k - 1))
\end{equation}
for $1\leq k\leq 5$.
Based on this definition, the perturbated amplitude varies from
$\tilde{A}_k(0) = 0.99A_k$ to $\tilde{A}_k(1) = 1.01A_k$, for every
$1\leq k \leq 5$. We define also $\uu^{\mathrm{truth}}$ as
the vector whose all its components are set to \num{0.5}, so when no
amplitude is perturbated: $\tilde{A}_k(\uu_k^{\mathrm{truth}}) = A_k$
for $1\leq k \leq 5$.


In the next section, we are going to estimate the parameter $\kk$ by
minimising the objective function using a gradient descent algorithm.
\section{Deterministic calibration of the bottom friction}
\label{sec:deterministic_calibration_bott}
The calibration of the bottom friction will first be studied without
external uncertainties, which corresponds to an unperturbated tidal
forcing: the environmental variable is set to $\uu^{\mathrm{truth}}$.
    %     $N_{\mathrm{obs}} = N_{\mathrm{mesh}}\cdot N_{\mathrm{time}}$

Following the notations introduced in~\cref{chap:inverse_problem} for
the model, let us define
$(\mathcal{M}(\cdot,\uu^{\mathrm{truth}}),\Kspace)$ as the numerical
model to calibrate. The forward operator $\mathcal{M}(\cdot, \uu^{\mathrm{truth}})$ is
defined by
\begin{equation}
  \begin{array}{rcl}
    \mathcal{M}: \Kspace  &\longrightarrow& \mathbb{R}^{N_{\mathrm{obs}}} \\
    \kk & \longmapsto & \mathcal{M}(\kk, \uu^{\mathrm{truth}}) = \left(\eta_{t,i}(\kk, \uu^{\mathrm{truth}})\right)_{\substack{1 \leq i \leq N_{\mathrm{mesh}} \\ 1 \leq t \leq N_{\mathrm{time}}}} \\ 
  \end{array}
\end{equation}
where $\eta_{t,i}(\kk,\uu)$ is the free surface height of the ocean at
the mesh point $i$, and at the time-step $t$, obtained using the model
and the bottom friction associated with $\kk$ and the environmental
variable $\uu$, and
$N_{\mathrm{mesh}} \cdot N_{\mathrm{time}} = N_{\mathrm{obs}}$.  In
this configuration, $N_{\mathrm{time}} =\num{49}$, and corresponds to
the number of records saved. The time step of the simulation in itself
is \SI{10}{\second}, and its total duration is \SI{24}{\hour}, meaning that the
water height $\eta$ is saved every \SI{30}{\minute}.

\subsection{Twin experiment setup}
In a twin experiment setup, the observation $y$ are generated using
the numerical model, and a predefined truth value
$\kk^{\mathrm{truth}}$.  This means that the ``physical model'' is
defined using the forward operator $\mathscr{M}$, based on the forward
numerical model, evaluated with the environmental parameter
$\uu^{\mathrm{truth}}$.
\begin{equation}
  \label{eq:twin_exp}
  \begin{array}{rcl}
    \mathscr{M}: \Kspace &\longrightarrow & \mathbb{R}^{N_{\mathrm{obs}}} \\
    \kk & \longmapsto &\mathscr{M}(\kk) = \mathcal{M}(\kk, \uu^{\mathrm{truth}})
  \end{array}
\end{equation}

Based on the forward operator $\mathscr{M}$, we can generate the
observations $y\in \Yspace = \mathbb{R}^{N_{\mathrm{obs}}}$ using the
truth value $\kk^{\mathrm{truth}}=\log \zob^{\mathrm{truth}}$, as
defined \cref{tab:size_sediments}:
\begin{equation}
  y = \mathscr{M}(\kk^{\mathrm{truth}}) = \mathcal{M}(\kk^{\mathrm{truth}}, \uu^{\mathrm{truth}})
\end{equation}

In the following, if the $\uu$ argument is omitted, it means that the
model, or subsequent functions are evaluated with
$\uu^{\mathrm{truth}}$.
\subsection{Cost function definition}
Once the observation $y \in \mathbb{R}^{N_{\mathrm{obs}}}$ has been generated, we can
define the objective function $J$:
\begin{align}
  \label{eq:cost_fun_definition}
  J(\kk) &= \sum_{t=1}^{ N_{\mathrm{time}}}\sum_{i=1}^{N_{\mathrm{mesh}}}  \left(\eta_{t,i}(\kk,\uu^{\mathrm{truth}}) - y_{t, i}\right)^2 \\
         &= \|\mathcal{M}(\kk,\uu^{\mathrm{truth}}) - y\|_2^2
\end{align}
Equivalently, as mentioned in~\cref{chap:inverse_problem}, by assuming
that the distribution of the (random) observation vector is known and
$Y \mid \kk \sim \mathcal{N}(\mathcal{M}(\kk), I)$ (with $I$ being the
identity matrix of dimension $p$), $J$ is proportional to the negative
log-likelihood of the data.

\subsection{Gradient-descent optimisation}
\label{ssec:optim_gradient}
The optimisation is carried using M1QN3, a version of a
gradient-descent procedure, as described
in~\cite{gilbert_numerical_1989}. We can first look to control $\zob$
at every cell of the mesh: $\kk = (\kk_1,\cdots, \kk_p)$ where
$\kk_i = \log\zob^i$ and $p=\num{15684}$.

Due to the large number of points whose friction can be controlled, a
finite-difference method to get the gradient is unfeasible. Instead,
Tapenade~\citep{hascoet_tapenade_2013}, an Automatic Differentiation
tool has been used in order to get the gradient (with respect to
$\kk$) of the cost function $J$ using the adjoint method, as
described~\cref{sec:calibration_adjoint_optimization}. The
optimisation procedure is stopped after \num{400} iterations, and the
estimated control parameter is
shown~\cref{fig:optimization_map_399}. The evolution of the cost
function and the squared norm of the gradient during the optimisation
procedure is shown~\cref{fig:ctrl_true}.  In the
appendix~\cref{sec:appendix_truth_optim} is presented the result of
the optimisation over the whole domain, for other choices of the
environmental parameter, \emph{i.e.} in a misspecified case.

\begin{figure}[ht]
  \centering
  \includegraphics{/home/victor/optimisation_dahu/optim_sediments/map_log_400.png% /home/victor/optimisation_dahu/optim_true/map_150.png
  }
  \caption[Optimisation of $\zob$ on the whole
  space]{\label{fig:optimization_map_399} Optimisation of $\zob$ on
    the whole space using gradient obtained via adjoint method, after
    $400$ iterations.}
\end{figure}

\begin{figure}[ht]
  \centering
  \input{/home/victor/optimisation_dahu/optim_sediments/ctrl_true400.pgf}
  \caption{\label{fig:ctrl_true} Evolution of the cost function and
    the squared norm of the gradient}
\end{figure}
By comparing the result of the
optimisation~\cref{fig:optimization_map_399} with the sediment
charts~\cref{fig:sediments_reduced} and the bathymetry
map~\cref{fig:depth_maps}, we can have a first overview on which
regions of the domain are properly estimated (\emph{i.e.} where the
estimation is close to the truth value).  On a first look, we can see
that the abyssal plain (the deep region off the Bay of Biscay) remains
mostly unaffected by the optimisation, while the continental shelf,
except for some parts of the English Channel, is well retrieved.

\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath optimisation_type_sediments.pdf}
  \caption[Final values of the optimisation procedure, based on the sediment type]{\label{fig:optimisation_type_sediments} Results of the
    optimisation procedure, depending on the type of sediments. The
    initial value is \SI{5e-3}{\meter}}
\end{figure}

On~\cref{fig:optimisation_type_sediments}, we can see that indeed,
points of the mesh corresponding to sand, \emph{i.e.} most of the
continental shelf, tends to get closer to the truth value $\zob^{\mathrm{truth}}$.  For
silts and muds however, the procedure did not change significantly
their roughness, and thus stays close to the initial value. This can
be probably explained by the great depth at which those sediments lay,
and thus it mitigates their influence on the drag coefficient
per~\cref{eq:quadratic_friction_vonkarman}. In the English Channel,
the size of the pebbles is quite well retrieved albeit a bit
underestimated, but the points mapped to gravel do seem to compensate:
on the northern part of the channel the size of the gravel is
overestimated, while it is underestimated on the southern part.
Finally the rocks appear to be hard to estimate properly: only about
3\% of the domain is listed as rocks, and their corresponding $\zob$
is quite large in contrast to the other sediments.

This optimisation showed that all the regions are not crucial for a
significant reduction of the values of the objective function.
We are now going to study the influence of the different inputs of the
objective function. More specifically, we are going to quantify the
influence of each region defined by its sediment type, and quantify
the influence of the uncertainties defined
in~\cref{ssec:tidal_modelling} on the output of the objective
function, by means of sensitivity analysis.


\section{Sensitivity analysis of the objective function}
\label{sec:sensitivity-analysis}
Sensitivity analysis (often abbreviated as \emph{SA}), aims at
quantifying the effect of the variation of some input variable to the
output of the model~\citep{iooss_revue_2011,janon_analyse_2012}.
Intuitively, SA aims at understanding how much the variations
of each input or combinations of inputs explain the variations of the
output.

It can then be approached at two different scales:
around a nominal value, using the gradient, and at a global scale, by
considering the inputs as random variables, and by measuring the
variance of the output. In this work, we are going to focus
exclusively on global sensitivity analysis.

Here, sensitivity analysis is performed as a dimension-reduction
method, because it will be used to reduce the input space, based on
prior assumption that the $\zob$ is considered constant for
each regions. Indeed, we will make the link between \emph{sensitivity}
and \emph{identifiability}~\citep{dobre_global_2010}. %, as
%non-sensitivity implies non-identifiability.
If a parameter shows a very small influence on the output of the
objective function, any choice of its value will yield sensibly the
same output of the objective, provided that the other input parameters
are the same, justifying their overlook.


% \subsection{Methods of Sensitivity analysis}
% \label{sec:methods_SA}
% \subsubsection{Local sensitivity analysis}
% \label{sec:loca_SA} Local sensitivity
% analysis~\cite{morio_global_2011} refers to the study of how a small
% perturbation $\delta \kk$ of a nominal value $\kk$ affects the output
% of the numerical model. As we assume that the numerical model is
% accessible through the cost function $J$, a straightforward way to
% quantify this perturbation is to consider the partial derivative of
% $J$, with respect to each component of the control variable
% $\kk=(\kk_1,\dots,\kk_p)$:
% \begin{equation} \frac{\partial J}{\partial \kk_i}(\kk)
% \end{equation} The normalized local sensitivity
% at $\kk$ associated with the $i$-th component is then
% \begin{equation} \frac{{\Delta J}/{J}}{{\Delta
%       \kk_i}/{\kk_i}} = \frac{\kk_i}{J(\kk)} \frac{\partial J}{\partial\kk_i}
% \end{equation}

\subsection{Global Sensitivity Analysis: Sobol' indices}
\label{sec:sobol-indices}
As global SA calls for a probabilistic framework, we are going to
consider a real-valued random vector $X=(X_1,\cdots X_p)$ whose
components are independent, which represents the inputs of a real
function $f: \mathbb{R}^p\rightarrow \mathbb{R}$. As $X$ is a random
vector, we can introduce $Y$, the real-valued random variable defined
as $Y=f(X)$.

 % \cite{iooss_revue_2011,gilquin_echantillonnages_2016,janon_analyse_2012}
The $i$-th Sobol' indice of order $1$ is defined
as~\citep{sobol_sensitivity_1993,sobol_global_2001}
\begin{equation}
  S_i = \frac{\Var_{X_i}\left[\Ex_{Y}\left[Y \mid X_i\right]\right]}{\Var_{Y}\left[Y\right]}
\end{equation}
and can be interpreted as the fraction of variance of the output
$Y$ explained by the variation of $X_i$ \emph{alone}. Indices of
order $2$ are defined as
\begin{equation}
  S_{i\times j} = \frac{\Var_{X_i, X_j}\left[\Ex_{Y}\left[Y \mid X_i, X_j\right]\right]}{\Var_{Y}\left[Y\right]} - S_i -S_j
\end{equation}
and account for the interactions of the inputs labelled $i$ and $j$.
Higher-order Sobol' indices can then be defined
sequentially. Total-effect indices are also central in global
sensitivity analysis: those indices measure the contributions of a
single input $X_i$ through all its possible interactions,
\textit{i.e.} by considering the effect of its own variability (as the
order $1$) and the effect of all its interactions (order $2$ and
above). Those total effect indices can be expressed as
\begin{equation}
  S_{T_i} = 1 - \frac{\Var_{X_{-i}}\left[\Ex_{Y}\left[Y \mid X_{-i}\right]\right]}{\Var_{Y}\left[Y\right]}
\end{equation}
where $X_{-i} = (X_1,\dots X_{i-1},X_{i+1},\dots,X_p)$ is a random vector of $p-1$ components.

In our study, the Sobol' indices of order $1$, $2$ and total effect
are computed using a replicated
method~\citep{gilquin_making_2019,gilquin_echantillonnages_2016},
allowing for bootstrap confidence intervals for the first and second
order effects.

\subsection{SA of the objective function for the calibration of CROCO}
\subsubsection{SA on the regions defined by the sediments}
The drag coefficient, which affects the ocean circulation, is the
result of two factors as shown
in~\cref{eq:quadratic_friction_vonkarman}: the bottom roughness
$\zob$ and the ocean depth $H$.

We are first going to perform a sensitivity analysis in order to
quantify the role of each sediment-based \emph{region}, without
incorporating the knowledge on the typical size of the sediment there.
Considering the similar expected size of silts (Si) and muds particles
(V) in~\cref{tab:size_sediments}, and the limited amount of silts, we
will merge those regions, and label the result with the code
$\mathrm{Si,V}$. Finally, the function which will be analyzed is the one
defined \cref{eq:cost_fun_definition}:
\begin{equation}
\kk \mapsto \|\mathcal{M}(\kk, \uu^{\mathrm{truth}}) - y \|_2^2
\end{equation}
with
\begin{equation}
  \kk = (\kk_{\mathrm{R}},\kk_{\mathrm{C}},\kk_{\mathrm{G}},\kk_{\mathrm{S}},
  \kk_{\mathrm{SF}},\kk_{\mathrm{Si,V}})\in\Kspace, \text{ with }
  \Kspace = \interval{\kk_{\min}}{\kk_{\max}}^6
\end{equation}
In the context of the sensitivity analysis, all the inputs are assumed
to be independent, and to be uniformly distributed on their support $\interval{\kk_{\min}}{\kk_{\max}}\approx \interval{\num{-11.5}}{\num{-3}}$.
The first, second and total order effect are
displayed~\cref{fig:SA_sediments}, where the regions defined by each
sediment is \cref{fig:sediments_reduced}. The experimental design used
here comprises \num{7888} points.

\label{ssec:SA_sediments}
\begin{figure}[ht]
  \centering
  \input{\imgpath SA_sediments.pgf}
  \caption[SA on the sediments-based regions]{\label{fig:SA_sediments} Global SA on the regions defined by the sediment type, and bootstrap confidence intervals}
\end{figure}

We can see that the most influential region is the one defined by the
pebbles (with code C). More generally, except for the rocks, we can
see that the shallower the region, the larger impact it has on the
objective function.  Based on geographic considerations and the result
of this sensitivity analysis, we can adopt a new segmentation: the
regions of Pebbles (C), and Gravel (G) will be kept intact, but the
remaining regions (R, S, SF, Si, V) will be bundled together.
\subsubsection{SA on the tide components}
\label{ssec:SA_tide}
As introduced~\cref{ssec:tidal_modelling}, \CROCO{} incorporates
different tide constituents that we perturbate through the uncertain
variable $\uu\in\Uspace$. In order to quantify the influence of each
component of the environmental parameter, we performed also a
sensitivity analysis. As before, the SA is performed on the sum of
squares of the difference between the forward operator and the
observations. This time however, the control parameter $\kk$ is set to its
truth value, and the sum of squares depends only on $\uu$:
\begin{equation}
  \uu \mapsto  \| \mathcal{M}(\kk^{\mathrm{truth}}, \uu) - y \|_2^2
\end{equation}
for $\uu = (\uu_1,\cdots,\uu_5)$, and $\Uspace = \interval{0}{1}^5$.
Once again, in the SA context, every component is assumed to be
uniformly distributed on $\interval{0}{1}$ and independent.
\Cref{fig:SA_tides} shows the Sobol indices of first order (left),
second (right), and the total effect indices, along with bootstrap
confidence intervals, obtained with a design of experiments of
\num{2888} points.
\begin{figure}[ht]
  \centering
  \input{\imgpath SA_tides.pgf}
  \caption[SA on the tide components]{\label{fig:SA_tides} Global SA
    on the different components of the tide, and bootstrap confidence
    intervals}
\end{figure}

We can see that the component of the vector affecting the amplitude of
the $M_2$ component of the tide has the most impact on the cost
function, and the $S_2$ component seems to have a non negligible
effect as well. For the other tide constituents, the SA reveals that
perturbating their amplitude has little to no-effect in this
configuration, and thus those variable will be discarded in further
analysis. The uncertain variable can then be redefined in what follows as
\begin{align}
  \UU = (\UU_1, \UU_2)%  \\
  % \UU_1 \text{ and } \UU_2 \text{i.i.d.} \\
  % \UU_i \sim \mathcal{U}\left(\interval{0}{1}\right) \text{ i=1,2}\\
\end{align}
where $\UU_1$ is the error on the $M_2$ amplitude, and $\UU_2$ is the
error on the $S_2$ amplitude, $\UU_1$ and $\UU_2$ are independent, and
$\UU \sim \mathcal{U}\left(\Uspace\right)$ with
$\Uspace = \interval{0}{1}^2$.

\section{Robust Calibration of the bottom friction}
\label{sec:robust_calibration}
In this section, we are going to study the robust calibration of the
numerical model, on a reduced space, according to the sensitivity
analysis performed above.  On this reduced space, the objective
function $J$ will first be optimised globally on
$\Kspace \times \Uspace$ in~\cref{ssec:glob_mini}, and its results
will be compared with the truth value (used to generate the
observations). Then, after having defined the GP $Z$, we estimate the
conditional minimums and minimisers
in~\cref{ssec:croco_cond_minimum_minimisers}. Finally,
in~\cref{ssec:rr_estimators_croco}, we compute relative-regret
estimates using GP enriched with SUR strategies: reduction of the
augmented IMSE in~\cref{ssec:optim_prob_threshold}, and sampling in
the margin of uncertainty for the estimation of the quantile of the
relative-regret in~\cref{ssec:optim_quantile_rr}.

\subsection{Objective function and global minimum}
\label{ssec:glob_mini}
Based on the sensitivity analysis carried in the previous section, we
are going to consider the following setting for robust calibration.
The control variable is defined as
\begin{equation}
  \kk=\left(\kk^{(1)},\kk^{(2)},\kk^{(3)}\right) \in \Kspace % \quad \kk^{(i)} = \log(\zob^i),
  \quad \Kspace = \interval{\kk_{\min}}{\kk_{\max}}^3
\end{equation}
 where the superscript $(1)$ corresponds to the region
defined as Pebbles, $(2)$ is for the regions defined as Gravel, and
$(3)$ corresponds to the merged regions of Rocks, Sand, Silts, Mud,
and Fine Sands.

% $(2)$ is for the regions defined as Rocks, Gravel,
% and Sand, and the index $3$ is for the Silts, Mud, and Fine Sands.


Following the SA on the tide constituents, the uncertain variable is a
random vector with \num{2} components, representing the error on
amplitude of the $M_2$ and the $S_2$ tide constituents:
\begin{equation}
  \UU = (\UU_1,\UU_2), \quad \UU_i \sim \mathcal{U}(\interval{0}{1}) \text{ for } i=1,2
\end{equation}
As previously, the forward operator of the numerical model gives the sea-surface height $\eta$:
\begin{equation}
  \begin{array}{rcl}
    \mathcal{M}: \Kspace \times \Uspace &\longrightarrow& \mathbb{R}^{N_{\mathrm{obs}}} \\
    (\kk, \uu)& \longmapsto & \mathcal{M}(\kk, \uu) = \left(\eta_{i,t}(\kk, \uu)\right)_{\substack{1 \leq i \leq N_{\mathrm{mesh}} \\ 1 \leq t \leq N_{\mathrm{time}}}} \\ 
  \end{array}
\end{equation}
and the objective function $J$ is the squared difference between the
observations $y$ and the forward operator.
\begin{equation}
  \begin{array}{rcl}
    J: \Kspace \times \Uspace & \longrightarrow & \mathbb{R} \\
    (\kk, \uu) & \longmapsto & \|\mathcal{M}(\kk, \uu) - y \|^2_2
  \end{array}
\end{equation}
For the sake of the study, the original function $J$ has been
optimised over $\Kspace\times\Uspace$, giving
\begin{equation}
  \min_{(\kk,\uu) \in \Kspace \times \Uspace} J(\kk, \uu) = J(\hat{\kk}_{\mathrm{opt}}, \hat{\uu}_{\mathrm{opt}})= \num{29.749}
\end{equation}
for
\begin{align}
  \hat{\kk}_{\mathrm{opt}}                     & = (\num[round-mode=places,round-precision=4]{-3.5161661} , \num[round-mode=places,round-precision=4]{-5.07764701}, \num[round-mode=places,round-precision=4]{-6.34588442})                                                                                                            \\\hat{\uu}_{\mathrm{opt}} & = (\num[round-mode=places,round-precision=4]{0.6347829},\num[round-mode=places,round-precision=4]{0.29890637})
\end{align}
In order to compare the result of this optimisation with the truth
value,~\cref{tab:ktruthkopt} shows the different values of the
different components of $\kk^{\mathrm{truth}}$ and
$\hat{\kk}_{\mathrm{opt}}$.

\begin{table}[!h]
  \centering
  \begin{tabular}{rrrr}\toprule
   Component              & $\kk^{\mathrm{truth}}$                                                                                                                                                                                                                                                                      & Component                & $\hat{\kk}_{\mathrm{opt}}$                                              \\ \midrule
    $\kk_{\mathrm{C}}$    & \num[round-mode=places,round-precision=4]{-3.68887}                                                                                                                                                                                                                                         & $\kk^{(1)}$                  & \num[round-mode=places,round-precision=4]{-3.5161661}                   \\
    $\kk_{\mathrm{G}}$    & \num[round-mode=places,round-precision=4]{-4.96184}                                                                                                                                                                                                                                         & $\kk^{(2)}$                  & \num[round-mode=places,round-precision=4]{-5.07764701}                  \\
    $\kk_{\mathrm{R}}$    & \num[round-mode=places,round-precision=4]{-2.99573}                                                                                                                                                                                                                                         & \multirow{4}{*}{$\left. \vphantom{\begin{tabular}{c}3\\3\\3\\3\end{tabular}}\right\}\kk^{(3)}$} & \multirow{4}{*}{\num[round-mode=places,round-precision=4]{-6.34588442}} \\
    $\kk_{\mathrm{S}}$    & \num[round-mode=places,round-precision=4]{-6.907755}                                                                                                                                                                                                                                        &                          &                                                                         \\
    $\kk_{\mathrm{SF}}$   & \num[round-mode=places,round-precision=4]{-8.804875}                                                                                                                                                                                                                                        &                          &                                                                         \\
    $\kk_{\mathrm{Si,V}}$ & \num[round-mode=places,round-precision=4]{-10.81977}                                                                                                                                                                                                                                        &                          &                                                                         \\ \bottomrule
  \end{tabular}
  \caption[Values of the $\kk$ component of the global optimiser, and
  truth value]{\label{tab:ktruthkopt} Values of the $\kk$ component of
    the global optimiser, and truth value. The region associated with
    $\kk^{(3)}$ is the union of the regions defined with code R, S,
    SF, Si and V.}
\end{table}
We can also notice that
$\uu^{\mathrm{truth}}=(\num{0.5},\num{0.5}) \neq
\hat{\uu}_{\mathrm{opt}}$. This can be explained by the choice of
controlling the parameter on a space of dimension 3, while the truth
value used to generate the observations can be seen as 6
dimensional. The difference in the environmental variables $\hat{\uu}^{\mathrm{opt}}$ and
$\uu^{\mathrm{truth}}$ compensates for the
difference of dimensionality between $\kk\in\Kspace$, the control
variable, and $\kk^{\mathrm{truth}}$.

As mentioned in the previous chapter, $J$ can be expensive to evaluate
computational-wise, so we propose to use GP in order to model it, and
to enrich its design in order to estimate members of the
relative-regret family of estimators.

We will first define the initial design, which will be evaluated by
$J$. A Latin Hypersquare of \num{100} points on
$\Kspace \times \Uspace$ is first sampled in order to construct a GP
that can be used as a surrogate.  In this work, the GP will be
constructed using the Python module
Scikit-learn~\citep{pedregosa_scikit-learn_2011}.

We denote $\mathcal{X}_{\mathrm{LHS}}$ the initial LHS, and $Z$ the
Gaussian Process constructed and fitted using
$\mathcal{X}_{\mathrm{LHS}}$.  We will write
\begin{equation}
  Z \sim \GP\left(m_Z, C_Z\right) \quad C_Z((\kk, \uu),(\kk, \uu)) = \sigma^2_Z(\kk, \uu)
\end{equation}
We then have, for any $(\kk, \uu) \in \Kspace\times\Uspace$,
\begin{equation}
  Z(\kk, \uu) \sim \mathcal{N}\left(m_Z(\kk, \uu), \sigma^2_Z(\kk, \uu)\right)
\end{equation}

By definition, for any $\kk\in \Kspace$ and any $\uu \in \Uspace$, we
have $J(\kk, \uu) \geq 0$.  However, the positivity of the objective
function is not necessarily verified by the surrogate $m_Z$, and thus
the notion of relative-regret is not defined. We have to first ensure
that the GP $Z$ (and consequently $Z^*$) is positive with large enough
probability.  To do so, we are first going to enrich the design near
the conditional minimisers, using the PEI criterion.  Alternatively,
one could have added points to the design according to the reliability
index, defined~\cref{eq:reliability_rho}, and thus look to find points
which have the largest probability of being negative.


\subsection{Conditional minimums and conditional minimisers}
\label{ssec:croco_cond_minimum_minimisers}

In the previous chapter, we defined the conditional minimum as the
minimum of the objective function at a given $\uu\in \Uspace$:
\begin{equation}
  J^* : \uu \mapsto J^*(\uu) = \min_{\kk\in\Kspace} J(\kk, \uu)
\end{equation}
The conditional minimisers function is defined as
\begin{equation}
  \kk^*:\uu  \mapsto \kk^*(\uu) = \argmin_{\kk \in \Kspace} J(\kk, \uu)
\end{equation}
Since both of these functions require an optimisation of the objective
function, they are quite expensive to compute, so, as done before, we
use the GP prediction of $Z$ in order to approximate them:
\begin{align}
  m_{Z^*}: \uu\mapsto &\min_{\kk \in \Kspace} m_Z(\kk, \uu) \\
  \kk^*_{Z}: \uu  \mapsto &\argmin_{\kk \in \Kspace} m_{Z^*}(\uu)
\end{align}
In order to improve the accuracy of those two functions and to ensure
the positivity of $m_Z$, we are first going to enrich the design using
the PEI criterion, as detailed \cref{sec:PEI_criterion}
on~\cpageref{sec:PEI_criterion},
from~\cite{ginsbourger_bayesian_2014}.  Choosing \num{200} additional
points this way, the new obtained design is denoted $\mathcal{X}_0$,
that will serve as the ``initial'' design for the enrichment
procedures described later.

For the sake of this work, we are going to estimate the conditional
minimisers as accurately as possible, using in total \num{750}
evaluations of the objective function $J$. By sampling $\uu_i$ from
$\UU$ for $1\leq i \leq n_{\uu}=\num{2000}$, we can first estimate
$m_{Z^*}(\uu)$ using this set of samples, as shown
in~\cref{fig:contour_Jstar}. As expected from the result of the
optimisation carried in~\cref{ssec:glob_mini}, we can see that the
minimum of $m_{Z^*}(\uu)$ (\emph{i.e.} the global minimum) is not
attained at $(0.5, 0.5)=\uu^{\mathrm{truth}}$ but rather at
approximatively $(\num{0.6},\num{0.3})$.


\begin{figure}[ht]
 \centering
 \includegraphics{\imgpath contour_Jstar_800_.png}
 \caption{\label{fig:contour_Jstar} Conditional minimum $m_{Z^*}(\uu)$
   estimated using the GP $Z$}
\end{figure}
Based on these samples, we can also approximate the distribution of
the random variable $\kk^*(\UU)$ by
$\kk_Z^*(\UU) = (\kk^{(1) *}_Z(\UU), \kk^{(2) *}_Z(\UU), \kk^{(3)
  *}_Z(\UU))$.  \Cref{fig:pairplot} shows the pairwise relations
between the components of the random variable $\kk^*_Z(\UU)$, based on
the samples $\kk^*_Z(\uu_i)$ for $1\leq i \leq n_{\uu}$. On the
diagonal plots is shown the Kernel Density Estimation of the marginal
distributions of $\kk^*(\UU)$, and the corresponding sampled
$m_{Z^*}(\UU)$. The off-diagonal plots show the relations between the
components. We can then observe a negative correlation between
$\kk^{(1) *}_Z(\UU)$ and $\kk^{(2) *}_Z(\UU)$ for instance, indicating that
an increase in one of those value of the friction is compensated by
the decrease in the other component.  We can also notice that the
marginal distribution of the first component $\kk_Z^{(1)*}(\UU)$ seems
to exhibit two modes: one at \num{-3.6}, and one at around
\num{-3.45}, and both are quite close to the value found when
optimising globally:
$\hat{\kk}^{(1)}_{\mathrm{opt}}\approx \num{-3.5}$, which make it a
clear candidate for robust optimisation. For the two other components,
we can observe that the range values taken by the samples
$\kk^*_Z(\uu_i)$ for $1 \leq i \leq n_{\uu}$ is larger, indicating
that for these components, a robust candidate may be less
\emph{identificable}, as discussed~\cref{sec:MPE},~\cpageref{sec:MPE}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{\imgpath pairplot_800_.png}
  \caption[Distribution of the minimisers
  $\kk^*(\UU)$]{\label{fig:pairplot} Estimated distribution of the
    minimisers $\kk^*_Z(\UU)$. The diagonal plots show the KDE of the
    marginal distributions of the minimisers. The non-diagonal plots
    show the pairwise relation between the different components of the
    minimisers }
\end{figure}

\clearpage
\subsection{Relative-regret based estimators}
\label{ssec:rr_estimators_croco}
We are now going to estimate members of the relative-regret family of
estimators as introduced~\cref{def:RR_family}, on
\cpageref{def:RR_family}:
\begin{equation}
  \left\{\kk_{\mathrm{RR,\alpha}} = \argmax_{\kk\in\Kspace} \Gamma_{\alpha}(\kk) \mid \alpha \geq 1\right\}
\end{equation}
where
\begin{equation}
  \Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk,\UU) \leq \alpha J^*(\UU)\right]
\end{equation}
is the probability that $\kk$ is $\alpha$-acceptable.  In the
following, we will assume also a limited budget of \num{500} runs of
the forward model, so after the initial design and the initial
iterations of the PEI criterion, we assume available \num{200}
additional runs.
%\begin{figure}[ht]
%  \centering
%  \input{\imgpath evo_alpha_p_kp.pgf}
%  \caption{\label{fig:evo_alphap_kp} Evolution of the threshold $\alpha_p$ as a function of $p$, and $\kk_{\mathrm{RR},\alpha_p}$}
%\end{figure}

\subsubsection{Optimisation of the probability of exceeding a threshold}
\label{ssec:optim_prob_threshold}
\paragraph{Plug-in approximation of the probability of acceptability}
The $\Gamma_{\alpha}$ function is quite expensive to compute, so we
are going to use a plug-in approach (see~\cref{def:plugin}) in order
to avoid exhaustive computations of the objective function
$J$. Instead, we will use $m_Z$ and $m_{Z^*}$ as surrogates of $J$ and
$J^*$.  For the approximation of the probability, we are going to use
a Sample Average Approximation (SAA) approach, by using a set of
$n_\uu$ i.i.d.\ samples $\{\uu_i\}_{1\leq i \leq n_{\uu}}$.  All in
all, we define
\begin{align}
  \hat{\Gamma}^{\mathrm{PI}}_{\alpha}(\kk) &= \frac{1}{n_\uu}\sum_{i=1}^{n_{\uu}} \mathbbm{1}_{\{m_Z(\kk, \uu_i) \leq \alpha m_{Z^*}(\uu_i)\}}
\end{align}

Maximising this expression yields
\begin{equation}
  \hat{\kk}_{\mathrm{RR},\alpha} = \argmax_{\kk\in\Kspace} \hat{\Gamma}^{\mathrm{PI}}_{\alpha}(\kk) = (\hat{\kk}^{(1)}_{\mathrm{RR},\alpha}, \hat{\kk}^{(2)}_{\mathrm{RR},\alpha},\hat{\kk}^{(3)}_{\mathrm{RR},\alpha})
\end{equation}
In order to get a relevant value of $\hat{\kk}_{\mathrm{RR},\alpha}$,
we are going to reduce sequentially the augmented IMSE, as introduced
in~\cref{sec:evaluation_gamma}.%\todo{La suite risque de changer}
\paragraph{Stepwise reduction of the augmented IMSE}
We will first look to improve the estimation of $\Gamma_{\alpha}$, by
improving the plug-in approximation. We define
$\Delta_{\alpha} = Z - \alpha Z^* \sim \GP(m_{\Delta_{\alpha}},
C_{\Delta_\alpha})$, which is a GP as defined in the previous chapter.
The prediction variance, or mean square error, is
$\sigma^2_{\Delta_{\alpha}}: (\kk, \uu) \mapsto
C_{\Delta_{\alpha}}\big((\kk, \uu), (\kk, \uu) \big)$. In the
following, we are going to estimate the relative-regret, associated
with the value $\alpha=1.3$, meaning that we are looking to maximise
the probability of being within $30\%$ of the optimal value.


To do so, we are going to reduce the augmented IMSE of the random
process $\Delta_{\alpha}$. Recalling that the $\IMSE$ associated with
the GP $\Delta_{\alpha}=Z-\alpha Z^*$ constructed using the design
$\mathcal{X}_n$ is defined as
\begin{equation}
  \IMSE(\mathcal{X}_n) = \int_{\Kspace \times \Uspace} \sigma^2_{\Delta_{\alpha}}(x) \,\mathrm{d}x
\end{equation}
we select the next point to evaluate as:
\begin{equation}
  (\kk_{n+1}, \uu_{n+1}) = \argmin_{(\kk,\uu)\in\Kspace\times \Uspace} \Ex_{Z(\kk, \uu)}\left[\IMSE(\mathcal{X}_n \cup \left((\kk, \uu), Z(\kk, \uu)\right))\right]
\end{equation}
In practice, we will choose the new point $(\kk_{n+1}, \uu_{n+1})$ in
a stochastic manner: first the IMSE is estimated using a Monte-Carlo
method:
\begin{align}
  \IMSE(\mathcal{X}_n) %&= \int_{\Kspace \times\Uspace} \sigma^2_{\Delta_{\alpha}}(\kk, \uu) \,\mathrm{d}\kk\, \mathrm{d}\uu \\
                       &\approx \frac{1}{n_{\IMSE}} \sum_{i=1}^{n_{\IMSE}} \sigma_{\Delta_{\alpha}}^2(\kk_i, \uu_i)
\end{align}
where the points $(\kk_i, \uu_i)$ for $1\leq i \leq n_{\IMSE}$ are
resampled each iteration using a LHS on $\Kspace \times \Uspace$ for
$n_{\IMSE}=100$, and we choose the point with the lowest augmented
IMSE among \num{100} randomly sampled points of
$\Kspace\times\Uspace$.

~\Cref{fig:aIMSE} shows the reduction of the IMSE with the number of
additional iterations, until reaching a number of \num{500}
evaluations of the model in total. Abrupt changes in the IMSE can be
explained by a significant changes in the hyperparameters of the GP.\@

\begin{figure}[ht]
  \centering
  \input{\imgpath aIMSE.pgf}
  \caption{\label{fig:aIMSE} Evolution of the IMSE during the
    enrichment strategy}
\end{figure}


Based on the enriched GP, we can maximise the plug-in approximation
$\hat{\Gamma}_{\alpha}^{\mathrm{PI}}$. We chose here
$n_{\uu}=\num{500}$, and the maximum found is
\begin{equation}
  \label{eq:max_gamma_alpha}
  \max_{\kk} \hat{\Gamma}_{\alpha}^{\mathrm{PI}}(\kk) = \num[separate-uncertainty=true]{0.93\pm 0.0224}
%num[separate-uncertainty=true]{0.848\pm 0.031}
\end{equation}
where the given confidence interval is computed using the normal
approximation of the binomial proportion, at a \num{95}\% level. As we
are using a plug-in approximation, we overlook the intrinsic
uncertainty which is represented by the GP, and thus use directly
$m_Z$ instead of $J$.

Even though the design has been enriched for $\alpha=1.3$, the
resulting GP can be used to evaluate quantities associated with
other thresholds.~\Cref{fig:evo_alphap_kp} shows the maximal
probability reached by $\hat{\Gamma}^{\mathrm{PI}}_{\alpha}$ as a
function of $\alpha$, and the different components of the control
variable $\hat{\kk}_{\mathrm{RR},\alpha}$, all these quantities
computed using the metamodel $m_Z$ after the additional iterations.

\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath palpha.png}
  \caption[Evolution of the maximal probability of
    acceptablility $\max \Gamma_{\alpha}^{\mathrm{PI}}$]{\label{fig:p_alpha} Evolution of the maximal probability of
    acceptablility $\max \Gamma_{\alpha}^{\mathrm{PI}}$, and 95\% CI
    interval associated with the SAA approximation of the estimation
    of the probability.}
\end{figure}

On~\cref{fig:evo_alphap_kp}, we can see that the estimated components
of $\hat{\kk}_{\mathrm{RR},\alpha}(\UU)$ stay in a rather small range
for all $\alpha >1$. Recalling the the global optimiser whose values
are introduced~\cref{tab:ktruthkopt}~\cpageref{tab:ktruthkopt},
$\hat{\kk}_{\mathrm{opt}} = (-3.516, -5.078, -6.3459 )$, we can
observe that $\hat{\kk}^{(1)}_{\mathrm{RR}, \alpha}$ is slightly
higher than $\hat{\kk}^{(1)}_{\mathrm{opt}}$ for all $\alpha > 1$.

% Regarding the second and third components, we can see that
% $\hat{\kk}_{\mathrm{RR},\alpha}$ shows lower values than
% $\hat{\kk}_{\mathrm{opt}}$, but considering the 

\begin{figure}[ht]
  \centering
  %\input{\imgpath evo_alpha_p_kp.pgf}
  \includegraphics{\imgpath evo_alpha_p_kp_bnd.png}
  \caption[Components of $\hat{\kk}_{\mathrm{RR},\alpha}$ after
  augmented IMSE reduction]{\label{fig:evo_alphap_kp} Components of
    $\hat{\kk}_{\mathrm{RR},\alpha}$, using the GP enriched with 200
    additional points minimising the augmented IMSE.}
\end{figure}


% \paragraph{Sampling in the margin of uncertainty}

% In the previous chapter, we defined the probability of coverage of the
% random set $\{\Delta_\alpha \leq 0\}$ to be
% \begin{equation}
%   \pi_{\alpha}(\kk, \uu) = \Phi\left(-\frac{m_{\Delta_{\alpha}}(\kk, \uu)}{\sigma_{\Delta_\alpha}(\kk, \uu)}\right)
%   \end{equation}
%   and the margin of uncertainty at the level $\eta$ to be $\mathbb{M}_{\eta}$:
%   \begin{equation}
%     \mathbb{M}_{\eta} = \left\{(\kk, \uu)\in \Kspace \times \Uspace \mid \frac{\eta}{2} \leq \pi_{\alpha}(\kk, \uu) \leq 1-\frac{\eta}{2}\right\}
%   \end{equation}
%   We can then sample points in this margin, find $K$ centroids that
%   represent statistically those samples, and adjust them in order to
%   reduce the most the uncertainty, as explained
%   in~\cref{alg:sampling_enrichment_star}.  As the margin
%   $\mathbb{M}_{\eta}$ becomes thinner and thinner when adding points
%   to the design (as it is easier to classify points either \emph{in}
%   or \emph{out} of $\{\Delta_{\alpha} \leq 0\}$), the sampling step
                         %                          can become difficult,\todo{finir}
\clearpage
\subsubsection{Optimisation of the quantile of the relative-regret}
\label{ssec:optim_quantile_rr}
\paragraph{Plug-in approximation of the quantile}
Alternatively, for a level of confidence
$p \in \interval{0}{1}$, we can define the quantile function of the
ratio:
\begin{equation}
  q_p(\kk) = Q_{\UU}\left(\mathrm{RR}(\kk, \UU);p \right)
\end{equation}
where $\mathrm{RR}(\kk, \UU)=\frac{J(\kk, \UU)}{J^*(\UU)}$ is the
relative-regret, that we introduce for notational convenience.  As
$\mathrm{RR}$ is unknown directly, we can also apply the plug-in
approach, and define
\begin{equation}
  \mathrm{RR}^{\mathrm{PI}}(\kk, \uu) = \exp\left[m_{\Xi}(\kk, \uu) + \frac{1}{2}\sigma^2_{\Xi}(\kk, \uu)\right]
\end{equation}
where $\Xi$ is the lognormal approximation as defined
in~\cref{eq:log_ratio}, \cpageref{eq:log_ratio}. Once again, the
estimation is done using a set of i.i.d.\ samples of $\UU$:
$\{\uu_i\}_{1\leq i \leq n_{\uu}}$
\begin{equation}
  \label{eq:RRPI}
  \hat{q}_p^{\mathrm{PI}}(\kk) = {\mathrm{RR}}^{\mathrm{PI}}(\kk, \uu)_{(\left[n_{\uu}p\right])}
\end{equation}
where the subscript indicates the order statistic, \emph{i.e.} the
$\left[n_{\uu}p\right]$ smallest value of
$\{\mathrm{RR}^\mathrm{PI}(\kk,\uu_i)\}_{1\leq i\leq n_\uu}$ (with
$[\cdot]$ as the rounding operator). 

As defined in the previous chapter, the minimiser of
$\hat{q}_p^{\mathrm{PI}}$ is the member of the relative-regret family
associated with $\alpha_p$:
\begin{equation}
  \hat{\kk}_{\mathrm{RR},\alpha_p} = \argmin_{\kk \in \Kspace} \hat{q}^{\mathrm{PI}}_p(\kk) = (\hat{\kk}^{(1)}_{\mathrm{RR},\alpha_p}, \hat{\kk}^{(2)}_{\mathrm{RR},\alpha_p},\hat{\kk}^{(3)}_{\mathrm{RR},\alpha_p})
\end{equation}
\paragraph{Sampling-based method for the estimation of the quantile}
We are now going to treat this problem using a sampling-based method,
as introduced~\cref{ssec:quantile_qeakmcs}, which is derived
from~\cite{razaaly_rare_2019}.  The log-normal approximation of the
relative-regret, as introduced~\cref{ssec:lognormal_approx}, allow us
to define the random process $\Xi$ as
\begin{equation}
  \log \frac{Z(\kk, \uu)}{Z^*(\uu)} \approx \Xi(\kk, \uu) \sim \mathcal{N}\left(m_{\Xi}(\kk, \uu),\sigma^2_{\Xi}(\kk, \uu)\right)
\end{equation}

Using the Gaussian nature of $\Xi$, we define $\mathfrak{q}_1$,
$\mathfrak{q}_2$ and $\mathfrak{q}_3$, as a lower bound, a central
estimate and an upper bound respectively, of the value
$\min_\kk \hat{q}_p(\kk)=\hat{\alpha}_p$, minimum which is attained at
$\hat{\kk}= \argmin_{\kk\in\Kspace} \hat{q}_p(\kk)$. Following the
notation introduced in the previous chapter, we chose $K_q = 3$.
With $k=\Phi^{-1}(1 - 0.025)$ the quantile of order \num{0.975} of the
standard normal distribution, we define
\begin{align}
  \log {\mathfrak{q}}_1 &= \left(m_{\Xi}(\tilde{\kk}, \uu) - k \sigma_{\Xi}(\tilde{\kk}, \uu)\right)_{([n_{\uu}p])} \approx Q_U(m_{\Xi}(\tilde{\kk}, \UU) - k \sigma_{\Xi}(\tilde{\kk}, \UU); p) \\
  \log \mathfrak{q}_2 &= \log \left(\min_{\kk\in\Kspace}\hat{q}^{\mathrm{PI}}_p(\kk) \right) = \log \hat{q}^{\mathrm{PI}}_p(\tilde{\kk}) = \log \hat{\alpha}_p \\
  \log \mathfrak{q}_3 &= \left(m_{\Xi}(\tilde{\kk}, \uu) + k \sigma_{\Xi}(\tilde{\kk}, \uu)\right)_{([n_{\uu}p])} \approx Q_U(m_{\Xi}(\tilde{\kk}, \UU) + k \sigma_{\Xi}(\tilde{\kk}, \UU); p)
\end{align}

We can then define the margin of uncertainty of level $\eta$,
associated with each of those values:
\begin{equation}
  \mathbb{M}_{\eta}(\mathfrak{q}_l) = \left\{(\kk, \uu) \mid \frac{\eta}{2} \leq \Phi\left(-\frac{m_{\Xi}(\kk, \uu) - \log \mathfrak{q}_l}{\sigma_{\Xi}(\kk, \uu)}\right) \leq 1 - \frac{\eta}{2}\right\}
\end{equation}
and sample points in each of those margins, perform a statistical
reduction method in order to get $K_{\mathbb{M}}$ points for each
margin, adjust them and finally evaluate the $3\cdot K_{\mathbb{M}}$
points by the objective function.


Numerically speaking, the margins revealed themselves to be quite
small, so we chose a level $\eta=0.0005$ in order to increase its
size.  Also, we chose to change the number of points in each margin
$K_{\mathbb{M}}$ along the iterations, by starting with a small number
of points, $K_{\mathbb{M}}=3$, and by increasing it, until
$K_{\mathbb{M}}=10$. We can see on~\cref{fig:vol_Meta} the estimated
volume of the margin of uncertainty (\emph{i.e.} its measure according
to Lebesgue's measure on $\Kspace \times \Uspace$), using Monte-Carlo
method, along with the 95\% confidence intervals associated with its
estimation (Wilson's score method as introduced~\cite{wilson_probable_1927} to
account for the small relative volume of the margin of
uncertainty). We can see that first, its volume increases
slightly. This can be seen as an exploration phase, where the
additional evaluations change the hyperparameters of the GP, which
affects the estimation of the conditional minimums, and the candidate
quantiles $\mathfrak{q}_l$, for $1\leq l \leq 3$. After enough
additional points, the volume of the margin decreases, as points are
added that do not change significantly the other estimations.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.9]{\imgpath volume_margin_uncertainty.png}
  \caption[Volume of the margin of uncertainty associated with
  $\mathfrak{q}_2 = \hat{\alpha}_p$]{\label{fig:vol_Meta} Volume of
    the margin of uncertainty associated with
    $\mathfrak{q}_2 = \hat{\alpha}_p$, and 95\% confidence intervals
    of its estimation}
\end{figure}


Using the GP conditioned on the final design, for $p=0.95$ we can compute
\begin{align}
  \hat{\alpha}_p &= \num{1.3791} \\
  \interval{\mathfrak{q}_1}{\mathfrak{q}_3} &= \interval{1.36618}{1.391868} \nonumber
\end{align}
and using the same GP,~\cref{fig:hat_alpha_p} shows the estimation of
the threshold for other values of $p$.  One main difference between
the confidence interval given there through $\mathfrak{q}_1$ and
$\mathfrak{q}_3$ and the one given~\cref{eq:max_gamma_alpha} is that
the former translates the uncertainty originating from the Gaussian
Process, while the latter accounts for the error in the \emph{sample
  average approximation} of the probability
$\hat{\Gamma}_{\alpha}^{\mathrm{PI}}$.
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.9]{\imgpath hatalphap.png}
  \caption[Estimation of the threshold $\alpha_p$, depending on the
  level $p$]{\label{fig:hat_alpha_p} Estimation of the threshold
    $\alpha_p$, depending on the level $p$. The lower and upper bounds
    $\mathfrak{q}_1$ and $\mathfrak{q}_3$ are also displayed}
\end{figure}

Finally,~\cref{fig:theta_rr_alpha} shows the different components of
$\hat{\kk}_{\mathrm{RR},\alpha_p}$ for different $p> 0.5$.
\begin{figure}[ht]
  \centering
  \includegraphics[]{\imgpath theta_rr_ap.png}
  \caption{\label{fig:theta_rr_alpha} Relative-regret based estimates
    $\hat{\kk}_{\mathrm{RR}, \alpha_p}$, depending on the level $p$}
\end{figure}
We can notice that similarly as in~\cref{fig:evo_alphap_kp}, the range
of values taken by each component is rather small. We can however
notice that the first component of $\hat{\kk}_{\mathrm{RR}, \alpha_p}$
is again slightly larger than the one the global optimum
$\hat{\kk}_{\mathrm{opt}}$.


All in all, this study suggests that choosing a value slightly higher
than the global optimiser for the first component (which corresponds
to Pebbles) would be more robust, than keeping this
minimiser. Regarding the other components, their respective influence
(given by the SA) and the relative-regret estimates would point toward
a value close or equal to the global optimiser.

\section{Partial conclusion}
\label{sec:partial-conclusion}
In this chapter, we addressed the problem of calibration under
uncertainties of the numerical model CROCO.\@ After having defined the
control and environmental parameters, we performed different studies,
in order to have a better understanding of the behaviour of the
forward model.

First, we optimised the objective function on a high-dimensional input
space, but without external uncertainties. We chose then to segment
the domain according to the type of sediments that can be found at the
bottom, and by performing a sensitivity analysis, we could reduce
further the dimension of the input space.
A similar study has been done in order to reduce the dimension of the
environmental parameter as well.

Finally, we applied some of the methods that rely on GP as introduced
in~\cref{chap:adaptative_design_gp}, in order to explore further the
behaviour of the numerical model. First, to have a better estimation
of the conditional minimum and minimisers we enriched the design using
the PEI criterion. Then, in order to get robust estimators of the
bottom friction, we used two different approaches to enrich the GP. In
the first one, we reduced iteratively the augmented IMSE in order to
improve the plug-in approximation of the probability of exceeding a
threshold. On the other hand, in order to minimise a specific quantile
of the relative-regret, we defined and sampled in the margin of
uncertainty in order to add points by batches. This finally lead to
relative-regret estimates of the bottom friction, which differ from
the global minimiser: according to this study, taking a value slightly
larger for the first component of the bottom friction lead seemingly
to a more robust solution.

From a performance point of view, GP can indeed reduce the
computational requirements needed to compute relative-regret
estimates. However as mentioned before, GP do not scale particularly
well for problem of more than a dozen input variables, rendering a
dimension reduction almost mandatory for tractability.

The computational cost of the enrichment process, either using
stepwise or sampling procedures, can quickly become
non-negligible. Indeed, the estimation of the parameters of the
distributions of $\Delta_{\alpha}(\kk, \uu)$ and $\Xi(\kk, \uu)$
require at each point $(\kk, \uu)$ a global optimisation of $m_Z$,
then the prediction and covariance matrix of this point and the global
optimiser. In itself, this procedure is not that expensive. However,
it needs to be performed a large number of times: for a single
evaluation of the augmented IMSE for instance, estimation of nested
integrals are needed, thus the number of computations of the
parameters of $\Delta_{\alpha}$ grows quite large, even more when
considering an optimisation of the augmented IMSE on
$\Kspace \times\Uspace$. Similarly, the volume of the margin of
uncertainty can be very small compared of the volume of the whole
space $\Kspace \times \Uspace$, thus without specific sampling
schemes, it can quickly become a computational bottleneck as well,
even more so when the prediction variance becomes small.



% END OF CHAPTER --------------------
\markchapterend


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
