\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter5/#1}
}
\newcommand{\CROCO}{CROCO}
\newcommand{\zob}{z_b}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\imgpath{/home/victor/acadwriting/Manuscrit/Text/Chapter5/img/} 


\begin{document}
% \subfileLocal{\dominitoc} \subfileLocal{\setcounter{chapter}{4}}
% \subfileLocal{\chapter{Application to the numerical coastal model
% CROCO}}
\chapter{Application to the numerical coastal model \CROCO}
\label{chap:croco}
\minitoc
\newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% La section 4.5 est un peu mieux introduite qu’avant mais ça reste
% assez difficile à suivre. Bon c’est un peu normal, c’est dense.

% Je ne sais si une sorte de diagramme permettrait de bien expliquer
% ce qui va être fait dans cette sous-section, les outils math qui
% vont être utilisés, à quel niveau …

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Introduction}
\label{sec:intro_croco}

In this chapter, we will study the problem of calibration under
uncertainties of the bottom friction of the ocean bed off the coast of
France, from the English Channel to the Bay of Biscay. This will be
realised using a realistic numerical model based on
CROCO\footnotemark[1] (Coastal and Regional Ocean COmmunity model).

Since the bottom friction depends directly on the size of the
asperities on the ocean bed, the length scales involved in this
process are way smaller than the scales of the computational grid. In
consequence, the numerical model does not solve the equations of
motions of the fluid around those asperities. Instead, the dissipation
coming from the associated rugosity is parametrized at every cell of
the mesh.

The bottom friction has been identified as a crucial parameter that,
if ill-specified, limits the accuracy of the
predictions~\citep{sinha_principal_1997,kreitmair_effect_2019},
especially in shallow regions; consequently, there has been an effort
to control this parameter in various studies, for instance
in~\cite{das_variational_1992,das_estimation_1991,boutet_estimation_2015}.
We will detail how bottom friction affects the oceanic circulation
in~\cref{ssec:modelling_bottom}, in order to get a first insight on
the regions that may influence the most the calibration.

The deterministic problem of calibration will then be addressed in
\cref{sec:deterministic_calibration_bott}, by first defining the
objective function and the input space. We will then calibrate the
model without external uncertainties using adjoint-based gradient, in
high-dimension ($\approx$\num{15000}).

However for such problems, as the parameter may be spatially
distributed and thus high-dimensional, any estimation procedure may
become quickly expensive.  In consequence, instead of considering each
grid cell individually, we will segment the geographical input space
in different independent regions, which are based on the type of
sediments listed at the bottom of the water.

In this problem of calibration we will assume that the uncertainties
take the form of an environmental parameter which perturbates the
amplitude of some tidal constituents.  In order to quantify the
influence of each of the sediment-based regions and the influence of
each of the components of the environmental variable, we will carry a
global sensitivity analysis in~\cref{sec:sensitivity-analysis}.

Finally based on this study, we will reduce significantly the input
space, and then apply some of the methods proposed in the previous
chapter, in order to get a robust estimation of the bottom friction
using Gaussian processes in~\cref{sec:robust_calibration}.


\section{\CROCO\ and bottom friction modelling}
\label{sec:croco_bottom_fr}
\CROCO{}\footnote{\CROCO\ and CROCO\_TOOLS are provided by
  \url{https://www.croco-ocean.org}} (Coastal and Regional Ocean
COmmunity model) is a numerical model that describes the motion of the
ocean by solving the \emph{primitive equations}, which are simplified
versions of the Navier-Stokes equations, taking into account the
particular scales at play at the surface of the Earth. \CROCO{} has
been developed upon
ROMS\_AGRIF %\footnote{\url{https://www.myroms.org/},\url{http://www-ljk.imag.fr/MOISE/AGRIF/}}
(Regional Ocean Modeling System, Adaptive Grid Refinement in
Fortran~\cite{debreu_two-way_2012}), and is designed to be coupled
with other modelling systems, such as atmospheric, biological or
ecosystem models.

% gradually including algorithms from MARS3D (sediments) and HYCOM
% (vertical coordinates). An important objective for \CROCO\ is to
% resolve very fine scales (especially in the coastal area), and their
% interactions with larger scales. It is the oceanic component of a
% complex coupled system including various components, e.g.,
% atmosphere, surface waves, marine sediments, biogeochemistry and
% ecosystems.


\subsection{Parameters and configuration of the model}
\label{sec:geographical_setting}

The configuration used in this thesis is based on the one used
in~\cite{boutet_estimation_2015}. The spatial domain ranges from
\ang{9}W to \ang{1}E (comprising \num{139} internal points in this
direction) and from \ang{43}N to \ang{51}N (\num{164} in this
direction), and spans most of the Bay of Biscay, the English Channel
and the eastern part of the Celtic Sea.  The resolution is
\SI{1/14}{\degree}, which leads to a mesh size between
\SI{5}{\kilo\metre} and \SI{6}{\kilo\metre}. The bathymetry map and
the spatial domain is shown~\cref{fig:depth_maps}. The ocean can be
split roughly in two regions, based on its depth : the region near the
coasts which corresponds to the continental shelf, where the water
depth is less than \SI{200}{\meter}, and the offshore region of the
Bay of Biscay, where the depth is closer to \SI{5000}{\meter}.
\begin{figure}[ht]
  \centering \includegraphics{\imgpath depth_maps_log.png}
  \caption[Bathymetry chart of the domain
  modelled]{\label{fig:depth_maps} Bathymetry used in \CROCO, and
    geographical landmarks. The continental shelf corresponds roughly
    to the area with depth less than \SI{200}{\meter} (green hue),
    while the abyssal plain has a depth larger than \SI{4000}{\meter}
    (blue hue)}
\end{figure}


\CROCO{} can solve the fluid motion equations in 3D, but in this
configuration, solves the rotating shallow water equations instead,
which are obtained by vertically averaging the primitive equations,
leading to:
\begin{align}
  \left\{
  \begin{array}{lll}
    \pfrac{\mathbf{v}}{t} + (\mathbf{v} \cdot \nabla )\mathbf{v} + 2 \bm{\Omega} \wedge \mathbf{v} & = & -g\nabla H + \frac{\bm{\tau}_b}{\rho H} + F \\
    \pfrac{\zeta}{t} + \nabla (H \cdot \mathbf{v})                                                     & = & 0
  \end{array}
                                                   \right.
\end{align}
where $\mathbf{v} = (v_x,v_y)$ is the velocity field of the fluid,
$\bm{\Omega}$ is the rotational angular vector of Earth. $H$, the
total water column height and $\zeta$, the free-surface height
(relative to the geoid) satisfy the relation $H = \zeta + b$ where $b$
is the bathymetry.  The effect of the bottom topography and the
friction are modelled using $g$ the gravitational constant, $\rho$ the
fluid density, and $\bm{\tau}_b$ the shear stress at the
bottom. Finally, $F$ represents the external forcing of the model.
The forcing due to the tides is done at the boundaries. The bottom
friction affects the circulation through $\bm{\tau}_b$, and different
parameterizations of this stress can be derived.

\subsection{Modelling of the bottom friction}
\label{ssec:modelling_bottom}
In \CROCO, the shear stress at the bottom is modelled using a quadratic drag
coefficient $C_d$:
\begin{equation}
  \label{eq:bottom_stress_tau}
  \bm{\tau}_b= -C_d \|\mathbf{v}_b\|\mathbf{v}_b 
\end{equation}
where $\mathbf{v}_b$ is the velocity at the bottom, so in the case of
the Shallow Water equations, $\mathbf{v}_b = \mathbf{v}$.  The drag
coefficient can in turn be formulated as a function of the water
column height and the \emph{bottom roughness} $\zob$ by assuming a
logarithmic profile of the velocity at bottom (a derivation can be
found in~\cite{le_bars_amandes_2010} for instance)
\begin{equation}
  \label{eq:quadratic_friction_vonkarman}
  C_d = \left(\frac{\kappa}{\log\left(\frac{H}{\zob}\right) - 1}\right)^2% \text{for } C_d \in [C_d^{\min}, C_d^{\max}]
\end{equation}
where $\kappa$ is the Von K\'arm\'an constant, usually taken equal to
$0.41$.  The bottom roughness $\zob$, or \emph{rugosity} in this
document, can be interpreted as the size of the turbulent layer at the
bottom, induced by the asperities of the sediments.
\cite{boutet_estimation_2015} shows that in a calibration context,
controlling the rugosity $\zob$ yields better result than controlling
the drag coefficient $C_d$ due to the influence of the water column
height $H$.
On~\cref{fig:cd_zob} is shown the drag coefficient $C_d$ as a function
of the roughness $\zob$ of the ocean floor, for different heights of
the water column $H$.
\begin{figure}[ht]
  \centering \input{\imgpath cd_zob.pgf}
  \caption[Drag coefficient $C_d$ as a function of the height and the
  roughness]{\label{fig:cd_zob} Drag coefficient $C_d$ as a function
    of the column water height and the roughness at the bottom}
\end{figure}


We can see that the higher the water column height, the less variation
appears when adjusting the bottom roughness $\zob$.  Considering the
physical properties of the bottom friction and the types of sediments,
it can be expected that the English Channel, and at a lesser extent
the rest of the continental shelf are the areas which are the most
influential for the calibration.

We are now going to develop more precisely which inputs we are going
to consider for the numerical problem of calibration.


\subsection{Definition of the control and environmental parameters}
\subsubsection{Bottom roughness and sediments size}
In this work, we are going to use a twin experiment setup: the
calibration will be performed with respect to some observation
generated using CROCO.  This observation
$y\in\mathbb{R}^{N_{\mathrm{obs}}}$ is computed using a specific
configuration of the forward model, meaning that we are going to
define a \emph{truth} value for the bottom friction.


To do so, we are going to make the assumption that the size of the
turbulent layer at the bottom is equal to the size of the sediments
there, so the rugosity is directly linked to the type of sediment
found on the ocean bed. \Cref{tab:size_sediments} presents a coarse
classification, along with the typical size of the sediments that can
be found, that will serve as \emph{truth value}:
$\zob^{\mathrm{truth}}$.  % We can see that the rugosity spans several
% order of magnitude, hence it may be worth considering controlling the
% logarithm of the rugosity instead of the rugosity itself.

\begin{table}[!ht]
  \centering
  \begin{tabular}{rrrl} \toprule Code & Description & Size of the
    majority of particles             & $\zob^{\mathrm{truth}}$                                                                                  \\ \midrule
    R                                 & Rock        & Larger                                                         & \SI{50}{\milli\meter}     \\
    C                                 & Pebble      & $>$\SI{20}{\milli\metre}                                       & \SI{25}{\milli\meter}     \\
    G                                 & Gravel      & $\interval{\SI{20}{\milli\metre}}{\SI{2}{\milli\metre}}$       & \SI{7}{\milli\meter}      \\
    S                                 & Sand        & $ \interval{\SI{2}{\milli\metre}}{\SI{0.5}{\milli\metre}}$     & \SI{1}{\milli\meter}      \\
    SF                                & Fine Sand   & $ \interval{\SI{0.5}{\milli\metre}}{\SI{0.05}{\milli\metre}}$  & \SI{1.5e-1}{\milli\meter} \\
    Si                                & Silt        & $ \interval{\SI{0.05}{\milli\metre}}{\SI{0.01}{\milli\metre}}$ & \SI{2e-2}{\milli\meter}   \\
    V                                 & Muds        & $< \SI{0.05}{\milli\metre}$                                    & \SI{2e-2}{\milli\meter}
                                                                                                                                                 \\ \bottomrule
            %             A           & Clay        & $< \SI{0.01}{\milli\metre}$                                    & \bottomrule
  \end{tabular}
  \caption[Types and sizes of each sediment
  class]{\label{tab:size_sediments} Type of sediments and size of the
    majority of particles for each type of sediment. Data source:
    SHOM, used under
    \href{https://creativecommons.org/licenses/by-sa/4.0/}{CC BY-SA
      4.0} license}
\end{table}

Based on the documentation of the SHOM\footnote{Service hydrographique
  et océanographique de la Marine, \url{https://www.shom.fr/fr}},
\cref{fig:sediments_reduced} shows a map of the repartition of the
different types of sediments introduced there. A more complete chart
with a finer classification of the types of sediments can be found in
the appendix, on~\cref{fig:sediments_full}.
\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath sediments_reduced.png}
  \caption[Repartition of the sediments on the ocean
  floor]{\label{fig:sediments_reduced} Repartition of the sediments on
    the ocean floor. Data source: SHOM, used under
    \href{https://creativecommons.org/licenses/by-sa/4.0/}{CC BY-SA
      4.0} license}
\end{figure}


Based on this classification, we can see that most of the ocean floor
of the studied domain is composed of sand. Even though siltic soil is listed,
it is only scarcely present. The figure also shows that the largest
sediments are rocks but are mostly located in the Bay of Biscay, near
the boundary of the continental shelf. Pebbles however are mostly
located in the shallow region in the English Channel, thus it may be
expected that controlling the roughness in the regions listed as
pebbles will affect significantly the water circulation, and thus the sea
surface height. Incidentally, we can notice the inverse correlation
between the size of the sediments, and the depth at which they are
found.

According to~\cref{tab:size_sediments}, the rugosity $\zob$ spans
multiple orders of magnitude, so we are then going to define the control
variable $\kk$ as
\begin{equation}
  \kk = \log \zob \in \Kspace = \interval{\kk_{\min}}{\kk_{\max}}^p
\end{equation}
where $\kk_{\min} = \log(10^{-5}) \approx -11.5$, and
$\kk_{\max} = \log(5\cdot 10^{-2}) \approx -3$.  The dimension of
$\Kspace$ is noted $p$, and will be specified later depending on the
chosen segmentation.

\subsubsection{Tidal modelling and uncertainties}
\label{ssec:tidal_modelling}
The ocean, especially near the English Channel is driven by tidal
forces that produce currents at the surface. As a periodic signal, the
tidal forcing is usually analysed harmonically, in order to separate
its influence by frequency. In \CROCO, this forcing can come from the
TPXO model of tides~\citep{egbert_efficient_2002}, and in our
configuration, we use the 5 primary harmonic constituents
as described \cref{tab:tides_components}.
\begin{table}[!h]
  \centering
  % % Chose order from the rank in the TPXO file :
                                                                                                               %                                                                                                                "M2 S2 N2 K2 K1 O1 P1 Q1 Mf Mm"
                                                                                                               %                                                                                                                " 1  2  3  4  5  6  7  8  9 10"
  \begin{tabular}{rrr}\toprule
    Darwin Symbol & Period (h)   & Species                           \\ \midrule
    $M_2$         & 12.4206      & Principal Lunar Semidiurnal       \\
    $S_2$         & 12           & Principal Solar Semidiurnal       \\
    $N_2$         & 12.65834751  & Larger Lunar Elliptic Semidiurnal \\
    $K_2$         & 11.96723606  & Lunisolar Semidiurnal             \\
    $K_1$         & 23.93447213  & Lunar Diurnal                     \\
            %             \midrule
            %             $O_1$         & 25.81933871  & Lunar Diurnal                     \\ 
            %             $P_1$         & 24.06588766  & Solar Diurnal                     \\
            %             $Q_1$         & 26.868350    & Larger Lunar Elliptic Diurnal     \\
            %             $M_f$         & 13.660830779 & Lunisolar Fortnightly             \\
            %             $M_m$         & 27.554631896 & Lunar Monthly                     \\
    \bottomrule
  \end{tabular}
  \caption{Harmonic constituents used in the configuration}
  \label{tab:tides_components}
\end{table}

By perturbating some properties of those tide components, we can
artificially introduce some error in the numerical model, \emph{i.e.}
a parametric misspecification that we will define as the environmental
variable. In this work, this takes the form of a small multiplicative
error on the amplitude of the different components of the tide. Let
$\uu = (\uu_1,\dots,\uu_5)\in \Uspace = \interval{0}{1}^5$ be the
environmental variable, that will be considered as a random variable
later, and let $A_k$ be the amplitude of the $k$th component of the
tide.  The perturbated amplitude $\tilde{A}_k$ is defined as
\begin{equation}
  \label{eq:tide_error}
  \tilde{A}_k(\uu_k) = A_k (1 + 0.01(2\uu_k - 1))
\end{equation}
for $1\leq k\leq 5$.
Based on this definition, the perturbated amplitude varies from
$\tilde{A}_k(0) = 0.99A_k$ to $\tilde{A}_k(1) = 1.01A_k$, for every
$1\leq k \leq 5$. We define also $\uu^{\mathrm{truth}}$ as
the vector whose all its components are set to \num{0.5}, so when no
amplitude is perturbated: $\tilde{A}_k(\uu_k^{\mathrm{truth}}) = A_k$
for $1\leq k \leq 5$.


In the next section, we are going to estimate the parameter $\kk$ by
minimising the objective function using a gradient-descent algorithm.
\section{Deterministic calibration of the bottom friction}
\label{sec:deterministic_calibration_bott}
The calibration of the bottom friction will first be studied without
external uncertainties, which corresponds to an unperturbated tidal
forcing. In consequence, the environmental variable is set to
$\uu^{\mathrm{truth}}$.

In ocean modelling, the free-surface height $\zeta$ is often used as
an observable quantity, because it can be measured using satellites or
tide gauges near the coasts. We consider then
$\zeta\in \mathbb{R}^{N_\mathrm{obs}}$ as the output of the numerical
model.

Following the notations introduced in~\cref{chap:inverse_problem} for
the model, let us define
$(\mathcal{M}(\cdot,\uu^{\mathrm{truth}}),\Kspace)$ as the numerical
model to calibrate. The forward operator
$\mathcal{M}(\cdot, \uu^{\mathrm{truth}})$ is defined by
\begin{equation}
  \begin{array}{rcl}
    \mathcal{M}: \Kspace  &\longrightarrow& \mathbb{R}^{N_{\mathrm{obs}}} \\
    \kk & \longmapsto & \mathcal{M}(\kk, \uu^{\mathrm{truth}}) = \left(\zeta_{t,i}(\kk, \uu^{\mathrm{truth}})\right)_{\substack{1 \leq i \leq N_{\mathrm{mesh}} \\ 1 \leq t \leq N_{\mathrm{time}}}} \\ 
  \end{array}
\end{equation}
where $\zeta_{t,i}(\kk,\uu)$ is the free-surface height of the ocean
at the mesh point $i$, and at the time-step $t$, obtained using the
model and the bottom friction associated with $\kk$ and the
environmental variable $\uu$, and
$N_{\mathrm{mesh}} \cdot N_{\mathrm{time}} = N_{\mathrm{obs}}$. In this configuration,
$N_{\mathrm{time}} =\num{49}$, and corresponds to the number of
records saved, while $N_{\mathrm{mesh}}=\num{15684}$ is the number of
cells of the computational grid not located on land. The time step of
the simulation in itself is \SI{10}{\second}, and its total duration
is \SI{24}{\hour}, meaning that the water height $\zeta$ is saved
every \SI{30}{\minute}.

\subsection{Twin experiment setup}
Recalling the definition of a twin experiment setup, the observation
$y$ is generated using the numerical model, and a predefined truth
value $\kk^{\mathrm{truth}}$.  This means that the ``physical model''
is defined using the forward operator $\mathscr{M}$, based on the
forward numerical model, evaluated with the environmental parameter
$\uu^{\mathrm{truth}}$.
\begin{equation}
  \label{eq:twin_exp}
  \begin{array}{rcl}
    \mathscr{M}: \Kspace &\longrightarrow & \mathbb{R}^{N_{\mathrm{obs}}} \\
    \kk & \longmapsto &\mathscr{M}(\kk) = \mathcal{M}(\kk, \uu^{\mathrm{truth}})
  \end{array}
\end{equation}

Based on the forward operator $\mathscr{M}$, we can generate the
observations $y\in \Yspace = \mathbb{R}^{N_{\mathrm{obs}}}$ using the
truth value $\kk^{\mathrm{truth}}=\log \zob^{\mathrm{truth}}$, as
defined \cref{tab:size_sediments}:
\begin{equation}
  y = \mathscr{M}(\kk^{\mathrm{truth}}) = \mathcal{M}(\kk^{\mathrm{truth}}, \uu^{\mathrm{truth}})
\end{equation}

In the following, if the $\uu$ argument is omitted, it means that the
model, or subsequent functions are evaluated with
$\uu^{\mathrm{truth}}$.
\subsection{Cost function definition}
Once the observation $y \in \mathbb{R}^{N_{\mathrm{obs}}}$ has been generated, we can
define the objective function $J$:
\begin{align}
  \label{eq:cost_fun_definition}
  J(\kk) &= \sum_{t=1}^{ N_{\mathrm{time}}}\sum_{i=1}^{N_{\mathrm{mesh}}}  \left(\zeta_{t,i}(\kk,\uu^{\mathrm{truth}}) - y_{t, i}\right)^2 \\
         &= \|\mathcal{M}(\kk,\uu^{\mathrm{truth}}) - y\|_2^2
\end{align}
Equivalently, as mentioned in~\cref{chap:inverse_problem}, by assuming
that the distribution of the (random) observation vector is known and
$Y \mid \kk \sim \mathcal{N}(\mathcal{M}(\kk), I)$ (with $I$ being the
identity matrix of dimension $p$), $J$ is proportional to the negative
log-likelihood of the data.

\subsection{Gradient-descent optimisation}
\label{ssec:optim_gradient}
The optimisation is carried using M1QN3, a version of a
gradient-descent procedure, as described
in~\cite{gilbert_numerical_1989}. We can first look to control $\zob$
at every cell of the mesh: $\kk = (\kk_1,\cdots, \kk_p)$ where
$\kk_i = \log\zob^i$ and $p=\num{15684}$.

Due to the large number of points whose friction can be controlled, a
finite-difference method to get the gradient is unfeasible. Instead,
Tapenade~\citep{hascoet_tapenade_2013}, an Automatic Differentiation
tool has been used in order to get the gradient (with respect to
$\kk$) of the cost function $J$ using the adjoint method, as
described~\cref{sec:calibration_adjoint_optimization}. The starting
point of the optimisation is $\kk = \log \num{5e-3}$, and the
procedure is stopped after \num{400} iterations. The estimated control
parameter (\emph{i.e.} the minimiser found) is
shown~\cref{fig:optimization_map_399}.  The evolution of the cost
function and the squared norm of the gradient during the optimisation
procedure is shown~\cref{fig:ctrl_true}.

\begin{figure}[ht]
  \begin{subfigure}{\textwidth}
  \centering
  \includegraphics{/home/victor/optimisation_dahu/optim_sediments/map_lognorm_400.png}
  \caption[Optimisation of $\zob$ on the whole
  space]{\label{fig:optimization_map_399} Optimisation of $\zob$ on
    the whole space using gradient obtained via adjoint method, after
    $400$ iterations.}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
  \input{/home/victor/optimisation_dahu/optim_sediments/ctrl_true400.pgf}
  \caption{\label{fig:ctrl_true} Evolution of the cost function and
    the squared norm of the gradient}
\end{subfigure}
\caption{Calibration of the bottom friction using gradient-descent
  with well-specified environmental variables}
\end{figure}
By comparing the result of the
optimisation~\cref{fig:optimization_map_399} with the sediment
chart~\cref{fig:sediments_reduced} and the bathymetry
map~\cref{fig:depth_maps}, we can have a first overview on which
regions of the domain are properly estimated (\emph{i.e.} where the
estimation is close to the truth value).  On a first look, we can see
that the abyssal plain (the deep region off the Bay of Biscay) remains
mostly unaffected by the optimisation, while the continental shelf,
except for some parts of the English Channel, is well retrieved.


On~\cref{fig:optimisation_type_sediments} is shown the value of the
optimised rugosity $\zob$, depending on the type of sediment
associated. Each point on this figure corresponds then to a mesh
point.
We can see that indeed, as~\cref{fig:optimization_map_399} shows,
points of the mesh corresponding to sand, \emph{i.e.} most of the
continental shelf, tends to get closer to the truth value
$\zob^{\mathrm{truth}}$. For silts and muds however, the procedure
did not change significantly their roughness, and thus stays close to
the initial value.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.9]{\imgpath optimisation_type_sediments.pdf}
  \caption[Final values of the optimisation procedure, based on the
  sediment type]{\label{fig:optimisation_type_sediments} Results of
    the optimisation procedure, depending on the type of
    sediments. The starting value in the optimisation procedure for
    $\zob$ is \SI{5e-3}{\meter}}
\end{figure}


This can be probably explained by the great depth at which those
sediments lay, and thus it mitigates their influence on the drag
coefficient per~\cref{eq:quadratic_friction_vonkarman}. In the English
Channel, the size of the pebbles is quite well retrieved albeit a bit
underestimated, but the points mapped to gravel do seem to compensate:
on the northern part of the channel the size of the gravel is
overestimated, while it is underestimated on the southern part.
Finally the rocks appear to be hard to estimate properly, as only about
3\% of the domain is listed as rocks, and their corresponding $\zob$
is quite large in contrast to the other sediments.


\clearpage
The optimisation procedure has been done also in the misspecified
case, \emph{i.e.}
$\uu^b \neq \uu^{\mathrm{truth}}=(\num{0.5}, \num{0.5})$ where the
first component refers to the error on $M_2$, and the second on $S_2$
(the others are supposed to be well-specified).

~\Cref{fig:optim_croco_misspecified} shows the resulting optimisation
for $\uu^b=(0, 0) $ and for $\uu^b=(1, 1)$. % \todo{meme echelle 4.6 et 4.4 ? cartes zb(ub) - zb(utruth)}
\begin{figure}[ht]
  \centering
  \begin{subfigure}{0.9\textwidth}
    \centering
  \includegraphics{/home/victor/optimisation_dahu/1b/map_lognorm_200.png}
  \caption[Optimisation of $\zob$ on the whole space,
  $\uu^b = (0, 0)$]{\label{fig:optimization_1b} $\uu^b=(0, 0)$}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
  \centering
  \includegraphics{/home/victor/optimisation_dahu/9b/map_lognorm_200.png}
  \caption[Optimisation of $\zob$ on the whole
  space, $\uu^b = (1, 1)$]{\label{fig:optimization_9b} $\uu^b=(1, 1)$}
\end{subfigure}
\caption[Optimisation of $\zob$, misspecified case]{\label{fig:optim_croco_misspecified} Optimisation of $\zob$ on the whole space, with a
  misspecification of the environmental variable}
\end{figure}

 We can see that while the Bay of Biscay is
left unaffected by the optimisation procedure, the values in the
English Channel seem be well estimated, no matter the
misspecification. However, the bottom friction near the Celtic Sea
seems to compensate the error due to the misspecification. In the
appendix~\cref{sec:appendix_truth_optim} is presented the result of
the optimisation over the whole domain, for other choices of the
environmental parameter.


This optimisation showed that all the regions are not crucial for a
significant reduction of the objective. So, in order to clarify this,
we are now going to study the influence of the different inputs of the
objective function. More specifically, we are going to quantify the
influence of each region defined by its sediment type, and quantify
the influence of the uncertainties defined
in~\cref{ssec:tidal_modelling} on the output of the objective
function, by means of sensitivity analysis.

\section{Sensitivity analysis of the objective function}
\label{sec:sensitivity-analysis}
Sensitivity analysis (often abbreviated as \emph{SA}), aims at
quantifying the effect of the variation of some input variable to the
output of the model~\citep{iooss_revue_2011,janon_analyse_2012}.
Intuitively, SA aims at understanding how much the variations
of each input or combination of inputs explain the variations of the
output.

It can then be approached at two different scales:
around a nominal value, using the gradient, and at a global scale, by
considering the inputs as random variables, and by measuring the
variance of the output. In this work, we are going to focus
exclusively on global sensitivity analysis.

Here, sensitivity analysis is performed as a dimension-reduction
method, because it will be used to reduce the input space, based on
the prior assumption that the rugosity $\zob$ is considered constant
for each regions. Indeed, we will make the link between
\emph{sensitivity} and
\emph{identifiability}~\citep{dobre_global_2010}. %, as
%non-sensitivity implies non-identifiability.
If a parameter shows a very small influence on the output of the
objective function, any choice of its value will yield sensibly the
same output of the objective, provided that the other input parameters
are the same, justifying their overlook.


% \subsection{Methods of Sensitivity analysis}
% \label{sec:methods_SA}
% \subsubsection{Local sensitivity analysis}
% \label{sec:loca_SA} Local sensitivity
% analysis~\cite{morio_global_2011} refers to the study of how a small
% perturbation $\delta \kk$ of a nominal value $\kk$ affects the output
% of the numerical model. As we assume that the numerical model is
% accessible through the cost function $J$, a straightforward way to
% quantify this perturbation is to consider the partial derivative of
% $J$, with respect to each component of the control variable
% $\kk=(\kk_1,\dots,\kk_p)$:
% \begin{equation} \frac{\partial J}{\partial \kk_i}(\kk)
% \end{equation} The normalized local sensitivity
% at $\kk$ associated with the $i$-th component is then
% \begin{equation} \frac{{\Delta J}/{J}}{{\Delta
%       \kk_i}/{\kk_i}} = \frac{\kk_i}{J(\kk)} \frac{\partial J}{\partial\kk_i}
% \end{equation}

\subsection{Global Sensitivity Analysis: Sobol' indices}
\label{sec:sobol-indices}
As global SA calls for a probabilistic framework, we are going to
consider a real-valued random vector $X=(X_1,\cdots X_p)$, whose
components are assumed independent, to represent the inputs (\emph{i.e.}
$\kk$ and $\uu$) of a real function
$f: \mathbb{R}^p\rightarrow \mathbb{R}$ (the objective function in this
thesis). As $X$ is a random vector, we can introduce $Y$, the
real-valued random variable defined as $Y=f(X)$.

 % \cite{iooss_revue_2011,gilquin_echantillonnages_2016,janon_analyse_2012}
The $i$-th Sobol' indice of order $1$ is defined
as~\citep{sobol_sensitivity_1993,sobol_global_2001}
\begin{equation}
  S_i = \frac{\Var_{X_i}\left[\Ex_{Y}\left[Y \mid X_i\right]\right]}{\Var_{Y}\left[Y\right]}
\end{equation}
and can be interpreted as the fraction of variance of the output
$Y$ explained by the variation of $X_i$ \emph{alone}. Indices of
order $2$ are defined as
\begin{equation}
  S_{i\times j} = \frac{\Var_{X_i, X_j}\left[\Ex_{Y}\left[Y \mid X_i, X_j\right]\right]}{\Var_{Y}\left[Y\right]} - S_i -S_j
\end{equation}
and account for the interactions of the inputs labelled $i$ and $j$.
Higher-order Sobol' indices can then be defined
sequentially. Total-effect indices are also central in global
sensitivity analysis: those indices measure the contributions of a
single input $X_i$ through all its possible interactions,
\textit{i.e.} by considering the effect of its own variability (as the
order $1$) and the effect of all its interactions (order $2$ and
above). Those total-effect indices can be expressed as
\begin{equation}
  S_{T_i} = 1 - \frac{\Var_{X_{-i}}\left[\Ex_{Y}\left[Y \mid X_{-i}\right]\right]}{\Var_{Y}\left[Y\right]}
\end{equation}
where $X_{-i} = (X_1,\dots X_{i-1},X_{i+1},\dots,X_p)$ is a random vector of $p-1$ components.

In our study, the Sobol' indices of order $1$, $2$ and total ones are
computed using a replicated
method~\citep{gilquin_making_2019,gilquin_echantillonnages_2016},
allowing for bootstrap confidence intervals for the first and second
order effects.

\subsection{SA of the objective function for the calibration of CROCO}
\subsubsection{SA on the regions defined by the sediments}
The drag coefficient, which affects the ocean circulation, is the
result of two factors as shown
in~\cref{eq:quadratic_friction_vonkarman}: the bottom roughness $\zob$
and the ocean depth $H$. So depending on the depth, the influence of
the rugosity at the bottom on the objective function may change.

We are first going to perform a sensitivity analysis in order to
quantify the role of each sediment-based \emph{region}, without
incorporating the knowledge on the typical size of the sediment there.
Considering the similar expected size of silts (Si) and muds particles
(V) in~\cref{tab:size_sediments}, and the limited amount of silts, we
will merge those regions, and label the result with the code
$\mathrm{Si,V}$. Finally, the function which will be analyzed is
\begin{equation}
\kk \mapsto \|\mathcal{M}(\kk, \uu^{\mathrm{truth}}) - y \|_2^2
\end{equation}
with
\begin{equation}
  \kk = (\kk_{\mathrm{R}},\kk_{\mathrm{C}},\kk_{\mathrm{G}},\kk_{\mathrm{S}},
  \kk_{\mathrm{SF}},\kk_{\mathrm{Si,V}})\in\Kspace, \text{ with }
  \Kspace = \interval{\kk_{\min}}{\kk_{\max}}^6
\end{equation}
In the context of the sensitivity analysis, all the inputs are assumed
to be independent, and to be uniformly distributed on their support
$\interval{\kk_{\min}}{\kk_{\max}}\approx
\interval{\num{-11.5}}{\num{-3}}$.  The first, second and total-order
effects are displayed~\cref{fig:SA_sediments}, where the regions
defined by each sediment is \cref{fig:sediments_reduced}. The
experimental design used here comprises \num{7888} points.

\label{ssec:SA_sediments}
\begin{figure}[ht]
  \centering
  \input{\imgpath SA_sediments.pgf}
  \caption[SA on the sediments-based regions]{\label{fig:SA_sediments} Global SA on the regions defined by the sediment type, and bootstrap confidence intervals}
\end{figure}

We can see that the most influential region is the one defined by the
pebbles (with code C). More generally, except for the rocks, we can
see that the sediments that lies in shallower regions have a larger
impact on the objective function.  Based on geographic considerations
and the result of this sensitivity analysis, we can adopt a new
segmentation: the regions of Pebbles (C), and Gravel (G) will be kept
intact, but the remaining regions (R, S, SF, Si, V) will be bundled
together.
\subsubsection{SA on the tide components}
\label{ssec:SA_tide}
As introduced~\cref{ssec:tidal_modelling}, \CROCO{} incorporates
different tide constituents that we perturbate through the uncertain
variable $\uu\in\Uspace$. In order to quantify the influence of each
component of the environmental parameter, we performed also a
sensitivity analysis. As before, the SA is performed on the sum of
squares of the difference between the forward operator and the
observations. This time however, the control parameter $\kk$ is set to its
truth value, and the sum of squares depends only on $\uu$:
\begin{equation}
  \uu \mapsto  \| \mathcal{M}(\kk^{\mathrm{truth}}, \uu) - y \|_2^2
\end{equation}
for $\uu = (\uu_1,\cdots,\uu_5)$, and $\Uspace = \interval{0}{1}^5$.
Once again, in the SA context, every component is assumed to be
uniformly distributed on $\interval{0}{1}$ and independent.
\Cref{fig:SA_tides} shows the Sobol' indices of first order (left),
second (right), and the total effect indices, along with bootstrap
confidence intervals, obtained with a design of experiments of
\num{2888} points.
\begin{figure}[ht]
  \centering
  \input{\imgpath SA_tides.pgf}
  \caption[SA on the tide components]{\label{fig:SA_tides} Global SA
    on the different components of the tide, and bootstrap confidence
    intervals}
\end{figure}

We can see that the component of the vector affecting the amplitude of
the $M_2$ component of the tide has the most impact on the cost
function, and the $S_2$ component seems to have a non negligible
effect as well. For the other tide constituents, the SA reveals that
perturbating their amplitude has little to no-effect in this
configuration, and thus those variables will be discarded in further
analysis. The uncertain variable can then be redefined in what follows as
\begin{align}
  \UU = (\UU_1, \UU_2)%  \\
  % \UU_1 \text{ and } \UU_2 \text{i.i.d.} \\
  % \UU_i \sim \mathcal{U}\left(\interval{0}{1}\right) \text{ i=1,2}\\
\end{align}
where $\UU_1$ is the error on the $M_2$ amplitude, and $\UU_2$ is the
error on the $S_2$ amplitude, $\UU_1$ and $\UU_2$ are independent, and
$\UU \sim \mathcal{U}\left(\Uspace\right)$ with
$\Uspace = \interval{0}{1}^2$.

\section{Robust Calibration of the bottom friction}
\label{sec:robust_calibration}
In this section, we are going to study the robust calibration of the
numerical model, on the space defined according to the sensitivity
analysis performed before. On this reduced space, the objective
function $J$ will first be optimised globally on
$\Kspace \times \Uspace$ in~\cref{ssec:glob_mini}, and its results
will be compared with the true value (used to generate the
observations).

Afterwards, we are going to estimate relative-regret based estimates
of the bottom friction as described in the previous chapters. The
general approach is summarised~\cref{fig:diagram_recap}.  We are first
going to define and fit a Gaussian Process $Z$ with respect to an
initial design evaluated by the objective function $J$,
as introduced in~\cref{chap:adaptative_design_gp}.  As $J$ is supposed to
be positive, we are first going to ensure that the surrogate
constructed using $J$, namely $m_Z$ is positive as well, by enriching
the design using the PEI
criterion in~\cref{ssec:croco_cond_minimum_minimisers}. This will allow
us to estimate the conditional minimums and conditional minimisers.
This initialisation step is represented as the top row
of~\cref{fig:diagram_recap}.

The second row of the figure represents the enrichment step, which
consists in adding points to the design according to some adaptive
strategies. These strategies are implemented in order to improve the
estimation of some quantities of interest linked to the
relative-regret estimates: estimation of $\Gamma_{\alpha}$ using the
augmented IMSE in~\cref{ssec:optim_prob_threshold}, and sampling in
the margins of uncertainty for $q_p$ in~\cref{ssec:optim_quantile_rr}.

Finally, using the Gaussian Process $Z$ fitted based on this final
enriched design, we then construct the associated surrogate $m_Z$ to
emulate $J$ and to compute the estimations of $\Gamma_{\alpha}$ or
$q_p$, and optimise them to get the members of the relative-regret
family of estimators (as summarised in the last row
of~\cref{fig:diagram_recap}).


%The approach we present here is summarised~\cref{fig:diagram_recap}.

\begin{figure}[ht]
  \centering
  \input{\imgpath schema_robust.pgf}
  \caption{\label{fig:diagram_recap} Representation of the different
    steps for the robust calibration of the numerical model using
    relative-regret estimates}
\end{figure}


\subsection{Objective function and global minimum}
\label{ssec:glob_mini}
Based on the sensitivity analysis carried in the previous section, we
are going to consider the following setting for robust calibration.
The control variable is defined as
\begin{equation}
  \kk=\left(\kk^{(1)},\kk^{(2)},\kk^{(3)}\right) \in \Kspace % \quad \kk^{(i)} = \log(\zob^i),
  \quad \Kspace = \interval{\kk_{\min}}{\kk_{\max}}^3
\end{equation}
 where the superscript $(1)$ corresponds to the region
defined as Pebbles, $(2)$ is for the regions defined as Gravel, and
$(3)$ corresponds to the merged regions of Rocks, Sand, Silts, Mud,
and Fine Sands.

% $(2)$ is for the regions defined as Rocks, Gravel,
% and Sand, and the index $3$ is for the Silts, Mud, and Fine Sands.


Following the SA on the tide constituents, the uncertain variable is a
random vector with two components, representing the error on
amplitude of the $M_2$ and the $S_2$ tide constituents:
\begin{equation}
  \UU = (\UU_1,\UU_2), \quad \UU_i \sim \mathcal{U}(\interval{0}{1}) \text{ for } i=1,2
\end{equation}
As previously, the forward operator of the numerical model yields the
sea-surface height $\zeta$:
\begin{equation}
  \begin{array}{rcl}
    \mathcal{M}: \Kspace \times \Uspace &\longrightarrow& \mathbb{R}^{N_{\mathrm{obs}}} \\
    (\kk, \uu)& \longmapsto & \mathcal{M}(\kk, \uu) = \left(\zeta_{t,i}(\kk, \uu)\right)_{\substack{1 \leq i \leq N_{\mathrm{mesh}} \\ 1 \leq t \leq N_{\mathrm{time}}}} \\ 
  \end{array}
\end{equation}
and the objective function $J$ is the squared difference between the
observations $y$ and the forward operator.
\begin{equation}
  \begin{array}{rcl}
    J: \Kspace \times \Uspace & \longrightarrow & \mathbb{R} \\
    (\kk, \uu) & \longmapsto & \|\mathcal{M}(\kk, \uu) - y \|^2_2
  \end{array}
\end{equation}

For the sake of the study, we can optimise the original function $J$
over
$\Kspace\times \Uspace = \interval{\kk_{\min}}{\kk_{\max}}^3 \times
\interval{0}{1}^2$.  Because we are controlling the calibration
parameter on a space of dimension 3 while the observation $y$ has been
generated without the dimension reduction, the result of the
optimisation can be different from the truth values.
 We obtain:
\begin{equation}
  \min_{(\kk,\uu) \in \Kspace \times \Uspace} J(\kk, \uu) = J(\hat{\kk}_{\mathrm{global}}, \hat{\uu}_{\mathrm{global}})= \num{29.749}
\end{equation}
for
\begin{align}
  \hat{\kk}_{\mathrm{global}}                     & = (\num[round-mode=places,round-precision=4]{-3.5161661} , \num[round-mode=places,round-precision=4]{-5.07764701}, \num[round-mode=places,round-precision=4]{-6.34588442})                                                                                                            \\\hat{\uu}_{\mathrm{global}} & = (\num[round-mode=places,round-precision=4]{0.6347829},\num[round-mode=places,round-precision=4]{0.29890637})
\end{align}
We can then indeed notice that
$\uu^{\mathrm{truth}}=(\num{0.5},\num{0.5}) \neq
\hat{\uu}_{\mathrm{global}}$. The difference in the environmental
variables $\hat{\uu}_{\mathrm{global}}$ and $\uu^{\mathrm{truth}}$
compensates for the difference of dimensionality between
$\kk\in\Kspace$, the control variable, and $\kk^{\mathrm{truth}}$.


In order to compare the result of this optimisation with the truth
value,~\cref{tab:ktruthkopt} shows the different values of the
different components of $\kk^{\mathrm{truth}}$ and
$\hat{\kk}_{\mathrm{global}}$.

\begin{table}[!h]
  \centering
  \begin{tabular}{rrrr}\toprule
   Component              & $\kk^{\mathrm{truth}}$                                                                                                                                                                                                                                                                      & Component                & $\hat{\kk}_{\mathrm{global}}$                                              \\ \midrule
    $\kk_{\mathrm{C}}$    & \num[round-mode=places,round-precision=4]{-3.68887}                                                                                                                                                                                                                                         & $\kk^{(1)}$                  & \num[round-mode=places,round-precision=4]{-3.5161661}                   \\
    $\kk_{\mathrm{G}}$    & \num[round-mode=places,round-precision=4]{-4.96184}                                                                                                                                                                                                                                         & $\kk^{(2)}$                  & \num[round-mode=places,round-precision=4]{-5.07764701}                  \\
    $\kk_{\mathrm{R}}$    & \num[round-mode=places,round-precision=4]{-2.99573}                                                                                                                                                                                                                                         & \multirow{4}{*}{$\left. \vphantom{\begin{tabular}{c}3\\3\\3\\3\end{tabular}}\right\}\kk^{(3)}$} & \multirow{4}{*}{\num[round-mode=places,round-precision=4]{-6.34588442}} \\
    $\kk_{\mathrm{S}}$    & \num[round-mode=places,round-precision=4]{-6.907755}                                                                                                                                                                                                                                        &                          &                                                                         \\
    $\kk_{\mathrm{SF}}$   & \num[round-mode=places,round-precision=4]{-8.804875}                                                                                                                                                                                                                                        &                          &                                                                         \\
    $\kk_{\mathrm{Si,V}}$ & \num[round-mode=places,round-precision=4]{-10.81977}                                                                                                                                                                                                                                        &                          &                                                                         \\ \bottomrule
  \end{tabular}
  \caption[Values of the $\kk$ component of the global optimiser, and
  truth value]{\label{tab:ktruthkopt} Values of the $\kk$ component of
    the global optimiser, and truth value. The region associated with
    $\kk^{(3)}$ is the union of the regions defined with code R, S,
    SF, Si and V.}
\end{table}


As mentioned in the previous chapter, $J$ can be expensive to evaluate
computational-wise, so we propose to use GP in order to model it, and
to enrich its design for the computation of members of the
relative-regret family of estimators.

We will first define the initial design, which will be evaluated by
$J$. A Latin Hypersquare of \num{100} points on
$\Kspace \times \Uspace$ is first sampled in order to construct a GP
that can be used as a surrogate.  In this work, the GP will be
constructed using the Python module
Scikit-learn~\citep{pedregosa_scikit-learn_2011}.

We denote $\mathcal{X}_{\mathrm{LHS}}$ the initial LHS, and $Z$ the
Gaussian Process constructed and fitted using
$\mathcal{X}_{\mathrm{LHS}}$.  We will write
\begin{equation}
  Z \sim \GP\left(m_Z, C_Z\right) \quad\text{ and } \quad C_Z((\kk, \uu),(\kk, \uu)) = \sigma^2_Z(\kk, \uu)
\end{equation}
We then have, for any $(\kk, \uu) \in \Kspace\times\Uspace$,
\begin{equation}
  Z(\kk, \uu) \sim \mathcal{N}\left(m_Z(\kk, \uu), \sigma^2_Z(\kk, \uu)\right)
\end{equation}

By definition, for any $\kk\in \Kspace$ and any $\uu \in \Uspace$, we
have $J(\kk, \uu) \geq 0$.  However, the positivity of the objective
function is not necessarily verified by the surrogate $m_Z$, and thus
the notion of relative-regret is not defined. We have to first ensure
that the GP $Z$ (and consequently $Z^*$) is positive with large enough
probability.  To do so, we are first going to enrich the design near
the conditional minimisers, using the PEI criterion.  Alternatively,
one could have added points to the design according to the reliability
index, defined~\cref{eq:reliability_rho}, and thus look and evaluate points
having the largest probability of being negative.


\subsection{Conditional minimums and conditional minimisers}
\label{ssec:croco_cond_minimum_minimisers}

In the previous chapter, we defined the conditional minimum as the
minimum of the objective function at a given $\uu\in \Uspace$:
\begin{equation}
  J^* : \uu \mapsto J^*(\uu) = \min_{\kk\in\Kspace} J(\kk, \uu)
\end{equation}
The conditional minimisers function is defined as
\begin{equation}
  \kk^*:\uu  \mapsto \kk^*(\uu) = \argmin_{\kk \in \Kspace} J(\kk, \uu)
\end{equation}
Since both of these functions require an optimisation of the objective
function, they are quite expensive to compute, so, as done before, we
use the GP prediction of $Z$ in order to approximate them:
\begin{align}
  m_{Z^*}: \uu\mapsto &\min_{\kk \in \Kspace} m_Z(\kk, \uu) \\
  \kk^*_{Z}: \uu  \mapsto &\argmin_{\kk \in \Kspace} m_{Z^*}(\uu)
\end{align}
As mentioned before, in order to improve the accuracy of those two functions and to ensure
the positivity of $m_Z$, we are first going to enrich the design using
the PEI criterion~\citep{ginsbourger_bayesian_2014}, as
introduced~\cref{sec:PEI_criterion}.  Choosing \num{200}
additional points this way, the new obtained design is denoted
$\mathcal{X}_0$, that will serve as the ``initial'' design for the
enrichment procedures described later.

For the sake of this work, we are going to estimate the conditional
minimisers as accurately as possible, using in total \num{750}
evaluations of the objective function $J$. By sampling $\uu_i$ from
$\UU$ for $1\leq i \leq n_{\uu}=\num{2000}$, we can first estimate
$m_{Z^*}(\uu)$ using this set of samples, as shown
in~\cref{fig:contour_Jstar}. As expected from the result of the
optimisation carried in~\cref{ssec:glob_mini}, we can see that the
minimum of $m_{Z^*}(\uu)$ (\emph{i.e.} the global minimum) is not
attained at $(0.5, 0.5)=\uu^{\mathrm{truth}}$ but rather at
approximatively $(\num{0.6},\num{0.3})$.


\begin{figure}[ht]
 \centering
 \includegraphics{\imgpath contour_Jstar_800_again.png}
 \caption{\label{fig:contour_Jstar} Conditional minimum $m_{Z^*}(\uu)$
   estimated using the GP $Z$}
\end{figure}
Based on these samples, we can also approximate the distribution of
the random variable $\kk^*(\UU)$ by
$\kk_Z^*(\UU) = (\kk^{(1) *}_Z(\UU), \kk^{(2) *}_Z(\UU), \kk^{(3)
  *}_Z(\UU))$.  \Cref{fig:pairplot} shows the pairwise relations
between the components of the random variable $\kk^*_Z(\UU)$, based on
the samples $\kk^*_Z(\uu_i)$ for $1\leq i \leq n_{\uu}$. On the
diagonal plots are shown the Kernel Density Estimation of the marginal
distributions of $\kk^*(\UU)$, and the corresponding samples from
$m_{Z^*}(\UU)$. The off-diagonal plots show the relations between the
components. We can then observe a negative correlation between
$\kk^{(1) *}_Z(\UU)$ and $\kk^{(2) *}_Z(\UU)$ for instance, indicating
that an increase in one of those value of the friction is compensated
by the decrease in the other component.  We can also notice that the
marginal distribution of the first component $\kk_Z^{(1)*}(\UU)$ seems
to exhibit two modes: one at \num{-3.6}, and one at around
\num{-3.45}, and both are quite close to the value found when
optimising globally:
$\hat{\kk}^{(1)}_{\mathrm{global}}\approx \num{-3.5}$, which makes it
a clear candidate for robust optimisation. For the two other
components, we can observe that the range of values taken by the
samples $\kk^*_Z(\uu_i)$ for $1 \leq i \leq n_{\uu}$ is larger,
indicating that for these components, a robust candidate may be less
\emph{identifiable}, as discussed~\cref{sec:MPE},~\cpageref{sec:MPE},
but then also less influential.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{\imgpath pairplot_800_again.png}
  \caption[Distribution of the minimisers
  $\kk^*(\UU)$]{\label{fig:pairplot} Estimated distribution of the
    minimisers $\kk^*_Z(\UU)$. The diagonal plots show the KDE of the
    marginal distributions of the minimisers. Each point represents a
    sample, and its vertical component is the conditional minimum
    associated. The non-diagonal plots show the pairwise relation
    between the different components of the minimisers }
\end{figure}

\clearpage
\subsection{Relative-regret based estimators}
\label{ssec:rr_estimators_croco}
We are now going to estimate members of the relative-regret family of
estimators as introduced~\cref{def:RR_family}, on
\cpageref{def:RR_family}:
\begin{equation}
  \left\{\kk_{\mathrm{RR,\alpha}} = \argmax_{\kk\in\Kspace} \Gamma_{\alpha}(\kk) \mid \alpha \geq 1\right\}
\end{equation}
where the function to be maximised,
\begin{equation}
  \Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk,\UU) \leq \alpha J^*(\UU)\right]
\end{equation}
is the probability that $\kk$ is $\alpha$-acceptable.  In the
following, we will assume also that we have at disposal a budget of \num{500} runs of
the forward model, so after the initial design and the initial
iterations of the PEI criterion, we assume available \num{200}
additional runs.
%\begin{figure}[ht]
%  \centering
%  \input{\imgpath evo_alpha_p_kp.pgf}
%  \caption{\label{fig:evo_alphap_kp} Evolution of the threshold $\alpha_p$ as a function of $p$, and $\kk_{\mathrm{RR},\alpha_p}$}
%\end{figure}

\subsubsection{Optimisation of the probability of exceeding a threshold}
\label{ssec:optim_prob_threshold}
\paragraph{Plug-in approximation of the probability of acceptability}

Let $\alpha > 1$. The $\Gamma_{\alpha}$ function is quite expensive to compute, so we
are going to use a plug-in approach (see~\cref{def:plugin}) in order
to avoid exhaustive computations of the objective function
$J$. Instead, we will use $m_Z$ and $m_{Z^*}$ as surrogates of $J$ and
$J^*$.  For the approximation of the probability, we are going to use
a Sample Average Approximation (SAA) approach, by using a set of
$n_\uu$ i.i.d.\ samples $\{\uu_i\}_{1\leq i \leq n_{\uu}}$.  All in
all, both approximations leads to
\begin{align}
  \hat{\Gamma}^{\mathrm{PI}}_{\alpha}(\kk) &= \frac{1}{n_\uu}\sum_{i=1}^{n_{\uu}} \mathbbm{1}_{\{m_Z(\kk, \uu_i) \leq \alpha m_{Z^*}(\uu_i)\}}
\end{align}

Maximising this expression yields
\begin{equation}
  \hat{\kk}_{\mathrm{RR},\alpha} = \argmax_{\kk\in\Kspace} \hat{\Gamma}^{\mathrm{PI}}_{\alpha}(\kk) = (\hat{\kk}^{(1)}_{\mathrm{RR},\alpha}, \hat{\kk}^{(2)}_{\mathrm{RR},\alpha},\hat{\kk}^{(3)}_{\mathrm{RR},\alpha})
\end{equation}
In order to get a relevant value of $\hat{\kk}_{\mathrm{RR},\alpha}$,
we are going to reduce sequentially the augmented IMSE, as introduced
in~\cref{sec:evaluation_gamma}.

\paragraph{Stepwise reduction of the augmented IMSE}
We will first look to improve the estimation of $\Gamma_{\alpha}$, by
improving the plug-in approximation. We define
$\Delta_{\alpha} = Z - \alpha Z^* \sim \GP(m_{\Delta_{\alpha}},
C_{\Delta_\alpha})$, which is a GP as defined in the previous chapter.
The prediction variance, or mean square error, is
$\sigma^2_{\Delta_{\alpha}}: (\kk, \uu) \mapsto
C_{\Delta_{\alpha}}\big((\kk, \uu), (\kk, \uu) \big)$. In the
following, we are going to estimate the relative-regret, associated
with the value $\alpha=1.3$, meaning that we are looking to maximise
the probability of being within $30\%$ of the optimal value. Such a
value has been chosen based on a preliminary analysis, using the GP
constructed on the initial design.


We are then going to reduce the augmented IMSE of the random process
$\Delta_{\alpha}$. Recalling that the $\IMSE$ associated with the GP
$\Delta_{\alpha}=Z-\alpha Z^*$ constructed using the design
$\mathcal{X}_n$ is defined as
\begin{equation}
  \IMSE(\mathcal{X}_n) = \int_{\Kspace \times \Uspace} \sigma^2_{\Delta_{\alpha}}(x) \,\mathrm{d}x
\end{equation}
we select the next point to evaluate as:
\begin{equation}
  \label{eq:aIMSE_criterion}
  (\kk_{n+1}, \uu_{n+1}) = \argmin_{(\kk,\uu)\in\Kspace\times \Uspace} \Ex_{Z(\kk, \uu)}\left[\IMSE(\mathcal{X}_n \cup \left((\kk, \uu), Z(\kk, \uu)\right))\right]
\end{equation}
In practice, we will choose the new point $(\kk_{n+1}, \uu_{n+1})$ in
a stochastic manner, as the augmented IMSE will be estimated using a
Monte-Carlo method.
Let us consider the design augmented with the point
$ \left((\kk, \uu), z(\kk, \uu)\right)$, which is equivalent to making the
assumption that the function takes the value $z(\kk, \uu)>0$ at the
point $(\kk,\uu)$, we note $\sigma_{{\Delta_\alpha} \mid z}^2$ the
prediction variance associated with this design. The estimation of
the augmented $\IMSE$ is then
\begin{align}
  \IMSE(\mathcal{X}_n\cup \left((\kk, \uu), z(\kk, \uu)\right) ) &\approx \frac{1}{n_{\IMSE}} \sum_{i=1}^{n_{\IMSE}} \sigma_{\Delta_{\alpha} \mid z}^2(\kk_i, \uu_i)
\end{align}
where the points $(\kk_i, \uu_i)$ for $1\leq i \leq n_{\IMSE}$ are
resampled each iteration using a LHS on $\Kspace \times \Uspace$ for
$n_{\IMSE}=150$.

The expectation operator of~\cref{eq:aIMSE_criterion} is also
approximated by choosing $z_j(\kk, \uu)$ for $1\leq j \leq n_Z$ as
different quantiles of $Z(\kk, \uu)$ (which is normally distributed). Finally, we have
\begin{align}
  \Ex_{Z(\kk, \uu)}\left[\IMSE(\mathcal{X}_n \cup \left((\kk, \uu), Z(\kk, \uu)\right))\right] &\approx \frac{1}{n_Z} \sum_{j=1}^{n_Z} \frac{1}{n_{\IMSE}} \sum_{i=1}^{n_{\IMSE}} \sigma^2_{\Delta_{\alpha} \mid z_j}(\kk_i, \uu_i) \\
   &\propto \sum_{j=1}^{n_Z} \sum_{i=1}^{n_{\IMSE}} \sigma^2_{\Delta_{\alpha} \mid z_j}(\kk_i, \uu_i)
\end{align}

and finally we choose the point with the lowest augmented IMSE among
\num{100} randomly sampled points in $\Kspace\times\Uspace$.

~\Cref{fig:aIMSE} shows the reduction of the augmented-IMSE with the number of
additional iterations, until reaching a number of \num{500}
evaluations of the model in total. Abrupt changes in the IMSE can be
explained by a significant changes in the hyperparameters of the GP.\@

\begin{figure}[ht]
  \centering
  \input{\imgpath aIMSE.pgf}
  \caption{\label{fig:aIMSE} Evolution of the IMSE during the
    enrichment strategy}
\end{figure}


Based on the enriched GP, we can maximise the plug-in approximation
$\hat{\Gamma}_{\alpha}^{\mathrm{PI}}$. We chose here
$n_{\uu}=\num{500}$, and the maximum found is
\begin{equation}
  \label{eq:max_gamma_alpha}
  \max_{\kk\in\Kspace} \hat{\Gamma}_{\alpha}^{\mathrm{PI}}(\kk) = \num[separate-uncertainty=true]{0.93\pm 0.0224}
%num[separate-uncertainty=true]{0.848\pm 0.031}
\end{equation}
where the given confidence interval is computed using the normal
approximation of the binomial proportion, at a \num{95}\% level. As we
are using a plug-in approximation, we overlook the intrinsic
uncertainty which is represented by the GP, and thus use directly
$m_Z$ instead of $J$.

Even though the design has been enriched for $\alpha=1.3$, the
resulting GP can be used to evaluate quantities associated with
other thresholds.~\Cref{fig:evo_alphap_kp} shows the maximal
probability reached by $\hat{\Gamma}^{\mathrm{PI}}_{\alpha}$ as a
function of $\alpha$, and the different components of the control
variable $\hat{\kk}_{\mathrm{RR},\alpha}$, all these quantities
computed using the metamodel $m_Z$ after the additional iterations.

\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath palpha.png}
  \caption[Evolution of the maximal probability of
    acceptablility $\max \Gamma_{\alpha}^{\mathrm{PI}}$]{\label{fig:p_alpha} Evolution of the maximal probability of
    acceptablility $\max \Gamma_{\alpha}^{\mathrm{PI}}$, and 95\% CI
    interval associated with the SAA approximation of the estimation
    of the probability.}
\end{figure}

We can see that the estimated components of
$\hat{\kk}_{\mathrm{RR},\alpha}(\UU)$ stay in a rather small range for
all $\alpha >1$. Recalling the the global optimiser whose values are
introduced~\cref{tab:ktruthkopt}~\cpageref{tab:ktruthkopt},
$\hat{\kk}_{\mathrm{global}} = (-3.516, -5.078, -6.3459 )$, we can
observe that $\hat{\kk}^{(1)}_{\mathrm{RR}, \alpha}$ is slightly
higher than $\hat{\kk}^{(1)}_{\mathrm{global}}$ for all $\alpha > 1$.
Globally however, we can see that the different values of the
components do not seem to change a lot for different $\alpha$.

% Regarding the second and third components, we can see that
% $\hat{\kk}_{\mathrm{RR},\alpha}$ shows lower values than
% $\hat{\kk}_{\mathrm{global}}$, but considering the 

\begin{figure}[ht]
  \centering
  %\input{\imgpath evo_alpha_p_kp.pgf}
  \includegraphics{\imgpath evo_alpha_p_kp_bnd.png}
  \caption[Components of $\hat{\kk}_{\mathrm{RR},\alpha}$ after
  augmented IMSE reduction]{\label{fig:evo_alphap_kp} Components of
    $\hat{\kk}_{\mathrm{RR},\alpha}$, using the GP enriched with 200
    additional points minimising the augmented IMSE.}
\end{figure}


\clearpage
\subsubsection{Optimisation of the quantile of the relative-regret}
\label{ssec:optim_quantile_rr}
\paragraph{Plug-in approximation of the quantile}
Alternatively, for a level of confidence
$p \in \interval{0}{1}$, we can define the quantile function of the
ratio:
\begin{equation}
  q_p(\kk) = Q_{\UU}\left(\mathrm{RR}(\kk, \UU);p \right)
\end{equation}
where $\mathrm{RR}(\kk, \UU)=\frac{J(\kk, \UU)}{J^*(\UU)}$ is the
relative-regret, that we introduce for notational convenience.  As
$\mathrm{RR}$ is unknown directly, we can also apply the plug-in
approach, and define
\begin{equation}
  \mathrm{RR}^{\mathrm{PI}}(\kk, \uu) = \exp\left[m_{\Xi}(\kk, \uu) + \frac{1}{2}\sigma^2_{\Xi}(\kk, \uu)\right]
\end{equation}
where $\Xi$ is the lognormal approximation as defined
in~\cref{eq:log_ratio}, \cpageref{eq:log_ratio}. Once again, the
estimation of the quantile of order $p$ of $\mathrm{RR}$ is done using
a set of i.i.d.\ samples of $\UU$: $\{\uu_i\}_{1\leq i \leq n_{\uu}}$
\begin{equation}
  \label{eq:RRPI}
  \hat{q}_p^{\mathrm{PI}}(\kk) = {\mathrm{RR}}^{\mathrm{PI}}(\kk, \uu)_{(\left[n_{\uu}p\right])}
\end{equation}
where the subscript indicates the order statistic, \emph{i.e.} the
$\left[n_{\uu}p\right]$ smallest value of
$\{\mathrm{RR}^\mathrm{PI}(\kk,\uu_i)\}_{1\leq i\leq n_\uu}$ (with
$[\cdot]$ as the rounding operator). 

As defined in the previous chapter, the minimiser of
$\hat{q}_p^{\mathrm{PI}}$ is the estimated member of the
relative-regret family associated with $\alpha_p$:
\begin{equation}
  \hat{\kk}_{\mathrm{RR},\alpha_p} = \argmin_{\kk \in \Kspace} \hat{q}^{\mathrm{PI}}_p(\kk) = (\hat{\kk}^{(1)}_{\mathrm{RR},\alpha_p}, \hat{\kk}^{(2)}_{\mathrm{RR},\alpha_p},\hat{\kk}^{(3)}_{\mathrm{RR},\alpha_p})
\end{equation}
\paragraph{Sampling-based method for the estimation of the quantile: QeAK-MCS}
We are now going to treat this problem using a sampling-based method,
as introduced~\cref{ssec:quantile_qeakmcs}, which is derived
from~\cite{razaaly_rare_2019}.  The log-normal approximation of the
relative-regret, as introduced~\cref{ssec:lognormal_approx}, allows us
to define the random process $\Xi$ as
\begin{equation}
  \log \frac{Z(\kk, \uu)}{Z^*(\uu)} \approx \Xi(\kk, \uu) \sim \mathcal{N}\left(m_{\Xi}(\kk, \uu),\sigma^2_{\Xi}(\kk, \uu)\right)
\end{equation}

Using the Gaussian nature of $\Xi$, we define $\mathfrak{q}_1$,
$\mathfrak{q}_2$ and $\mathfrak{q}_3$, as a lower bound, a central
estimate and an upper bound respectively, of the value
$\min_\kk \hat{q}_p(\kk)=\hat{\alpha}_p$, minimum which is attained at
$\hat{\kk}= \argmin_{\kk\in\Kspace} \hat{q}_p(\kk)$. Following the
notation introduced in the previous chapter, we chose $K_q = 3$.
With $k=\Phi^{-1}(1 - 0.025)$ the quantile of order \num{0.975} of the
standard normal distribution, we define
\begin{align}
  \log {\mathfrak{q}}_1 &= \left(m_{\Xi}(\tilde{\kk}, \uu) - k \sigma_{\Xi}(\tilde{\kk}, \uu)\right)_{([n_{\uu}p])} \approx Q_U(m_{\Xi}(\tilde{\kk}, \UU) - k \sigma_{\Xi}(\tilde{\kk}, \UU); p) \\
  \log \mathfrak{q}_2 &= \log \left(\min_{\kk\in\Kspace}\hat{q}^{\mathrm{PI}}_p(\kk) \right) = \log \hat{q}^{\mathrm{PI}}_p(\tilde{\kk}) = \log \hat{\alpha}_p \\
  \log \mathfrak{q}_3 &= \left(m_{\Xi}(\tilde{\kk}, \uu) + k \sigma_{\Xi}(\tilde{\kk}, \uu)\right)_{([n_{\uu}p])} \approx Q_U(m_{\Xi}(\tilde{\kk}, \UU) + k \sigma_{\Xi}(\tilde{\kk}, \UU); p)
\end{align}

We can then define the margins of uncertainty of the relative-regret of
level $\eta$, for each of the values $\mathfrak{q}_l$:
\begin{equation}
  \label{eq:def_metaql}
  \mathbb{M}_{\eta}(\mathfrak{q}_l) = \left\{(\kk, \uu) \mid \frac{\eta}{2} \leq \Phi\left(-\frac{m_{\Xi}(\kk, \uu) - \log \mathfrak{q}_l}{\sigma_{\Xi}(\kk, \uu)}\right) \leq 1 - \frac{\eta}{2}\right\}
\end{equation}
We can then sample points in each of those margins, perform a statistical
reduction method in order to get $K_{\mathbb{M}}$ points for each
margin, adjust them and finally evaluate the $K_q \cdot K_{\mathbb{M}} =  3\cdot K_{\mathbb{M}}$
points by the objective function.


Numerically speaking, the margins revealed themselves quite small, so
we chose a level $\eta=0.0005$ in order to increase their volume, and
thus facilitate the sampling procedure.

After the sampling of \num{2000} points in those margins
$\mathbb{M}_{\eta}(\mathfrak{q}_l)$ for $1\leq l \leq 3$ , we use the
KMeans clustering algorithm in order to select $K_{\mathbb{M}}$
\emph{representative} points among those samples.  This number of
points selected in each margin $K_{\mathbb{M}}$ changes along the
iterations: it starts with a small number of points,
$K_{\mathbb{M}}=3$, and increases until $K_{\mathbb{M}}=10$.


We can see on~\cref{fig:vol_Meta} the estimated volume of the margin
of uncertainty (\emph{i.e.} its measure according to Lebesgue's
measure on $\Kspace \times \Uspace$), using Monte-Carlo method, along
with the 95\% confidence intervals associated with the Monte-Carlo
estimation (Wilson's score method as
introduced~\cite{wilson_probable_1927} to account for the small
relative volume of the margin of uncertainty).  We can notice that
first, its volume increases slightly. This can be seen as an
exploration phase, where the additional evaluations change the
hyperparameters of the GP, which affects the estimation of the
conditional minimums, and the candidate quantiles $\mathfrak{q}_l$,
for $1\leq l \leq 3$. After enough additional points, the volume of
the margin decreases, as points are added that do not change
significantly the other estimations.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.9]{\imgpath volume_margin_uncertainty.png}
  \caption[Volume of the margin of uncertainty associated with
  $\mathfrak{q}_2 = \hat{\alpha}_p$]{\label{fig:vol_Meta} Volume of
    the margin of uncertainty associated with
    $\mathfrak{q}_2 = \hat{\alpha}_p$, and 95\% confidence intervals
    of its estimation}
\end{figure}


Using the GP conditioned on the final design, for $p=0.95$ we can compute
\begin{align}
  \hat{\alpha}_p &= \num{1.3791} \\
  \interval{\mathfrak{q}_1}{\mathfrak{q}_3} &= \interval{1.36618}{1.391868} \nonumber
\end{align}
and using the same GP,~\cref{fig:hat_alpha_p} shows the estimation of
the threshold for other values of $p$.  One main difference between
the confidence interval given there by $\mathfrak{q}_1$ and
$\mathfrak{q}_3$, and the one given~\cref{eq:max_gamma_alpha} is that
the former translates the uncertainty originating from the Gaussian
Process, while the latter accounts for the error in the \emph{sample
  average approximation} of the probability
$\hat{\Gamma}_{\alpha}^{\mathrm{PI}}$.
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.9]{\imgpath hatalphap.png}
  \caption[Estimation of the threshold $\alpha_p$, depending on the
  level $p$]{\label{fig:hat_alpha_p} Estimation of the threshold
    $\alpha_p$, depending on the level $p$. The lower and upper bounds
    $\mathfrak{q}_1$ and $\mathfrak{q}_3$ are also displayed}
\end{figure}

Finally,~\cref{fig:theta_rr_alpha} shows the different components of
$\hat{\kk}_{\mathrm{RR},\alpha_p}$ for different $p> 0.5$.
\begin{figure}[ht]
  \centering
  \includegraphics[]{\imgpath theta_rr_ap.png}
  \caption{\label{fig:theta_rr_alpha} Relative-regret based estimates
    $\hat{\kk}_{\mathrm{RR}, \alpha_p}$, depending on the level $p$}
\end{figure}
We can notice that similarly as in~\cref{fig:evo_alphap_kp}, the range
of values taken by each component is rather small. We can however
notice that the first component of $\hat{\kk}_{\mathrm{RR}, \alpha_p}$
is again slightly larger than the one the global optimum
$\hat{\kk}_{\mathrm{global}}$.


All in all, this study suggests that choosing a value slightly higher
than the global optimiser for the first component (which corresponds
to Pebbles) would be more robust than choosing the global
minimiser. Regarding the other components, their respective influence
(given by the SA) and the relative-regret estimates would point toward
a value close or equal to the global optimiser.

A comparison of the two methods is summarised in~\cref{tab:results_croco}.

\begin{table}
  \centering
  \begin{tabular}{rrr}\toprule
    Method & augmented IMSE & Qe-AK MCS \\ \midrule
    Quantity of interest & $\Ex_{Z}[\int_{\Kspace\times\Uspace}\sigma^2_{\Delta_\alpha \mid Z}]$ & $\mathbb{M}_{\eta}(\mathfrak{q}_l)$ \\
    Type & 1-step & $K$-step \\
    Main Bottleneck &
                      \begin{tabular}{@{}r@{}}
                        Evaluate and optimise integral in \\
                        $(1 + \dim( \Kspace\times \Uspace))$ dimensions
                      \end{tabular}
                      & \begin{tabular}{@{}r@{}}
                          Sampling in unknown \\
                          regions in $\dim( \Kspace\times \Uspace)$
                        \end{tabular} \\
    Advantage & Optimal criterion of enrichment & $K$ chosen arbitrary \\ \midrule
    $\hat{\kk}_{\mathrm{RR},\alpha}$ $\alpha=1.3$ & $(\num{-3.43},\num{-5.2},\num{-6.48})$  & $(\num{-3.375},\num{-5.05}, \num{-6.61})$ \\
    $\hat{\kk}_{\mathrm{RR},\alpha_p}$ $p=0.95$ & $(\num{-3.39},\num{-5.28},\num{-6.5})$  & $(\num{-3.36},\num{-5.10}, \num{-6.63})$ \\
    $\hat{\kk}_{\mathrm{global}}$ & \multicolumn{2}{c}{$(-3.516, -5.078, -6.3459 )$} \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of methods and numerical results for the robust
    calibration of CROCO}
  \label{tab:results_croco}
\end{table}




\section{Partial conclusion}
\label{sec:partial-conclusion}
In this chapter, we addressed the problem of calibration under
uncertainties of the numerical model CROCO.\@ After having defined the
control and environmental parameters, we performed different studies,
in order to have a better understanding of the behaviour of the model
at stake.

First, we optimised the objective function on a high-dimensional input
space, but without external uncertainties. We chose then to segment
the domain according to the type of sediments that can be found at the
bottom, and by performing a sensitivity analysis, we could reduce
further the dimension of the input space.
A similar study has been done in order to reduce the dimension of the
environmental parameter as well.

Finally, we applied some of the methods that rely on GP as introduced
in~\cref{chap:adaptative_design_gp}, in order to explore further the
behaviour of the numerical model under uncertainties. First, to have a
better estimation of the conditional minimum and minimisers we
enriched the design using the PEI criterion. Then, in order to get
robust estimators of the bottom friction, we used two different
approaches to enrich the GP. In the first one, we reduced iteratively
the augmented IMSE in order to improve the plug-in approximation of
the probability of exceeding a threshold. For the other one, in order
to minimise a specific quantile of the relative-regret, we defined and
sampled in the margin of uncertainty in order to add points by
batches.

This finally leads to relative-regret estimates of the bottom
friction, which differ from the global minimiser: according to this
study, taking a value slightly larger for the first component of the
bottom friction leads seemingly to a more robust solution. The small
variations in the values of $\hat{\kk}_{\mathrm{RR}}$ for the
different levels of confidence also suggest that a compromise between
performances and robustness is easily reachable. Indeed, in this
configuration, we can see that the calibration problem behaves
``nicely'' under uncertainties: the environmental
parameters seem to only have a limited impact on the estimation of the
bottom friction. 

% that the calibration of the
% numerical model under uncertainties is not that problematic in this
% configuration, since the global optimiser $\hat{\kk}_{\mathrm{opt}}$
% is quite close to the relative-regret estimates.




From a performance point of view, GP can indeed reduce the
computational requirements needed to compute relative-regret
estimates. However as mentioned before, GP do not scale particularly
well for problems of more than a dozen input variables, rendering a
dimension reduction almost mandatory for tractability.

The computational cost of the enrichment process, either using
stepwise or sampling procedures, can quickly become
non-negligible. Indeed, the estimation of the parameters of the
distributions of $\Delta_{\alpha}(\kk, \uu)$ and $\Xi(\kk, \uu)$
require at each point $(\kk, \uu)$ a global optimisation of $m_Z$,
which can amount to a few hundreds of calls to the surrogate $m_Z$.
In itself, this procedure is not that expensive. However, it needs to
be performed a large number of times: for a single evaluation of the
augmented IMSE for instance, estimation of nested integrals are
needed, thus the number of computations of the parameters of
$\Delta_{\alpha}$ grows quite large, even more when considering an
optimisation of the augmented IMSE on $\Kspace
\times\Uspace$. Similarly, the volume of the margin of uncertainty can
be very small compared of the volume of the whole space
$\Kspace \times \Uspace$, thus without specific sampling schemes, it
can quickly become a computational bottleneck as well, even more so
when the prediction variance becomes small.

Some improvements can be considered in order to perform more
efficiently those estimations. A two-stage approach could be derived,
in order to first identify a value $\tilde{\kk}$ for which we want to
reduce the augmented IMSE, and secondly to integrate the augmented
IMSE over $\tilde{\kk} \times \Uspace$. By doing so, each iteration
would instead require the optimisation of an integral of dimension
$1 + \dim \Uspace$, while at the same time focus some computational
effort toward the estimation of $\hat{\kk}_{\mathrm{RR},\alpha}$, by
correctly choosing a candidate $\tilde{\kk}$.  Also, instead of
considering the augmented IMSE, we could instead look for the point
maximising the prediction variance of $\Delta_{\alpha}$, and
``adjust'' its $\kk$ component, similarly as proposed for the sampling
based scheme, bypassing completely the evaluation of the integral.



A better sampling scheme for Monte-Carlo based methods can also be
imagined: the margin of uncertainty comprises points $(\kk, \uu)$ for
which $\Xi(\kk,\uu)$ is ``close'' to $\log \mathfrak{q}_l$ (for
$1\leq l \leq K_q$), thus for importance sampling, we could construct
a proposal density which consists in first sampling $\uu \sim \UU$,
then sampling $\kk$ ``close'' to $\kk^*_Z(\uu)$, for instance by
choosing $\kk \sim \mathcal{N}(\kk^*_Z(\uu), \gamma^2 I)$, where $I$
is the identity matrix in dimension $\dim \Kspace$, and $\gamma$ is
chosen with respect to the target quantiles $\mathfrak{q}_l$.


Both methods introduced in this chapter rely on the ability to compute
quickly the conditional minimum $m_{Z^*}$ and the conditional
minimiser $\kk_Z^*$ in order to get the mean and variance of the
processes $\Delta_\alpha$ and $\Xi$. For each of this optimisation, we
used a gradient-descent methods, where the starting point is selected
randomly in the search space each restart, in order to avoid getting
stuck in a local minimiser. This step could be improved as well, by
using directly a global optimisation
method.% such as CMA-ES~\citep{hutchison_evaluating_2004} since
% the evaluation of the function to be minimised is not so expensive.

Finally, aside from those technical details of implementation, a
similar study could be derived on slightly different configurations,
by increasing for instance the assimilation window, in order to
increase the observable interactions in the tide constituents, or by
considering a different segmentation with more sediment types, or a
segmentation based on the depth for instance.

% END OF CHAPTER --------------------
\markchapterend


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
