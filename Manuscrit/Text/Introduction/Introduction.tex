\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Introduction/#1}
}

% \subfileLocal{
% \externaldocument{../../Text/Chapter2/build/Chapter2}
% \externaldocument{../../Text/Chapter3/build/Chapter3}
% \externaldocument{../../Text/Chapter4/build/Chapter4}
% \externaldocument{../../Text/Chapter5/build/Chapter5}
% \externaldocument{../../Text/Conclusion/build/Conclusion}
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagestyle{introStyle}
\chapter*{Introduction}
\TitleBtwLines
\phantomsection
\addstarredchapter{Introduction}
\label{chap:Introduction}
% \subfileLocal{\pagestyle{contentStyle}}
% \todo{\cite{mcwilliams_irreducible_2007,zanna_ocean_2011}}
To understand and to be able to forecast natural phenomena is crucial
for many applications with high social, environmental and economic
stakes.  In earth sciences especially, the modelling of the ocean and
the atmosphere is important for day to day weather forecasts,
hurricanes tracking, pollutant dispersion, or biological monitoring.

Those natural phenomena are then modelled mathematically, usually by
representing the physical reality with some general equations
(Navier-Stokes equations for Computational Fluid Dynamics for
instance), and by making successive reasonable assumptions and
simplifications in order to account for the relative scales of the
processes involved, and discretizations, in order to be able to
implement appropriate solvers.
%Indeed, some small scale processes, such as
% turbulences, are notoriously hard to model.
% and a fine knowledge of
% those may not be completely relevant for the foreseen application of
% the model.

Models are then only a partial representation of the reality, which
aim at representing complex processes that occur across a large range
of scales, which interact with each other. For computational reasons,
no modelling system would be able to take all those different scales
into account, but instead, their effect are incorporated in the
modelling by overly simplifying them and by \emph{parametrization}.

In ocean modelling, and especially in coastal regions, a telling
example of this is the parametrization of the bottom friction. This
phenomenon occurs as the asperities of the ocean bed dissipates energy
through turbulences, and thus affects the circulation at the
surface. Since this happens at a subgrid level, \emph{i.e.} at a scale
usually several order of magnitudes below the scale of the domain
studied, modelling all those turbulences is completely unfeasible in
practice: the knowledge of the ocean bed is too limited for such
applications and the computational power required would be
unthinkable. Instead, the effect of the bottom friction is accounted
for through parametrization, so by introducing a new parameter which
is defined at every mesh point.

As this modelling is supposed to represent the reality, the prediction
should be compared with some data acquired through observations. This
comparison usually takes the form of the definition of a misfit
function $J$ that measures the error between the forecast and the
reality. This objective function is then minimized with respect to
some chosen parameters
$\kk$~\cite{das_estimation_1991,das_variational_1992,boutet_estimation_2015}
in order to get a calibrated model. Those parameters will be called
the \emph{control parameters}.

% Those calibrated models are often used to make decisions afterward,
% such as emergency evacuations plans, or optimisation of the position
% of turbines, or for forecasts. 
However, the parameters introduced are not the only source of errors
in the modelling.  % Garder ?
For such complex systems, some additional inputs are subject to
unrepresented statistical fluctuations~\cite{zanna_ocean_2011},
manifesting themselves at the boundary conditions, or in the forcing
of the model for instance. Such \emph{intrinsic} uncertainties are
often subject to variability, and neglecting this can lead to further
errors~\cite{mcwilliams_irreducible_2007}. We chose to model this additional
source of uncontrollable uncertainty with a random variable $\UU$.
Based on this, the objective function is then a function which takes
two arguments: the parameter that we wish to calibrate $\kk$, and some
other parameter $\uu$, which can be thought as a realisation of the random variable
$\UU$, that we shall call \emph{environmental} parameter.

Due to the presence of this random source of uncertainty, we wish to
calibrate the model, \emph{i.e.} to select a value of the control
parameter $\kk$, in a manner that guarantees that the model represents
accurately enough the reality, as often as possible given the value of
the environmental parameters. In other words, as the objective
function is a first glimpse at the quality of the calibration, we wish
that this function exhibits \emph{acceptable} values as often as
possible, when $\kk$ is fixed. This defines intuitively the underlying
notion of \emph{robustness} with respect to the variability of the
uncertain variable. Omitting this can lead to \emph{localized optimization}~\cite{huyse_probabilistic_2002}, which is the 

In this thesis, we will study an aspect of the calibration of a
numerical model under uncertainty, by discussing the notion of
robustness, and by proposing a new criterion of robustness.  Specific
methods will also be introduced, and applied to the calibration of a
numerical model of the ocean. The thesis is organized as follows:
\begin{itemize}
\item in~\cref{chap:inverse_problem}, we introduce notions of
  statistics and probabilities that we will use to define the
  calibration problem. More specifically, the statistical and Bayesian
  inference problems will be broached, as well as some aspects of
  nested model selection using the likelihood ratio test, while
  emphasizing the link between probabilistic formulations of the
  inference problem, and variational approach, based on the
  optimisation of a specified objective function.
\item in~\cref{chap:robust_estimators}, we are going to discuss some
  of the notions of robustness that can be found in the literature,
  either from a probabilistic inference aspect, or by the field of
  optimisation under uncertainties.  Some of the optimisation under
  uncertainties methods rely on the optimisation of the moments of
  $\kk\mapsto J(\kk,\UU)$
  (in~\cite{lehman_designing_2004,janusevskis_simultaneous_2010}),
  while other methods are based on multiobjective problems, such as
  in~\cite{baudoui_optimisation_2012,ribaud_krigeage_2018}. These
  approaches may compensate some bad performances by some very good
  ones, as we are averaging with respect to $\UU$.

  We propose especially a new criterion of optimisation, which is
  based on the comparison between the objective function at a couple
  $(\kk,\uu)$ and its optimal value for the same environmental
  variable. This notion of regret, either relative or additive, is
  then optimised in the sense of minimizing the probability of
  exceeding a specified threshold, or to minimize one of its
  quantile. This work has led to the publication of an
  article~\cite{trappler_robust_2020}.
  
\item The family of criteria introduced in
  \cref{chap:robust_estimators} can be quite expensive to evaluate,
  that is why in~\cref{chap:adaptative_design_gp}, we will discuss the
  use of metamodels, Gaussian Processes especially, in order to choose
  iteratively the new points to evaluate. The process of selection,
  called \emph{SUR} method~\cite{bect_sequential_2012} (Stepwise
  Uncertainty Reduction) will depend on the type of robust estimation
  we wish to carry. Different methods will be proposed, which differ
  by the measure of uncertainty on the function we wish to
  optimize. We will also introduce methods in order to select a batch
  of points, in order to take advantage of parallelism when available.

\item Finally, in~\cref{chap:croco}, we will study the calibration of
  an regional coastal model, based on CROCO\@. We will first define an
  objective function for this problem, and optimize it using an
  gradient-descent algorithm. After having 
\end{itemize}


\etoile
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%           PREVIOUSLY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Numerical models are widely used to study or forecast natural
% phenomena and improve industrial processes.
% However, by essence models
% only partially represent reality and sources of uncertainties are
% ubiquitous (discretisation errors, missing physical processes, poorly
% known boundary conditions). Moreover, such uncertainties may be of
% different nature.~\cite{walker_defining_2003} proposes to consider two
% categories of uncertainties:
% \begin{itemize}
% \item Aleatoric uncertainties, coming from the inherent variability of
% a phenomenon, \emph{e.g.} intrinsic randomness of some environmental
% variables
% \item Epistemic uncertainties coming from a lack of knowledge about
% the properties and conditions of the phenomenon underlying the
% behaviour of the system under study
% \end{itemize} The latter can be accounted for through the introduction
% of ad-hoc correcting terms in the numerical model, that need to be
% properly estimated. Thus, reducing the epistemic uncertainty can be
% done through parameters estimation approaches. This is usually done
% using optimal control techniques, leading to an optimisation of a well
% chosen cost function which is typically built as a comparison with
% reference observations.
%   %
%   An application of such an approach, in the context of ocean
% circulation modeling, is the estimation of ocean bottom friction
% parameters in~\cite{das_estimation_1991}
% and~\cite{boutet_estimation_2015}.

 
  
%   The calibration often takes the form of the minimisation of a
% function $J$, that describes a distance between the output of the
% numerical model and some given observed data, plus generally some
% regularization terms.  In our study, this cost function takes two
% types of arguments: $\kk\in\Kspace$ that represents the parameters to
% calibrate, and $\uu\in\Uspace$, that represents the environmental
% conditions.  We assume that the environmental conditions are uncertain
% by nature, and thus will be modelled with a random variable $\UU$, to
% account for these aleatoric uncertainties.  This is then the random
% variable $J(\kk,\UU)$ that we want to minimize ``in some sense'' with
% respect to $\kk$.



  
%   We propose to compare the value of the objective function to the
% best value attainable given the environmemtal conditions at this
% point, with the idea that we want to be as close as possible, and as
% often as possible, to this optimal value. Introducing the relative
% regret, that is the ratio of the objective function by its conditional
% optimum, we can define a new family of robust estimators.

%   Within this family, choosing an estimator consists in favouring
% either its robustness, \emph{e.g} its ability to perform well under
% all circumstances, or on the contrary favour near-optimal
% performances, transcribing a risk-averse or a risk-seeking behaviour
% from the user.
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{../../bibzotero}
}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
