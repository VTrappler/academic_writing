\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter4/#1}
}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter2/build/Chapter2}
\externaldocument{../../Text/Chapter3/build/Chapter3}
\externaldocument{../../Text/Chapter4/build/Chapter4}
\externaldocument{../../Text/Chapter5/build/Chapter5}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagestyle{conclusionStyle}

% \relax

% \begingroup
%% ---- On veut que "conclusion" soit entre les trait au début du chapitre

%% ---- On veut que ce soit le chapitre numéro 3 en notation alphabétique pour avoir un C
% \clearpage
% \setcounter{chapter}{2}
% \renewcommand{\thechapter}{\Alph{chapter}}%
\TitleBtwLines
\chapter*{Conclusion and perspectives}
\phantomsection
\addstarredchapter{Conclusion and perspectives}
\label{chap:Conclusion}
\renewcommand{\thesection}{} % In this thesis, we studied the problem of
% the calibration of a numerical model under uncertainties, by proposing a new criterion based on the regret, relative or additive

\paragraph{Summary}

Numerical models are ubiquitous nowadays for the prediction of various
phenomena, which have a large impact especially for policy- and
decision-making.  Due to the increase in computing power and to the
various advances in research that allow the quantification of
additional effects or interactions that were overlooked until then,
realistic models may become increasingly complex, as additional
parameters are taken into account. In order to get a meaningful
representation of the reality, those parameters have to be chosen
accordingly. Furthermore, the numerical model studied may also be
affected by some random inputs, making the calibration more
difficult. In this thesis, we tackled the estimation problem of
parameters under uncertainty. More precisely, we studied the notion of
robustness of a calibrated model when a random variable which
represent some uncontrollable environmental conditions is taken into
account.


In~\cref{chap:inverse_problem}, after having covered common notions
of probability and aspects of statistical inference, we described 
the calibration problem as an optimisation problem by introducing an
objective function, that we wish to minimise with respect to the
control parameter $\kk$.

However, due to the presence of some random environmental variables
$\UU$ in the study, a plain minimisation of the objective function is
not completely relevant, as its result will depend on the realisation of this
random variable assumed.  By
taking into account the random nature of the {environmental
  parameters}, the calibration can be seen as a problem of
\emph{optimisation under uncertainties}, and many specific methods and
criteria can be defined to treat accordingly this new problem of
\emph{robust} calibration.


Some classical criteria are first introduced
in~\cref{chap:robust_estimators}. Depending on the framework used to
describe the quantities involved, we can define Bayesian or
frequentist estimates by keeping a fully probabilistic inference
framework. On the other hand, when considering a variational approach,
estimates such as the minimiser of the mean value of the objective
function, \emph{i.e.} the minimiser of the \emph{expected loss}, are
often used.

In this thesis, we focused on estimates based on the notion of regret:
instead of comparing directly the values taken by the objective
function for different configurations given by $\uu$, the regret
allows the modeller to compare the value of the objective with the
best attainable performance given this specific environmental
variable.  This allows to put less emphasis on configurations which
lead to bad performances, and to focus more on ``salveagable''
situations.

Moreover, the user can adjust a parameter in order to reflect either a
risk-adverse behaviour, by favourising a control of the regret with
high probability, or a risk-seeking one, by favourising an estimate
that will yield values of the regret closer to its optimum, albeit
with lower probability.


In general, criteria of robust optimisation require a global knowledge
of the function, since they often involve several evaluations of
expectations and probabilities with respect to $\UU$. In addition to
that, the family of regret-based estimates we introduced depends
directly on the conditional minimum and
minimiser. In~\cref{chap:adaptative_design_gp}, we proposed to use
Gaussian Processes (GP) in order to compute the quantities associated
with regret-based estimators. More precisely, we proposed a few
methods which aim at improving this estimation by choosing iteratively
a new, or a batch of new points to evaluate and to add to the design.

Finally in~\cref{chap:croco}, we studied an academic problem of
calibration of a coastal model based on CROCO.\@ After having reduced
the dimension of the input space based on the sediment type at the
bottom, we enriched the design in order to improve the estimation of
the functions that define the regret-based estimates, which are then
optimised.


\paragraph{Limitations and perspectives}
Throughout this thesis, we assumed that the forward operator was a
deterministic black-box, or \emph{deterministic simulator}, in the
sense that the uncertainties in the modelling are ``controlled'' by
the modeller. \emph{Stochastic simulator} in contrast do not take an
environmental parameter as parameter, so each evaluation of the
forward operator can be seen as a realisation of a random
variable. This may be the case when the environmental variables are
not easily parametrised, such as in the presence of functional
inputs~\citep{el_amri_analyse_2019}. In this case, it is not possible
to control the value of $\uu$ chosen for an enrichment strategies for
instance. An alternative strategy would be to consider this
uncertainty as \emph{noise} in the output of the simulator, leading to
\emph{noisy kriging} and/or or noisy optimisation
methods~\citep{picheny_noisy_2014}.
% 2-stage enrichment strategies

Based on the assumption that the environmental parameter $\uu$ was
indeed controllable, using the properties of the Gaussian processes we
developed criteria which aim at improving the estimations of the
functions $\Gamma_\alpha$ and $q_p$. In order to get the regret-based
estimators, those functions had to be optimised, using a set of
samples of $\UU$ to approximate expectations and quantiles, in a
sample average approximation (SAA) fashion. Because of this, for
levels of confidence very close to $1$, the estimation of quantities
in such high probabilities can be difficult, and usually needs
specific methods instead of simple Monte-Carlo sampling in order to
get accurate enough results, as in~\cite{razaaly_rare_2019}.

Also, instead of improving the estimation of $\Gamma_{\alpha}$ and of
$q_p$ on the whole space, we could develop methods in order to
optimise directly those functions, and thus combine the estimation and
the optimisation.  This could for instance be done in a $2$-stage
enrichment strategy, as
in~\cite{janusevskis_simultaneous_2010}. First, a value
$\tilde{\kk} \in \Kspace$ is chosen, with a ``high'' potential to be
the optimiser (quite similarly as the EI criterion), and then the
couple $(\kk_{n+1}, \uu_{n+1})$ to evaluate is chosen in order to
reduce a measure of uncertainty associated with the space
$\{\tilde{\kk}\} \times \Uspace$ (\emph{e.g.} the IMSE integrated over
this space). This would focus the enrichment in regions of interest,
and also reduce the dimension of the integral to evaluate.

We introduced also a few sampling methods, that rely on efficient
sampling in different margin of uncertainties. Depending on the
problem, this task can reveal itself quite challenging: the more
accurate the GP is, the less prediction variance it shows, and thus
the margin of uncertainty can become very thin. Simple sampling
schemes such as the acceptance-rejection method can become
inefficient, even more so when the dimension of the problem
increases. Refinement such as importance sampling can be considered
such as in~\cite{razaaly_rare_2019}.

% Calibration using a Bayesian approach
Once set aside potential technical improvements in the methods
proposed, we can propose a few other possibilities that may warrant
further investigations.

In this thesis we focused on the variational
formulation of the calibration problem, \emph{i.e.} by defining an
objective function, akin to the log-likelihood, or to the
log-posterior density that is then optimised. A Bayesian method could
be performed in order to estimate the posterior distribution of the
parameter $\kk$ given the observations. Moreover, this task could be
performed using adapted sampling schemes, such as Hamiltonian
Monte-Carlo~\citep{betancourt_conceptual_2017}, in order to use the
gradient that may be available using adjoint method.

This gradient, taken with respect to the control parameter, may be
useful at different stages: we can incorporate this additional
knowledge in the modelling of the GP, so that the predictions are
improved, as done
in~\cite{bouhlel_gradient-enhanced_2019,laurent_overview_2019}.
However, GP are flexible and useful tools but are not well suited for
modelling problems with dimensions higher than about 10: when the
designs considered are too large, fitting the GP can also be
problematic, as large matrices have to be inverted, and the
optimisation of the hyperparameters can become difficult. Reducing the
dimension of the input spaces is then often necessary.
The segmentation we performed in~\cref{chap:croco} for instance is
rather coarse and based on external information. A finer dimension
reduction method could be done, without prior information, using the
gradient for instance, as done in~\cite{benameur_refinement_2002} or
in~\cite{zahm_certified_2018}.

% Choice of quantile ? to remove maybe
Finally, in this work, the random variables $J^*(\UU)$ and
$\kk^*(\UU)$ which represent the conditional minimum and minimisers
respectively, play a central role in our definitions of robustness,
and further developements could be imagined around those random
variables.
% Those distributions
% are involved in the expression of the regret, that we control through
% its quantile.
% However, we do not control the values of the function when
% exceeding this threshold. A similar approach could be developed
% analogous to the CVaR (Conditional Value at
% Risk)~\cite{rockafellar_conditional_2002}, also called
% \emph{expected shortfall}, in order to control the values of the
% objective function when they \emph{do} exceed the threshold.
% Horsetail matching, comparison J*(U) and J(k, U)
The distribution of the conditional minimum $J^*(\UU)$ can be seen as
an ``ideal'' distribution for the objective (\emph{i.e.} the
distribution that one could get if the calibrated parameter had always
been chosen optimally for all realisations of the uncertain
variable). The minimisation of a measure of misfit between the
distribution of $J^*(\UU)$ and the distribution of $J(\hat{\kk},\UU)$
(the objective function of the model calibrated with $\hat{\kk}$) may
be worth exploring. Horsetail matching for
instance~\citep{cook_extending_2017,cook_horsetail_2018} could be used
so that the a metric between cdf of the two distribution is minimised.
\thispagestyle{plain}
\markchapterend
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
        \bibliography{/home/victor/acadwriting/bibzotero}
}
% \relax

% \endgroup
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
