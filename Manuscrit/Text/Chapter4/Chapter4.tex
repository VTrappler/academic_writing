\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter4/#1}
}
% \DeclareMathOperator{\IMSE}{\mathrm{IMSE}}
% \newcommand{\IMSE}{\mathop*{\mathrm{IMSE}}}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction}
\externaldocument{../../Text/Chapter2}
\externaldocument{../../Text/Chapter3}
\externaldocument{../../Text/Chapter5}
\externaldocument{../../Text/Annexes}
\externaldocument{../../Text/Conclusion}
}
\newcommand\imgpath{/home/victor/acadwriting/Manuscrit/Text/Chapter4/img/} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \dominitoc
% \faketableofcontents
% \subfileLocal{\setcounter{chapter}{2}}
\chapter{Adaptive strategies for calibration using Gaussian Processes}
\label{chap:adaptative_design_gp}
\minitoc
\newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SECTION 0 : Introduction	   %%%

\section{Introduction}

In the previous chapter, we introduced different notions of robustness
for the estimation of a calibration parameter. Those objectives
usually require the evaluation of various integrals, and to perform
several optimisations. A large number of model evaluations is then
needed, but it may bring some practical issues, as numerical models
are usually very expensive to run in terms of computational
resources. Indeed, for most realistic physical simulations, systems of
PDEs over large discretized domains have to be solved. Even though the
programs are optimised and parallelized to take best advantage of
high-performance computers, the time required to compute the
quantities of interest may range from a few seconds to days. Because
of that, methods requiring a large number of runs of the model for
exhaustivity should be avoided.

% On the other hand, one other issue commonly encountered is the dimensionality of the parameter spaces. The number of samples required to get the fixed coverage of a space scales exponentially with the number of dimensions: this is the \emph{curse of dimensionality}. This motivates the ongoing research on dimension reduction. Some classical techniques, such as Principal Component analysis (PCA) \cite{jolliffe_principal_2002}, and Singular Value Decomposition (SVD) look . \cite{zahm_certified_2018} look to find the subspaces where the Bayesian update has the most effects.\cite{blanchet-scalliet_specific_2017,ribaud_krigeage_2018}

% \subsubsection{Sensitivity analysis}

In this chapter, we will focus on the use of \emph{surrogate models}
to solve robust optimisation problems, according to some of the
criteria introduced in the previous chapter.
\begin{definition}[Surrogate function]
  Let $f: \Xspace \rightarrow \mathbb{R}$ be a function representing
  the computation of a quantity of interest. A \emph{surrogate}, or
  \emph{metamodel}, or \emph{emulator} of $f$, say $g$, is a function
  from $\Xspace$ to $\mathbb{R}$ which possesses two main properties:
  \begin{itemize}
  \item $g$ is an approximation of $f$
  \item $g$ is cheaper to evaluate than $f$ 
  \end{itemize}
\end{definition}
We will focus exclusively on
kriging~\citep{krige_statistical_1951,matheron_traite_1962}, also
called Gaussian Process regression, but other methods can be used to
solve such problems. Polynomial Chaos regression for instance
\cite{wiener_homogeneous_1938,xiu_wiener--askey_2002,sudret_polynomial_2015,miranda_adjoint-based_2016}. %\todo{motivation choix du krigeage}

Those surrogates then be used directly instead of $f$ in a
\emph{plug-in} approach.

\begin{definition}[Plug-in method]
  \label{def:plugin}
  In this work, we will use the term \emph{plug-in} as the fact of
  using directly an estimated quantity, such as a surrogate $g$
  instead of the expensive-to-evaluate original function $f$ in
  computations.
\end{definition}

In this chapter, after defining the usual kriging equations
in~\cref{sec:krigin_equations}, we are going to introduce a few
classic and useful criteria in \cref{sec:enrichment_strategies} for
global optimisation and/or exploration of an unknown function
$f$. Finally, in~\cref{sec:robust_criteria_gp}, we are going to focus
on robust optimisation, by splitting the input space in $\Kspace$ and
$\Uspace$, and introduce other strategies to take advantage of the
nature of the surrogate in order to efficiently compute the robust
estimates introduced before, especially regret-based estimators.

% Surrogate models are often built using initial evaluations of $f$.
\section{Gaussian process regression}
In the following, we will consider a generic function $f$, that maps
a space $\Xspace$ to $\mathbb{R}$. Depending on the application,
$\Xspace = \Kspace$ or $\Xspace = \Kspace \times \Uspace$. This
function is unknown, and expensive to evaluate.

\subsection{Random processes}
Let us assume that we have a map $f$ from a $p$-dimensional space to $\mathbb{R}$:
\begin{align}
  \begin{array}{rrcl}
    f: & \mathbb{X} \subset \mathbb{R}^p& \longrightarrow & \mathbb{R} \\
       & x & \longmapsto & f(x)
  \end{array}
\end{align}

This function is assumed to have been evaluated on a design of $n$
points,
$\mathcal{X} = \left\{ (x_i, f(x_i) \right)\}_{1\leq i\leq n}$, called
the \emph{initial design}. For notational simplicity, we write
$x\in \mathcal{X}$ if $(x, f(x)) \in \mathcal{X}$, \textit{i.e.} if
the point $x$ has been evaluated by $f$.  As this function is unknown,
there is uncertainty on the values outside the initial design, which
can be classified as \emph{epistemic} since it can be reduced by
directly evaluating the function (see \cref{sec:def_robustness}).
This uncertainty on the value taken by the function leads us to the
definition of random processes:
\begin{definition}[Random process]
  Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space, and
  $\Xspace\subset \mathbb{R}^p$.  A random process $Z$ is a collection
  of random variables indexed on $\Xspace$, so for each
  $x \in \Xspace$, $Z(x)$ is a real random variable (\textit{i.e.}
  $Z(x):\Omega \rightarrow \mathbb{R}$):
 \begin{equation}
  \begin{array}{rcl}
    Z: \mathbb{X} & \longrightarrow & \left(\Omega \rightarrow \mathbb{R} \right)\\
    x& \longmapsto & Z(x)
  \end{array}
\end{equation}
A sample from this random process, that is $Z(\cdot)(\omega)$ for
$\omega \in \Omega$ will be shortened as $Z(\cdot, \omega)$ for
notational purpose, and is called a \emph{sample trajectory}, or a
\emph{sample path}.  When $\omega$ is omitted, $Z(x)$ represents the
random process at the point $x \in \Xspace$.
\end{definition}
From a Bayesian point of view, such a random process can act as a
prior on the function $f$, or in other words, $f$ can be thought as a
particular sample path of $Z$.  Evaluating the function at an
additional point $x \notin \mathcal{X}$ provides new information on
the random process, and we can update our belief on $f$.


In this work, we are going to focus exclusively on a specific type of
random process, namely the Gaussian process (abbreviated as GP). Other
types of random process can be encountered in the literature: Student
t-processes in~\cite{shah_student-t_2014} are introduced as
alternatives to GP to account for larger tails, or various graphical
models such as Gaussian and Markov Random Fields
in~\cite{bishop_pattern_2006,li_markov_2009} are used to model images
for instance.

\begin{definition}[Gaussian process]
  Let $Z$ be a random process on $\Xspace$, \textit{i.e}. a collection
  of random variables indexed by $\Xspace$. $Z$ is a Gaussian process
  (GP) if any finite number of those random variables have a
  multivariate joint Gaussian distribution.  In that case, $Z$ is
  uniquely defined by its mean function
  $m_Z:\Xspace \rightarrow \mathbb{R}$ and its covariance function
  $C_Z:\Xspace \times \Xspace \rightarrow \mathbb{R}$:
  \begin{align}
    m_Z(x) &= \Ex\left[Z(x)\right] \\
    C_Z(x, x^{\prime}) &= \Cov[Z(x), Z(x^{\prime})]
  \end{align}
  and we write $Z \sim \GP(m_Z, C_Z)$.
  Due to the definition of a GP, we also have for all $x\in \Xspace$
  \begin{equation}
    Z(x) \sim \mathcal{N}(m_Z(x), \sigma^2_Z(x)) 
  \end{equation}
  with $\sigma_Z^2(x) = C_Z(x, x)$
\end{definition}
A more thorough description of Gaussian processes and their applications can be found in~\cite{rasmussen_gaussian_2006}.


Let $Z$ be a GP with a known covariance function. The construction of
a covariance function will be discussed~\cref{sec:cov_fun}.  Based on
the initial design $\mathcal{X}$, we can construct a surrogate for the
unknown function $f$ by conditioning the GP on the design which
comprises some evaluations of $f$.

\subsection{Kriging equations}
\label{sec:krigin_equations}
Given a Gaussian process $Z$ as a prior on $f$, and by conditioning it by the initial design $\mathcal{X} = \{(x_i, f(x_i)\}_{1 \leq i \leq n}$, the conditioned random process is still a GP:
\begin{equation}
  \label{eq:GP_Z_cond}
  Z \mid \mathcal{X} \sim \GP(m_{Z\mid \mathcal{X}}, C_{Z\mid \mathcal{X}})
\end{equation}
Given the nature of $Z$ and the initial design, we can derive the
joint distribution of the GP at the points of the design and the GP at
an unobserved point $x \in \Xspace$:
\begin{equation}
  \label{eq:GP_joint_distrib}
  \begin{pmatrix}
    Z(\mathbf{x}) \\
    Z(x)
  \end{pmatrix} \sim
  \mathcal{N}\left(
    \begin{pmatrix}
      \mu_Z \\
      m_{Z}(x)
    \end{pmatrix} ;
    \begin{pmatrix}
      \mathbf{K}_{\mathcal{X}} & K_{\mathcal{X}}(x) \\
       K_{\mathcal{X}}(x)^T & C_Z(x, x)
    \end{pmatrix}
\right)
\end{equation}
where $\mathbf{x} = (x_1,\dots,x_n)$, $x_i\in\mathcal{X}$, for $ 1\leq i\leq n$ are the points of the design and
\begin{align}
   Z(\mathbf{x}) &= (Z({x}_1),\dots,Z({x}_n)) \\
  K_{\mathcal{X}}(x) &= \left(C_Z(x, x_1),C_Z(x, x_2),\dots,C_Z(x,x_n)\right)^T \\
  \mathbf{K}_{\mathcal{X}} &= \left(C_Z(x_i, x_j)\right)_{1 \leq i,j \leq n}
\end{align}
and $\mu_Z = \Ex\left[Z(\mathbf{x})\right]$ is the mean of the
unconditionned GP. When this mean is assumed to be known, the kriging
procedure is qualified of \emph{Simple Kriging}, otherwise, we often
talk about \emph{Ordinary Kriging}. Finally, when this mean is a
deterministic function (that may be estimated), we talk about
\emph{Universal Kriging}~\citep{le_riche_introduction_2014}. In the
following, we consider Simple Kriging.

Using the properties of the multivariate distribution, the GP
conditioned on the observations of~\eqref{eq:GP_Z_cond} has mean and
covariance function defined by
\begin{align}
  m_{Z \mid \mathcal{X}}(x) &= m_Z(x) + K_{\mathcal{X}}(x)^T \mathbf{K}_{\mathcal{X}}^{-1}(f(\mathbf{x}) - \mu_Z ) \\
  C_{Z\mid \mathcal{X}}(x, x^\prime) &= C_Z(x, x^\prime)  - K_{\mathcal{X}}(x)^T\mathbf{K}_{\mathcal{X}}^{-1}K_{\mathcal{X}}(x^\prime)
\end{align}
These are called the \emph{kriging equations}.


Given the fact that a conditioned GP is still a GP, at a point $x \in \Xspace$, we have
\begin{equation}
  \label{eq:cond_gp}
  Z(x) \mid \mathcal{X} \sim \mathcal{N}\left(m_{Z\mid \mathcal{X}}(x), \sigma^2_{Z\mid \mathcal{X}}(x)\right) \quad \text{ with } \quad \sigma^2_{Z\mid \mathcal{X}}(x) = C_{Z \mid \mathcal{X}}(x, x)
\end{equation}
The function $m_{Z \mid \mathcal{X}}: \Xspace \rightarrow \mathbb{R}$ can then be defined as a surrogate for $f$. This surrogate provides an interpolation of the unknown function $f$, since for $x \in \mathcal{X}$, $f(x) = m_{Z \mid \mathcal{X}}(x)$.
In addition to that interpolation property, for points not in the design, we have a measure of the epistemic uncertainty modelled by the normal r.v.\ in~\eqref{eq:cond_gp}. As an illustration, for a few training points and their respective evaluations, we represented the GP regression of an unknown function $f$ \cref{fig:example_GP}.

\begin{figure}[ht]
  \centering
  \input{\imgpath example_GP.pgf}
  \caption[Illustration of GP regression]{\label{fig:example_GP} Example of a Gaussian Process, as a surrogate for a function $f$ evaluated at 4 inputs. The shaded regions correspond to the regions $m_Z \pm i \cdot \sigma_Z$ for $i=1, 2,3$.}
\end{figure}

From a computational point of view, GP are relatively cheap to handle:
\begin{itemize}
\item If the covariance function $C_Z$ is known, the most expensive step in the estimation of $m_{Z\mid \mathcal{X}}$ and $C_{Z \mid \mathcal{X}}$ is the inversion of the matrix $\mathbf{K}_{\mathcal{X}}$. This matrix does not depend on the points $x$ where we wish to evaluate $m_{Z\mid \mathcal{X}}$, thus its inversion needs only to be performed once. However, if there is a very large number of points in the design, and thus a very large matrix $\mathbf{K}_{\mathcal{X}}$, this inversion can reveal itself numerically complicated in practice. % \todo{ça s'adapte à des pbs de taille moyenne}
\item In order to get a sample path from this GP, one need to be able to sample from a multivariate normal distribution, and thus only need to compute the Cholesky decomposition of the conditioned covariance matrix.
\end{itemize}
However, if some points of the design are very close to each other, the covariance matrix can be ill-conditioned. In this case,
for numerical stability, we can add a \emph{nugget} effect to $\mathbf{K}_{\mathcal{X}}$, and~\eqref{eq:GP_joint_distrib_nugget} becomes:
\begin{equation}
  \label{eq:GP_joint_distrib_nugget}
  \begin{pmatrix}
    Z(\mathbf{x}) \\
    Z(x)
  \end{pmatrix} \sim
  \mathcal{N}\left(
    \begin{pmatrix}
      \mu_Z \\
      m_{Z}(x)
    \end{pmatrix} ;
    \begin{pmatrix}
      \mathbf{K}_{\mathcal{X}} + \varsigma^2\mathrm{Id}& K_{\mathcal{X}}(x) \\
       K_{\mathcal{X}}(x)^T & C_Z(x, x)
    \end{pmatrix}
\right)
\end{equation}
From a probabilistic point of view, we can see the nugget effect as an
added Gaussian noise when evaluating the function, \emph{i.e.} that we
are observing $f(x_i)+\epsilon$ with
$\epsilon \sim \mathcal{N}(0, \varsigma^2)$. When
$\varsigma^2 \neq 0$, the GP is no longer interpolant, as
$\Var\left[Z(x) \mid \mathcal{X}\right] \neq 0$ for
$x\in \mathcal{X}$.

\subsection{Covariance functions}
\label{sec:cov_fun}
In the previous section, we described the equations to solve to get
the surrogate $m_{Z\mid \mathcal{X}}$ of $f$ based on GP.  The kriging
equations are based on the covariance function $C_Z$.  A covariance is
said to be stationary, if for all $x, x^{\prime} \in \Xspace$, the
covariance of the GP between those two points depends only on the
difference $h = x-x^{\prime}=(h_1,\cdots, h_{p})$. In that case, we
will write $C_Z(x, x^{\prime}) = C_Z(h)$.  For multidimensional
problems, covariance functions are usually chosen as the product of
one dimensional covariance functions:
\begin{equation}
  C_Z(h) = s^2\prod_{i=1}^{p} C_i(h_i;l_i)
\end{equation}
These covariance functions introduce an additional parameter
$l=(l_1,\dots,l_p)$ of dimension $p=\dim \Xspace$, and a variance
parameter $s^2$.  $l$ is called the \emph{length scale} and quantify
the radius of influence of an evaluation along the different input
variables. If the length scales are all equals, the covariance kernel
is said \emph{isotropic}. Otherwise, the kernel is \emph{anisotropic}.
A few common stationary $1D$-covariance functions are
introduced~\cref{tab:common_cov_fc}.

  \begin{table}[ht]
    \centering 
    \begin{tabular}{lrc}
      \toprule
      Name        & $C(h;l)$                                                                                                 & Regularity of sample paths \\ \midrule
      Gaussian    & $\exp\left(- \frac{h^2}{2 l^2}\right)$                                                                   & $C^{\infty}$               \\
      Exponential & $\exp\left(- \frac{\lvert h \rvert}{l}\right)$                                                           & $C^0$                      \\
      Matérn 3/2  & $\left(1 + \sqrt{3}\frac{h}{l}\right)\exp\left(-\sqrt{3}\frac{h}{l}\right)$                              & $C^1$                      \\
      Matérn 5/2  & $\left(1+ \sqrt{5}\frac{h}{l} + \frac{5}{3}\frac{h^2}{l^2}\right) \exp\left(-\sqrt{5}\frac{h}{l}\right)$ & $C^2$                      \\ \bottomrule
    \end{tabular}
    \caption{\label{tab:common_cov_fc} Common stationary covariance functions}
  \end{table}

  The choice of a particular covariance function is usually motivated
  by the wanted regularity of the sample paths. For example, if the
  unknown function $f$ is assumed to be infinitely differentiable, a
  Gaussian kernel is suited for the modelling. One common choice is
  the Matérn kernel of order $5/2$, so that the sample paths are
  twice-differentiable. \Cref{fig:cov_fc_examples} shows the shape of
  the different covariance functions
  introduced~\cref{tab:common_cov_fc}, and also a few sample paths of
  an unconditioned Gaussian Process $Z\sim\GP(0, C(\cdot, \cdot))$ for
  each of the covariance functions.
  
\begin{figure}[ht]
  \centering
  \input{\imgpath covariance_functions.pgf}
  \caption[Common covariance functions for GP]{\label{fig:cov_fc_examples} Common covariance functions for GP regression. The right plots show (unconditioned) sample paths for those different covariance functions with same length scale.}
\end{figure}

\subsection{Initial design and validation}

The $(p + 1)$ hyperparameters of the covariance, \textit{i.e.} the
length scales $(l_1,\dots,l_{p})$ and the variance parameter $s$ have
to be estimated based on the training set $\mathcal{X}$. This is
usually done by MLE~(see for instance~\cite{ribaud_robustness_2019}),
or by cross-validation (see~\cite{ginsbourger_note_2009}).

In order to construct a Gaussian process, or a general metamodel for
that matter, the model must first be evaluated on the initial design
$\mathcal{X}$ which should present good space-filling properties. An
usual choice is to use Latin Hypercube Sampling (LHS), and a
widespread rule of thumb for the initial number of points of $\Xspace$
to be evaluated is about $10p$. In order to validate the GP,
\emph{i.e.} to verify that the GP does not overfit the data, one can
use cross-validation as described in~\cite{dubrule_cross_1983}.


\section{General enrichment strategies for Gaussian Processes}
\label{sec:enrichment_strategies}

We detailed so far the construction of a GP on a unknown function $f$,
based on an initial design. However the surrogate is usually not an
end in itself, as it may be used afterward to replace the original
function for expensive procedures in a plug-in approach (see
\cref{def:plugin}). Obviously, if the surrogate is not accurate
enough, the subsequent computations can become meaningless, that is
why we are going to see how one can improve such surrogate.

The approximation given by the metamodel can be improved by adding
points to the design, \emph{i.e.} by evaluating some points in the
input space. In order to incorporate as most information on $f$ as
possible, choosing those points can be done sequentially, by
optimising a specified criterion, as described~\cref{ssec:1stepSUR},
or we can look to add a batch of $K$ points each iteration, as
described in~\cref{sec:sampling_based_criterion}.  Those procedures
can be performed until some specific stopping criterion is
met. Alternatively, as the unknown function $f$ is expensive to
evaluate, we can define a maximal number of calls allowed to the
function, and the procedures stop when the budget of evaluations is
spent.



\subsection{1-step lookahead strategies}
\label{ssec:1stepSUR}
For an unknown function $f$, a GP is initially constructed based on a
design
$\mathcal{X}_0 = \left\{\left(x_1,f(x_1)\right), \dots, \left(x_{n_0},
    f(x_{n_0})\right)\right\}$, that consists of $n_0$ points of
$\Xspace$, and their corresponding evaluations. This GP is denoted
$Z \mid \mathcal{X}_0$ and is defined as
\begin{equation}
  \label{eq:ZgivenXGP}
  Z\mid \mathcal{X}_0 \sim \mathcal{N}(m_{Z\mid\mathcal{X}_0}(x),\sigma^2_{Z\mid\mathcal{X}_0}(x))
\end{equation}
For notational convenience, the conditioning with respect to
$\mathcal{X}_0$ will be omitted if the experimental design is clear
from the context.

In order to enrich the design of experiments sequentially, we can
adopt a \emph{Stepwise Uncertainty Reduction}
strategy~\citep{villemonteix_informational_2006}, which is based on
the definition of a \emph{criterion} (also called \emph{learning
  function}), say $\kappa$, that measures the uncertainty upon a
certain objective associated with the GP and with $f$. This criterion
implicitly depends on the GP and thus the design used to construct it.

To select the next point to evaluate with $f$, this criterion is
maximised on $\Xspace$.  This procedure is then repeated at each
iteration: at the iteration $n$, with the set of evaluated points
$\mathcal{X}_n$, the next point is then chosen as
\begin{equation}
  x_{n+1} = \argmax_{x\in\Xspace} \kappa(x) = \argmax_{x\in\Xspace} \kappa(x; Z \mid \mathcal{X}_n)
\end{equation}
The GP is updated according to the new evaluation which is added to
the design. The general principle of this strategy is summarized
in~\cref{alg:SUR_strat}
% This new point is then evaluated by $f$, and the pair is added to the design: $\mathcal{X} \gets \mathcal{X} \cup \{(x_{n+1}, f(x_{n+1}))\}$.

\begin{algorithm}
  \caption{\label{alg:SUR_strat} SUR strategy: adaptive enrichment using a 1-step criterion}
\begin{algorithmic}
\REQUIRE Initial design $\mathcal{X}_0$, criterion function $\kappa$
\STATE Fit $Z$, a GP using the design $\mathcal{X}_0$
\STATE $n \leftarrow 0$
\WHILE{(stopping criterion not met) \OR (evaluation budget not reached)}
\STATE $x_{n+1} \leftarrow \argmax_{x \in \Xspace} \kappa(x; Z \mid \mathcal{X}_{n})$
\STATE Evaluate $f(x_{n+1})$
\STATE $\mathcal{X}_{n+1} \leftarrow \mathcal{X}_n \cup \left\{\left(x_{n+1}, f(x_{n+1})\right)\right\}$
\STATE Update the GP using $\mathcal{X}_{n+1}$
\STATE $n \leftarrow n + 1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{Batch selection of points: sampling-based methods}
\label{sec:sampling_based_criterion}

Due to the structure of many computer codes, it may sometimes be
beneficial to evaluate a batch of $K$ points instead of selecting a
unique point at each step of the procedure. SUR strategies as
introduced before require the optimisation of a criterion function to
select a single point, but in~\cite{ginsbourger_kriging_2010}, the
authors introduce a few ways to transform the criterion in order to be
able to select $K$ points that will be evaluated by the true function.

% \paragraph{Transforming a 1-step into a $q$-step criterion}
% Using a greedy approach, we can modify $1$-step criteria to $q-$step
% criteria, by performing successive optimisation. Let $\kappa$ be an
% enrichment criterion, $\mathcal{X}_n$ the design at the iteration
% $n$, and we define for $1\leq i \leq q$,
% \begin{align}
%   x^{n+1} &= \argmax_{x\in\Xspace} \kappa\left(x; Z \mid \mathcal{X}_n\right) \\
%   x^{n+2} &= \argmax_{x\in\Xspace} \kappa\left(x; Z \mid \mathcal{X}_n\cup \left\{(x^{n+1}, z^{n+1}) \right\}\right) \\ &\vdots \\
%   %  x^{n+i} &= \argmax_{x \in \Xspace} \kappa\left(x; Z \mid \mathcal{X}_n\cup\left\{(x^{n+1}, z^{n+1}), \dots ,( x^{n+i-1}, z^{n+i-1})\right\} \right)
% \end{align}
% Each point $x^{n+i}$ is associated with $z^{n+i}$, a
% \emph{potential} evaluation of $f(x^{n+i})$, \emph{i.e.} a $z^{n+i}$
% is a realization of $Z(x^{n+i})$.  Depending on the choice of the
% realizations $z^{n},\dots,z^{n+i-1}$. We can augment the design in
% different ways:
% \begin{itemize}
% \item $\left(x^j, z^j\right) = \left(x^j, m_Z(x^j)\right)$ is the
%   \emph{kriging believer} (KB) method, as we the observation is
%   replaced by the kriging prediction
% \item $\left(x^j, z^j\right) = \left(x^j, L\right)$ leads to the
%   \emph{Constant liar} (CL) method, as we replace the observation
%   with an arbitrary choice of $L$, such as $\min f$, $\max f$,
%   $\mathrm{mean} f$.
% % \item $\left(x^j, z^j\right) = \left(x^j, Z(x^j)\right)$ assumes
% %   unknown the realisation. , require the evaluation of several
% %   expensive integrals in order to evaluate $\kappa$, so it may not
% %   be suited for realistic applications
% \end{itemize}
% By choosing a specific realization instead of averaging over all
% possible realizations, we avoid the avoid the successive
% optimisation of expectations, which can be very expensive.  However,
% despite avoiding the successive integrals, the optimisations can be
% expensive.

Instead of adapting a 1-step criterion, we can rely on sampling in
order to get $K$ points of $\Xspace$ to evaluate.  This technique,
named AK-MCS in~\cite{echard_ak-mcs_2011}, which stands for
\emph{Adaptive-Kriging Monte-Carlo Sampling} is described
in~\cite{dubourg_reliability-based_2011}, and has been applied and
refined more recently
in~\cite{schobi_rare_2017,razaaly_rare_2019,razaaly_quantile-based_2020}
for the estimation of extreme quantiles and small probabilities.
Those are qualified as Monte-Carlo sampling methods, as they rely on
obtaining samples drawn from a given distribution which represent
points whose evaluation could be interesting with regards to the
problem at stake. % \todo{anglais ?}

Let $\kappa$ be a 1-step criterion, and let
$\bar{\kappa}(x) =
\frac{\kappa(x)}{\int_{\Xspace}\kappa(u)\,\mathrm{d}u}$ be the
normalized criterion, so that $\int_{\Xspace} \bar{\kappa}=1$. In this
case, $\bar{\kappa}$ can be seen as a density.  Using an appropriate
sampler, we can generate $N$ i.i.d.\ samples
$S=\{x_j^s\}_{1\leq j \leq N}$ from this criterion.

  
However, as $N$ should be large, there is no point in evaluating all
the samples in $S$: using statistical reduction of the samples, we can
find a reduced number of points, which are the most representative of
the $N$ samples. This is usually done by means of clustering
algorithms, especially those who can take a fixed number of clusters
such as the KMeans algorithm~\citep{macqueen_methods_1967}, in order
to select precisely $K$ points, number chosen in compliance with the
wanted batch size. For each one of those centroids, we then look for
the closest sampled point as the centroid can be located in a region
of low or even zero $\bar{\kappa}$. Finally, the closest sampled
points can then be considered to be evaluated.  Additional filtering
can then be performed in order to avoid numerical issues, if the
chosen points are too close to each other or to points of the design.
The principle of AK-MCS (using KMeans) is
described~\cref{alg:sampling_enrichment}.

\begin{algorithm}
\caption{AK-MCS: enrichment of the design using sampling}
\label{alg:sampling_enrichment}
\begin{algorithmic}
  \REQUIRE Initial design $\mathcal{X}_0$, Sampler according to $\bar{\kappa}$
  \REQUIRE Batch size $K$, number of samples $N$
\STATE Fit $Z$, a GP using the design $\mathcal{X}_0$
\STATE $n \leftarrow 0$
\WHILE{(stopping criterion not met) \OR (evaluation budget not reached)} 
\STATE Sample $N$ points $S = \{x^s_j\}_{1 \leq j \leq N}$ according to the distribution with pdf $\bar{\kappa}$
\STATE Get the $K$ centroids $\{x_j^{\mathrm{center}}\}_{1 \leq j \leq K}$ using KMeans on $S$
\FOR{$j=1$ to $K$} % \COMMENT{Find the $N_s$ closest points}
\STATE $x_{n+j}=\argmin_{x \in S \setminus \{x_{n+1}, \dots, x_{n+j-1}\}} \|x - x_j^{\mathrm{center}} \|$
\ENDFOR
\STATE Evaluate $f(x_{n+1}),\dots, f(x_{n+K})$
\STATE $\mathcal{X}_{n+K} \leftarrow \mathcal{X}_n \cup \left\{\left(x_{n+1}, f(x_{n+1})\right),\dots,  \left(x_{n+K}, f(x_{n+K})\right)\right\}$
\STATE Condition the GP according to $\mathcal{X}_{n+K}$
\STATE $n \leftarrow n + K$
\ENDWHILE
\end{algorithmic}
\end{algorithm}



\section{Criteria of enrichment}

We are now going to introduce a few criteria which are used for three
different problems involving an unknown function $f$:
\begin{itemize}
\item The exploration of the input space will be
  addressed~\cref{sec:exploration_criteria}, in order to target
  regions where the kriging prediction may differ significantly from
  the true function;
\item the global optimisation of the function $f$
  in~\cref{sec:GP_optimization_criteria}, to focus the exploration of
  the input space $\Xspace$ in the regions where $f$ is minimal;
\item the estimation of different level sets $f^{-1}(\{T\})$ and/or
  excursion sets in the form
  $f^{-1}(\interval[open left]{-\infty}{T})$ for $T\in\mathbb{R}$
  in~\cref{sec:GP_vol_estim}.
\end{itemize}

\subsection{Criteria for exploration of the input space}
\label{sec:exploration_criteria}
We are going to introduce first some common criteria of enrichment,
that aim at exploring the input space $\Xspace$.  We suppose that the
current GP has been conditioned with the design $\mathcal{X}_n$,
composed of the points $x_i$ and their evaluations $f(x_i)$ for
$1\leq i \leq n$.

\paragraph{Maximum of variance}
A measure of uncertainty on the GP is
$\max_{x\in\Xspace} \sigma_{Z \mid \mathcal{X}_n}^2(x)$, the maximum
value of the prediction variance on the space. A simple criterion is
to select and evaluate the point corresponding to this maximum of
variance:
\begin{equation}
  x_{n+1} = \argmax_{x \in \Xspace} \sigma^2_{Z \mid \mathcal{X}_n}(x)
\end{equation}
This criterion by its simplicity is easy to implement, as the
prediction variance is cheap to compute given a GP.\@ It does not
depend directly on the evaluations of the function at $x_i$ for
$1\leq i\leq n$, uniquely on the distance between the candidate point
$x$ and the points of the design $\mathcal{X}_n$ and the covariance
parameters.

\paragraph{Integrated Mean Square Error}
The prediction variance is directly given by
$\sigma^2_{Z\mid \mathcal{X}_n}$ and represents the uncertainty on the
Gaussian regression. To summarize this uncertainty on the whole space
$\Xspace$, we define the Integrated Mean Square Error
(IMSE)~\citep{sacks_designs_1989} as
\begin{equation}
  \IMSE(Z \mid \mathcal{X}_n) = \int_{\Xspace} \sigma_{Z\mid\mathcal{X}_n}^2(x)\,\mathrm{d}x
\end{equation}
For practical reasons, we can consider to integrate the MSE only on a
subset $\mathfrak{X}\subset \mathcal{X}_n$ that yields
\begin{equation}
  \IMSE_{\mathfrak{X}}\left(Z \mid \mathcal{X}_n \right) = \int_{\Xspace} \sigma^2_{Z\mid \mathcal{X}_n}(x)  \mathbbm{1}_{\mathfrak{X}}(x)\,\mathrm{d}x = \int_{\mathfrak{X}} \sigma^2_{Z\mid \mathcal{X}_n}(x)\,\mathrm{d}x
\end{equation}
For notational convenience, when the GP considered is clear from the
context, only the design used to fit it will be an argument:
$\IMSE(Z \mid \mathcal{X}_n) = \IMSE(\mathcal{X}_n)$.  Unfortunatley, 
exact evaluation of this integral is impossible, so it needs to be
approximated using numerical integration, such as Monte-carlo or
quadrature rules such as Gaussian, or Hermite rule to cite a
few.% that take the form
% \begin{equation}
%   \IMSE(Z\mid \mathcal{X}_n) \approx \sum_{i=1}^{n_{\mathrm{quad}}} w_i \sigma^2_{Z\mid\mathcal{X}_n}(x_i)
% \end{equation}
% for $n_{\mathrm{quad}}$ quadrature points $x_i$ associated with weights $w_i$, both given by 

We can then study how adding a certain point would affect the
$\IMSE$. For a given $x\in \Xspace$ and an outcome
$z=f(x)\in \mathbb{R}$, the \emph{augmented design} is defined as
$\mathcal{X}_n \cup \left\{(x, z)\right\}$, and the IMSE of the
augmented design is
$\IMSE\left(\mathcal{X}_n \cup \left\{(x, z)\right\} \right)$.  Before
the actual experiment though, $z$ is unknown, but we can model it by
its distribution given by the GP (per~\cref{eq:ZgivenXGP}). So for a
given candidate $x$, the IMSE when evaluating the point $x$ will be on
average
\begin{equation}
  \label{eq:IMSE_augmented}
  \Ex_{Z(x)}\Big[\IMSE\big(\mathcal{X}_n \cup \left\{(x, Z(x))\right\} \big)\Big]
\end{equation}
where the expectation is to be taken with respect to the random
variable $Z(x)$. The expression of~\cref{eq:IMSE_augmented} requires then the evaluation of an integral in dimension $1+\dim \mathbb{X}$ .

As each outcome of $Z(x)$ requires to fit a GP and to compute the
IMSE, a precise evaluation is quite expensive. A strategy found for
instance in~\cite{villemonteix_informational_2006} is to take $M$
possible outcomes for $Z(x)$, corresponding to evenly spaced quantiles
of its distribution, or using Gauss-Hermite
quadratures~\citep{bernard_methodes_2019} in order to take advantage
of the Gaussian nature of $Z(x)$.  The hyperparameters of the GP
should not be reevaluated when augmenting the design, in order to get
comparable values for the IMSE.\@

Finally, we can maximise a criterion which is the opposite of the
expression in~\cref{eq:IMSE_augmented} to enrich the design:
\begin{equation}
  x_{n+1} = \argmax_{x\in \Xspace}-\Ex_{Z(x)}\Big[\mathrm{IMSE} \big(\mathcal{X}_n \cup \left\{(x, Z(x))\right\} \big)\Big]
\end{equation}

% \tocheck{Et si au lieu d'estimer l'espérance, on choisit un échantillon. Stochastic simulation ?}
% \begin{figure}[ht]
%   \centering
%   \input{\imgpath IMSE_variance.pgf}
%   \caption{\label{fig:IMSE_variance} IMSE and variance comparison}
% \end{figure}


\subsection{Criteria for optimisation of the objective function}
\label{sec:GP_optimization_criteria}
The criteria detailed above aim at reducing the epistemic uncertainty
modelled through the Gaussian Process. In other words, we try to
improve our knowledge on the unknown function globally. We are now
going to evoke a few criteria which are driven by the global
optimisation of the function.

Those methods usually aim at striking a balance between the
\emph{exploration} of the whole space $\Xspace$ and
\emph{intensification}, \textit{i.e.} evaluating the function near its
optimum.  Let $f$ be the unknown function, and $Z$ be a GP constructed
based on an initial design $\mathcal{X}_0 = \{(x_i, f(x_i))\}$.
\paragraph{Probability of improvement}
We are first going to introduce the probability of improvement
$\mathrm{PI}$, which is the probability that the GP is smaller than a
threshold $f_{\min}$. Due to the Gaussian nature of $Z(x)$, this
probability can be written in closed form using
$\Phi = F_{\mathcal{N}(0, 1)}$ the cdf of a centered standard Gaussian
r.v.\
\begin{align}
  \mathrm{PI}(x) &= \Prob% _{Z(x)}
                   \left[Z(x) < f_{\min}\right] \\
                 &= \Phi\left(\frac{m_Z(x) - f_{\min}}{\sigma_Z(x)}\right)
\end{align}
This threshold can have different forms
\begin{itemize}
\item $f_{\min} = \min_{i} f(x_i)$: the GP is compared with the
  current minimal value reached by the function
\item $f_{\min} = \min_i f(x_i) + \epsilon$: by introducing a small
  tolerance $\epsilon$, we encourage exploration instead of
  intensification.
\end{itemize}
Using the probability of improvement tends to select points quite
close to the point evaluated so far, thus does favor intensification
at the expense of exploration.
\begin{equation}
  x_{n+1} = \argmax_{x\in \Xspace} \mathrm{PI}(x)
\end{equation}
\paragraph{Expected improvement and EGO}
One of the most common criteria for global optimisation is the
\emph{Expected Improvement} (EI)~\citep{mockus_bayesian_1974}, and the
SUR strategy using it as an enrichment criterion is called
\emph{Efficient Global Optimisation}
(EGO)~\citep{jones_efficient_1998}.  Analogously to the probability of
improvement, we define the improvement $I(x)$ as the random variable
defined as
\begin{equation}
  \label{eq:def_improvement}
  I(x) = {\left[f_{\min} - Z(x)\right]}_+
\end{equation}
where $[y]_+ = \max(y, 0)$.
The Expected Improvement $\mathrm{EI}$ is 
\begin{align}
  \label{eq:def_ei}
  \mathrm{EI}(x) = \Ex_{Z(x)}[I(x)]  = \Ex_{Z(x)}\left[\left[f_{\min} - Z(x) \right]_+\right]
\end{align}
Again, a closed form is available to compute the expected improvement,
that does not require the direct evaluation of the expectation
\cref{eq:def_ei}.
\begin{equation}
  \mathrm{EI}(x) = \left(f_{\min} - m_Z(x)\right) \Phi\left(\frac{f_{\min} - m_Z(x)}{\sigma_Z(x)}\right) + \sigma_Z(x) \phi\left(\frac{f_{\min} - m_Z(x)}{\sigma_Z(x)}\right)
\end{equation}
The EI is then quite easy to evaluate and furthermore it is possible
to compute the gradient of the EI and use it for the optimisation as
done in~\cite{pardalos_differentiating_2015}.
\begin{equation}
  x_{n+1} = \argmax_{x\in \Xspace} \mathrm{EI}(x)
\end{equation}

\paragraph{IAGO}
\label{ssec:IAGO}
Another criterion worth mentioning is a criterion based on the
distribution of the
minimisers~\citep{villemonteix_informational_2006,hennig_entropy_2011},
called IAGO (\emph{Informational Approach to Global Optimisation}) Let
$z_i$ be a sample path of $Z$, and let $x_i^*$ the global minimiser of
$z_i$.  We denote then $X^*$ the random variable corresponding to the
global minimiser of $Z$.  We consider the differential entropy of
$X^*$ introduced \cref{def:KL_entropy}, given the augmented design
$\mathcal{X}_n \cup \left\{\left(x,Z(x)\right)\right\}$:
$H[X^*\mid \mathcal{X}_n \cup \left\{(x, Z(x))\right\}]$.  So at each
step, we choose the point that gives the smallest expected uncertainty
on the location of the global minimisers of the sample paths.  The
criterion can then be written as
\begin{equation}
  x_{n+1}= \argmax_{x \in \Xspace} -\Ex_{Z(x)}\Big[H\left[X^* \mid \mathcal{X}_n \cup \left\{(x, Z(x))\right\} \right]\Big]
\end{equation}


\Cref{fig:example_optimisation_criteria} shows different criteria
introduced before for an unknown function.
\begin{figure}[ht!]
  \centering
  \input{\imgpath example_optimization_criteria.pgf}
  \caption[Optimisation criteria for GP]{\label{fig:example_optimisation_criteria} Example of optimisation criteria. At this iteration, $\mathrm{EI}$ and $\mathrm{PI}$ aim toward intensification, while $\mathrm{IAGO}$ and the maximum of variance favorize exploration}
\end{figure}

% \clearpage
%
\subsection{Contour and volume estimation}
\label{sec:GP_vol_estim}
In this section, we are interested in the estimation of the inverse
image of sets: $\{x \in \Xspace \mid f(x) = T\}=f^{-1}(\{T\})$ for
level sets, or
$\{x\in\Xspace \mid f(x) \leq T\} = f^{-1}(]-\infty, T])$ called
excursion sets. An exemple of this estimation is illustrated
\cref{fig:prob_coverage_example}, where the focus is on the estimation
of the set $f^{-1}(\interval[open left]{-\infty}{T})$. For that, we
are going to introduce enrichment strategies which rely on the
evalution of the function around the level set $\{f=T\}$.

\subsubsection{Reduction of the augmented IVPC}
For a function $f$, let us assume that we are interested in the
estimation of the set
$f^{-1}(B) = \{x \in \Xspace \mid f(x) \leq T\}$, with
$B = ]-\infty, T]$. Given $\Prob_{X}$ a probability
measure on $\Xspace$, and $B\subset \mathbb{R}$, we can compute
$V=\Prob_{X}\left[f^{-1}(B)\right]$. For
$B = \interval[open left]{-\infty}{T}$, it is equivalent to compute the
volume of the \emph{excursion set} of $f$ below $T$:
$V=\Prob_{X}[f(X) \leq T]$. Such an example is illustrated~\cref{fig:prob_coverage_example}.% \todo{GP regression ou prediction}
\begin{figure}[ht]
  \centering
  \input{\imgpath prob_coverage_exemple.pgf}
  \caption[Estimation of $f^{-1}(B)$ using GP]{\label{fig:prob_coverage_example} Estimation of $f^{-1}(\interval[open left]{\infty}{T})$ using GP and probability of coverage associated % attention regression changé à la main dans le pgf en prediction
  }
\end{figure}

% Let us start by introducing diverse tools based around Vorob'ev expectation of closed sets (\cite{el_amri_analyse_2019,heinrich_level_2012,vorobyev_new_2003}). 
Let $Z$ be a GP, indexed by $\Xspace$ with continuous sample
paths. For a measurable set $B\subset \mathbb{R}$, $A = Z^{-1}(B)$ is
a random closed set.  We define $\pi_A$ its probability of coverage:
\begin{equation}
  \pi_A(x) = \Prob_{Z(x)}\left[x\in A\right] = \Prob_{Z(x)}[Z(x) \in B]
\end{equation}
\Cref{fig:prob_coverage_example} shows the probability of coverage of
the set $A$, the true set $f^{-1}(B)$, and the \emph{plug-in} estimation of
this set: $m_Z^{-1}(B) = \{x\in\Xspace \mid m_Z(x) \leq T\}$.

For a given $x\in\Xspace$, the event ``$x$ belongs to $A$'' is
Bernoulli random variable, with probability $\pi_A(x)$, thus has
variance %\todo{expliciter Loi de bernoulli}
\begin{equation}
  \label{eq:bernoulli_cov_variance}
\mathscr{V}_A(x)=\pi_A(x)(1 - \pi_A(x))
\end{equation}
We can also define the probability of missclassification. If
$\pi_A(x) > 0.5$, $x$ is classified in $A$ so the probability of
misclassification is $1-\pi_A(x)$, and on the other hand, if
$\pi_A(x) < 0.5$, $x$ is classified in $A^{C}$, and the probability of
missclassification is $\pi_A(x)$. This can be condensed as
\begin{equation}
\mathscr{P}_{\mathrm{mis}}(x) = \min(\pi_A(x), 1-\pi_A(x))
\end{equation}

For both $\mathscr{P}_{\mathrm{mis}}$ and $\mathscr{V}_A$, the maximum is reached for $\pi_A(x) = 1/2$.

As we want to classify each point either \emph{in} $A$ or \emph{out
  of} $A$, we can look for the different level-sets of the probability
of coverage: for $\eta \in [0, 1]$, we define the $\eta$-level set of
$\pi_A$, also called \emph{Vorob'ev quantiles}
(see~\cite{vorobyev_new_2003})
\begin{equation}
  Q_{\eta} = \{x\in\Xspace \mid \pi_A(x) \geq \eta \}
\end{equation}
Those sets are decreasing (with respect to the inclusion) when $\eta$ increases:
\begin{equation}
  0\leq \eta \leq \xi \leq 1 \implies Q_{\xi} \subseteq Q_{\eta}
\end{equation}

Using the properties of the GP stating that $x\in\Xspace$,
$Z(x)\sim\mathcal{N}\left(m_Z(x), \sigma^2_Z(x)\right)$,
we can express the probability of
coverage of the random set $A$ using $F_{Z(x)}$, the cdf of the r.v.\
$Z(x)$, or $\Phi$, the cdf of the centered standard Gaussian
distribution:
\begin{align}
  \pi_A(x) &= \Prob_{Z(x)}\left[Z(x) \leq T\right] = F_{Z(x)}(T)=\Phi\left(-\frac{m_Z(x) - T}{\sigma_Z(x)}\right) \\
  \mathscr{V}_A(x) &= \pi_A(x)(1 - \pi_A(x)) \\
  \mathscr{P}_{\mathrm{mis}}(x) &= \min\left(F_{Z(x)}(T), 1-F_{Z(x)}(T)\right)=\min\left(F_{Z(x)}(T),F_{-Z(x)}(-T)\right) \\
  % &=\min\left(F_{Z(x)}(T),F_{Z(x)-2m_{Z}(x)}(-T)\right) \\ &=
  % \min\left(\Phi\left(-\frac{m_Z(x) - T}{\sigma_Z(x)}\right),
  %   \Phi\left(-\frac{-m_Z(x) + T}{\sigma_Z(x)}\right)\right) \\
  % &= \min\left(\Phi\left(-\frac{m_Z(x) -
  %     T}{\sigma_Z(x)}\right),\Phi\left(+\frac{m_Z(x) -
  %     T}{\sigma_Z(x)}\right)\right)\\
           &= \Phi\left(- \frac{\lvert m_Z(x) - T \rvert}{\sigma_Z(x)} \right) \label{eq:prob_missclassification_GP}
\end{align}
\cite{echard_ak-mcs_2011} defines the argument of the cdf
\cref{eq:prob_missclassification_GP} as the \emph{reliability index}
$\rho$ (denoted $U$ in its original definition
\cite{echard_ak-mcs_2011})):
\begin{equation}
  \label{eq:reliability_rho}
  \rho(x) = \frac{\lvert m_Z(x) - T \rvert}{\sigma_Z(x)}
\end{equation}

Based on those quantities, we will introduce two criteria which aim at
improving the classification problem of $x\in A=f^{-1}(B)$.

First, we can choose to evaluate the point which has the maximal
probability of misclassification, or equivalently since $\Phi$ is
monotonously increasing, maximise the criterion given by
\begin{equation} x_{n+1} =\argmax_{\kk\in\Kspace} -\frac{\lvert m_Z(x)- T \rvert}{\sigma_Z(x)}
\end{equation}

This criteria will be maximal when either the kriging prediction is
close to the level set $T$ (intensification aspect), or if there is a
large prediction variance (exploration aspect).


Alternatively, we can also use the variance of the probability of
coverage $\mathscr{V}_A$ as an indication of the uncertainty of the
classification problem. As the maximum of variance will be obtained
for points having a probability of coverage $\pi_A(x) =0.5$, looking
and evaluating the maximiser of this criterion is not really relevant:
any point verifying $m_Z(x)=T$ would be suited, without taking into
account its prediction error $\sigma^2_Z(x)$, and a point already well
predicted (\emph{i.e.} close to other points already in the design)
could be evaluated, incorporating little new knowledge.


Instead, by integrating $\mathscr{V}_A$ over the whole space
$\Xspace$, we get a measure of the uncertainty in the classification
problem associated with the design $\mathcal{X}_n$, which we will
denote $\mathrm{IVPC}(\mathcal{X}_n)$ (Integrated Variance of the
Probability of Coverage).
\begin{equation}
  \label{eq:IVPC_def}
\mathrm{IVPC}(\mathcal{X}_n) =  \int_{\Xspace} \mathscr{V}_A(x)\,\mathrm{d}x
\end{equation}
A similar function has been introduced in~\cite{bect_sequential_2012}.
We can finally introduce a criterion based on the augmented design, as
done for the $\IMSE$.
\begin{equation}
  \label{eq:expected_aIVPC}
  x_{n+1} = \argmin_{x \in\Xspace} \Ex_{Z(x)}\Big[\mathrm{IVPC}(\mathcal{X}_n \cup \{   \left(x, Z(x) \right)  \})\Big]
\end{equation}

Other criteria have also been developed, such as the use of the theory
of random sets~\citep{el_amri_data-driven_2019}, in order to define a
measure of uncertainty based on Vorob'ev mean and
deviation~\citep{vorobyev_new_2003}.


\subsubsection{Sampling in the Margin of uncertainty}
\label{sec:margin_of_uncertainty}

Using the level sets, we can construct the $\eta$-margin of
uncertainty, as introduced in~\cite{dubourg_reliability-based_2011},
that is the set of points $x \in \Xspace$ that we cannot classify in
or out of $A$ with high enough probability.  Setting the classical
level $\eta=0.05$ for instance, $Q_{1-\frac{\eta}{2}}=Q_{0.975}$ is
the set of points whose probability of coverage is higher than
$0.975$, while $Q_{\frac{\eta}{2}}=Q_{0.025}$ is the set of points
whose probability of coverage is higher than $0.025$, thus its
complement in $\Xspace$, denoted by $Q_{\frac{\eta}{2}}^C$ is the set
of points whose probability of coverage is lower than
$0.025$. Obviously, $Q_{1-\frac{\eta}{2}} \subset Q_{\frac{\eta}{2}}$.

The $\eta$-margin of uncertainty $\mathbb{M}_{\eta}$ for $\eta=0.05$
is then defined as the sets of points whose coverage probability is
between $0.025$ and $0.975$:
\begin{align}
  \label{eq:margin_unc}
  \mathbb{M}_{\eta} &= \left(Q_{1-\frac{\eta}{2}} \cup Q^C_{\frac{\eta}{2}} \right)^C = Q_{1-\frac{\eta}{2}}^C \cap Q_{\frac{\eta}{2}} = Q_{\frac{\eta}{2}} \setminus Q_{1-\frac{\eta}{2}} \\
                    &= \left\{x \in \Xspace \mid \frac{\eta}{2} \leq \pi_A(x) \leq 1 - \frac{\eta}{2} \right\}
\end{align}

Based on this set, we can construct easily a sampling criterion by
using the indicator function of the margin of uncertainty:
$\mathbbm{1}_{\mathbb{M}_{\eta}}(x)=\kappa_{\mathbb{M}}(x) =
\bar{\kappa}_{\mathbb{M}}(x) \cdot \mathrm{Vol}(\mathbb{M}_{\eta})$.
 Evaluating points in this region allows to reduce
the uncertainty at $x$ (completely if the GP is interpolant, as
$\sigma_Z(x)$ becomes $0$ after evaluating $f(x)$) on the
classification of the point $x$.

Finally, using this indicator function,~\cref{alg:sampling_enrichment}
can be applied to enrich the design, by adding a batch of $K$ points
at each iterations.  The margin of uncertainty, samples
in this margin and the centroids that represent statistically this set
of samples are shown~\cref{fig:margin_unc}. The samples have been
obtained using an acceptance-rejection method, since the pdf of the
sampling distribution is an indicator function. The $K=\num{10}$
centroids have been computed using the KMeans algorithm.

We can see that the centroids are located close to the true level-set of the function in this case.
%\todo{batch + centroids + samples}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{\imgpath margin_unc.pdf}
  \caption[Samples in the margin of uncertainties and centroids]{\label{fig:margin_unc} Margin of uncertainty, and
    construction of the $K$ points of the batch to be evaluated, using
    samples and centroids}
\end{figure}

% \clearpage
\section{Adaptive strategies for robust optimisation using GP}
\label{sec:robust_criteria_gp}
In the previous section, we introduced enrichment strategies to solve
different problems for a generic function
$f:\Xspace\rightarrow \mathbb{R}$.  Before introducing specific
strategies for robust calibration (as defined in
\cref{chap:robust_estimators}), we are first going to recall briefly
the setting and corresponding problems of robust calibration.  Let $J$
be an objective function taking two inputs:
\begin{equation}
  \begin{array}{rcl}
    J: \Kspace \times \Uspace& \longrightarrow& \mathbb{R} \\
    (\kk, \uu) & \longmapsto & J(\kk, \uu)
  \end{array}
\end{equation}
In a context of calibration, this function measures the misfit between
the output of the numerical model and the observations.

The first input of the function, $\kk\in\Kspace$, is the calibration
parameter which we wish to control and $\uu\in\Uspace$ is the
environmental parameter. We model this intrinsic variability of the
environmental parameter with a random variable $\UU$.  In this
context, different objectives can be derived in order to get a
satisfying robust estimate $\hat{\kk}$. Robustness here is to be understood as
the ability to get satisfying performances of the objective function
$J(\hat{\kk},\uu)$ for any $\uu\in\Uspace$, realisation of $\UU$.

Based on an initial design
$\mathcal{X}_0 = \left\{\left((\kk_i,\uu_i),J(\kk_i, \uu_i)
  \right)\right\}_{1\leq i \leq n_0}$ of $n_0$ points in
$\Kspace\times\Uspace$, we assume that we constructed a GP, say $Z$, on
$\Kspace \times \Uspace$. In the following, if the design used to
construct the GP is clear from the context, it will be omitted in the
notation.

As a GP, $Z$ is described by its mean function
$m_{Z}:\Kspace \times \Uspace\rightarrow\mathbb{R}$ and a covariance
function $C:(\Kspace\times\Uspace)^2\rightarrow \mathbb{R}$, and the
prediction variance is
$\sigma^2_Z(\kk,\uu) = C\left((\kk,\uu), (\kk,\uu)\right)$.

As a GP, for any $(\kk, \uu)\in\Kspace\times\Uspace$, $Z(\kk, \uu)$ is normally distributed:
\begin{equation}
  Z(\kk,\uu) \sim \mathcal{N}\left(m_{Z}(\kk,\uu), \sigma^2_Z(\kk,\uu) \right)
\end{equation}

A surrogate of $J$, constructed using the GP $Z$ is then $m_Z$, which
will be used to compute the different estimates. However, the initial
design $\mathcal{X}_0$ is probably too sparse to get $m_Z$ accurately
enough to compute the wanted estimates. For that purpose, we propose
enrichment strategies in order to add \emph{relevant} points to the
design $\mathcal{X}_0$.

One main challenge for this problem lies in the decomposition of the
input space, ($\Xspace$ previously) into $\Kspace \times \Uspace$. The
objective on each of these spaces is different: one needs to explore
sufficiently the space $\Uspace$ according to the distribution of
$\UU$, in order to be able to compute $\Prob_{\UU}$ and $\Ex_{\UU}$,
while $\Kspace$ does not need to be explored as much because the
objective function is supposed to be \emph{optimised} with respect to
$\kk$.

In what follows, we are going to focus on the estimation of the
conditional minimisers $\kk^*: \uu\mapsto \kk^*(\uu)$, which can be
used to estimate the mode of the distribution of the conditional
minimisers $\estimtxt{\kk}{MPE}$, and on the estimators which satisfy
properties based on regret.  For that purpose, the methods introduced
here will usually involve the computation and the optimisation of a
criterion on the joint space directly, following the method
of~\cref{alg:SUR_strat} on \cpageref{alg:SUR_strat}, and sampling in
regions of large uncertainties, as detailed
in~\cref{alg:sampling_enrichment}~\cpageref{alg:sampling_enrichment}.

For other robust estimators, such as the minimum of the expectation
$\estimtxt{\kk}{mean}$, other strategies have been derived, which
consist first in removing the dependency on $\UU$. For instance
in~\cite{janusevskis_simultaneous_2010}, the authors integrate
directly the GP with respect to $\UU$, in order to get another GP,
which models the function
$\kk\mapsto \Ex_{\UU}\left[J(\kk, \UU)\right]$. This integrated GP
will then be used to select a point $\tilde{\kk}$, and then a couple
of points $(\kk_{n+1},\uu_{n+1})$ is chosen in order to reduce a
measure of uncertainty on the space $\{\tilde{\kk}\}\times \Uspace$.
Alternatively, when introducing the variance in a multiobjective
optimisation problem,~\cite{miranda_adjoint-based_2016} proposes a
method based on polynomial chaos expansion in order to get both the
value and the gradient of the expected value and the variance.


%, some others use the GP $Z$ to remove the dependence on $\UU$, by projecting the GP for instance



% \subsection{Minimisation of the expected loss}
% \label{ssec:expected_loss_GP_projection}
% Recalling the definition of $\estimtxt{\kk}{mean} = \argmin_{\kk \in \Kspace} \Ex_{\UU}\left[J(\kk, \UU)\right]$, we can look to minimise the expected loss. 
% In~\cite{janusevskis_simultaneous_2010}, the authors define the \emph{projected process} $W$, the stochastic process $\kk \rightarrow \left(\Omega \rightarrow \mathbb{R}\right)$ which verifies
% \begin{equation}
%   W(\kk) = \Ex_U[Z(\kk, \UU)] = \int_{\Uspace} Z(\kk, \uu) p_{\UU}(\uu)\,\mathrm{d}\uu
% \end{equation}
% $W$ is then a Gaussian process on $\Kspace$. As done before, we want to minimise the unknown function $\Ex_U[J(\kk, \UU)]$ using $W$, thus we can maximise the expected improvement~\cref{eq:def_ei} to find a potentially interesting point $\tilde{\kk}$.
% Then, we will look for the pair $(\kk, \uu) \in \Kspace\times \Uspace$ which minimises the variance of the augmented projected process at this candidate.
% \begin{align}
%   \tilde{\kk} &= \argmax_{\kk \in \Kspace} \mathrm{EI}_W(\kk) \\
%   (\kk_{n+1}, \uu_{n+1}) &= \argmin_{(\kk, \uu)\in\Kspace\times \Uspace} \Var\left[W(\tilde{\kk}) \mid \mathcal{X} \cup (\kk,\uu) \right]
% \end{align}
% The variance of the augmented projected process does not depend on the realization of r.v.\ $Z(\kk, \uu)$, but only on the distance of $(\kk, \uu)$ to the current experimental design.
% \todo{FAIRE REFERENCE PLUTOT}




\subsection{PEI for the conditional minimisers}
\label{sec:PEI_criterion}
In the previous chapter, we introduced the conditional minimums
$\uu\mapsto J^*(\uu)$ and the conditional minimisers
$\uu \mapsto \kk^*(\uu)$. Each evaluation of these functions require
an optimisation procedure, thus can be expensive from a computational
point of view.~\cite{ginsbourger_bayesian_2014} proposes a criterion,
adapted from the EI, which aims at solving such a problem: the
\emph{Profile Expected Improvement} $\mathrm{PEI}$ is defined as
\begin{align}
  \label{eq:def_PEI}
  \mathrm{PEI}(\kk, \uu)&= \Ex\left[\left[f_{\min}(\uu) - Z(\kk, \uu)\right]_+\right]\\  \text{ with } f_{\min}(\uu) &= \max(\min_{x\in\mathcal{X}} f(x), \min_{\kk \in \Kspace} m_Z(\kk, \uu))
\end{align}
for the design $\mathcal{X} = \{(x_i, f(x_i)\}_{1\leq i\leq n}$ with
the slight abuse of notation $J(\kk, \uu) = f(x)$ for $x=(\kk, \uu)$.

This allows us to see the similarity with the $\mathrm{EI}$
criterion: instead of having a fixed threshold, the $\mathrm{PEI}$
introduces a criterion that depends on $\uu$. \Cref{fig:example_PEI}
shows an example of GP enriched using the $\mathrm{PEI}$ criterion.
The criterion can then be used in a classical SUR strategy introduced
in~\cref{alg:SUR_strat}, where at each iteration, the PEI is maximised
over $\Kspace \times \Uspace$:
\begin{equation}
  \label{eq:PEI_criterion_SUR}
  x_{n+1}=(\kk_{n+1}, \uu_{n+1}) =\argmax_{(\kk, \uu)\in \Kspace \times \Uspace} \mathrm{PEI}(\kk, \uu)
\end{equation}
and the maximiser is evaluated by $f$ and added to the design.
\begin{figure}[ht]
  \centering
  \input{\imgpath PEI_example.pgf}
  \caption[Illustration of enrichment using the PEI
  criterion]{\label{fig:example_PEI} GP after 30 additional iterations
    chosen using PEI. The GP prediction using the initial design is
    represented on the top left plot. The PEI criterion for this
    GP is evaluated over the whole space and plotted top right. Bottom
    left plot is the GP prediction after 30 iterations. Bottom right
    is the surrogate estimation of the conditional minimum. The shaded
    region corresponds to the initial surrogate conditional minimum,
    $\pm$ one standard deviation. After the iterations, the surrogate
    estimate and the true value coincide.}
\end{figure}
Once a stopping criterion is met, such as the maximal variance
attained at the conditional minimum is below a specified threshold
$\epsilon_{\mathrm{stop}}$,
\begin{equation}
  \max_{\uu\in\Uspace} \sigma^2_Z(m_{Z^*}(\uu), \uu) \leq \epsilon_{\mathrm{stop}}
\end{equation}
we can construct accurate surrogates of $J^*(\uu)$ and $\kk^*(\uu)$,
using the GP in a plug-in approach:
\begin{equation}
 m_{Z^*}(\uu) = \min_{\kk\in\Kspace} m_Z(\kk,\uu)  \quad \text{ and } \quad \kk_Z^*(\uu) =\argmin_{\kk\in\Kspace} m_Z(\kk, \uu)
\end{equation}
and with a set of i.i.d.\ samples $\{\uu_i\}_i$ of $\UU$, use the
surrogates to find the MPE, based on the distribution of
$\{\kk_Z^*(\uu_i)\}$.


% \clearpage

\subsection{Gaussian formulations for the relative and additive regret families of estimators}
\label{ssec:gp_delta_alpha}

We are now going to detail how Gaussian processes can be used to
estimate regret-based estimators.  Recalling the previous chapter, the
family of regret-based estimators are defined as
\begin{align}
  \{ \estimtxt{\kk}{AR,\beta} &= \argmax_{\kk\in\Kspace} \Gamma_\beta(\kk) = \argmax_{\kk\in\Kspace} \Prob_{\UU}\left[J(\kk, \UU) \leq J^*(\UU) + \beta \right] \mid \beta \geq 0 \}
\end{align}
in the case of additive-regret, and
\begin{align}
  \{ \estimtxt{\kk}{RR,\alpha} &= \argmax_{\kk\in\Kspace} \Gamma_\alpha(\kk) = \argmax_{\kk\in\Kspace}\Prob_{\UU}\left[J(\kk, \UU) \leq \alpha J^*(\UU)\right] \mid \alpha \geq 1 \}
\end{align}
for the relative-regret.

As explained~\cref{sec:rr_family}, in order to get an estimator, two
starting points can be considered: either fixing the threshold
$\alpha$ (or $\beta$), and computing the associated estimator
$\estimtxt{\kk}{RR,\alpha}$ (or $\estimtxt{\kk}{AR,\beta}$), or
instead, looking to get a probability of acceptability of level $p$,
and then compute the associated estimator and threshold. For the first
alternative, we are going to focus on improving the estimation of
$\Gamma$ using GP in~\cref{sec:evaluation_gamma}, while for the
second, we will try to improve the estimation of the quantile of the
ratio in~\cref{sec:quantile_rr}.


We first need to detail some notations with respect to the conditional
minimum and conditional minimisers, and their equivalent for the GP.
% Let us first consider the conditional minimiser:
% \begin{align}
    %     J^*(\uu) = J(\kk^*(\uu),\uu) = \min_{\kk\in\Kspace} J(\kk,\uu)
    %   \end{align}

We define $Z^*$ as
\begin{equation}
  Z^*(\uu) \sim \mathcal{N}\left(m_{Z^*}(\uu), \sigma^{2}_{Z^*}(\uu)\right)
\end{equation}
where
\begin{align}
  m_{Z^*}(\uu) &= \min_{\kk\in\Kspace} m_Z(\kk,\uu) \label{eq:def_mstar}\\
  \kk_Z^*(\uu)&=\argmin_{\kk\in\Kspace} m_Z(\kk, \uu) \label{eq:theta_star}\\
  \sigma^{2}_{Z^*}(\uu) &= \sigma^{2}_Z(\kk^*_Z(\uu), \uu) \label{eq:sigma_star} 
\end{align}
We use then the minimiser and the minimum of the GP prediction as
estimates of the conditional minimum and minimiser, similarly as
in~\cite{ginsbourger_bayesian_2014}. % for instance, but other choices could be considered, such as $m_Z(\kk^*_Z(\uu)) - \gamma \sigma^{2}_{Z^*}(\kk_Z^*(\uu))$. This choice would lead to be more ``optimistic'' in the estimation of the minimum (i.e.\ a lower minimum), and in turn, would have a tendency to overestimate the estimated value of $\alpha$, thus trending toward more conservative estimates.
To generalize notions of the additive- and relative-regret, we can
define $\Delta_{\alpha, \beta} = Z - \alpha Z^* - \beta$.  This difference
$\Delta_{\alpha,\beta}$ is a linear combination of correlated Gaussian
processes, thus is a GP as well and its distribution can be derived by
first considering the joint distribution of $Z(\kk,\uu)$ and
$Z^*(\uu) = Z(\kk_Z^*(\uu), \uu)$:
\begin{equation}
  \begin{bmatrix}
    Z(\kk,\uu) \\
    Z^*(\uu)
  \end{bmatrix}
  \sim \mathcal{N}\left(
    \begin{bmatrix}
      m_Z(\kk,\uu) \\
      m_{{Z^*}}(\uu)
    \end{bmatrix}
    ;\,
    \begin{bmatrix}
      C\left((\kk,\uu),(\kk,\uu)\right) & C\left((\kk,\uu),(\kk^*_Z(\uu),\uu)\right) \\
      C\left((\kk,\uu),(\kk_Z^*(\uu),\uu)\right) & C\left((\kk^*_Z(\uu),\uu),(\kk_Z^*(\uu),\uu)\right)
    \end{bmatrix}
  \right)
\end{equation}

Multiplying by the $1\times 2$ matrix
$\begin{bmatrix}1                & -\alpha \end{bmatrix}$ and translating by $-\beta$
yields
\begin{align}
  \Delta_{\alpha,\beta}(\kk,\uu) & \sim \mathcal{N}\left(m_{\Delta}(\kk,\uu); \sigma^2_{\Delta}(\kk,\uu)\right)  \label{eq:delta_GP_ab} \\
  m_{\Delta}(\kk,\uu)            & = m_Z(\kk,\uu) - \alpha m_{{Z^*}}(\uu) - \beta \label{eq:mu_delta_GP_ab}                             \\
  \sigma^2_{\Delta}(\kk,\uu)     & = \sigma_Z^2(\kk,\uu) + \alpha^2 \sigma_{Z^*}^2(\uu) - 2\alpha C\left((\kk,\uu),(\kk^*_Z(\uu),\uu)\right) \label{eq:variance_delta_GP_ab}
\end{align}

We are then interested in the random set
$\{(\kk, \uu) \mid \Delta_{\alpha, \beta}(\kk, \uu) \leq 0\}$. The
different combinations of $\alpha$ and $\beta$ dictate if we are either
interested in the additive or the relative regret:
\begin{itemize}
\item $(\alpha, \beta) = (1, \beta)$ corresponds to the additive regret
\item $(\alpha, \beta) = (\alpha, 0)$ corresponds to the relative regret
\end{itemize}

                                   %                                    \todo{Réfléchir beta role ?}

Decomposing the variance $\sigma^2_{\Delta}$
in~\cref{eq:variance_delta_GP_ab}, 3 sources of uncertainty appear:
\begin{itemize}
\item $\sigma^2_{Z}$ is the prediction variance of the GP on $J$, that
  is directly reduced when additional points are evaluated
\item $\sigma^2_{Z^*}$ is the variance of the predicted value of the
  minimisers.
\item Assuming a stationary form of the covariance, the third term is
  dependent only on the distance between $\kk$ and $\kk_{Z^*}(\uu)$,
  (the estimated conditional minimiser). By separating the $\kk$ and
  $\uu$ components, the covariance term can be written
  $C((\kk,\uu), (\kk^{\prime},\uu^{\prime})) = s
  \mathcal{K}_{\kk}(\|\kk - \kk^{\prime}\|) \mathcal{K}_{\uu}(\|\uu -
  \uu^{\prime}\|)$, and
  % $C((\kk,\uu), (\kk^*_Z',\uu')) = s \prod_{i\in\mathcal{I}_{\kk}}\rho_{\theta_i}(\|k_i - k'_i\|) \prod_{j\in\mathcal{I}_{\uu}} \rho_{\uu_j}(\|u_j - u'_j\|)$
  substituting $\kk^*_Z(\uu)$ for $\kk^{\prime}$ gives
  \begin{align}
    C\left((\kk,\uu),(\kk^*_Z(\uu),\uu)\right) &= s  \mathcal{K}_{\kk}(\|\kk - \kk^*_Z(\uu)\|) \mathcal{K}_{\uu}(0) \\
                                                &= s  \mathcal{K}_{\kk}(\|\kk - \kk^*_Z(\uu)\|)
  \end{align}
\end{itemize}
This decomposition highlights the fact that the prediction error
$\sigma_{\Delta}^2$ of the difference $\Delta_{\alpha,\beta}$ measured
at a point $(\kk, \uu)$ will not be reduced completely by evaluating
the function $J$ at this point, as only the prediction variance
$\sigma_Z^2$ will be significantly affected in general. In other
words, reducing the uncertainty at a point $(\kk, \uu)$ may require
the evaluation of another point
$(\tilde{\kk},\tilde{\uu}) \neq (\kk, \uu)$.

In the following, unless explicitly stated, we will focus on the
relative-regret family of estimators, so $\beta=0$ and
$\alpha \geq 1$, thus only keeping $\alpha$ in the subscript.  The
strategies introduced hereafter can be easily be adapted for the
additive-regret, by setting $\alpha=1$, and translating the mean
functions accordingly, thus defining
$\{\Delta_{\alpha=1} \leq \beta \}$.


                                                  %                                                   \subsection{2 stages enrichment strategies}

                                                  %                                                   We have seen so far strategies that rely on a global reduction
                                                  %                                                   A measure of the uncertainty on this derived quantity is evaluated and used to choose a ``candidate'' $\tilde{\kk}$, so that we can reduce the uncertainty on $\{\tilde{\kk}\} \times \Uspace$.
                                                  %                                                   The next point $(\kk_{n+1}, \uu_{n+1})$ to evaluate is then chosen in $\Kspace \times \Uspace$, where $\kk_{n+1}$ may not be equal to $\tilde{\kk}$.
                                                  %                                                   A general 2-stage SUR strategy is described~\cref{alg:2stage_SUR}.


                                                  %                                                   \begin{algorithm}
                                                  %                                                   \caption{\label{alg:2stage_SUR} Two-stages SUR strategies for robust optimisation problem}
                                                  %                                                   \begin{algorithmic}
                                                  %                                                   \REQUIRE Initial design $\mathcal{X}_0$, criterion function $\kappa$
                                                  %                                                   \STATE Fit $Z$, a GP using the design $\mathcal{X}_0$
                                                  %                                                   \STATE $n \leftarrow 0$
                                                  %                                                   \WHILE{Stopping criterion not met or evaluation budget not reached} 
                                                  %                                                   \STATE $\tilde{\kk} \leftarrow \argmax_{\kk \in \Kspace} \kappa(\kk; Z \mid \mathcal{X}_{n})$
                                                  %                                                   \STATE $(\kk_{n+1},\uu_{n+1}) \leftarrow \argmax_{(\kk,\uu) \in \Kspace\times\Uspace} \kappa((\kk,\uu); \tilde{\kk}, Z \mid \mathcal{X}_{n})$
                                                  %                                                   \STATE Evaluate $J(\kk_{n+1}, \uu_{n+1})$
                                                  %                                                   \STATE $\mathcal{X}_{n+1} \leftarrow \mathcal{X}_n \cup \left\{\left((\kk_{n+1},\uu_{n+1}), J(\kk_{n+1}, \uu_{n+1})\right)\right\}$
                                                  %                                                   \STATE $n \leftarrow n + 1$
                                                  %                                                   \ENDWHILE
                                                  %                                                   \end{algorithmic}
                                                  %                                                   \end{algorithm}




\subsection{GP-based methods for the estimation of $\Gamma_{\alpha}$}
\label{sec:evaluation_gamma}
Let consider $\alpha\geq 1$ fixed. In order to compute
$\estimtxt{\kk}{RR, \alpha}$, we need to estimate and optimise the
function $\Gamma_{\alpha}$. For that purpose, we are going to explore
the space to improve the estimation of $\Gamma_{\alpha}$, and once
sufficient knowledge is acquired, use the plug-in % \todo{definir}
estimate $\hat{\Gamma}_{\alpha}$ for the optimisation, to get the
wanted estimator.

\subsubsection{Estimation of $\Gamma_{\alpha}$}

For a given $\kk\in\Kspace$, the coverage probability of the
$\alpha$-acceptable region, \emph{i.e.} the probability for $\kk$ to be
$\alpha$-acceptable is
\begin{align}
  \Gamma_{\alpha}(\kk)  & = \Prob_{U}\left[J(\kk,\UU) \leq \alpha J^*(\UU)\right] \\
                      & =\Ex_{U}\left[\mathbbm{1}_{J(\kk,\UU) \leq \alpha J^*(\UU)}\right]
\end{align}
As $J$ is not known perfectly, it can be seen as a classification
problem.  This classification problem can be approached with a plug-in
approach, that is to consider $m_Z$ instead of $J$:
\begin{align}
  \mathbbm{1}_{J(\kk,\uu) \leq \alpha J^*(\uu)} & \approx   \mathbbm{1}_{m_Z(\kk,\uu) \leq \alpha m_{{Z^*}}(\uu)} \label{eq:plugin_indicator}
\end{align}
Alternatively we can also approximate this indicator function by
consider the probability of the random variable $\Delta$ to be less
than $0$:
\begin{align}
  \mathbbm{1}_{J(\kk,\uu) \leq \alpha J^*(\uu)} & \approx   \Prob_{Z}\left[ \Delta_{\alpha}(\kk,\uu) \leq 0 \right] = \pi_{\alpha}(\kk,\uu) \label{eq:prob_indicator}
\end{align}

Based on those two approximation, we can define two different
estimations of $\Gamma_\alpha$, namely
$\hat{\Gamma}_\alpha^{\mathrm{PI}}$ with the plug-in approach
of~\cref{eq:plugin_indicator}, and $\hat{\Gamma}_{\alpha}^{\pi}$ for
the probabilistic one in~\cref{eq:prob_indicator}, that we will detail
shortly after. Based on those estimations, two strategies emerge:
\begin{itemize}
\item For $\hat{\Gamma}_{\alpha}^{\mathrm{PI}}$, we are going to
  reduce the expected augmented $\IMSE$ of the GP $Z - \alpha Z^*$.
\item For $\hat{\Gamma}_{\alpha}^{\pi}$, we are going to reduce the
  expected augmented integrated variance of probability of coverage
  $\mathrm{IVPC}$.
\end{itemize}

\subsubsection{Reduction of the augmented IMSE}
For the plug-in approach, the chosen estimator is defined by:
\begin{equation}
  \label{eq:def_gamma_PI}
  \Gamma_{\alpha}^{\mathrm{PI}}(\kk) = \Prob_U\left[m_Z(\kk,\uu) \leq \alpha m_{{Z^*}}(\uu) \right]
\end{equation}
The outer expectation operator is to be computed numerically, using
quadrature rule, or Monte-Carlo methods. In general, from a set of
i.i.d.\ samples $\{\uu_i\}_{1\leq i \leq n_{\uu}}$ sampled from $\UU$,
we define
\begin{equation}
    \label{eq:def_hatgamma_PI}
  \hat{\Gamma}_{\alpha}(\kk) = \frac{1}{n_{\uu}}\sum_{i=1}^{n_{\uu}} \mathbbm{1}_{m_Z(\kk, \uu_i) - \alpha m_{Z^*}(\uu_i)\leq 0}
\end{equation}

Due to the fact that the GP surrogate is cheap to evaluate, the
computation of the outer expectation with respect to $\UU$ is
assumed to be performed without too much problems.

In order to improve the accuracy of this estimator, one need to
improve the GP prediction $m_Z$ of the objective function $J$.
% \todo{learning function}
In this case, we propose to use the augmented IMSE of the GP
$Z - \alpha Z^* = \Delta_{\alpha}$ %,  where at a point $(\kk, \uu)$,$\Delta_{\alpha} \sim \mathcal{N}\left(m_\Delta(\kk, \uu), \sigma^2_{\Delta}(\kk, \uu)\right)$
as a learning function. The choice of the IMSE
(instead of choosing the point of maximal variance for instance) comes
from the decomposition of the variance~\cref{eq:variance_delta_GP_ab}.


Let $\hat{\Gamma}^{\mathrm{PI}}_{\alpha,n}$ be the plug-in
approximation of $\Gamma_\alpha$, constructed using the Gaussian
Process surrogate with $n$ points added according to the augmented
IMSE criterion.~\Cref{fig:IMSE_enrichment} illustrates the $L^2$ and
$L^{\infty}$ between the truth $\Gamma_\alpha$ and the estimation
$\hat{\Gamma}_{\alpha,n}$. In this figure, and in what follows, the
error of two different estimations of $\Gamma_{\alpha}$ are
considered: the plug-in estimate of \cref{eq:def_hatgamma_PI} with the
label $\mathrm{PI}$, and the probabilistic one with the label $\pi$,
which will be introduced in the next section.  Finally, the subscript
$n$ will indicate a quantity computed with $n$ \emph{additional}
evaluations of the function.

\begin{figure}[ht]
  \centering
  \input{\imgpath IMSE_enrichment.pgf}
  \caption[Augmented IMSE for the estimation of
  $\Gamma_{\alpha}$]{\label{fig:IMSE_enrichment} Evolution of the
    $L^2$ and $L^\infty$ error of the estimation of $\Gamma_\alpha$
    and IMSE of the successive constructed GP. The points added are
    chosen by the augmented expected IMSE. The curve labelled
    $\mathrm{PI}$ refers to the plug-in estimation of
    \cref{eq:def_hatgamma_PI}, while the one labelled $\pi$ refers to
    the probabilistic estimation, defined later in~\cref{eq:def_hatgamma_prob}}
\end{figure}


\subsubsection{Reduction of the augmented IVPC}

Recalling the definition of the probabilistic approach
\cref{eq:prob_indicator}, given the GP $Z$, we can write an estimator
of $\Gamma_{\alpha}$:
\begin{align}
  \label{eq:def_gamma_prob}
  \Gamma_{\alpha}^{\pi}(\kk) &= \Ex_U\left[ \Prob_{Z}\left[ \Delta_{\alpha}(\kk,\UU) \leq 0\right]\right] = \Ex_U\left[ \Prob_{Z}\left[ Z(\kk, \UU) - \alpha Z^*(\UU) \leq 0\right]\right] \\
                                   &= \Ex_U\left[\pi_{\alpha}(\kk,\UU)\right]%=\int_{\Uspace}\pi_{\alpha}(\kk,\uu)p_{\UU}(\uu) \,\mathrm{d}\uu = \int_{\Uspace}\Phi\left(-\frac{m_{\Delta_\alpha}(\kk,\uu)}{\sigma_{\Delta_\alpha}(\kk,\uu)}\right)p_{\UU}(\uu) \,\mathrm{d}\uu
\end{align}
The set $\{(\kk, \uu) \mid Z(\kk, \uu) - \alpha Z^*(\uu)\leq 0\}$ has
a probability of coverage written $\pi_{\alpha}$, which can be
computed using the CDF of the standard normal distribution $\Phi$,
because $\Delta_{\alpha}$ is a GP with parameters
described~\cref{eq:delta_GP_ab,eq:mu_delta_GP_ab,eq:variance_delta_GP_ab}:
\begin{equation}
  \label{eq:def_pialphaku}
  \pi_{\alpha}(\kk,\uu) = \Phi\left(-\frac{m_{\Delta_\alpha}(\kk,\uu)}{\sigma_{\Delta_\alpha}(\kk,\uu)}\right)
\end{equation}


Finally, the outer expectation can be computed in a similar fashion as
in~\cref{eq:def_hatgamma_PI}; given a set of i.i.d.\ samples
$\{\uu_i\}_{1\leq i\leq n_{\uu}}$ from $\UU$, we define the
probabilistic estimation of $\Gamma_{\alpha}$ as
\begin{equation}
  \label{eq:def_hatgamma_prob}
  \hat{\Gamma}_{\alpha}^{\pi}(\kk) = \frac{1}{n_{\uu}}\sum_{i=1}^{n_{\uu}} \pi_{\alpha}(\kk, \uu_i)
\end{equation}




% Finally, averaging the coverage probability over $\Uspace$ gives what
% we are going to call a probabilistic estimation of $\Gamma_{\alpha}$:
% \begin{equation}
%   {\Gamma}_{\alpha}^{\pi}(\kk) = \Ex_U\left[\pi_{\alpha}(\kk,\uu)\right]
% \end{equation}

Recalling~\cref{eq:bernoulli_cov_variance}, the variance of the
probability of coverage is
$\pi_\alpha(\kk, \uu) \left(1 - \pi_{\alpha}(\kk,\uu)\right)$, and by
integrating this variance over the whole space
$\Kspace \times \Uspace$, similarly as in~\cite{bect_sequential_2012},
we can definethe integrated variance of the probability of coverage
$\mathrm{IVPC}$.
\begin{equation}
  \label{eq:IVPC_def}
  \mathrm{IVPC}(\mathcal{X}_n) =  \int_{\Kspace \times \Uspace} \pi_{\alpha}(\kk, \uu) \left(1 - \pi_{\alpha}(\kk, \uu)\right)p_{\UU}(\uu)\,\mathrm{d}\kk \,\mathrm{d}\uu
\end{equation}
The $\mathrm{IVPC}$ is a measure of the uncertainty on the global
coverage of the set $\{Z - \alpha Z^* \leq 0\}$, where $Z$ is
conditioned by the design $\mathcal{X}_n$. Instead of evaluating this
integrated variance directly on the current design $\mathcal{X}_n$, we
can once again augment the design at an (unevaluated) point
$(\kk, \uu)$, assuming that its evaluation is the random variable
$Z(\kk, \uu)$:
\begin{equation}
  \mathrm{IVPC}(\mathcal{X}_n \cup \{((\kk, \uu), Z(\kk, \uu))\}
\end{equation}

Finally, we can define a new learning function, which is the expected
$\mathrm{IVPC}$ with respect to the random variable $Z(\kk, \uu)$:
\begin{equation}
  \label{eq:expected_aIVPC}
  (\kk_{n+1},\uu_{n+1}) = \argmin_{(\kk, \uu)\in \Kspace\times\Uspace} \Ex_{Z(\kk, \uu)}\Big[\mathrm{IVPC}(\mathcal{X}_n \cup \{   \left(\left(\kk, \uu \right), Z(\kk, \uu) \right)  \} )     \Big]
\end{equation}
\Cref{fig:IVPC_enrichment} shows the evolution of the error in the
estimation of $\Gamma_{\alpha}$, with respect to the $L^2$ and
$L^{\infty}$ norm.
\begin{figure}[ht]
  \centering
  \input{\imgpath IVPC_enrichment.pgf}
  \caption{\label{fig:IVPC_enrichment} Enriching the design according to the criterion of \cref{eq:expected_aIVPC}}
\end{figure}

\subsubsection{Sampling in the margin of uncertainty}
\paragraph{Sampling and translating to the source of uncertainties}
We can also enrich the design by means of sampling within the margin
of uncertainty defined~\cref{sec:margin_of_uncertainty}
\cpageref{sec:margin_of_uncertainty}, introduced
in~\cite{echard_ak-mcs_2011,schobi_rare_2017,razaaly_rare_2019}. % \todo{bof in this case}
In this case, using the probability of coverage defined as
\begin{equation}
  \pi_{\alpha}(\kk, \uu) = \Prob_{Z}\left[\Delta_{\alpha} \leq 0\right] = \Phi\left(-\frac{m_{\Delta_\alpha}(\kk, \uu)}{\sigma_{\Delta_\alpha}(\kk, \uu)}\right)
\end{equation}
the margin of uncertainty of level $\eta$ is by definition
\begin{equation}
  \mathbb{M}_{\eta} = \left\{ (\kk, \uu) \mid \frac{\eta}{2} \leq \pi_{\alpha}(\kk, \uu) \leq 1 - \frac{\eta}{2}\right\}
\end{equation}
The margin of uncertainty of the random set $\{\Delta_{\alpha} \leq 0\}$ has been represented \cref{fig:prob_coverage_margin}.
\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath prob_coverage_margin.pdf}
  \caption[Probability of coverage and margin of uncertainty]{\label{fig:prob_coverage_margin} Probability of coverage
    and Margin of uncertainty for $\{\Delta_{\alpha} \leq 0\}$ at
    level $\eta=0.05$. The variance of the probability of coverage is
    integrated on $\Uspace$, giving the IVPC at constant $\kk$}
\end{figure}

Sampling in this margin of uncertainty and evaluating some of those
points makes sense if the points selected help reduce the margin of
uncertainty.  Let us assume that the current design used to construct
$Z$ comprises $n$ points, and let us assume that we obtained $K$
points which represent statistically the margin of uncertainty:
$\tilde{x}_{n+i}$ for $1\leq i\leq K$. Those points being in our case
the centroids produced by the KMeans algorithms.


As mentioned before in~\cref{eq:variance_delta_GP_ab}
on~\cpageref{eq:variance_delta_GP_ab}, the uncertainty on the
acceptability of those points can be reduced either by evaluating the
function at this point (thus reducing $\sigma^2_Z$), or by improving the
knowledge of the conditional minimiser (reducing
$\alpha^2 \sigma^2_{Z^*}$). For each centroid
$\tilde{x}_{n+i} = (\tilde{\kk}_{n+i}, \tilde{\uu}_{n+i})$, we can
then adjust its $\kk$-component if the uncertainty on the conditional
minimiser is large in order to reduce $\alpha^2 \sigma_{Z^*}^2$
(instead of $\sigma^2_Z$). This is done by introducing an adjustment
function:
\begin{equation}
  \label{eq:adjust_centroid}
  \mathrm{adjust}(\kk,\uu) =
  \begin{cases}
    (\kk, \uu) & \text{ if }\sigma^2_Z(\kk, \uu) \geq \alpha^2\sigma^2_{Z^*}(\uu)\\
    \left(\max_{\kk\in\Kspace} \Ex_Z\left[\left[Z(\kk,\uu) - m_{{Z^*}}(\uu)\right]_+ \right], \uu \right) & \text{ otherwise}
  \end{cases}
\end{equation}
where $m_{{Z^*}}(\uu) = \min_{\kk\in\Kspace} m_Z(\kk, \uu)$ as
defined~\eqref{eq:def_mstar}. In this case, the adjusted value can be
seen as an EGO iteration on $\Kspace \times \Uspace$. The adjusted
centroids are then expected to reduce the most the uncertainty at the
original centroids, given by the KMeans algorithm.

\paragraph{Hierarchical clustering for a second adjustment}
However, as we can see on the left plot
of~\cref{fig:adjusted_centroids}, all centroids are adjusted
independently, and since only their $\kk$ component is changed, the
adjusted centroids may end up very close to each other. The issue is
twofold: the evaluation of those points brings redundant information
to the problem, and thus we do not take fully advantage of the batch
evaluations. Moreover when adding close points to the design, the
proximity can lead to an ill-conditioned covariance matrix of the GP
and in turn to numerical problems in the estimation of its
hyperparameters.


In order to identify the nearby points, we can perform once
again % \todo{préciser (classification?)}
a clustering procedure such as Hierarchical
clustering~\citep{nielsen_hierarchical_2016}, as the number of points
to consider is much more limited, and only a labelling is needed. 
\begin{figure}[ht]
  \centering
  \resizebox{0.9\textwidth}{!}{\input{\imgpath schema_double_adjustment.pgf}}
  \caption[Principle of the two consecutive adjustments for batch
  selection]{\label{fig:schema_double_adj} Principle of the two
    consecutive adjustments: The first adjustment is done in order to
    reduce the most the uncertainty, and the second to select points
     far enough from each other}
\end{figure}

For each point in each cluster, as they are in close vicinity, we can
assume that their uncertainty on the conditional minimiser
$\alpha^2\sigma^2_{Z^*}$ are sensibly the same. So instead, we are
going to make the adjustment described \cref{eq:adjust_centroid} only
for the point which presents the smallest $\sigma^2_Z$.  The whole
algorithm is described~\cref{alg:sampling_enrichment_star}, and the
double adjustment is described schematically
in~\cref{fig:schema_double_adj}. In the end, the points added to the
design are the points labeled ``Adjusted centroids 2''.



\begin{algorithm}
  \caption{Enrichment of the design using sampling to reduce the margin of uncertainty of $\{\Delta_{\alpha}\leq 0\}$}
  \label{alg:sampling_enrichment_star}
\begin{algorithmic}
  \REQUIRE Initial design $\mathcal{X}_0$, 
  \REQUIRE number of points to evaluate each iterations $K$, number of samples $N$
  \REQUIRE $Z$, a GP using the design $\mathcal{X}_0$
  \REQUIRE Probability of coverage $\pi_{\alpha}$, and sampling pdf $\kappa_{\mathbb{M}} = \mathbbm{1}_{\mathbb{M}_{\eta}}$
\STATE $n \leftarrow 0$
\WHILE{(stopping criterion not met) \OR (evaluation budget not reached)} 
\STATE Sample $N$ points $S = \{x^s_i\}_{1 \leq i \leq N}$ in $\mathbb{M}_{\eta}\subset \Kspace \times \Uspace$ using $\kappa_{\mathbb{M}}$
\STATE Get the $K$ centroids $\{x_i^{\mathrm{center}}\}_{1 \leq i \leq K}$ using KMeans on $S$
\FOR{$i=1$ to $K$} % \COMMENT{Find the $N_s$ closest points}
\STATE $\tilde{x}_{n+i}=(\kk_{n+i},\uu_{n+i})=\argmin_{x \in S \setminus \{\tilde{x}_{n+1}, \dots, \tilde{x}_{n+i-1}\}} \|x - x_i^{\mathrm{center}} \|$
\ENDFOR

\STATE Hierarchical clustering of $\{\mathrm{adjust}(\tilde{x}_{n+i})\}$ for $1 \leq i \leq K$
\STATE Define $\{x_{n+i}\}_{1\leq i\leq K}$ as the readjusted clusters using hierarchical clustering
\STATE Evaluate $f(x_{n+i})=J(\kk_{n+i},\uu_{n+i})$ for $1\leq i \leq K$
\STATE $\mathcal{X}_{n+K} \leftarrow \mathcal{X}_n \cup \left\{\left(x_{n+1}, f(x_{n+1})\right),\dots,  \left(x_{n+K}, f(x_{n+K})\right)\right\}$
\STATE Condition the GP according to $\mathcal{X}_{n+K}$
\STATE $n \leftarrow n + K$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

We apply an iteration of this method on the real function,
in~\cref{fig:adjusted_centroids}. At first, the $K=10$ centroids
obtained by KMeans are represented with the red stars. As the measured
uncertainty is due to the conditional minimisers, all the centroids
are adjusted, yielding the blue stars near the locus of the conditional minimisers. At
$\uu \approx \num{0.6}$, $\uu\approx\num{0.4}$ and $\uu\approx \num{0.15}$, the hierarchical clustering

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{\imgpath adjusted_centroids.pdf}
  \caption[Full batch iteration with double
  adjustment]{\label{fig:adjusted_centroids} Sampling in the margin of
    uncertainty, and adjustment of centroids. The centroids of the
    KMeans algorithm are in red, the adjusted centroids are in blue,
    and the adjusted centroids after the hierarchical clustering step
    are in magenta}
\end{figure}

On~\cref{fig:AK_MCS_Gamma} is shown the evolution of the norm of the
difference between the estimated $\hat{\Gamma}_{\alpha,n}$ (after
having added $n$ points to the design) and the true value
$\Gamma_{\alpha}$, when adding simultaneously $K=5$ and $K=10$ points.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{\imgpath AK_MCS_Gamma.pdf}
  \caption[Error in the estimation of $\Gamma_{\alpha}$ using a
  sampling-based method]{\label{fig:AK_MCS_Gamma} Evolution of the
    error with the number of added points in the estimation of
    $\Gamma_{\alpha}$ when selecting points using the sampling based
    methods described \cref{alg:sampling_enrichment_star}. $\pi$
    refers to the estimation of $\Gamma$ using a probabilistic
    approach, while $\mathrm{PI}$ is the estimation using a plug-in
    approach}
\end{figure}
The added points are shown~\cref{fig:AK_MCS_design}, and the margin of
uncertainty $\mathbb{M}_{\eta}$ after the $n=70$ evaluations. We can
see that the added point are distributed either toward the conditional
minimisers, or toward the frontier of
$\{(\kk, \uu) \mid J(\kk, \uu) = \alpha J^*(\uu)\}$, as expected.
\begin{figure}[ht]
  \centering
  \includegraphics[width = \textwidth]{\imgpath AK_MCS_design.pdf}
  \caption[GP prediction, final experimental design and margin of uncertainty]{\label{fig:AK_MCS_design} GP prediction and final
    experimental design (left) and resulting margin of uncertainty
    $\mathbb{M}_{\eta}$}
\end{figure}


% \subsubsection{Vorob'ev expectation as placeholder}
% We can also consider the set $Z - \alpha Z^* \leq 0$ from a random set perspective:
% For each $\kk\in\Kspace$, we can consider the random closed set $A_\kk =  \{\uu \in \Uspace \mid Z(\kk, \uu) - \alpha Z^*(\uu) \leq 0\}$.
% In this case, we can set $\mu = \Prob_{\UU}$, so that $\mu(A_{\kk}) = \Prob_{\UU}[A_{\kk}]$.
% For a given $\kk$, the probability of coverage of the set $A_{\kk}$
% is
% \begin{equation}
%   \label{eq:prob_cov_randclosedset}
%   \pi_{A_{\kk}}(\uu) = \frac{\pi_{\alpha}(\kk, \uu)}{\int_{\Uspace} \pi_{\alpha}(\kk, \uu)p_{\UU}(\uu)\,\mathrm{d}\uu}
% \end{equation}
% The normalization constant of \cref{eq:prob_cov_randclosedset} can be obtained quite easily numerically, as $\pi_{\alpha}$ can be computed in a closed from~\cref{eq:def_pialphaku}.

% The Vorob'ev mean is defined as the $\eta^*$-level set of $\pi_{A_\kk}(\uu)$, that verifies
% \begin{equation}
%   \Prob_{\UU}[Q_{\eta^*}(\kk)] = \Ex_{Z}\left[\Prob_{\UU}\left[A_{\kk}\right]\right] %= \int_{\Zspace} \int_{\Uspace} \mathbbm{1}_{\{(y(\kk, \uu) - \alpha y^*(\uu)\leq 0\}} \,\mathrm{d}u \mathrm{d}y
% \end{equation}
% We can then define a new estimator of $\Gamma$:
% \begin{equation}
%   \Gamma_{\alpha}^{\mathrm{Voro}}(\kk) = \Prob_{\UU}\left[Q_{\eta^*}(\kk)\right]
% \end{equation}
% % \tocheck{Compare Vorob'ev deviation with pi(1-pi) ?}


\clearpage

\subsection{Optimisation of the quantile of the relative regret}
\label{sec:quantile_rr}
In the previous section, we introduced methods in order to improve the
estimation of
$\Gamma_{\alpha}(\kk)= \Prob_{\UU}\left[J(\kk, \UU) \leq \alpha
  J^*(\UU)\right]$, the probability of exceeding a threshold
$\alpha \geq 1$ chosen and fixed beforehand.
However, once properly estimated and optimised, the maximal
probability $\max \Gamma_{\alpha}$ may not ensure a large enough
reliability in terms of relative-regret, especially if the threshold
has been chosen arbitrarily.


As mentioned in~\cref{sec:choice_threshold}, we can instead look to tune
$\alpha$ such that $\max \Gamma_{\alpha}$ is large enough, \emph{i.e.}
exceeds a specified level of confidence
$p$. % \todo{Plus de rappels, points d'étapes}
Let us recall~\cref{eq:opt_alpha_p_formulation} which defines
$\alpha_p$ as the smallest threshold giving a maximal probability of
atleast $p$:

\begin{equation}
  \label{eq:alpha_p_inf_formation}
  \alpha_p = \inf_{\alpha \geq 1}\{\max_{\kk \in \Kspace}\Gamma_{\alpha}(\kk)\geq p \}
\end{equation}

As $J^*(\uu) > 0$ for all $\uu$, we can define $q_{p}(\kk)$ as the quantile of level $p$ of the ratio $J(\kk, \UU) / J^*(\UU)$:
\begin{align}
  \label{eq:qp_definition}
       q_{p}(\kk)= Q_{\UU}\left(\frac{J(\kk, \UU)}{J^*(\UU)};p\right) \iff  p  = \Prob_{\UU}\left[\frac{J(\kk, \UU)}{J^*(\UU)} \leq q_{p}(\kk)\right] = \Gamma_{q_p(\kk)}(\kk)
\end{align}
where $Q_{\UU}(\cdot\,;p)$ is the quantile function of order $p$ with respect to the random variable $\UU$.
$\alpha_p$ verifies then
\begin{equation}
  \label{eq:alpha_p_min}
\alpha_p = \min_{\kk\in\Kspace} q_p(\kk)
\end{equation}
The relation between $\Gamma_{\alpha_p}$ and $q_p$ is illustrated~\cref{fig:q_p_illu}.
\begin{figure}[ht]
  \centering
  \input{\imgpath q_p_illustration.pgf}
  \caption[Relation between $\Gamma_{\alpha}$ and $q_p$]{\label{fig:q_p_illu} Relation between the probability
    $\Gamma_{\alpha}$ and its maximum, and the quantile function of the
    ratio $q_p$}
\end{figure}

Given~\cref{eq:alpha_p_min}, we are going to focus on the estimation
of the quantiles of order $p$ of the ratio $J/J^*$
defined \cref{eq:qp_definition} as $q_p(\kk)$ for $\kk \in \Kspace$.

Numerically speaking and given $N$ i.i.d.\ samples $x_i \sim X$, where
$X$ is a real-valued random variable, the estimation of a quantile is
quite easy. A common estimation of the quantile of order $p$ of $X$ is
$x_{(\left[Np\right])}$, where $x_{(i)}$ is the $i$th order statistic
of samples (\emph{i.e.} the $i$th smallest value of the set of
samples), and $[\cdot]$ is the rounding operator.

As done for $\Delta_{\alpha} = Z - \alpha Z^*$, and the estimation of
$\Gamma_{\alpha}$, using the GP $Z$ constructed on $J$, we are going
to approximate the ratio $J/J^*$, in order to have an estimation which
depends on the properties of the surrogate $Z$.
In~\cref{ssec:lognormal_approx}, we are going to derive some
properties of the ratio $Z/Z^*$, before introducing a $1$-step
criterion in \cref{ssec:reduction_imse_quantile}, and a sampling
method in \cref{ssec:quantile_qeakmcs}.

\subsubsection{Lognormal approximation of the ratio}
\label{ssec:lognormal_approx}
Let $\kk\in\Kspace$ and $\uu\in\Uspace$.  The true value of the ratio
$J(\kk,\uu)/J^*(\uu)$ is modelled as the random variable
$Z(\kk,\uu)/Z^*(\uu)$, where the joint distribution of $Z(\kk, \uu)$
and $Z^*(\uu)$ is
\begin{equation}
  \begin{bmatrix}
    Z(\kk, \uu) \\ Z^*(\uu)
  \end{bmatrix}
  \sim \mathcal{N}\left(
    \begin{bmatrix}
      m_Z(\kk, \uu) \\ m_{Z^*}(\uu)
    \end{bmatrix}
    ;
    \begin{bmatrix}
      \sigma^2_Z(\kk, \uu) &\rho \sigma_Z(\kk, \uu)\sigma_{Z^*}(\uu) \\
      \rho \sigma_Z(\kk, \uu)\sigma_{Z^*}(\uu) & \sigma^2_{Z^*}(\uu) 
    \end{bmatrix}
  \right)
\end{equation}
where $\rho$ is the correlation coefficient
$\rho = \frac{\Cov\left(Z(\kk, \uu), Z^*(\uu)\right)}{\sigma_Z(\kk,
  \uu)\sigma_{Z^*}(\uu)}$.
% What we are calling the plug-in approach is to replace the ratio $Z(\kk, \uu)/Z^*(\uu)$, with its mean $\Ex_Z\left[\frac{Z(\kk, \uu)}{Z^*(\uu)}\right]$. 
Under the condition that $m_{Z^*}(\uu)/\sigma_{Z^*}(\uu)$ is large,
\textit{i.e.} that $Z^*(\uu) \geq 0$ with high probability, we can
make the approximation that at each $(\kk, \uu)$, the ratio is
lognormally distributed (see \cref{sec:lognorm_ratio}):
\begin{align}
  \label{eq:log_ratio_eq}
  \frac{Z(\kk, \uu)}{Z^*(\uu)}&\sim \log \mathcal{N}\left(\log\frac{m_Z(\kk, \uu)}{m_{Z}^*(\uu)};\frac{\sigma^2_Z(\kk, \uu)}{m_Z(\kk, \uu)^2}+\frac{\sigma^2_{Z^*}(\uu)}{m_{{Z^*}}(\uu)^2} -  2 \rho \frac{\sigma_Z(\kk,\uu) \sigma_{Z^*}(\uu)}{m_{Z}(\kk, \uu)m_{Z^*}(\uu)}\right) \\
  &\sim \log\mathcal{N}(m_{\Xi}(\kk, \uu), \sigma^2_{\Xi}(\kk, \uu))
\end{align}
By definition of the lognormal distribution, the logarithm of $Z/ Z^*$
can then be approximated by a normally distributed random variable, denoted $\Xi(\kk, \uu)$:
\begin{equation}
  \label{eq:log_ratio}
  \Xi(\kk,\uu) =  \log\left(\frac{Z(\kk, \uu)}{Z^*(\uu)}\right)\sim \mathcal{N}\left(m_{\Xi}(\kk, \uu),\sigma_{\Xi}^2(\kk, \uu)\right)
\end{equation}

The mean value of the lognormally distributed random variable is used
as an estimate of the ratio:
\begin{equation}
  \label{eq:mean_ratio_logn}
\Ex_Z\left[\frac{Z(\kk, \uu)}{Z^*(\uu)}\right] = \frac{m_Z(\kk, \uu)}{m_{Z^*}(\uu)}\exp\left(\frac{1}{2}\sigma^2_{\Xi}(\kk, \uu)\right) 
\end{equation}
and then the plug-in estimation of the quantile of
order $p$ is:
\begin{align}
  {q}^{\mathrm{PI}}_p(\kk) &= Q_{\UU}\left(\frac{m_Z(\kk, \UU)}{m_{Z^*}(\UU)}\exp\left(\frac{1}{2}\sigma^2_{\Xi}(\kk, \UU)\right),p \right)\\ &= Q_{\UU}\left(\exp\left(m_{\Xi}(\kk, \uu)+\frac{1}{2}\sigma^2_{\Xi}(\kk, \UU)\right),p \right)
\end{align}
And based on a set of samples $\{\uu_i\}_{1\leq i \leq n_{\uu}}$, the estimated quantile becomes
\begin{equation}
  \hat{q}^{\mathrm{PI}}_p(\kk) = {\left(\exp\left(m_{\Xi}(\kk, \uu)+\frac{1}{2}\sigma^2_{\Xi}(\kk, \uu)\right)\right)}_{\left(\left[n_{\uu}p\right]\right)}
\end{equation}

% Additionally, to clear any confusion, the ratio $m_Z(\kk, \uu)/m_{Z^*}(\uu)$ is the median of the lognormally distribued random variable $Z(\kk, \uu)/Z^*(\uu)$.

We can use directly the Gaussian formulation of $\Xi$ in order to
derive strategies of enrichment. In the following, we will set the
confidence level to $p=0.95$.
  
  \subsubsection{Reduction of the IMSE}
  \label{ssec:reduction_imse_quantile}
  As previously done for the random process $Z - \alpha Z^*$, we can
  look to reduce at each iteration the integrated prediction variance
  of the ratio, which is $\sigma^2_{\Xi}$ by defining the IMSE
\begin{equation}
  \IMSE(\mathcal{X}_n) = \int_{\Kspace\times\Uspace} \sigma_{\Xi}^2(\kk, \uu) \,\mathrm{d}\kk\,\mathrm{d}\uu
\end{equation}
where $\sigma_\Xi^2$ is constructed using $Z/Z^*$ and the design $\mathcal{X}_n$.

We can then look to improve the expected prediction error on the
log-ratio described \cref{eq:log_ratio}, by minimising the augmented
IMSE:
\begin{equation}
  \label{eq:aIMSE_Xi}
 (\kk_{n+1}, \uu_{n+1}) = \argmin_{(\kk, \uu) \in \Kspace \times \Uspace}\Ex_{Z(\kk,\uu)}\left[\IMSE(\mathcal{X}_n \cup \left\{\left((\kk, \uu),Z(\kk, \uu)\right)\right\}\right]
\end{equation}

meaning that $\sigma_{\Xi}$ is computed using \cref{eq:log_ratio_eq},
and the GP augmented by the couple
$\left((\kk, \uu), Z(\kk, \uu)\right)$.  The result of this adaptive
strategy is illustrated~\cref{fig:qPI_aIMSE}. Along with
$\hat{q}_{p,n}^{\mathrm{PI}}$, we plotted Monte-Carlo estimation of
the quantile: $\hat{q}_{p,n}^{\mathrm{MC}}$, which has been obtained
as a Monte-Carlo estimation of
$\Ex_{Z}\left[Q_{\UU}\left(\frac{Z(\kk,
      \uu)}{Z^*(\uu)};p\right)\right]$ for comparison.  We can see
that this method is able to reduce steadily the error on the
estimation of the quantile of order $p=0.95$ in this case. The large
discontinuities in the error are probably due to the evaluation points
in a region previously unexplored, and/or the effect of a significant
change in the hyperparameters of the GP.
\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath qPI_aIMSE.pdf}
  \caption[Error of the estimation when reducing the augmented ISME of $\Xi$]{\label{fig:qPI_aIMSE} Evolution of the error of the estimation when reducing the augmented IMSE of the log-ratio $\Xi$}
\end{figure}

Each step of this method requires the optimisation of the expected
IMSE described~\cref{eq:aIMSE_Xi}, which can become expensive. We can also
define a sampling based method, which is based on plausible values of
the quantile of order $p$.

\subsubsection{Sampling-based method, adaptation of the QeAK-MCS}
\label{ssec:quantile_qeakmcs}
We introduced previously a 1-step strategy, which looks to reduce the
prediction error of the ratio
$Z/Z^*$. In~\cite{razaaly_rare_2019,razaaly_quantile-based_2020}, the
author introduce a sampling-based method, named QeAK-MCS, for the estimation of extreme
quantiles, that we can adapt for the estimation of the quantiles of
the ratio.  Recalling that
\begin{equation}
  \Xi(\kk, \uu) = \log\frac{Z(\kk, \uu)}{Z^*(\uu)} \sim \mathcal{N}\left(m_{\Xi}(\kk, \uu),\sigma^2_{\Xi}(\kk, \uu)\right)
\end{equation}
we can get bounds of the true value of the quantile of order $p$ at a
given $\kk\in\Kspace$ by using the quantiles of $\Xi(\kk, \uu)$:
\begin{align}
  \log q_p^{+}(\kk) = Q_{\UU}\left(m_{\Xi}(\kk, \UU) + k \sigma_{\Xi}(\kk, \UU), p\right) \\
  \log q_p^{-}(\kk) = Q_{\UU}\left(m_{\Xi}(\kk, \UU) - k \sigma_{\Xi}(\kk, \UU), p\right)
\end{align}
where $k$ is a quantile of the standard Gaussian random variable.  Let us define
$\tilde{\kk}=\argmin_{\kk \in\Kspace} \hat{q}_p(\kk)$, that is the
value that minimises the quantile of order $p$ of the plug-in estimate
of the ratio, and $\hat{\alpha}_p = \hat{q}_p(\tilde{\kk})$.

By linearly discretizing the interval
$\interval{q^-_p(\tilde{\kk})}{q^+_p(\tilde{\kk})}$, we can define
$\mathfrak{q}_l$ for $1 \leq l \leq K_q$, with
$\mathfrak{q}_1 = q^-_p$ and $\mathfrak{q}_{K_q} = q^+_p$. The
$\{\mathfrak{q}_l\}_{1\leq l \leq K_q}$ are then expected to cover the
plausible range of values that the sought quantile
$\alpha_p = \min q_p(\kk)$ may take.

For each of those $K_q$ quantiles, we can look to sample
$K_{\mathbb{M}}$ points in the margin of uncertainty of the log-ratio,
which is defined as
\begin{align}
  \mathbb{M}_{\eta}(\mathfrak{q}_l)  &= \left\{ (\kk, \uu) \mid \frac{m_{\Xi}(\kk, \uu) - \log \mathfrak{q}_l}{\sigma_{\Xi}(\kk, \uu)}< k \text{ and } -\frac{m_{\Xi}(\kk, \uu)-\log \mathfrak{q}_l}{\sigma_\Xi(\kk, \uu)}  <  k\right\} \\
                                     &= \{(\kk, \uu) \mid m_{\Xi}(\kk,\uu) - k\sigma_{\Xi}(\kk, \uu) < \log \mathfrak{q}_l < m_{\Xi}(\kk, \uu) + k\sigma_{\Xi}(\kk, \uu)\}
                                     % &= \left\{ (\kk, \uu) \mid m_{\Xi} - k \sigma_{\Xi} < \log \mathfrak{q}_l \land  m_{\Xi} + k \sigma_{\Xi} > \log \mathfrak{q}_l\right\} \\
\end{align}
with $k$ an appropriate quantile of the standard Gaussian random
variable. Equivalently, by defining the coverage probability of the
set $\{\log \frac{Z(\kk,\uu)}{Z^*(\uu)}\leq \log \mathfrak{q}_l\}$,
which is
$\pi_{\Xi,l}(\kk, \uu) = \Phi\left(-\frac{m_{\Xi}(\kk, \uu) - \log
    \mathfrak{q}_l}{\sigma_{\Xi}(\kk, \uu)}\right)$,
\begin{equation}
  \mathbb{M}_{\eta}(\mathfrak{q}_l) = \left\{(\kk, \uu) \mid \frac{\eta}{2}\leq\pi_{\Xi,l}(\kk, \uu) \leq 1-\frac{\eta}{2}\right\}
\end{equation}

In other words, this margin of uncertainty is the set of points whose
confidence interval of level $\eta$ of the log-ratio comprises the
targeted value $\log \mathfrak{q}_l$.

In the end, we have $K = K_qK_{\mathbb{M}}$ points to add to the
design, that we can adjust as in~\cref{alg:sampling_enrichment_star},
in order to reduce the uncertainty on the value of the conditional
minimum when required. This procedure is
illustrated~\cref{fig:QAK_MCS}, while
on~\cref{fig:evol_error_qAK_MCS}, the evolution of the error on the 
estimation of $q_p$ is shown.
\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath QAK_MCS.pdf}
  \caption{\label{fig:QAK_MCS} One iteration of the QeAK-MCS procedure, with $K_q=3$ and $K_{\mathbb{M}}=4$}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics{\imgpath evol_error_qAK_MCS.pdf}
  \caption[Evolution of the estimation
    error for $K=K_qK_{\mathbb{M}}=12$]{\label{fig:evol_error_qAK_MCS} Evolution of the estimation
    error as $K=K_qK_{\mathbb{M}}=12$ are added each iterations}
\end{figure}
We can see that we can reduce globally
the estimation error, but it seems slower than the augmented IMSE, as
shown~\cref{fig:qPI_aIMSE}.


An issue that may arise is that when the GP is accurate enough,
$q_p^-$ and $q_p^+$ become close, hence the different margin of
uncertainties produce centroids close to each others, which may lead
to numerical difficulties. A crude way to deal with this technical
issue is to discard randomly all but one point when some are close to
each other, as done with hierarchical clustering.

\section{Partial conclusion}

In this chapter, we introduced Gaussian Processes as surrogates for
the expensive-to-evaluate objective function $J$. This metamodel, once
constructed using an initial design of points on
$\Kspace \times \Uspace$, can then be used to compute the robust
estimators introduced in~\cref{chap:robust_estimators}.

However, due to the limited size of the initial design, and the
impossibility to perform exhaustive computations of the objective
function, this surrogate can be inaccurate in some regions, leading to
a bad estimation, and thus a questionable calibration.  Using the
properties of the GP, we can construct adaptive strategies in order
to enrich the design of experiment sequentially, where the points
chosen improve significantly the various estimations, and in this
thesis, the estimation of the regret-based estimators.

Several improvements can be considered. First, all the methods
introduced here focus on the improvement of the estimation of a
function, which will be optimised in order to get the estimator. The
adaptive strategy could be adapted to take into account the subsequent
optimisation. Similarly as in~\cite{janusevskis_simultaneous_2010}, we
could develop strategies in two parts: first, identify the point in
$\Kspace$ which has the most potential to be the wanted estimator, and
second, find the point in the joint space $\Kspace \times \Uspace$
which reduces the most the uncertainty on the candidate.

Another issue encountered is that Gaussian Processes are suited to
approximate functions of a moderate number of variables, because the
optimisation of the hyperparameters becomes increasingly difficult. In
order to apply the algorithms based on GP, the dimension may first
have to be reduced. Moreover, we did not take advantage of the
gradient information, which may be available through the adjoint
method and automatic differentation tools, as has been done
in~\cite{bouhlel_gradient-enhanced_2019,laurent_overview_2019,miranda_adjoint-based_2016,pardalos_differentiating_2015}.

In the next chapter, we will study the problem of robust calibration
of a realistic numerical model, using some of the methods we
introduced in this chapter.


% \begin{table}[ht]
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     Objective name & Objective to minimise wrt $\kk$  & Computational solution \\ \midrule
%     Profile Likelihood & $-\log \max_{\uu \in \Uspace} p_{Z \mid \KK, \UU}(y \mid \kk, \uu)$  \\
%     Integrated Likelihood & $-\log \int_{\Uspace} p_{Z \mid \KK, \UU}(y \mid \kk, \uu) \,\mathrm{d}\uu=-\log p_{Z\mid \KK}(y\mid \kk)$ \\
%     Marginal maximum a posteriori & $-\log p_{\KK\mid }(\kk \mid y)$  & MCMC based sampling (\cite{doucet_marginal_2002})\\ \midrule
%     Global Optimum & $\min_{\uu\in\Uspace} J(\kk, \uu)$ & EGO (\cite{jones_efficient_1998})\\
%     Worst-case & $\max_{\uu \in \Uspace} J(\kk, \uu)$ \\
%     Regret worst-case & $\max_{\uu\in\Uspace}\left\{J(\kk, \uu) - \min_{\kk^{\prime}\in\Kspace} J(\kk^{\prime}, \uu)\right\}$ \\ \midrule
%     Mean & $\Ex_{\UU}[J(\kk, \UU)]$ & Projected GP \\
%     Mean and variance & $ \lambda \Ex_{\UU}[J(\kk, \UU)] + (1-\lambda) \sqrt{\Var_{\UU}[J(\kk, \UU)]}$ & Projected GP \\ \bottomrule
%   \end{tabular}
%   \caption{Summary of single objective robust estimators}
% \end{table}
\markchapterend

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
}
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
