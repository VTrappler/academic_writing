\documentclass[../../Main_ManuscritThese.tex]{subfiles}

\subfileGlobal{
\renewcommand{\RootDir}[1]{./Text/Chapter4/#1}
}
% \DeclareMathOperator{\IMSE}{\mathrm{IMSE}}
% \newcommand{\IMSE}{\mathop*{\mathrm{IMSE}}}

% For cross referencing
\subfileLocal{
\externaldocument{../../Text/Introduction/build/Introduction}
\externaldocument{../../Text/Chapter2/build/Chapter2}
\externaldocument{../../Text/Chapter3/build/Chapter3}
\externaldocument{../../Text/Chapter5/build/Chapter5}
\externaldocument{../../Text/Conclusion/build/Conclusion}
}
\newcommand\imgpath{/home/victor/acadwriting/Manuscrit/Text/Chapter4/img/} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \dominitoc
% \faketableofcontents
% \subfileLocal{\setcounter{chapter}{2}}
\chapter{Adaptative design enrichment for calibration using Gaussian Processes}
\label{chap:adaptative_design_gp}
\minitoc
\newpage
\subfileLocal{\pagestyle{contentStyle}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% 				SECTION 0 : Introduction	   %%%

Numerical models are usually very expensive to run in terms of computer resources. Indeed, for most realistic physical simulations, the programs have to solve systems of PDEs over large grids. Even though the computations are optimized and parallelized to run on high performance computers, methods that require a large number of runs of the model for exhaustivity should be avoided. In this chapter, we will focus on \emph{surrogate models}.

\section{The computational bottleneck}

\subsection{The curse of dimensionality}

\subsubsection{Sensitivity analysis}

\subsubsection{Dimension Reduction}

\subsection{Surrogate models}
\begin{definition}[Surrogate model]
  A surrogate
\end{definition}


\section{Gaussian process regression}
In the following, we will introduce a generic function $f$, that maps a space $\Xspace$ to $\mathbb{R}$. Depending on the application, $\Xspace = \Kspace$ or $\Xspace = \Kspace \times \Uspace$. This function is unknown, and supposedly expensive to evaluate, but it has already been evaluated on a set of points $x_i$
\cite{rasmussen_gaussian_2006}
\subsection{Random processes}
Let us assume that we have a map $f$ from a $p$ dimensional space to $\mathbb{R}$:
\begin{align}
  \begin{array}{rrcl}
    f: & \mathbb{X} \subset \mathbb{R}^p& \longrightarrow & \mathbb{R} \\
       & x & \longmapsto & f(x)
  \end{array}
\end{align}

This function is assumed to have been evaluated on a design of $n$ points, $\mathcal{X} = \left\{ (x_i, f(x_i) \right)\}_{1\leq i\leq n}$, called the \emph{initial design} For notational simplicity, we write $x\in \mathcal{X}$ if $(x, f(x)) \in \mathcal{X}$.
As this function is unknown, there is (epistemic) uncertainty on the values outside of the initial design. This uncertainty can be reduced by directly evaluating the function.
This uncertainty is modelled by random processes as defined in the following

\begin{definition}[Random process]
  Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space, and $\Xspace\subset \mathbb{R}^p$.
  A random process $Z$ is a collection of random variables indexed on $\Xspace$, so for each $x \in \Xspace$, $Z(x)$ is a random variable:
 \begin{equation}
  \begin{array}{rcl}
    Z: \mathbb{X} & \longrightarrow & \left(\Omega \rightarrow \mathbb{R} \right)\\
    x& \longmapsto & Z(x)
  \end{array}
\end{equation}
A sample from this random process, that is $Z(\cdot)(\omega)$ for $\omega \in \Omega$ will be shortened as $Z(\cdot, \omega)$ for notational purpose, and is called a \emph{sample trajectory}, or a \emph{sample path}.
\end{definition}
From a Bayesian point of view, we can imagine constructing a random process $Z$ as a prior on the function $f$, or in other words, considering $f$ as a particular sample path of $Z$. Evaluating the function, thus \tocheck{augmenting} the design means to gather information on the random process, and we can update our belief on $f$.


\begin{definition}[Gaussian process]
  Let $Z$ be a random process on $\Xspace$, i.e. a collection of random variables indexed by $\Xspace$. It is defined as a Gaussian process if any finite number of those random variables have a multivariate joint Gaussian distribution.
  In that case, $Z$ is uniquely defined by its mean function $m_Z$ and its covariance function $C_Z$:
  \begin{align}
    m_Z(x) &= \Ex\left[Z(x)\right] \\
    C_Z(x, x^{\prime}) &= \Cov[Z(x), Z(x^{\prime})]
  \end{align}
  and we write $Z \sim \GP(m_Z, C_Z)$
\end{definition}

By imposing constraints on a random process, we can use it to create a surrogate for the function $f$. A surrogate estimated based on the random process $Z$ will be denoted $\hat{Z}$ and
\begin{equation}
  \hat{Z}: \Xspace \rightarrow \mathbb{R}
\end{equation}

\subsection{Linear Estimation}
\label{sec:linear_estimation}
Given a random process $Z$ as a prior on $f$, we want to construct a surrogate $\hat{Z}$, using the intial design $\mathcal{X} = \{(x_i, f(x_i)\}_{1 \leq i \leq n}$. This surrogate will be constructed as a linear estimation:
A linear estimation $\hat{Z}$ of $f$ at an unobserved point $x\notin \mathcal{X}$ can be written as
\begin{equation}
  \label{eq:lin_est}
  \hat{Z}(x) =
  \begin{bmatrix}
    w_1 \dots w_n
    \end{bmatrix}
    \begin{bmatrix}
      f(x_1) \\ \vdots \\ f(x_n)
    \end{bmatrix} = \mathbf{W}^Tf(\mathcal{X}) = \sum_{i=1}^n w_i(x) f(x_i)
\end{equation}
Using those \emph{kriging weights} $\mathbf{W}$, a few additional conditions must be added, in order to obtain the Best Linear Unbiased Estimator:
\begin{itemize}
\item Non-biased estimation: $\Ex[\hat{Z}(x) - Z(x)]=0$
\item Minimal variance: $\min~\Ex[(\hat{Z}(x) - Z(x))^2]$
\end{itemize}
The non-biasedness condition using~\cref{eq:lin_est} can be rewritten
\begin{equation}
  \Ex[\hat{Z}(x) - Z(x)]=0 \iff m\left(\sum_{i=1}^n w_i(x)-1\right) = 0 \iff \sum_{i=1}^n w_i(x) = 1 \iff \mathbf{1}^T \mathbf{W} = 1
\end{equation}
For the minimum of variance, we introduce the augmented random vectors $\mathbf{Z}_n(x) = [Z(x_1),\dots Z(x_n), Z(x)]$ and $\mathbf{Z}_n = [Z(x_1),\dots Z(x_n)]$, and
the variance can be expressed as:
\begin{align}
  \Ex[(\hat{Z}(x) - Z(x))^2] &= \Cov\left[[\mathbf{W}^T, -1] \cdot \mathbf{Z}_n(x) \right] \\
                             &= [\mathbf{W}^T, -1] \Cov\left[\mathbf{Z}_n(x) \right] [\mathbf{W}^T, -1]^T
\end{align}
In addition, we have
\begin{equation}
  \Cov\left[\mathbf{Z}_n(x) \right] =
  \begin{bmatrix}
    \Cov\left[ \mathbf{Z}_n^T\right]
    & \Cov\left[\mathbf{Z}_n^T, Z(x) \right]
  \\
  \Cov\left[\mathbf{Z}_n^T, Z(x) \right]^T & \Var\left[Z(x)\right]
  \end{bmatrix}
\end{equation}
Once expanded, the kriging weights solve then the following optimisation problem:
\begin{align}
  \min_{\mathbf{W}} ~&~\mathbf{W}^T \Cov\left[\mathbf{Z}_n\right] \mathbf{W}+ \Var\left[Z(x)\right] \\ &-\Cov\left[\mathbf{Z}_n^T, Z(x) \right]^T \mathbf{W}- \mathbf{W}^T\Cov\left[\mathbf{Z}_n^T, Z(x) \right] \\ 
  \text{s.t. }&  \mathbf{1}^T\mathbf{W} = 1
\end{align}
This leads to
\begin{align}
  \begin{bmatrix}
    \mathbf{W} \\ m
  \end{bmatrix}
  &=
  \begin{bmatrix}
    \Cov\left[\mathbf{Z}_n\right] & \mathbf{1} \\
  \mathbf{1}^T & 0
\end{bmatrix}^{-1}
                 \begin{bmatrix}
                  \Cov\left[\mathbf{Z}_n^T, Z(x) \right]^T \\ 1 
\end{bmatrix}
  \\ &=
    \begin{bmatrix}
      C(x_1, x_1) & \cdots & C(x_1, x_n) & 1 \\
      C(x_2, x_1) & \cdots & C(x_2, x_n) & 1 \\
      \vdots & \ddots & \vdots & \vdots \\
      C(x_n, x_1) & \cdots & C(x_n, x_n)& 1 \\
      1 & \cdots & 1 & 0
    \end{bmatrix}^{-1}
                       \begin{bmatrix}
                         C(x_1, x) \\
                         C(x_2, x) \\
                         \vdots \\
                         C(x_n, x) \\
                         1
                       \end{bmatrix}
\end{align}
and
\begin{equation}
  \hat{Z}(x) =
      \begin{bmatrix}
      C(x_1, x) &
      C(x_2, x) &
      \dots &
      C(x_n, x)
      \end{bmatrix}
      \left(\begin{bmatrix} C(x_1, x_1) & \cdots & C(x_1, x_n)\\
      C(x_2, x_1) & \cdots & C(x_2, x_n) \\
      \vdots & \ddots & \vdots  \\
      C(x_n, x_1) & \cdots & C(x_n, x_n) \\
    \end{bmatrix}^{-1}\right)^T
  \begin{bmatrix}
    f(x_1) \\ f(x_2) \\ \vdots \\ f(x_n)
  \end{bmatrix}
\end{equation}
One main interesting point of GP regression, is that more than the surrogate $\hat{Z} = m_Z$, we have a measure of the uncertainty on the estimation:
\begin{equation}
  Z(x) \sim \mathcal{N}\left(m_Z(x), \sigma^2_Z(x)\right) \quad \text{ with } \sigma^2_Z(x) = C_Z(x, x)
\end{equation}

\begin{figure}[ht]
  \centering
  \input{\imgpath example_GP.pgf}
  \caption{\label{fig:example_GP} Example of a Gaussian Process. The shaded regions correspond to the regions $m_Z \pm i \cdot \sigma_Z$ for $i=1, 2,3$.}
\end{figure}


\subsection{Covariance functions}
\label{sec:cov_fun}
In the previous section, we described the equations to solve to get the surrogate $\hat{Z}$ of $f$ based on GP.
The coefficients of the linear estimation are based on the covariance function $C_Z$.
A covariance is said to be stationary, if for all $x, x^{\prime} \in \Xspace$, the covariance of the GP between those two points depends only on the  difference $h = x-x^{\prime}=(h_1,\cdots, h_{\dim\Xspace})$. In that case, we will write $C_Z(x, x^{\prime}) = C_Z(h)$ 
For multidimensional problems, covariance functions are usually chosen as the product of $1D$ covariance functions:
\begin{equation}
  C_Z(h) = s^2\prod_{i=1}^{\dim \Kspace} C_i(h_i;l_i)
\end{equation}
These covariance functions introduce an additional parameter $l$ of dimension $\dim \Xspace$, and a variance parameter $s^2$.
$l$ called the \emph{length scale}, that measure the influence of each variable on its vicinity. If the length scales are all equals, the covariance kernel is said \emph{isotropic}. Otherwise, the kernel is \emph{anisotropic}.

A few common stationary $1D$-covariance functions are introduced~\cref{tab:common_cov_fc}.

  \begin{table}[ht]
    \centering
    \begin{tabular}{lrc}
      \toprule
      Name & $C(h;l)$ & Regularity of sample paths\\ \midrule
      Gaussian & $\exp\left(- \frac{h^2}{2 l^2}\right)$ & $C^{\infty}$\\
      Exponential &$\exp\left(- \frac{\lvert h \rvert}{l}\right)$ & $C^0$  \\
      Matérn 3/2 & $\left(1 + \sqrt{3}\frac{h}{l}\right)\exp\left(-\sqrt{3}\frac{h}{l}\right)$ & $C^1$\\
      Matérn 5/2 & $\left(1+ \sqrt{5}\frac{h}{l} + \frac{5}{3}\frac{h^2}{l^2}\right) \exp\left(-\sqrt{5}\frac{h}{l}\right)$ & $C^2$\\ \bottomrule
    \end{tabular}
    \caption{\label{tab:common_cov_fc} Common covariance functions}
  \end{table}



  One main difference that motivates one or the other covariance function is the assumption upon the regularity of the sample paths. For example, if the unknown function $f$ is assumed to be infinitely differentiable, a Gaussian kernel is suited for the modelling. One common choice is the Matérn kernel of order $5/2$, so that the samples paths are twice-differentiable.

  
\begin{figure}[ht]
  \centering
  \input{\imgpath covariance_functions.pgf}
  \caption{\label{fig:cov_fc_examples} Common covariance functions for GP regression. The right plots show sample paths for those different covariance functions with same length scale.}
\end{figure}

Those $(\dim \Xspace + 1)$ hyperparameters have to be estimated based on the training set $\mathcal{X}$. This is usually done by MLE~(see for instance~\cite{ribaud_robustness_2019}, or by cross-validation (see \cite{ginsbourger_note_2009})
\section{Stepwise Enrichment strategies for Gaussian Processes}
\label{sec:enrichment_strategies}
For a unknown function $f$, a GP is initally constructed based on a design $\mathcal{X} = \left\{\left(x_1,f(x_1)\right), \dots, \left(x_n, f(x_n)\right)\right\}$, that consists of $n$ points of $\Xspace$, and their corresponding evaluations. This GP is denoted $Y \mid \mathcal{X}$ and:
\begin{equation}
  \label{eq:YgivenXGP}
  Y\mid \mathcal{X} \sim \mathcal{N}(m_{Y\mid\mathcal{X}}(x),\sigma^2_{Y\mid\mathcal{X}}(x))
\end{equation}
The main idea behind Stepwise Uncertainty Reduction is to define a criterion, say $\kappa_n$, that measures in a way the uncertainty upon a certain objective associated with the GP and $f$, and to maximize this criterion, in order to select the next point:
\begin{equation}
  x_{n+1} = \argmax_{x\in\Xspace} \kappa_n(x) = \argmax_{x\in\Xspace} \kappa(x; Y \mid \mathcal{X})
\end{equation}
This new point is then evaluated by $f$, and the pair is added to the design: $\mathcal{X} \gets \mathcal{X} \cup \{(x_{n+1}, f(x_{n+1}))\}$.
\subsection{Exploration based criteria}
\label{sec:exploration_criteria}
We are going to introduce common criteria of enrichment, that aim at exploring the input space $\Xspace$

\subsubsection{Maximum of variance}
A measure of uncertainty on the GP is $\max_{x\in\Xspace} \sigma_{Y \mid \mathcal{X}}^2(x)$, the maximum value of the prediction variance on the space.
A simple criterion is to select and evaluate the point corresponding to this maximum of variance:
\begin{equation}
  x_{n+1} = \argmax_{x\in \Xspace} \kappa_n(x) = \argmax_{x \in \Xspace} \sigma^2_{Y \mid \mathcal{X}}(x)
\end{equation}
This criterion by its simplicity is easy to implement, as the prediction variance is cheap to compute given a GP, and does not depend directly on the evaluations of the function $f(x_i)$, uniquely on the distance between the inputs points and the covariance parameters.

\subsubsection{Integrated Mean Square Error}
\cite{sacks_designs_1989}
The prediction variance is directly given by $\sigma^2_{Y\mid \mathcal{X}}$ and represents the uncertainty on the Gaussian regression. To summarize this uncertainty on the whole space $\mathcal{X}$, we define the Integrated Mean Square Error (IMSE) as
\begin{equation}
  \IMSE(Y \mid \mathcal{X}) = \int_{\Xspace} \sigma_{Y\mid\mathcal{X}}^2(x)\,\mathrm{d}x
\end{equation}
For practical reasons, we can consider to integrate the MSE only on a subset $\mathfrak{X}\subset \mathcal{X}$ that yields
\begin{equation}
  \IMSE_{\mathfrak{X}}\left(Y \mid \mathcal{X} \right) = \int_{\mathcal{X}} \sigma^2_{Y\mid \mathcal{X}}(x)  \mathbbm{1}_{\mathfrak{X}}(x)\,\mathrm{d}x = \int_{\mathfrak{X}} \sigma^2_{Y\mid \mathcal{X}}(x)\,\mathrm{d}x
\end{equation}

Unfortunately, exact evaluation of this integral is impossible, so it needs to be approximated using numerical integration, such as Monte-carlo or quadrature rules:
\begin{equation}
  \IMSE_{\mathfrak{X}}(Y\mid \mathcal{X}) \approx \sum_{i=1}^{n_{\mathrm{quad}}} w_i \sigma_{Y\mid\mathcal{X}}(x_i)
\end{equation}

For a given $x\in \Xspace$ and an outcome $y\in\Yspace$, the augmented design is defined as the experimental design, $\mathcal{X} \cup \left\{(x, y)\right\}$, and the IMSE of the augmented design is $\IMSE\left(Y \mid \mathcal{X} \cup \left\{(x, y)\right\} \right)$.
Before the actual experiment though, $y$ is unknown, but we can model it by its distribution given by the GP (per~\cref{eq:YgivenXGP}). So for a given candidate $x$, the mean prediction error we will get when evaluating $x$ is given by
\begin{equation}
  \label{eq:IMSE_augmented}
  \Ex_{Y(x)}\Big[\IMSE\big(Y \mid \mathcal{X} \cup \left\{(x, Y(x))\right\} \big)\Big]
\end{equation}
where the expectation is to be taken with respect to different realisations of $Y(x)$. As each scenario requires to fit a GP, and to compute the IMSE, a precise evaluation is quite expensive. A strategy ffound for instance in~\cite{villemonteix_informational_2006} is to take $M$ possible outcomes for $Y(x)$, corresponding to evenly spaced quantiles of the its distribution.
It is maybe important to note that the hyperparameters of the GP should not be reevaluated when augmenting the design, in order to get comparable values for the IMSE.


To enrich the design with the best point, that reduces the most the prediction error, a simple $1$-step strategy is to minimize the expectation of~\cref{eq:IMSE_augmented}.
\begin{equation}
  x_{n+1} = \argmin_{x\in \Xspace}\Ex_{Y(x)}\Big[\IMSE\big(Y \mid \mathcal{X} \cup \left\{(x, Y(x))\right\} \big)\Big]
\end{equation}

\subsection{Optimization oriented criteria}
\label{sec:GP_optimization_criteria}
The criteria we detailed above aim at reducing the epistemic uncertainty modelled through the Gaussian Process. In other words, we try to improve our knowledge on the unknown function globally. We are now going to evoke a few criteria which are driven by the global optimization of the function.

Those methods usually aim at striking a balance between exploration and \emph{intensification}. We covered exclusive exploration in~\cref{sec:exploration_criteria}.
Let $f$ be the unknown function, and $Y$ be a GP constructed based on an initial design $\mathcal{X} = \{(x_i, f(x_i))\}$.
\subsubsection{Probability of improvement}
We are first going to introduce the probability of improvement $\mathrm{PI}$, which is the probability that the GP is smaller than a threshold $f_{\min}$. Due to the Gaussian nature of $Y(x)$, this probability can be written in closed form using $\Phi = F_{\mathcal{N}(0, 1)}$ the cdf of the standard gaussian.
\begin{align}
  \mathrm{PI}(x) &= \Prob% _{Y(x)}
                   \left[Y(x) < f_{\min}\right] \\
                 &= \Phi\left(\frac{m_Y(x) - f_{\min}}{\sigma_Y(x)}\right)
\end{align}
This threshold can have different forms
\begin{itemize}
\item $f_{\min} = \min_{i} f(x_i)$, so the GP is to be compared with the current minimal value reached by the function
\item $f_{\min} = \min_i f(x_i) + \epsilon$ so we introduce a small tolerance $\epsilon$, in order to encourage exploration instead of intensification.
\end{itemize}
\subsubsection{Expected improvement and EGO}
\cite{mockus_bayesian_1974}\cite{jones_efficient_1998}
Quite related to the probability of improvement, we define the improvement $I(x)$ as the random variable defined as
\begin{equation}
  \label{eq:def_improvement}
  I(x) = {\left[f_{\min} - Y(x)\right]}_+
\end{equation}
where $[y]_+ = \max(y, 0)$.
The \emph{Expected Improvement} $\mathrm{EI}$ is 
\begin{align}
  \label{eq:def_ei}
  \mathrm{EI}(x) = \Ex[I(x)]  = \Ex\left[\left[f_{\min} - Y(x) \right]_+\right]
\end{align}
Again, a closed form is available to compute the expected improvement, that does not require the evaluation of the expectation \cref{eq:def_ei}:
\begin{equation}
  \mathrm{EI}(x) = \left(f_{\min} - m_Y(x)\right) \Phi\left(\frac{f_{\min} - m_Y(x)}{\sigma_Y(x)}\right) + \sigma_Y(x) \phi\left(\frac{f_{\min} - m_Y(x)}{\sigma_Y(x)}\right)
\end{equation}

\subsubsection{IAGO}
\label{ssec:IAGO} Another criterion worth mentioning is a criterion based on the distribution of the minimizers \cite{villemonteix_informational_2006}:
Let $y_i$ be a sample path of $Y$, and let $x_i^*$ the global minimizer of $y_i$.
We denote then $X^*$ the random variable corresponding to the global minimizer of $Y$.
We consider the differential entropy of $X^*$ given the augmented design $\mathcal{X} \cup \left\{(x,Y(x))\right\}$
\begin{equation}
  \kappa_{\mathrm{IAGO}}(x \mid \mathcal{X})=\Ex_{Y(x)}\Big[H\left[X^* \mid \mathcal{X} \cup \left\{(x, Y(x))\right\} \big)\right]\Big]
\end{equation}
 So at each step, we choose the point that gives the smallest uncertainty on the location of the global minimizers of the sample paths.

\subsection{Contour and volume estimation}
Let us start by introducing diverse tools based around Vorob'ev expectation of closed sets (\cite{el_amri_analyse_2019},~\cite{heinrich_level_2012}). 


Let us consider $A$, a random closed set, such that its realizations are subsets of $\Xspace$, and $\pi_A$ is its coverage probability, that is
\begin{equation}
  \pi_A(x) = \Prob\left[x\in A\right], x\in\Xspace
\end{equation}
For a given $x\in\Xspace$, the event ``$x$ belongs to $A$'' happens with probability $\pi_A(x)$, thus has variance $\pi_A(x)(1 - \pi_A(x))$.

For $\eta \in [0, 1]$, we define the $\eta$-level set of $\pi_A$, also called \emph{Vorob'ev quantiles}(see~\cite{vorobyev_new_2003})
\begin{equation}
  Q_{\eta} = \{x\in\Xspace \mid \pi_A(x) \geq \eta \}
\end{equation}
Those sets are decreasing (with respect to the inclusion) when $\eta$ increases:
\begin{equation}
  0\leq \eta \leq \xi \leq 1 \implies Q_{\xi} \subseteq Q_{\eta}
\end{equation}



\begin{definition}[Vorob'ev expectation of random closed sets, Vorob'ev deviation]
  Let $A$ a random closed set of $\Xspace$, and $\mu$ a measure on $\Xspace$. We define the Vorob'ev expectation, as the $\eta^*$-level set of $A$ that verifies
  \begin{equation}
    \Ex[\mu(A)] = \mu(Q_{\eta^*})
  \end{equation}
  that is the level set of $p$, that has the volume of the mean of the volume of the random set $A$.
  If this equation does not have solutions, $\eta^*$ is chosen as
\begin{equation}
  \forall \beta < \eta^* \quad \mu(Q_{\beta}) \leq \Ex[\mu(A)] \leq \mu(Q_{\eta^*})
\end{equation}
Furthermore, as we have defined a kind of expectation of a random set, we can define a deviation as
\begin{equation}
  \Ex[\mu\left(Q_{\eta^*} \triangle A \right)]
\end{equation}
with $E\triangle F = \left(E \setminus F\right) \cup \left(F \setminus E\right)$ being the symmetric difference of the sets $E$ and $F$
\end{definition}

\begin{equation}
  \kappa(x \mid \mathcal{X}) = \Ex_{Y(x)}\left[\Ex\left[\mu\left(Q_{\eta^*_{n+1}}  \triangle A\right)\right]\right]
\end{equation}
where $Q_{\eta^*_{n+1}}$ is the Vorob'ev expectation of the random set $A$, constructed using the GP $Y$ with the augmented design $\mathcal{X}\cup\{(x, Y(x))\}$.
\subsubsection{Margin of uncertainty}
\label{sec:margin_of_uncertainty}
Using the level sets, we can construct the $\eta$-margin of uncertainty, as introduced in~\cite{dubourg_reliability-based_2011}, that is the set of points $x \in \Xspace$ that we cannot classify in or out of $A$ with high enough probability.
Setting the classical level $\eta=0.05$ for instance, $Q_{1-\frac{\eta}{2}}=Q_{0.975}$ is the set of points whose probability of coverage is higher than $0.975$, while $Q_{\frac{\eta}{2}}=Q_{0.025}$ is the set of points whose probability of coverage is higher than $0.025$. Obviously, $Q_{1-\frac{\eta}{2}} \subset Q_{\frac{\eta}{2}}$. The complement of $Q_{\frac{\eta}{2}}$ in $\Xspace$, denoted by $Q_{\frac{\eta}{2}}^C$ is the set of points whose probability of coverage is lower than $0.025$. The $\eta$-margin of uncertainty $\mathbb{M}_{\eta}$ is defined as the sets of points whose coverage probability is between $0.025$ and $0.975$.
\begin{equation}
  \label{eq:margin_unc}
  \mathbb{M}_{\eta} = \left(Q_{1-\frac{\eta}{2}} \cup Q^C_{\frac{\eta}{2}} \right)^C = Q_{1-\frac{\eta}{2}}^C \cap Q_{\frac{\eta}{2}} = Q_{\frac{\eta}{2}} \setminus Q_{1-\frac{\eta}{2}}
\end{equation}


\subsection{Robust criteria and GP}
So far, we introduced before different criteria on a generic space $\Xspace$. In order to optimize in a robust way, we are going to consider the joint space $\Xspace = \Kspace \times \Uspace$:
We assume that we constructed a GP on $J$ on the joint space $\Kspace \times \Uspace$, based on a design of $n$ points $\mathcal{X} = \left\{(\kk^{(1)},\uu^{(1)}),\dots,(\kk^{(n)},\uu^{(n)}) \right\}$, denoted as $(\kk,\uu)\mapsto Y(\kk,\uu)$.

As a GP, $Y$ is described by its mean function $m_{Y}$ and its covariance function $C(\cdot, \cdot)$, while $\sigma^2_Y(\kk,\uu) = C\left((\kk,\uu), (\kk,\uu)\right)$
\begin{equation}
  Y(\kk,\uu) \sim \mathcal{N}\left(m_{Y}(\kk,\uu), \sigma^2_Y(\kk,\uu) \right)
\end{equation}

We are going to introduce some criteria that are useful in a robust optimization context. One of the main challenge of these criteria is the differentiation between $\kk$ and $\uu$, as we want to minimize with respect to $\kk$, but explore at the same time with respect to $\UU$.


\subsubsection{Expected loss}
Recalling the definition of $\estimtxt{\kk}{mean} = \argmin_{\kk \in \Kspace} \Ex_{\UU}\left[J(\kk, \UU)\right]$, we can look to minimize the expected 
In~\cite{janusevskis_simultaneous_2010}, the author define the \emph{projected process} $Z$, a stochastic process $\kk \rightarrow \left(\Omega \rightarrow \mathbb{R}\right)$ as
\begin{equation}
  Z(\kk) = \Ex_U[Y(\kk, \UU)] = \int_{\Uspace} Y(\kk, \uu) p_{\UU}(\uu)\,\mathrm{d}\uu
\end{equation}


\subsubsection{Profile expected improvement}
In the previous chapter, we introduce the conditional minimum $J^*(\UU)$ and the conditional minimizers $\kk^*(\UU)$. To follow on that idea, the function $\uu \mapsto \kk^*(\uu)$ can be explored: \cite{ginsbourger_bayesian_2014} introduces the \emph{Profile Expected Improvement} $\mathrm{PEI}$, defined as
\begin{equation}
  \label{eq:def_PEI}
  \mathrm{PEI}(\kk, \uu)= \Ex\left[[f_{\min} - Y(\kk, \uu)]_+\right] \text{ with } f_{\min} = \max(\min_i f(x_i), \min_{\uu \in \UU} m_Y(\kk, \uu))
\end{equation}

This writing allow us to see the similarity between the 

\todo{faire figure}

\subsection{GP of the penalized cost function}
\label{ssec:gp_delta_alpha}

We are now going to detail how Gaussian processes can help in recovering the regret-based families of robust estimators:
\begin{align}
  \{ \estimtxt{\kk}{add,\beta} &= \max_{\kk\in\Kspace} \Gamma_\beta(\kk) = \Prob_{\UU}\left[J(\kk, \UU) \leq J^*(\UU) + \beta \right] \mid \beta \geq 0 \} \\
\{ \estimtxt{\kk}{rel,\alpha} &= \max_{\kk\in\Kspace} \Gamma_\alpha(\kk) = \Prob_{\UU}\left[J(\kk, \UU) \leq \alpha J^*(\UU)\right] \mid \alpha \geq 1 \}
\end{align}
We assume that we constructed a GP on $J$ on the joint space $\Kspace \times \Uspace$, based on a design of $n$ points $\mathcal{X} = \left\{(\kk^{(1)},\uu^{(1)}),\dots,(\kk^{(n)},\uu^{(n)}) \right\}$, denoted as $(\kk,\uu)\mapsto Y(\kk,\uu)$.

As a GP, $Y$ is described by its mean function $m_{Y}$ and its covariance function $C(\cdot, \cdot)$, while $\sigma^2_Y(\kk,\uu) = C((\kk,\uu), (\kk,\uu))$
\begin{equation}
  Y(\kk,\uu) \sim \mathcal{N}\left(m_{Y}(\kk,\uu), \sigma^2_Y(\kk,\uu) \right)
\end{equation}
Let us consider now the conditional minimiser:
\begin{align}
  J^*(\uu) = J(\kk^*(\uu),\uu) = \min_{\kk\in\Kspace} J(\kk,\uu)
\end{align}

Analogous to $J$ and $J^*$, we define $Y^*$ as
\begin{equation}
  Y^*(\uu) \sim \mathcal{N}\left(m^*_Y(\uu), \sigma^{2,*}_Y(\uu)\right)
\end{equation}
where
\begin{align}
  m^*_Y(\uu) = \min_{\kk\in\Kspace} m_Y(\kk,\uu) = m_Y(\kk^*(\uu)) \\
  \sigma^{2,*}_Y(\uu) = \sigma^{2,*}_Y(\kk^*(\uu)) 
\end{align}
The surrogate conditional minimiser is used in~\cite{ginsbourger_bayesian_2014} for instance, but other choices could be considered, such as $m_Y(\kk^*(\uu)) - \beta \sigma^{2,*}_Y(\kk^*(\uu))$. This choice for instance would lead to be more ``optimistic'' in the estimation of the minimum (i.e.\ a lower minimum), and in turn, would have a tendency to overestimate the estimated value of $\alpha$.

The $\alpha$-relaxed difference defined as  $\Delta_{\alpha} = Y - \alpha Y^*$ is a linear combination of correlated Gaussian processes. Its distribution is Gaussian and can be derived by first considering the joint distribution of $Y(\kk,\uu)$ and $Y^*(\uu) = Y(\kk^*(\uu), \uu)$:
\begin{equation}
  \begin{bmatrix}
    Y(\kk,\uu) \\
    Y^*(\uu)
  \end{bmatrix}
  \sim \mathcal{N}\left(
    \begin{bmatrix}
      m_Y(\kk,\uu) \\
      m_Y^*(\uu)
    \end{bmatrix}
    ;\,
    \begin{bmatrix}
      C\left((\kk,\uu),(\kk,\uu)\right) & C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) \\
      C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) & C\left((\kk^*(\uu),\uu),(\kk^*(\uu),\uu)\right)
    \end{bmatrix}
\right)
\end{equation}
Multiplying by the matrix $\begin{bmatrix}1 & -\alpha \end{bmatrix}$ yields
\begin{align}
  \Delta_{\alpha}(\kk,\uu) &\sim \mathcal{N}\left(m_{\Delta}(\kk,\uu); \sigma^2_{\Delta}(\kk,\uu)\right)  \label{eq:delta_GP}\\
  m_{\Delta}(\kk,\uu) &= m_Y(\kk,\uu) - \alpha m_Y^*(\uu) \label{eq:mu_delta_GP}\\
  \sigma^2_{\Delta}(\kk,\uu) &= \sigma_Y^2(\kk,\uu) + \alpha^2 \sigma_{Y^*}^2(\kk,\uu) - 2\alpha C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) \label{eq:variance_delta_GP}
\end{align}


Decomposing the variance $\sigma^2_{\Delta}$ in~\cref{eq:variance_delta_GP}, 3 sources of uncertainty arise:
\begin{itemize}
\item $\sigma^2_{Y}$ is the prediction variance of the GP on $J$, that is directly reduced when additional points are evaluated
\item $\sigma^2_{Y^*}$ is the variance of the predicted value of the minimizer.
\item Assuming a stationary form of the covariance, the third term is directly dependent on the distance between $\kk$ and $\kk^*(\uu)$. As the covariance term can be written $C((\kk,\uu), (\kk',\uu')) = s \prod_{i\in\mathcal{I}_{\kk}}\rho_{\theta_i}(\|k_i - k'_i\|) \prod_{j\in\mathcal{I}_{\uu}} \rho_{\theta_j}(\|u_j - u'_j\|)$, substituting $\kk^*(\uu)$ for $\kk^\prime$ gives
\begin{align}
  C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) &= s \prod_{i\in\mathcal{I}_{\kk}}\rho_{\theta_i}(\|k_i - k^*_i(\uu)\|)\prod_{j\in\mathcal{I}_{\uu}} \rho_{\theta_j}(0) \\
  &=s \prod_{i\in\mathcal{I}_{\kk}}\rho_{\theta_i}(\|k_i - k^*_i(\uu)\|)
\end{align}
\end{itemize}
This decomposition highlights the fact that the uncertainty measured at a point $(\kk, \uu)$ using $\sigma_{\Delta}^2$ will not be reduced completely by evaluating the function at this point, as only the prediction variance $\sigma_Y^2$ will be significantly affected in general. In this case, reducing the uncertainty on a slice of constant $\kk$ (candidate) will not result necessarily in an evaluation located on this slice.


\subsection{Evaluation and optimization of $\Gamma$}
Let consider $\alpha\geq 1$ fixed. In order to compute $\estimtxt{\kk}{rel, \alpha}$, we need to estimate and optimize the function $\Gamma_{\alpha}$. For that purpose, we can first explore the space to improve the estimation of $\Gamma_{\alpha}$, and once sufficient knowledge is acquired, use the plug-in estimate $\hat{\Gamma}_{\alpha}$ for the optimization, to get the wanted estimator.
\subsubsection{Estimation of the probability of coverage}

For a given $\kk\in\Kspace$, the coverage probability of the $\alpha$-acceptable region, i.e.\ the probability for $\kk$ to be $\alpha$-acceptable is
\begin{align}
  \Gamma_{\alpha}(\kk) &= \Prob_{U}\left[J(\kk,\UU) \leq \alpha J^*(\UU)\right] \\
                              & =\Ex_{U}\left[\mathbbm{1}_{J(\kk,\UU) \leq \alpha J^*(\UU)}\right]
\end{align}
As $J$ is not known perfectly, it can be seen as a classification problem.
This classification problem can be approached with a plug-in approach in~\cref{eq:plugin_indicator}, or a probabilistic one in~\cref{eq:prob_indicator}:
\begin{align}
  \mathbbm{1}_{J(\kk,\uu) \leq \alpha J^*(\uu)} &\approx   \mathbbm{1}_{m_Y(\kk,\uu) \leq \alpha m_Y^*(\uu)} \label{eq:plugin_indicator} \\
  \mathbbm{1}_{J(\kk,\uu) \leq \alpha J^*(\uu)} &\approx   \ProbGP\left[ \Delta_{\alpha}(\kk,\uu) \leq 0 \right] = \pi_{\alpha}(\kk,\uu) \label{eq:prob_indicator}
\end{align}

Based on those two approximation, we can define two different estimations of $\Gamma_\alpha$, namely $\hat{\Gamma}_\alpha^{\mathrm{PI}}$ with the plug-in approach, and $\Gamma_{\alpha}^{\pi}$ for the probabilistic one.

\subsubsection{Plug-in approach}
For the plug-in approach, the chosen estimator is defined \cref{eq:def_gamma_PI}:
  \begin{equation}
    \label{eq:def_gamma_PI}
    \hat{\Gamma}_{\alpha}^{\mathrm{PI}}(\kk) = \Prob_U\left[m_Y(\kk,\uu) \leq \alpha m_Y^*(\uu) \right]
  \end{equation}
  The outer expectation operator is to be computed numerically, using quadrature rule, or Monte-carlo methods. In order to improve the accuracy of this estimator, one need to improve the GP prediction $m_Y$ of the cost function $J$.
  In this case, we propose to reduce the IMSE of the GP $Y - \alpha Y^*$. The choice of the IMSE (instead of choosing the point of maximal variance for instance) comes from the decomposition of the variance~\cref{eq:variance_delta_GP}. 
  
  \subsubsection{Probabilistic approach}
\begin{align}
  \hat{\Gamma}_{\alpha}^{\pi}(\kk) &= \Ex_U\left[ \ProbGP\left[ \Delta_{\alpha}(\kk,\UU) \leq 0\right]\right] = \Ex_U\left[ \ProbGP\left[ Y(\kk, \UU) - \alpha Y^*(\UU) \leq 0\right]\right] \\ &= \Ex_U\left[\pi_{\alpha}(\kk,\uu)\right]
\end{align}

  
The probability of coverage for the set $\{Y - \alpha Y^*\leq 0\}$ is $\pi_{\alpha}$, and can be computed using the CDF of the standard normal distribution $\Phi$, because $\Delta_{\alpha}$ is a GP, as defined~\cref{eq:delta_GP,eq:mu_delta_GP,eq:variance_delta_GP}
\begin{equation}
  \label{eq:def_pialphaku}
  \pi_{\alpha}(\kk,\uu) = \Phi\left(-\frac{m_{\Delta_\alpha}(\kk,\uu)}{\sigma_{\Delta_\alpha}(\kk,\uu)}\right)
\end{equation}
Finally, averaging the coverage probability over $\uu$ yields
\begin{equation}
  \hat{\Gamma}_{\alpha}^{\pi}(\kk) = \Ex_U\left[\pi_{\alpha}(\kk,\uu)\right]=\int_{\Uspace}\pi_{\alpha}(\kk,\uu)p_{\UU}(\uu) \,\mathrm{d}\uu = \int_{\Uspace}\Phi\left(-\frac{m_{\Delta_\alpha}(\kk,\uu)}{\sigma_{\Delta_\alpha}(\kk,\uu)}\right)p_{\UU}(\uu) \,\mathrm{d}\uu
\end{equation}


We can also consider the set $Y - \alpha Y^* \leq 0$ from a random set perspective:
For each $\kk\in\Kspace$, we can consider the random closed set $A_\kk =  \{\uu \in \Uspace \mid Y(\kk, \uu) - \alpha Y^*(\uu) \leq 0\}$.
In this case, we can set $\mu = \Prob_{\UU}$, so that $\mu(A_{\kk}) = \Prob_{\UU}[A_{\kk}]$.
For a given $\kk$, the probability of coverage of the set $A_{\kk}$
is
\begin{equation}
  \label{eq:prob_cov_randclosedset}
  \pi_{A_{\kk}}(\uu) = \frac{\pi_{\alpha}(\kk, \uu)}{\int_{\Uspace} \pi_{\alpha}(\kk, \uu)p_{\UU}(\uu)\,\mathrm{d}\uu}
\end{equation}
The normalization constant of \cref{eq:prob_cov_randclosedset} can be obtained quite easily numerically, as $\pi_{\alpha}$ can be computed in a closed form~\cref{eq:def_pialphaku}.

The Vorob'ev mean is defined as the $\eta^*$-level set of $\pi_{A_\kk}(\uu)$, or
\begin{equation}
  \Prob_{\UU}[Q_{\eta^*}(\kk)] = \Ex\left[\Prob_{\UU}\left[A_{\kk}\right]\right]
\end{equation}


\subsubsection{Sampling-based enrichment: margin of uncertainty}
The $\eta$-margin of uncertainty $\mathbb{M}_{\eta}$ defined~\cref{eq:margin_unc} as the sets of points whose coverage probability is between $0.025$ and $0.975$: $\mathbb{M}_{\eta} = Q_{\frac{\eta}{2}} \setminus Q_{1-\frac{\eta}{2}}$
The points that belong in this set are points whose probability of missclassification is larger than $\eta$.



\subsection{$\alpha$ through quantiles}
Instead of maximizing for each $\alpha$ the estimated $\hat{\Gamma}$, we are now going to derive an approach based on the quantiles:
Let $Y(\kk, \uu) \sim \mathcal{N}(m_Y(\kk,\uu), \sigma^2_{Y}(\kk, \uu))$ be the GP fitted using $\mathcal{X}$. A realisation $y\sim Y$ is then a function from $\Kspace \times \Uspace$ to $\mathbb{R}^{+}_*$.
Again, the plug-in approach is to compute the ratio $m_Y(\kk, \uu) / m^*_Y(\uu)$ on a large grid for $\uu$ and for each $\kk$, look for the quantile of order $p$ with respect to $\UU$
\begin{equation}
  \alpha_{m_{Y}}(\kk) = Q_{U}\left(p;~\frac{m_Y(\kk, \UU)}{m^*_Y(\UU)}\right)
\end{equation}
the estimation of the relaxation value $\hat{\alpha}_p$ is then the minimal value of the quantiles with respect to $\kk$:
\begin{equation}
    \label{eq:def_plugin_alpha}
  \hat{\alpha}^{\mathrm{PI}}_p = \min_{\kk \in \Kspace} Q_U\left(p;~\frac{m_Y(\kk, \UU)}{m^*_Y(\UU)}\right)  \tag{plug-in}
\end{equation}
Moreover, as we can sample quite easily from the GP, we can have an idea of the uncertainty in the estimation of $\hat{\alpha}_p$.
Let us say that we sampled $N$ function from $Y$, namely $y^{(i)}$ for $1 \leq i \leq N$. For each of these samples, we can get $\alpha_{y^{(i)}}(\kk)$, shortened as $\alpha^{(i)}(\kk)$. Using Monte-Carlo, we can approximate the usual moments for $\alpha$.

\begin{equation}
 \Ex_{Y}\left[\alpha_{Y}(\kk) \right] \approx \frac{1}{N} \sum_{i=1}^N \alpha^{(i)}(\kk) = \alpha^{\mathrm{MC}}(\kk)
\end{equation}

and finally,
\begin{equation}
  \hat{\alpha}_p^{\mathrm{MC}} = \min_{\kk \in \Kspace} \alpha^{\mathrm{MC}}(\kk)
\end{equation}


\subsubsection{Iterative procedure}
The general
\cref{ssec:gp_delta_alpha}
\begin{itemize}
\item Find a measure of uncertainty that depends as a function of $\kk \times \Uspace$.
\item Find the point that reduces the most the uncertainty on this slice
\item Update
\end{itemize}

At the step $n$:
\begin{itemize}
\item First, using the GP, $\alpha_p$ is estimated using Monte-carlo and samples from the GP, giving $\hat{\alpha}_{n,.99}$
\item We choose the candidate $\kk_{\mathrm{candidate}}$ as the minimizer of the sampled quantiles: $\kk_{\mathrm{candidate}} = \argmin_{\kk} \alpha^{\mathrm{MC}}(\kk)$.
\item We optimize the wIMSE, defined as $\IMSE(Y_n \mid \mathcal{X}, \{\kk_{\mathrm{candidate}}\}\times \Uspace )$ to get the next point to sample
\end{itemize}
On Figure~\cref{fig:alpha_MC} is shown the procedure applied on BHs, based on a initial design of $30$ points. The wIMSE is computed by sampling a $50$-point LHS on $\{\kk_{\mathrm{candidate}} \times \Uspace\}$.
The estimation using Monte-carlo seems to show convergence towards the true value, while the plug-in approach does not evolve much with the iterations
\begin{figure}[!h]
  \centering
  \input{/home/victor/acadwriting/Misc/alpha_estimation_MC.pgf}
  \label{fig:alpha_MC}
  \caption{$\hat{\alpha}_{n,.99}$ estimated by MC }
\end{figure}

% \begin{figure}[!h]
%   \centering
%   \input{alpha_estimation_plugin.pgf}
% \end{figure}

% \subsubsection{Weighted IMSE}
% The IMSE presented above is not objective driven, as the IMSE (integrated implicitly on the whole space $\Xspace$) aims solely at improving the prediction (though that can be the objective in itself).

% To include a more precise objective than the enrichment of the design, one can add a weight function to the integral, giving the weighted IMSE.
% \begin{equation}
%   \label{eq:w-imse}
%   w\IMSE(Y\mid \mathcal{X}) = \int_{\Xspace} \sigma_{Y\mid\mathcal{X}}^2(x)w(x)\,\mathrm{d}x
% \end{equation}



\subsection{Sources, quantification of uncertainties, and SUR strategy?}
Formally, for a given point $(\kk,\uu)$, the event ``the point is $\alpha$-acceptable'' has probability $\pi_{\alpha}(\kk,\uu)$ and variance $\pi_{\alpha}(\kk,\uu) (1-\pi_{\alpha}(\kk,\uu))$. Obviously, the points with the highest uncertainty have the highest variance, so have a coverage probability around $0.5$.


Recalling the objective, it gives upper bounds and lower bounds of the confidence interval of level $\eta$ on the probability for each $\kk$:
\begin{align}
  \hat{\Gamma}_{\alpha}^{UB}(\kk) &= \Prob_U\left[\theta=(\kk,\uu) \in Q_{1-\frac{\eta}{2}}\right] \\
  \hat{\Gamma}_{\alpha}^{LB}(\kk) &= \Prob_U\left[\theta=(\kk,\uu) \in Q_{\frac{\eta}{2}}\right]
\end{align}



\subsubsection{UB-LB for $(p, \alpha_p, \kk_p)$}
Let us assume that we have set a probability $p\in [0,1]$. Let us recall that the triplet $(p, \alpha_p, \kk_p)$ verifies
\begin{align}
  \max_{\kk} \Gamma_{\alpha_p}(\kk) = \Gamma_{\alpha_p}(\kk_p) = \Prob_{\uu}\left[J(\kk_p,\uu) \leq \alpha_p J^*(\uu)\mid \uu = \uu\right] = p
\end{align}
Let us say that $\bar{\Gamma}$ is the $\eta$-upper-bound, while $\underline{\Gamma}$ is the $\eta$-lower bounds, so
\begin{equation}
  \ProbGP\left[\underline{\Gamma}(\kk) \leq \Gamma_n(\kk) \leq \bar{\Gamma}(\kk)\right] = \eta
\end{equation}
\begin{itemize}
\item If $\underline{\Gamma}(\kk)>p$, we are too permissive, so we should decrease $\alpha$
  \begin{itemize}
  \item by how much ?
  \end{itemize}
\item If $\bar{\Gamma}(\kk)<p$, we are too conservative, so we should increase $\alpha$
  \begin{itemize}
  \item by how much again ?
  \end{itemize}
 \item If $\underline{\Gamma}(\kk)<p<\bar{\Gamma}(\kk)$, reduce uncertainty on $\kk_p$
\end{itemize}
Changing the value of $\alpha$ does not require any further evaluation of the objective function, so can be increased until $\max \hat{\Gamma} = p$ ? by dichotomy for instance. This $\hat{\kk}_p$ is then the candidate.

Criterion: stepwise reduction of the variance of the estimation of $\hat{\Gamma}(\hat{\kk}_p) = \max_{\kk}\hat{\Gamma}(\hat{\kk})$

For a fixed $p\in (0, 1]$, and an initial design $\mathcal{X}$. Set an initial value for $\alpha \geq 1$. 
\begin{itemize}
\item Define $\Delta_{\alpha}$, using $Y \mid \mathcal{X}$
\item Update $\alpha$ such that $\max \hat{\Gamma}_{\alpha,n} = p$
\item Compute measure of uncertainty that we want to reduce:
  \begin{itemize}
  \item $\bar{\Gamma}_{\alpha,n}(\kk) - \underline{\Gamma}_{\alpha,n}(\kk)$
  \item $\pi_{\alpha}(\kk,\uu)(1-\pi_{\alpha}(\kk,\uu))$
  \end{itemize}
\end{itemize}

\subsubsection{Sampling based criterion}
\label{sec:sampling_based_criterion}
This technique is described in~\cite{dubourg_reliability-based_2011}
Let assume that we derived a criterion $\kappa$. And let $f(x) = \frac{\kappa(x)}{\int_{\Xspace}\kappa(u)\,\mathrm{d}u}$. $f$ can be seen as a density.
  Using an appropriate sampler, we can generate $N$ iid samples from this criterion $\{x_i\}_{1\leq i \leq N}$
  
  However, as $N$ should be large, there is no point in evaluating all the samples $x_i$. This goes by the statistical reduction of the samples:
  This can be done by KMeans algorithm, 

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=\textwidth]{/home/victor/acadwriting/sampling_estimation_Meta.pdf}
%   \caption{\label{fig:label} }
% \end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subfileLocal{
	\pagestyle{empty}
	\bibliographystyle{alpha}
	\bibliography{/home/victor/acadwriting/bibzotero}
}
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Main_ManuscritThese"
%%% End:
