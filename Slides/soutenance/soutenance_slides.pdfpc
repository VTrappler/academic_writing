[duration]
45
[last_minutes]
5
[notes]
### 1
Thank you for the introduction, and thank you to the jury for being here today. \par My name is Victor Trappler, and I'm going to defend my PhD, which is named ``Parameter control in the presence of uncertainties'' \par This work has been realized in the AIRSEA team, here in Grenoble, under the supervision of Ã‰lise Arnaud, Arthur Vidard and Laurent Debreu
### 2
Reality is often very complex to understand in its entirety, but at the same time, the ability to understand, and to predict our surroundings allow us to take decisions which have environmental, societal or economic impact. \par However, due to this complexity, a lot of simplifications are required in order to be able to represent mathematically natural phenomena. Those simplified versions can then be studied more precisely, and even implemented numerically, to create simulations, and provide prediction and forecasts
### 3
We are going to take for instance the modelling of the ocean. \par First thing first, most numerical models are usually based on a subdivision of the ocean into cells, which interacts with each others. Those cells have a size in the order of the kilometer. \par \par At the bottom of the ocean, the different types of soil and sediments have an influence on the circulation of the water at the surface. Indeed, the ocean bed is not completely smooth, there are rocks, sand, gravel, and all those asperities create turbulences, which dissipate energy. This phenomenon happens at a scale much smaller than the scale of the mesh provided for the simulation. \par \par All in all, instead of modelling individually each rock or asperity, we can introduce a parameter, which will quantify for each cell the effect of the friction. 
### 3
We are going to take for instance the modelling of the ocean. \par First thing first, most numerical models are usually based on a subdivision of the ocean into cells, which interacts with each others. Those cells have a size in the order of the kilometer. \par \par At the bottom of the ocean, the different types of soil and sediments have an influence on the circulation of the water at the surface. Indeed, the ocean bed is not completely smooth, there are rocks, sand, gravel, and all those asperities create turbulences, which dissipate energy. This phenomenon happens at a scale much smaller than the scale of the mesh provided for the simulation. \par \par All in all, instead of modelling individually each rock or asperity, we can introduce a parameter, which will quantify for each cell the effect of the friction. 
### 4
 To summarize, in the end, we have a model, with a lot of parameters, which is supposed to represent the reality. \par Some of those parameters cannot be chosen, as their value may come from real observations, measured physical properties, or other environmental conditions. \par Now, the question, is how to choose the value of the remaining parameters, so that the model, the simulations are accurate enough ? \par 
### 5
We are now going to translate these questions using a classical setting of inverse problem. \par The parameter we want to calibrate, to control is called the control parameter theta The other parameters, that we do not have much control are called the environmental variables, which are considered fixed, and known. \par our numerical model, M can be seen as an operator, taking as inputs theta and u, and outputs an observable quantity, for instance the sea water height. This is the direct simulation. \par Our goal, in an inverse problem setting, is, given some observations y, what is the best value of theta, so that the output of the numerical model matches as closely as possible the observations ? 
### 6
 In practice, we select a value of the environmental parameters u, and using the least square approach, we define J, an objective function, as the sum of the squares of the difference between the observations and the output of the numerical model. \par \par This is a deterministic optimisation problem, that we can solve using classical methods such as adjoint gradient. The resulting parameter theta hat is then optimal in this situation. \par However, this estimation depends on the value of the environmental parameter chosen initially. Now, what happens if this parameter changes ? or if the value of the environmental parameter is not good with respect to the observations \par The minimisation procedure is supposed to correct the error on theta but will it try to compensate too much ? 
### 6
 In practice, we select a value of the environmental parameters u, and using the least square approach, we define J, an objective function, as the sum of the squares of the difference between the observations and the output of the numerical model. \par \par This is a deterministic optimisation problem, that we can solve using classical methods such as adjoint gradient. The resulting parameter theta hat is then optimal in this situation. \par However, this estimation depends on the value of the environmental parameter chosen initially. Now, what happens if this parameter changes ? or if the value of the environmental parameter is not good with respect to the observations \par The minimisation procedure is supposed to correct the error on theta but will it try to compensate too much ? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 7
 We are going to illustrate this issue, with the calibration problem of the bottom friction in a model of the north atlantic ocean. This setting will be detailed later, but we can already see that the optimal value found does indeed depend on u. \par Some regions are left unaffected, but some other vary directly with the environmental parameter. \par So now the question that arise, is that how to select a value of the control parameter, such that it is quite close to the reality, even when u changes? 
### 8
This leads to the notion of robustness, so an estimate will be considered robust if the objective function gives good performances when u varies 
### 11
 We evoked earlier that the uncertainty on theta or on u is not the same, and we can make a rough distinction between two types: \par - First, the epistemic uncertainties that result from a lack of knowledge, but can be reduced. An example is the uncertainty during the estimation of the mean value. The more samples you take, the less uncertainty there is on your estimation \par - Secondly, there is the aleatoric uncertainty, that comes from the inherent variability of the system studied. Think of the different values that a random variable takes. \par Our goal, is then to be able to reduce the epistemic uncertainty on the value of theta, while taking into account the aleatoric uncertainty.
### 12
The aleatoric uncertainty is modelled by assuming that u is a random variable with known distribution 
### 12
The aleatoric uncertainty is modelled by assuming that u is a random variable with known distribution 
### 13
 In our study, the model is completely deterministic, so we can control its inputs The cost function, becomes then a function of two theta and u. We still wish to minimise with respect to theta, but what can we do for u ? 
### 14
 I'm going to present very quickly some estimates that can be considered robust, but will focus mainly on the last one. First we can think about minimising in the worst case sense. This usually leads to overly conservative estimates as we are maximizing over the whole space U. We can also think about minimising the moments, such as the mean or the variance, or even combine them in a multiobjective setting by looking for the pareto front. \par \par Every choice of environmental variable gives a distinct situation The aspect we are going to focus on is based on the regret, so it implies a comparison with the best performance attainable for each u. 
### 15
 
### 15
 
### 16
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 16
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 16
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 16
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 17
Great so now we have Gamma(theta) which is the probability that theta gives a cost alpha acceptable If we have an idea of a threshold we don't want to exceed, so if alpha is known we can maximize the probability of being alpha acceptable \par Or, on the other hand, as gamma is a probability, we can look for the smallest relaxation, where the probability of acceptability reaches a certain confidence 1-eta. \par We can then define the family of relative-regret estimators, which are the maximizers of such a probability of being alpha acceptable. Depending on the approach, we can nudge toward optimal performances with small alpha, or risk adverse preference, by setting a bigger relaxation.
### 21
We now have a family of estimators, but in practice, finding them is expensive, as it requires probability estimation, and various optimisations, that is why we are going to use surrogates to tackle this problem
### 22
We choose Gaussian Process regression as a metamodel, as it can replace cheaply the computer code
### 22
We choose Gaussian Process regression as a metamodel, as it can replace cheaply the computer code
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 27
One of the first problem we encountered is the computation of the conditional minimum and minimisers. Using Gaussian processes, and an enrichment criterion called the Profile Expected Improvement, we can reconstruct the Jstar and theta star function iteratively, and then use the kriging prediction as a surrogate since Jstar is well approximated
### 6
What about the estimation of the probability of being alpha acceptable ? As it is a linear combination of GP, we still have a Gaussian distribution at the end, and we can rewrite the probability of being acceptable as a probability of failure of the resulting GP, that we could consider to estimate using common techniques, level set, contour estimation. \par One interesting thing is in the decomposition of the variance of Delta alpha: two main sources of uncertainties are present: the uncertainty on the true value of the function at theta,u; and the true value of the minimizer.
