\documentclass[10pt,aspectratio=169]{beamer}
\usetheme{metropolis}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{booktabs}

%% Fig mgmt ---------------------------------------
\graphicspath{{../Figures/}}
\usepackage{adjustbox}
\usetikzlibrary{positioning}
\usepackage{subfig}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\newcommand\manupath{/home/victor/acadwriting/Manuscrit/Text/}


%% Bib mgmt ---------------------------------------
\usepackage[authoryear,square]{natbib}
% \setcitestyle{square,aysep={},yysep={;}}

%% New commands --------------------------------------- 
\newcommand{\Uspace}{\mathbb{U}}
\newcommand{\Kspace}{\Theta}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\GP}{\mathsf{GP}}
\newcommand{\Prob}{\mathbb{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Cov}{\textsf{Cov}}
\newcommand{\tra}{\mathrm{tr}}
\newcommand{\yobs}{\bm{y}^{\mathrm{obs}}}
\newcommand{\kest}{\hat{\bm{k}}}
\newcommand{\kk}{\theta}
\newcommand{\uu}{u}
\newcommand{\UU}{U}
\DeclareMathOperator*{\KL}{\textsf{KL}}

%% Pdfpc notes ---------------------------------------
\usepackage[duration=45, lastminutes=5]{pdfpcnotes}



\definecolor{blkcol}{HTML}{E1E1EA}
\definecolor{blkcol2}{RGB}{209, 224, 224}

\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}
\definecolor{halfgray}{gray}{0.55}
\definecolor{webgreen}{rgb}{0,.5,0}
\definecolor{webbrown}{rgb}{.6,0,0}
\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\definecolor{Black}{cmyk}{0, 0, 0, 0}
\definecolor{blueGreen}{HTML}{4CA6A7}
\definecolor{blueGreenN2}{HTML}{3964B4}
% Using colorbrewer, 1 is lightest colour, 4 is darkest
\definecolor{brewsuperlight}{HTML}{F0F9E8}
\definecolor{brewlight}{HTML}{BAE4BC}
\definecolor{brewdark}{HTML}{7BCCC4}
\definecolor{brewsuperdark}{HTML}{2B8CBE}


%%% VICTOR COLORS ----------------------------------------
\colorlet{cfgHeaderBoxColor}{brewsuperdark} %{green!50!blue!70}
\colorlet{cfgCiteColor}{RoyalBlue} % vLionel : RoyalBlue
\colorlet{cfgUrlColor}{RoyalBlue}  % vLionel : RoyalBlue
\colorlet{cfgLinkColor}{RoyalBlue} % vLionel : blueGreenN2

\setbeamercolor{block title}{bg = blkcol}
\setbeamercolor{block body}{bg = blkcol!50}
\setbeamerfont{author}{size=\footnotesize}

\usepackage{appendixnumberbeamer}
\metroset{progressbar=foot,
  numbering=fraction}




\title{Parameter Control in the presence of uncertainties}
\author{{\large \textbf{Victor Trappler}} \hfill \texttt{victor.trappler@univ-grenoble-alpes.fr} \\
  É. Arnaud, L. Debreu, A. Vidard \\
  AIRSEA Research team (Inria)\hfill \texttt{team.inria.fr/airsea/en/}\\
  Laboratoire Jean Kuntzmann}


\institute{\begin{center}
\includegraphics[scale=0.20]{INRIA_SCIENTIFIQUE_UK_CMJN}
\includegraphics[scale=0.20]{ljk}
\end{center}}

\date{\textbf{PhD Defense, Grenoble, 2021}
  % {\bf } \today
}
\setcounter{tocdepth}{1}

\begin{document}
\begin{frame}
  \pnote{Hello everyone, my name is Victor Trappler, I'm a PhD student
    in the AIRSEA team in Grenoble, under the supervision of Élise
    Arnaud, Arthur Vidard and Laurent Debreu.

    I'm here to present some of the work entitled ``Parameter Control
    in the Presence of uncertainties'' }
  \maketitle
\end{frame}

\section{Introduction}
\begin{frame}{Uncertainties in the modelling}
  \begin{center}
    % \begin{adjustbox}{clip, trim=36cm 0cm 0cm 0cm, max
    %   width=\textwidth, center}
    \input{\manupath Chapter3/img/modelling_uncertainties.pgf}
    % \end{adjustbox}
  \end{center}

  Does reducing the error on the parameters leads to the compensation
  of the unaccounted natural variability of the physical processes ?

  
  \pnote{During the whole process of the modelling of a physical
    system, that is from the observation of a natural phenomenon, to
    the simulation using numerical methods, we introduce
    uncertainties.  Those uncertainties take the form of errors
    introduced by the simplifications, discretizations and
    parametrizations needed to represent things numerically.

    In the end, we have a set of parameters, that we need to
    calibrate, but during this phase of calibration, how can we be
    sure that we try to act only the error due to the parametrization,
    and are not compensating errors coming from others sources.}
\end{frame}
\begin{frame}{Outline}
  \tableofcontents
  \pnote{We are first going to see the general setting of calibration
    problems, and then how to define robustness in this
    context. Finally, we are going to see how to tackle this problem
    in practice using surrogate models }
\end{frame}


\section{Calibration problem}
\subsection{Formulation as an optimisation problem}
\begin{frame}[t]{Computer code and inverse problem}
  \begin{itemize}
  \item[Input]
    \begin{itemize}
    \item $\kk$: Control
      parameter%Bottom friction (spatially distributed)
    \item $\uu$: Environmental variables (fixed and known)
    \end{itemize}
  \item[Output] \begin{itemize}
    \item $\mathcal{M}(\kk,\uu)$: Quantity to be compared to
      observations%Sea surface height, at predetermined time of the simulation and at certain location
    \end{itemize}
  \end{itemize}
  \vfill
  % \only<1>{\input{../Figures/comp_code.pgf}}
  \only<1>{
    \begin{center}
      \input{../Figures/inv_prob.pgf}
    \end{center}
  } \pnote{In quite a classical setting, we assume that we have a
    model M, that takes two inputs: Θ the control variable, that we
    aim at calibrating, and u some environmental variables, that we
    consider fixed and knowns.  We wish to calibrate the model wrt to
    some observations yobs }
\end{frame}

\begin{frame}{Data assimilation framework}
  % We have
  % $\yobs = \mathcal{M}(\kk_{\mathrm{obs}},\uu_{\mathrm{obs}})$ with
  % $\uu_{\mathrm{obs}} = \uu$
  Let $\uu\in\Uspace$. The calibration problem can be formulated as
  the following optimisation problem
  \begin{align}
    \min J(\kk, \uu) &=  \frac12 \|\mathcal{M}(\kk,\uu) - \bm{y}^{\mathrm{obs}} \|^2\\
 \text{such that }& \kk \in \Kspace
  \end{align}

  \begin{enumerate}[$\rightarrow$]
  \item Deterministic optimization problem
  \item Possibly add regularization
  \item Classical methods: Adjoint gradient and Gradient-descent
  \end{enumerate}
  The solution is $\hat{\kk} = \argmin_{\kk \in \Kspace} J(\kk, \uu)$
  
  \begin{itemize}
  \item What if $\uu$ does not reflect accurately the observations?
  \item Does $\hat{\kk}$ compensate the errors brought by this random
    misspecification? ($\sim$overfitting)
    % \item How well will $\bm{\hat{k}}$ perform under other
    %   conditions?
  \end{itemize}
  \pnote{ In practice, we select a likely value of the environmental
    parameters u, and Using the least square approach, we define J, a
    cost function, as the sum of the squares of the difference between
    the observations and the output of the numerical model.


    This is a deterministic optimisation problem, that we can solve
    using classical methods such as adjoint gradient.

    But what if the u we chose is not quite the same as the one of the
    observations. The minimisation procedure is supposed to correct
    the error on theta but will it try to compensate too much ? }
\end{frame}

%%%%
\begin{frame}{Context}
  \begin{itemize}
  \item The friction $\kk$ of the ocean bed has an influence on the
    water circulation
  \item Depends on the type and/or characteristic length of the
    asperities
  \item Subgrid phenomenon
  \item $\uu$ parametrizes the BC
  \end{itemize}
  \begin{center}
    \scalebox{.9}{\input{../Figures/hauteureau.tikz}}
  \end{center}
  \pnote{ In order to have an idea of such parameters, the case study
    for us is the estimation of the bottom friction.  The bottom
    friction theta has an influence on the oceanic circulation, as it
    dissipates some energy by turbulences and it depends on the type
    of soil, and on the characteristic length of the
    asperities. Something that is hard to observe directly.
    % In oceans modelling, this is a subgrid phenomenon.
    The environmental variable u parametrizes the BC, for instance the
    relative amplitude of tidal components.  }
\end{frame}


\subsection{Random parameteric misspecifications}
\begin{frame}{Different types of uncertainties}
  \begin{block}{Epistemic or aleatoric
      uncertainties?~\cite{walker_defining_2003}}
    \begin{itemize}
    \item Epistemic uncertainties: From a lack of knowledge, that can
      be reduced with more research/exploration
    \item Aleatoric uncertainties: From the inherent variability of
      the system studied, operating conditions
    \end{itemize}
  \end{block}
  $\rightarrow$ But where to draw the line?

  Our goal is to take into account the aleatoric uncertainties in the
  estimation of our parameter.  \pnote{ We evoked earlier that the
    uncertainty on theta or on u is not the same, and we can make a
    rough distinction between two types:

    - First, the epistemic uncertainties that result from a lack of
    knowledge, but can be reduced. An example is the uncertainty
    during the estimation of the mean value. The more samples you
    take, the less uncertainty there is on your estimation

    - Secondly, there is the aleatoric uncertainty, that comes from
    the inherent variability of the system studied. Think of the
    different values that a random variable takes.

    Our goal, is then to be able to reduce the epistemic uncertainty
    on the value of theta, while taking into account the aleatoric
    uncertainty.  }
\end{frame}

\begin{frame}[t]{Aleatoric uncertainties}
  Instead of considering $\uu$ fixed, we consider that $\uu\sim \UU$  r.v.\ (with known pdf $\pi(\uu)$), and the output of the model depends on its realization. \\
  \vfill \only<1>{
    \begin{center}
      \input{../Figures/inv_prob.pgf}
    \end{center}
  } \only<2>{
    \begin{center}
      \input{../Figures/comp_code_unc_inv.pgf}
    \end{center}
  } \vfill


  \pnote{ As hinted before, we are going to model the aleatoric
    uncertainty on u by a random variable.  The output of the model
    becomes a random variable }
\end{frame}

\begin{frame}{The cost function as a random variable}
  \begin{itemize}
  \item The computer code is deterministic, and takes $\kk$ and $\uu$
    as input:
    \begin{equation*}
      \mathcal{M}(\kk,\alert{\uu})
    \end{equation*}
  \item The deterministic quadratic error is now
    \begin{equation*}
      J(\kk,\alert{\uu}) =  \frac12\|\mathcal{M}(\kk,\alert{\uu}) - \yobs\|^2
    \end{equation*}
  \end{itemize}

\begin{equation*}
  ''\hat{\kk} = \argmin_{\kk\in\Kspace} J(\kk,\alert{\uu})'' \text{ but what can we do about } \uu ?
\end{equation*}
\pnote{ In our study, the model is completely deterministic, so we can
  control its inputs The cost function, becomes then a function of two
  theta and u. We still wish to minimise with respect to theta, but
  what can we do for u ?  }
\end{frame}

\begin{frame}{Misspecification of $u$: twin experiment setup }
  
  Minimization performed on $\kk\mapsto J\left(\kk,\uu\right)$, for
  different $\uu$: \vfill \only<2>{Well-specified model} \only<3>{1\%
    error on the amplitude of the M2 tide} \only<4>{1\% error on the
    amplitude of the M2 tide}
  % \only<5>{0.5\% error on the amplitude of the M2 tide, starting at
  % the truth}

  \begin{center}
    \includegraphics<2>[width=\textwidth]{/home/victor/optimisation_dahu/optim_true/map_155.png}
    \includegraphics<3>[width=\textwidth]{/home/victor/optimisation_dahu/optim_1_001/map_150.png}
    \includegraphics<4>[width=\textwidth]{/home/victor/optimisation_dahu/optim_0_001/map_160.png}
    % \includegraphics<5>[width=\textwidth]{/home/victor/optimisation_dahu/optim_025_001_frtr/map_199.pdf}
  \end{center}
  \pnote{ To have a first look at the problem of misspecification, we
    try to calibrate the bottom friction on this domain in a twin
    experiment setup, with or without misspecification. This figures
    shows the truth value of the bottom friction.

    
    When optimizing without misspecification, we can see that the
    region of the english channel is able to retrieve the truth value,
    while the regions farther from the coast, in the bay of biscay,
    stay quite unaffected by the procedure. So far so good.

    When we introduce a 1\% error in the amplitude of the M2 tide, we
    can see more variations, especially in the english channel, as it
    compensate the misspecification of the tide.

    The estimation of theta is sensitive to the value of the
    environmental parameter. So the question that arise is ``how to
    get a value of theta, which shows robust properties when the
    environmental parameter varies ?''  }
\end{frame}

\begin{frame}{Robustness and estimation of parameters}
  \pnote{ So that is how we define robustness in this work: So
    basically, we have two main objectives: - First to find some
    criteria of robustness to estimate theta - Be able to compute
    those estimates quickly } {\textbf{Robustness}}: get good
  performances when the environmental parameter varies

  \begin{itemize}
  \item Define criteria of robustness, based on $J(\kk,\uu)$, that
    will depend on the final application
  \item Be able to compute them in a reasonable time
  \end{itemize}
\end{frame}


\section{Criteria of robustness}
\begin{frame}{Non-exhaustive list of ``Robust'' Objectives }
  \pnote{ I'm going to present very quickly some estimates that can be
    considered robust, but will focus mainly on the last one.  First
    we can think about minimising in the worst case sense. This
    usually leads to overly conservative estimates as we are
    maximizing over the whole space U. We can also think about
    minimising the moments, such as the mean or the variance, or even
    combine them in a multiobjective setting by looking for the pareto
    front.


    Every choice of environmental variable gives a distinct situation
    The aspect we are going to focus on is based on the regret, so it
    implies a comparison with the best performance attainable for each
    u.  }
  \begin{itemize}
    % \item Global Optimum: $ \min_{(\kk,\uu)} J(\uu,\kk)$
    %   $ \longrightarrow $ EGO
  \item Worst case~\cite{marzat_worst-case_2013}:
    $$ \min_{\kk \in \Kspace} \left\{\max_{\uu \in \Uspace}
      J(\kk,\uu)\right\}$$
  \item M-robustness~\cite{lehman_designing_2004}:
    $$\min_{\kk\in\Kspace} \Ex_{\UU}\left[J(\kk,\UU)\right]$$
  \item V-robustness~\cite{lehman_designing_2004}:
    $$\min_{\kk\in\Kspace} \Var_{\UU}\left[J(\kk,\UU)\right]$$
  \item Multiobjective~\cite{baudoui_optimisation_2012}:
    $$ \text{Pareto frontier}
  $$
  % \item Region of failure given by
  %   $J(\kk,\uu)>T$~\cite{bect_sequential_2012}:
  %   $$\max_{\kk \in \Kspace} R(\kk) = \max_{\kk\in \Kspace}
  %   \Prob_{\uu}\left[J(\kk,\uu) \leq T \right]$$
\item Best performance achievable given $\uu \sim \UU$
\end{itemize}
\end{frame}


\begin{frame}{``Most Probable Estimate'', and relaxation}%
  \pnote{ The main idea is that we want to consider individually all
    situations induced by the value of the environmental variable.
    Basically, once a value u is sampled, the problem is
    deterministic, so under some assumption, we have a minimiser theta
    star that is a function of u


    Keeping in mind the random nature of U, we can define the random
    variable thetastar, and its density (if it is defined), can be
    seen as the frequency of which a value theta is optimal.


    That is an interesting information, but we can have a little more
    than that. We may want to include theta that yield values of the
    cost function close to a minimum. To do that, we introduce a
    relaxation of the equality constraint with alpha, so that for a
    given u, we consider acceptable the theta that give values of the
    cost function between Jstar, the optimal value and alpha times
    Jstar So finally, we compute the probability that this given theta
    is acceptable with respect to the level alpha }

  Given $\uu \sim \UU$, the optimal value is $J^*(\uu)$, attained at
  $\kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk,\uu)$.  \pause
  % \begin{columns}
  %   \begin{column}{0.6\textwidth}

  
  The minimizer can be seen as a random variable:
  \begin{equation*}
    \kk^*(\UU) = \argmin_{\kk\in\Kspace} J(\kk,\alert<1>{\UU})
  \end{equation*}
  $\longrightarrow$ estimate its density (how often is the value $\kk$
  a minimizer)
  \begin{align*}
    p_{\kk^*}(\kk) &= "\Prob_{\UU}\left[J(\kk,\UU)= J^*(\UU) \right]"
  \end{align*}
  \pause How to take into account values not optimal, but not too far
  either $\longrightarrow$ relaxation of the equality with
  $\alpha> 1$:
  \begin{equation*}
    \Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk,\UU) \leq \alpha J^*(\UU) \right]
  \end{equation*}
\end{frame}

 
\begin{frame}{Illustration}
  \pnote{ What does it look like on a concrete example. We have the
    plot of a cost function, where theta is the x axis, and u is on
    the y axis.

    As said earlier, for each horizontal cross section, so for u
    fixed, we compute the minimiser, theta star of u.

    We can then compute the whole set of the conditional minimisers

    Now, we set alpha: inside the yellow lines, we are between the
    minimum and alpha times the minimum

    Finally, we construct and measure for each theta the probability
    to be within this acceptable region.  Great, now we just have to
    know how to choose
    alpha. % Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile.
  }
  \begin{columns}
    \begin{column}{0.45\textwidth}
      \vfill
      \only<1>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_1.pgf}}}%
      \only<2>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_2.pgf}}}%
      \only<3>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_3.pgf}}}
      \only<4>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_4.pgf}}}%
      \vfill
    \end{column}
    \begin{column}{.6\textwidth}
      \begin{itemize}
      \item<1-> Sample $\uu\sim\UU$, and solve
        $\kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk,\uu)$
      \item<2->Set of conditional minimisers:
        $\{(\kk^*(\uu), \uu) \mid \uu \in \Uspace\}$
      \item<3-> Set $\alpha \geq 1$
      \item<4>
        $R_{\alpha}(\kk) = \{\uu \mid J(\kk,\uu) \leq \alpha J^*(\uu)
        \}$
      \item<4>
        $\Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[\UU\in
          R_{\alpha}(\kk) \right]$
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}{Getting an estimator}
  \pnote{Great so now we have Gamma(theta) which is the probability
    that theta gives a cost alpha acceptable If we have an idea of a
    threshold we don't want to exceed, so if alpha is known we can
    maximize the probability of being alpha acceptable

    Or, on the other hand, as gamma is a probability, we can look for
    the smallest relaxation, where the probability of acceptability
    reaches a certain confidence 1-eta.

    We can then define the family of relative-regret estimators, which
    are the maximizers of such a probability of being alpha
    acceptable.  Depending on the approach, we can nudge toward
    optimal performances with small alpha, or risk adverse preference,
    by setting a bigger relaxation.  }  $\Gamma_{\alpha}(\kk)$:
  probability that the cost (thus $\kk$) is $\alpha$-acceptable
  \begin{itemize}
  \item If $\alpha$ known, maximize the probability that $\kk$ gives
    acceptable values:
    \begin{equation}
      \max_{\kk\in\Kspace} \Gamma_{\alpha}(\kk) = \max_{\kk\in\Kspace}\Prob_{\UU}\left[J(\kk, \UU) \leq \alpha J^*(\UU)\right]
    \end{equation}
  \item Set a target probability $1-\eta$, and find the smallest
    $\alpha$.
    \begin{equation}
      \inf\{ \alpha \mid \max_{\kk\in\Kspace}\Gamma_{\alpha}(\kk) \geq 1 - \eta \}
    \end{equation}
  \end{itemize}

  \begin{block}{Relative-regret family of estimators}
    \begin{equation}
      \left\{ \hat{\kk} \mid \hat{\kk} = \argmax_{\kk \in \Kspace} \Gamma_{\alpha}(\kk), \alpha>1 \right\}
    \end{equation}
  \end{block}

\end{frame}

\begin{frame}{Why the relative regret ?}
  \pnote{we discussed so far the relative regret, that takes the form
    of a multiplicative relaxation. Why this over the additive regret
    ?

    Relative regret takes better into account the magnitude of the
    cost function, as the region of acceptability grows with alpha AND
    Jstar. When the situation is already bad, we don't want to put
    much effort to stay close to the minimum theta star of u. On the
    other hand, for Jstar close to 0, }

  
  \renewcommand\rmfamily{\sffamily}
  \begin{center}
    \resizebox{.6\textwidth}{!}{\input{\manupath
        Chapter3/img/illustration_region_regret.pgf}}
  \end{center}
  \begin{itemize}
  \item Relative regret
    \begin{itemize}
    \item $\alpha$-acceptability regions large for flat and bad
      situations ($J^*(\uu)$ large)
    \item Conversely, puts high confidence when $J^*(\uu)$ is small
    \item No units $\rightarrow$ ratio of costs
    \end{itemize}
  \end{itemize}
\end{frame}



\section{Adaptive strategies using Gaussian Processes}
\subsection{\small{How to compute $\hat{\kk}$ in a reasonable time?}}

\begin{frame}{Surrogates, and cost function}
  \pnote{We now have a family of estimators, but in practice, finding
    them is expensive, as it requires probability estimation, and
    various optimisations, that is why we are going to use surrogates
    to tackle this problem}
  \begin{itemize}
  \item Replace expensive model by a computationally cheap metamodel
    ($\sim$ plug-in approach)
  \item Adapted sequential procedures e.g. EGO
  \end{itemize}
  $\rightarrow$ Kriging (Gaussian Process
  Regression)~\cite{matheron_traite_1962,krige_statistical_1951}
  % \item Polynomial Chaos
  %   Expansion~\cite{xiu_wiener--askey_2002,sudret_polynomial_2015}
  % \end{itemize}
  \onslide<2>{ $Y\sim \GP\left(m_Y(\cdot),C_Y(\cdot, \cdot)\right)$ GP
    regression of $J$ on $\Kspace \times \Uspace$, using an initial
    design $\mathcal{X} = \{((\kk_i, \uu_i), J(\kk_i, \uu_i))\}}$
  \pnote{We choose Gaussian Process regression as a metamodel, as it
    can replace cheaply the computer code}
\end{frame}


\begin{frame}{Estimation of $\kk^*$, $J^*(\uu)$}
  \pnote{One of the first problem we encountered is the computation of
    the conditional minimum and minimisers.  Using Gaussian processes,
    and an enrichment criterion called the Profile Expected
    Improvement, we can reconstruct the Jstar and theta star function
    iteratively, and then use the kriging prediction as a surrogate
    since Jstar is well approximated}
  
  Estimation of $J^*(\uu)$ and $\kk^*(\uu)$: Enrich the design
  according to PEI criterion~\cite{ginsbourger_bayesian_2014}.

  % \begin{center}
  %   \includegraphics[scale=0.5]{PEI_branin}
  % \end{center}
  \begin{center}
    \renewcommand\rmfamily{\sffamily}
    \resizebox{.8\textwidth}{!}{\input{\manupath
        Chapter4/img/PEI_example.pgf}}
  \end{center}
\end{frame}


\begin{frame}{GP of the ``penalized'' cost function}
  \pnote{What about the estimation of the probability of being alpha
    acceptable ?  As it is a linear combination of GP, we still have a
    Gaussian distribution at the end, and we can rewrite the
    probability of being acceptable as a probability of failure of the
    resulting GP, that we could consider to estimate using common
    techniques, level set, contour estimation.

    One interesting thing is in the decomposition of the variance of
    Delta alpha: two main sources of uncertainties are present: the
    uncertainty on the true value of the function at theta,u; and the
    true value of the minimizer.}  What about
  $J(\kk, \uu) - \alpha J^*(\uu)$ ?
  % \begin{itemize}
  % \item Optimization of the $1-\eta$ quantile of $J/J^*$, or
  %   $J - J^*$
  %   \cite{razaaly_quantile-based_2020,quagliarella_optimization_2014}
  % \end{itemize}
  \begin{align}
    Y &\sim \GP\left(m_Y(\cdot); C_Y(\cdot, \cdot)\right) \text{ on } \Kspace \times \Uspace \\
    \Delta_{\alpha} &= Y - \alpha Y^*
  \end{align}
  Still a GP
  \begin{align}
    \Delta_{\alpha}(\kk, \uu) &\sim \GP\left(m_{\alpha}(\cdot);C_{\alpha}(\cdot, \cdot) \right) \\
    m_{\alpha}(\kk,\uu)   &= m_Y(\kk, \uu) - \alpha m_Y^*(\uu) \\
    \sigma^2_{\alpha}(\kk, \uu) &= \sigma^2_{Y}(\kk, \uu) +  \alert<2>{\alpha^2\sigma^2_{Y^*}(\uu)}- 2\alpha C_Y\left((\kk, \uu), ({\kk}_Y^ *(\uu), \uu)\right)
  \end{align}
  % \only<2>{$\rightarrow$ Evaluating $J(\kk, \uu)$ will not
  % necessarily reduce ``all'' the uncertainty for the regret}
  Estimate the ``probability of
  failure''~\cite{bect_sequential_2012,echard_ak-mcs_2011}
  $\Prob_{\UU}\left[J(\kk, \UU) - \alpha J^*(\UU) \leq 0\right]
  \approx \Prob_{\UU}\left[\Prob_Y\left[\Delta_{\alpha} \leq
      0\right]\right]$
\end{frame}
\begin{frame}{Joint space or objective-oriented exploration}
  Because of $J^*(\uu)$, it is often not enough to select the point
  where the uncertainty is high.  Generally, two main approaches can
  be considered
  \begin{itemize}
  \item Estimate the region
    $\{(\kk, \uu) \mid J(\kk,\uu) \leq \alpha J^*(\uu)\}$, then use
    the surrogate as a plug-in estimate to compute and maximize
    $\Gamma_{\alpha}$

    $\rightarrow$ reduce uncertainty on the whole space
  \item Select a candidate $\tilde{\kk}$, such that uncertainty on the
    estimation of $\Gamma_{\alpha}(\tilde{\kk})$ is reduced

    $\rightarrow$ reduce uncertainty on
    $\{\tilde{\kk}\}\times\Uspace$, with $\tilde{\kk}$ well-chosen.
  \end{itemize}
\end{frame}
% \begin{frame}
%   \frametitle{Surrogates and dimension reduction}
%   \begin{itemize}
%   \item Sensitivity
%     analysis~\cite{sudret_global_2008,le_gratiet_metamodel-based_2016}:
%     Based on intensive computation of the metamodel, or analytic
%     computation based on coefficients of the expansion computed
%   \item Isotropic by groups
%     kernels~\cite{blanchet-scalliet_specific_2017,ribaud_krigeage_2018-1}:
%     Group variables to have a few isotropic kernels
%   \end{itemize}

% \end{frame}
% \section{Calibration of a numerical model}
\begin{frame}
  \frametitle{Application to CROCO: Dimension reduction}
  \begin{center}
    \renewcommand\rmfamily{\sffamily}
    \begin{columns}
      \begin{column}{.5\textwidth}
        \resizebox{\textwidth}{!}{\includegraphics{\manupath
            Chapter5/img/depth_repartition.pdf}}
      \end{column}
      \begin{column}{.5\textwidth}
        \resizebox{\textwidth}{!}{\input{\manupath
            Chapter5/img/SA_croco.pgf}}
      \end{column}
    \end{columns}
  \end{center}
  Ad-hoc segmentation according to the depth, and sensitivity
  analysis: only the shallow coastal regions seem to have an
  influence.
\end{frame}
\begin{frame}
  \frametitle{Robust optimization}
  \begin{center}
    \begin{columns}
      \begin{column}{.5\textwidth}
        % \includegraphics[width=\textwidth]{\manupath
        % Chapter5/img/gaussian_english_channel.png}
      \end{column}
      \begin{column}{.5\textwidth}
        \includegraphics[width=\textwidth]{croco.png}
      \end{column}
    \end{columns}
  \end{center}

  \begin{itemize}
  \item $\UU\sim \mathsf{U}[-1, 1]$ uniform r.v.\ that models the
    percentage of error on the amplitude of the M2 component of the
    tide
  \item The ``truth'' ranges from $8$mm to $13$mm.
  \item $11.0$mm leads to a cost which deviates less than $1\%$ from
    the optimal value with probability $0.77$
  \end{itemize}
\end{frame}



\section{Conclusion}

\begin{frame}[label=conclusion]{Conclusion}
  
  \begin{block}{Wrapping up}
    \begin{itemize}
    \item Problem of a \emph{good} definition of robustness
    \item Tuning $\alpha$ or $\eta$ reflects risk-seeking or
      risk-adverse strategies
    \item Strategies rely heavily on surrogate models, to embed
      aleatoric uncertainties directly in the modelling
    \end{itemize}
  \end{block}


  \begin{block}{Perspectives}
    \begin{itemize}
    \item Cost of computer evaluations $\rightarrow$ limited number of
      runs?
    \item In low dimension, CROCO very well-behaved.
    \item Dimensionality of the input space $\rightarrow$ reduction of
      the input space?
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}[allowframebreaks]
  \frametitle{References} \bibliographystyle{authordate1}
  \bibliography{/home/victor/acadwriting/bibzotero.bib}
\end{frame}

\appendix

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
