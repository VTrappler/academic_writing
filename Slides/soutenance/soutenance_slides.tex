\documentclass[10pt,aspectratio=169,usepdftitle=false]{beamer}


\usetheme{metropolis}
\makeatletter
\setbeamertemplate{headline}{%
  \begin{beamercolorbox}[colsep=1.5pt]{upper separation line head}
  \end{beamercolorbox}
  \begin{beamercolorbox}{section in head/foot}
    \vskip2pt\insertnavigation{\paperwidth}\vskip2pt
  \end{beamercolorbox}%
  \begin{beamercolorbox}[colsep=1.5pt]{lower separation line head}
  \end{beamercolorbox}
}
\makeatother

\setbeamercolor{section in head/foot}{fg=normal text.bg, bg=structure.fg}

\AtBeginSection{
  \frame{
    \sectionpage
    \tableofcontents[currentsection]
  }
}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\newcommand\hmmax{0} % default 3
\newcommand\bmmax{0} % default 4
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{siunitx}
%% Fig mgmt ---------------------------------------
\graphicspath{{../Figures/}}
\usepackage{adjustbox}
\usetikzlibrary{positioning}
\usepackage{subfig}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\newcommand\manupath{/home/victor/acadwriting/Manuscrit/Text/}


%% Bib mgmt ---------------------------------------
\usepackage[authoryear,square]{natbib}
% \setcitestyle{square,aysep={},yysep={;}}

%% New commands --------------------------------------- 
\newcommand{\Uspace}{\mathbb{U}}
\newcommand{\Kspace}{\Theta}
\newcommand{\Xspace}{\mathbb{X}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\GP}{\mathsf{GP}}
\newcommand{\Prob}{\mathbb{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Cov}{\textsf{Cov}}
\newcommand{\tra}{\mathrm{tr}}
\newcommand{\yobs}{y}
% \newcommand{\yobs}{\bm{y}^{\mathrm{obs}}}
\newcommand{\kest}{\hat{\bm{k}}}
\newcommand{\kk}{\theta}
\newcommand{\uu}{u}
\newcommand{\UU}{U}
\DeclareMathOperator*{\KL}{\textsf{KL}}

\newcommand{\inputpgf}[2][\textwidth]{
  \renewcommand\rmfamily{\sffamily}
  \resizebox{#1}{!}{\input{#2}}}



%% Pdfpc notes ---------------------------------------
\usepackage[duration=45, lastminutes=5]{pdfpcnotes}



\definecolor{blkcol}{HTML}{E1E1EA}
\definecolor{blkcol2}{RGB}{209, 224, 224}

\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}
\definecolor{halfgray}{gray}{0.55}
\definecolor{webgreen}{rgb}{0,.5,0}
\definecolor{webbrown}{rgb}{.6,0,0}
\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\definecolor{Black}{cmyk}{0, 0, 0, 0}
\definecolor{blueGreen}{HTML}{4CA6A7}
\definecolor{blueGreenN2}{HTML}{3964B4}
% Using colorbrewer, 1 is lightest colour, 4 is darkest
\definecolor{brewsuperlight}{HTML}{F0F9E8}
\definecolor{brewlight}{HTML}{BAE4BC}
\definecolor{brewdark}{HTML}{7BCCC4}
\definecolor{brewsuperdark}{HTML}{2B8CBE}


%%% VICTOR COLORS ----------------------------------------
\colorlet{cfgHeaderBoxColor}{brewsuperdark} %{green!50!blue!70}
\colorlet{cfgCiteColor}{RoyalBlue} % vLionel : RoyalBlue
\colorlet{cfgUrlColor}{RoyalBlue}  % vLionel : RoyalBlue
\colorlet{cfgLinkColor}{RoyalBlue} % vLionel : blueGreenN2

\setbeamercolor{block title}{bg = blkcol}
\setbeamercolor{block body}{bg = blkcol!50}
\setbeamerfont{author}{size=\footnotesize}

\usepackage{appendixnumberbeamer}
\metroset{progressbar=foot,
  numbering=fraction}




\title{Parameter control in the presence of uncertainties}
\subtitle{PhD Defense, June 11th 2021}
\author[Victor Trappler]{%
  % \begin{center}%
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{center}
        {\Large \textbf{Victor Trappler} \vspace{.0cm}}%
      \end{center}
    \end{column} 
    \begin{column}{0.5\textwidth}
      \begin{center}
    \begin{tabular}{rrr}
      \textit{Advisors}: & Élise Arnaud  & Univ. Grenoble Alpes\\
                & Laurent Debreu & Inria Grenoble\\
                & Arthur Vidard & Inria Grenoble\\
          \textit{Jury}: & Youssef Marzouk & MIT\\
                & Pietro Congedo & Inria Paris Saclay\\
                & Olivier Roustant & INSA Toulouse\\
                & Rémy Baraille & SHOM
    \end{tabular}
  \end{center}
  \end{column} 
\end{columns}
% \end{center}%
}


\institute{%
  % \hrulefill
  \begin{center}
      \includegraphics[height=30pt]{INRIA_SCIENTIFIQUE_UK_CMJN}
    \hspace{1cm}
        \includegraphics[height=30pt]{ljk}
        \hspace{1cm}
        \includegraphics[height=30pt]{logo_UGA.pdf}
      \end{center}
      % \hrulefill%
}

% \date{\textbf{PhD Defence, Grenoble, June 11th 2021}}
%   % {\bf } \today
% }
\date{}
\setcounter{tocdepth}{1}

\begin{document}
\begin{frame}
  \pnote{Thank you for the introduction, and thank you to the jury for
    being here today.

    My name is Victor Trappler, and I'm going to present some of the
    work done during my phd, which is named ``Parameter control in the
    presence of uncertainties''

    For my PhD, I've been part of the AIRSEA team, here in Grenoble,
    and have been under the supervision of Élise Arnaud, Arthur Vidard
    and Laurent Debreu}
  \maketitle
\end{frame}
\AtBeginSection{
  % \frame{
  %   \sectionpage
  %   \tableofcontents[currentsection]
  % }
}
\section{Introduction}
\begin{frame}{Why do we need models ?}
  \pnote{Modelling a natural phenomenon, such as the weather often
    starts with}
  The ability to understand  is essential in order to forecast, and to take decisions.
  \begin{itemize}
  \item Natural phenomenon are often very complex to understand in their entirety
  \item Mathematical models are \alert{simplified} versions, which
    allow to study the relationships between some observed (or not) quantities
  \item The mathematical models can be used to construct numerical
    models, which are used for forecasts
  \end{itemize}
  % \includegraphics[scale=0.015]{france_satellite.jpg}
\end{frame}

\begin{frame}{Parametrization}
  A lot of choices are required to construct models, which depends on
  the scale we wish to represent
  
    \begin{itemize}
    \item Physical parameters, which represent some physical properties
    that influence the model. 
  \item Additional parameters, introduced for numerical reasons
  \end{itemize}

    \begin{center}
    \begin{adjustbox}{clip,
        trim=33cm 0cm 0cm 0cm,
        max width=0.6\textwidth, center}
    \input{\manupath Chapter3/img/modelling_uncertainties.pgf}
    \end{adjustbox}
  \end{center}

  % Does reducing the error on the parameters leads to the compensation
  % of the unaccounted natural variability of the physical processes ?

  
  \pnote{During the whole process of the modelling of a physical
    system, that is from the observation of a natural phenomenon, to
    the simulation using numerical methods, we introduce
    uncertainties.  Those uncertainties take the form of errors
    introduced by the simplifications, discretizations and
    parametrizations needed to represent things numerically.

    In the end, we have a set of parameters, that we need to
    calibrate, but during this phase of calibration, how can we be
    sure that we try to act only the error due to the parametrization,
    and are not compensating errors coming from others sources.}
  
\end{frame}

\begin{frame}{Uncertainties in the modelling}

\end{frame}

\begin{frame}{A fine tuning needed}

  $\rightarrow$ How well does the simulation using a set of parameters
  match the observations ?
\end{frame}



\AtBeginSection{
  \frame{
    \sectionpage
    \tableofcontents[currentsection,currentsubsection]
  }
}
\begin{frame}[t]{Computer code and inverse problem}
  \begin{itemize}
  \item[Input]
    \begin{itemize}
    \item $\kk$: Control
      parameter%Bottom friction (spatially distributed)
    \item $\uu$: Environmental variables (fixed and known)
    \end{itemize}
  \item[Output] \begin{itemize}
    \item $\mathcal{M}(\kk,\uu)$: Quantity to be compared to
      observations%Sea surface height, at predetermined time of the
                  %simulation and at certain location
    \end{itemize}
  \end{itemize}
  \vfill
  % \only<1>{\input{../Figures/comp_code.pgf}}
  \only<1>{
    \begin{center}
      \input{../Figures/inv_prob.pgf}
    \end{center}
  } \pnote{In quite a classical setting, we assume that we have a
    model M, that takes two inputs: Θ the control variable, that we
    aim at calibrating, and u some environmental variables, that we
    consider fixed and knowns.  We wish to calibrate the model wrt to
    some observations yobs }
\end{frame}

\begin{frame}{Data assimilation framework}
  % We have
  % $\yobs = \mathcal{M}(\kk_{\mathrm{obs}},\uu_{\mathrm{obs}})$ with
  % $\uu_{\mathrm{obs}} = \uu$
  Let $\uu \in \Uspace$. By defining a notion of distance between
  $\mathcal{M}(\kk, \uu)$ and $y$:
  \begin{block}{Objective function}
    We define $J$ as the squared difference between the output of the
    model and the observations
    \begin{equation}
      J(\kk, \uu) =  \frac12 \|\mathcal{M}(\kk,\uu) - \yobs \|^2
    \end{equation}
    \alert<1>{$\rightarrow$ the smaller $J$ is, the better the fit is}
  \end{block}
\onslide<2->{We can get an estimate by solving
  \begin{equation}
    \min_{\kk\in\Kspace} J(\kk, \uu) = J(\hat{\kk},\uu)
  \end{equation}
  % \begin{enumerate}[$\rightarrow$]
  % \item Deterministic optimization problem
  % \item Possibly add regularization
  % \item Classical methods: Adjoint gradient and Gradient-descent
  % \end{enumerate}
  \begin{itemize}
  \item \alert{$\hat{\kk}$ depends inherently on $\uu$}
  \item What if $\uu$ does not reflect accurately the observations?
  \item Does $\hat{\kk}$ then compensate the errors brought by this random
    misspecification? ($\sim$overfitting)
    % \item How well will $\bm{\hat{k}}$ perform under other
    %   conditions?
  \end{itemize}
}
  \pnote{ In practice, we select a likely value of the environmental
    parameters u, and Using the least square approach, we define J, a
    cost function, as the sum of the squares of the difference between
    the observations and the output of the numerical model.


    This is a deterministic optimisation problem, that we can solve
    using classical methods such as adjoint gradient.

    But what if the u we chose is not quite the same as the one of the
    observations. The minimisation procedure is supposed to correct
    the error on theta but will it try to compensate too much ? }
\end{frame}

%%%%
\begin{frame}{Context}
  \begin{itemize}
  \item The friction $\kk$ of the ocean bed has an influence on the
    water circulation
  \item Depends on the type and/or characteristic length of the
    asperities
  \item Subgrid phenomenon
  \item $\uu$ parametrizes the BC
  \end{itemize}
  \begin{center}
    \scalebox{.9}{\input{../Figures/hauteureau.tikz}}
  \end{center}
  \pnote{ In order to have an idea of such parameters, the case study
    for us is the estimation of the bottom friction.  The bottom
    friction theta has an influence on the oceanic circulation, as it
    dissipates some energy by turbulences and it depends on the type
    of soil, and on the characteristic length of the
    asperities. Something that is hard to observe directly.
    % In oceans modelling, this is a subgrid phenomenon.
    The environmental variable u parametrizes the BC, for instance the
    relative amplitude of tidal components.  }
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  \pnote{We are first going to see the general setting of calibration
    problems, and then how to define robustness in this
    context. Finally, we are going to see how to tackle this problem
    in practice using surrogate models }
\end{frame}



\subsection{Random parameteric misspecifications}
\begin{frame}{Different types of uncertainties}
  \begin{block}{Epistemic or aleatoric
      uncertainties?~\citep{walker_defining_2003}}
    \begin{itemize}
    \item Epistemic uncertainties: From a lack of knowledge, that can
      be reduced with more research/exploration
    \item Aleatoric uncertainties: From the inherent variability of
      the system studied, operating conditions
    \end{itemize}
  \end{block}
  $\rightarrow$ But where to draw the line?

  Our goal is to take into account the aleatoric uncertainties in the
  estimation of our parameter.  \pnote{ We evoked earlier that the
    uncertainty on theta or on u is not the same, and we can make a
    rough distinction between two types:

    - First, the epistemic uncertainties that result from a lack of
    knowledge, but can be reduced. An example is the uncertainty
    during the estimation of the mean value. The more samples you
    take, the less uncertainty there is on your estimation

    - Secondly, there is the aleatoric uncertainty, that comes from
    the inherent variability of the system studied. Think of the
    different values that a random variable takes.

    Our goal, is then to be able to reduce the epistemic uncertainty
    on the value of theta, while taking into account the aleatoric
    uncertainty.  }
\end{frame}

\begin{frame}[t]{Aleatoric uncertainties}
  Instead of considering $\uu$ fixed, we consider that $\uu\sim \UU$  r.v.\ (with known pdf $\pi(\uu)$), and the output of the model depends on its realization. \\
  \vfill \only<1>{
    \begin{center}
      \input{../Figures/inv_prob.pgf}
    \end{center}
  } \only<2>{
    \begin{center}
      \input{../Figures/comp_code_unc_inv.pgf}
    \end{center}
  } \vfill


  \pnote{As hinted before, we are going to model the aleatoric
    uncertainty on u by a random variable.  The output of the model
    becomes a random variable }
\end{frame}

\begin{frame}{The cost function as a random variable}
  \begin{itemize}
  \item The computer code is deterministic, and takes $\kk$ and $\uu$
    as input:
    \begin{equation*}
      \mathcal{M}(\kk,\alert{\uu})
    \end{equation*}
  \item The deterministic quadratic error is now
    \begin{equation*}
      J(\kk,\alert{\uu}) =  \frac12\|\mathcal{M}(\kk,\alert{\uu}) - \yobs\|^2
    \end{equation*}
  \end{itemize}

\begin{equation*}
  ''\hat{\kk} = \argmin_{\kk\in\Kspace} J(\kk,\alert{\uu})'' \text{ but what can we do about } \uu ?
\end{equation*}
\pnote{ In our study, the model is completely deterministic, so we can
  control its inputs The cost function, becomes then a function of two
  theta and u. We still wish to minimise with respect to theta, but
  what can we do for u ?  }
\end{frame}

\begin{frame}{Misspecification of $u$: twin experiment setup }
  
  Minimization performed on $\kk\mapsto J\left(\kk,\uu\right)$, for
  different $\uu$: \vfill % \only<2>{Well-specified model} \only<3>{1\%
  %   error on the amplitude of the M2 tide} \only<4>{1\% error on the
  %   amplitude of the M2 tide}
  % % \only<5>{0.5\% error on the amplitude of the M2 tide, starting at
  % % the truth}
  \begin{center}
    % \includegraphics<2>[width=\textwidth]{/home/victor/optimisation_dahu/optim_true/map_155.png}
    % \includegraphics<3>[width=\textwidth]{/home/victor/optimisation_dahu/optim_1_001/map_150.png}
    % \includegraphics<4>[width=\textwidth]{/home/victor/optimisation_dahu/optim_0_001/map_160.png}
    % \includegraphics<5>[width=\textwidth]{/home/victor/optimisation_dahu/optim_025_001_frtr/map_199.pdf}
    \includegraphics<2>[scale=.9]{/home/victor/optimisation_dahu/1b/map_lognorm_200.png}%
    \includegraphics<3>[scale=.9]{/home/victor/optimisation_dahu/2b/map_lognorm_200.png}%
    \includegraphics<4>[scale=.9]{/home/victor/optimisation_dahu/3b/map_lognorm_200.png}%
    \includegraphics<5>[scale=.9]{/home/victor/optimisation_dahu/4b/map_lognorm_200.png}%
    \includegraphics<6>[scale=.9]{/home/victor/optimisation_dahu/optim_sediments/map_lognorm_200.png}%
    \includegraphics<7>[scale=.9]{/home/victor/optimisation_dahu/6b/map_lognorm_200.png}%
    \includegraphics<8>[scale=.9]{/home/victor/optimisation_dahu/7b/map_lognorm_200.png}%
    \includegraphics<9>[scale=.9]{/home/victor/optimisation_dahu/8b/map_lognorm_200.png}%
    \includegraphics<10>[scale=.9]{/home/victor/optimisation_dahu/9b/map_lognorm_200.png}%
  \end{center}
  \pnote{ To have a first look at the problem of misspecification, we
    try to calibrate the bottom friction on this domain in a twin
    experiment setup, with or without misspecification. This figures
    shows the truth value of the bottom friction.

    
    When optimizing without misspecification, we can see that the
    region of the english channel is able to retrieve the truth value,
    while the regions farther from the coast, in the bay of biscay,
    stay quite unaffected by the procedure. So far so good.

    When we introduce a 1\% error in the amplitude of the M2 tide, we
    can see more variations, especially in the english channel, as it
    compensate the misspecification of the tide.

    The estimation of theta is sensitive to the value of the
    environmental parameter. So the question that arise is ``how to
    get a value of theta, which shows robust properties when the
    environmental parameter varies ?''  }
\end{frame}

\begin{frame}{Robustness and estimation of parameters}
  \pnote{ So that is how we define robustness in this work: So
    basically, we have two main objectives: - First to find some
    criteria of robustness to estimate theta - Be able to compute
    those estimates quickly } {\textbf{Robustness}}: get good
  performances when the environmental parameter varies

  \begin{itemize}
  \item Define criteria of robustness, based on $J(\kk,\uu)$, that
    will depend on the final application
  \item Be able to compute them in a reasonable time
  \end{itemize}
\end{frame}


\section{Robustness in calibration}
\begin{frame}{Robust objectives of the objective function}
  \pnote{ I'm going to present very quickly some estimates that can be
    considered robust, but will focus mainly on the last one.  First
    we can think about minimising in the worst case sense. This
    usually leads to overly conservative estimates as we are
    maximizing over the whole space U. We can also think about
    minimising the moments, such as the mean or the variance, or even
    combine them in a multiobjective setting by looking for the pareto
    front.


    Every choice of environmental variable gives a distinct situation
    The aspect we are going to focus on is based on the regret, so it
    implies a comparison with the best performance attainable for each
    u.  }
  \begin{itemize}
    % \item Global Optimum: $ \min_{(\kk,\uu)} J(\uu,\kk)$
    %   $ \longrightarrow $ EGO
  \item Worst case~\citep{marzat_worst-case_2013}:
    $$ \min_{\kk \in \Kspace} \left\{\max_{\uu \in \Uspace}
      J(\kk,\uu)\right\}$$
  \item M-robustness~\citep{lehman_designing_2004}:
    $$\min_{\kk\in\Kspace} \Ex_{\UU}\left[J(\kk,\UU)\right]$$
  \item V-robustness~\citep{lehman_designing_2004}:
    $$\min_{\kk\in\Kspace} \Var_{\UU}\left[J(\kk,\UU)\right]$$
  \item Multiobjective~\citep{baudoui_optimisation_2012}:
    $$ \text{Pareto frontier} $$
  % \item Region of failure given by
  %   $J(\kk,\uu)>T$~\cite{bect_sequential_2012}:
  %   $$\max_{\kk \in \Kspace} R(\kk) = \max_{\kk\in \Kspace}
  %   \Prob_{\uu}\left[J(\kk,\uu) \leq T \right]$$
\item Best performance achievable given $\uu \sim \UU$
\end{itemize}
\end{frame}


\begin{frame}{``Most Probable Estimate'', and relaxation}%
  \pnote{ The main idea is that we want to consider individually all
    situations induced by the value of the environmental variable.
    Basically, once a value u is sampled, the problem is
    deterministic, so under some assumption, we have a minimiser theta
    star that is a function of u


    Keeping in mind the random nature of U, we can define the random
    variable thetastar, and its density (if it is defined), can be
    seen as the frequency of which a value theta is optimal.


    That is an interesting information, but we can have a little more
    than that. We may want to include theta that yield values of the
    cost function close to a minimum. To do that, we introduce a
    relaxation of the equality constraint with alpha, so that for a
    given u, we consider acceptable the theta that give values of the
    cost function between Jstar, the optimal value and alpha times
    Jstar So finally, we compute the probability that this given theta
    is acceptable with respect to the level alpha }

  Given $\uu \sim \UU$, the optimal value is $J^*(\uu)$, attained at
  $\kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk,\uu)$.  \pause
  % \begin{columns}
  %   \begin{column}{0.6\textwidth}

  
  The minimizer can be seen as a random variable:
  \begin{equation*}
    \kk^*(\UU) = \argmin_{\kk\in\Kspace} J(\kk,\alert<1>{\UU})
  \end{equation*}
  $\longrightarrow$ estimate its density (how often is the value $\kk$
  a minimizer)
  \begin{align*}
    p_{\kk^*}(\kk) &= "\Prob_{\UU}\left[J(\kk,\UU)= J^*(\UU) \right]"
  \end{align*}
  \pause How to take into account values not optimal, but not too far
  either $\longrightarrow$ relaxation of the equality with
  $\alpha> 1$:
  \begin{equation*}
    \Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk,\UU) \leq \alpha J^*(\UU) \right]
  \end{equation*}
\cite{trappler_robust_2020}
\end{frame}

 
\begin{frame}{Construction of regions of acceptability}
  \pnote{ What does it look like on a concrete example. We have the
    plot of a cost function, where theta is the x axis, and u is on
    the y axis.

    As said earlier, for each horizontal cross section, so for u
    fixed, we compute the minimiser, theta star of u.

    We can then compute the whole set of the conditional minimisers

    Now, we set alpha: inside the yellow lines, we are between the
    minimum and alpha times the minimum

    Finally, we construct and measure for each theta the probability
    to be within this acceptable region.  Great, now we just have to
    know how to choose
    alpha. % Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile.
  }
  \begin{columns}
    \begin{column}{0.45\textwidth}
      \vfill
      \only<1>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_1.pgf}}}%
      \only<2>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_2.pgf}}}%
      \only<3>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_3.pgf}}}
      \only<4>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_4.pgf}}}%
      \vfill
    \end{column}
    \begin{column}{.6\textwidth}
      \begin{itemize}
      \item<1-> Sample $\uu\sim\UU$, and solve
        $\kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk,\uu)$
      \item<2->Set of conditional minimisers:
        $\{(\kk^*(\uu), \uu) \mid \uu \in \Uspace\}$
      \item<3-> Set $\alpha \geq 1$
      \item<4>
        $R_{\alpha}(\kk) = \{\uu \mid J(\kk,\uu) \leq \alpha J^*(\uu)
        \}$
      \item<4>
        $\Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[\UU\in
          R_{\alpha}(\kk) \right]$
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}{Regret-based estimates}
  \pnote{Great so now we have Gamma(theta) which is the probability
    that theta gives a cost alpha acceptable If we have an idea of a
    threshold we don't want to exceed, so if alpha is known we can
    maximize the probability of being alpha acceptable

    Or, on the other hand, as gamma is a probability, we can look for
    the smallest relaxation, where the probability of acceptability
    reaches a certain confidence 1-eta.

    We can then define the family of relative-regret estimators, which
    are the maximizers of such a probability of being alpha
    acceptable.  Depending on the approach, we can nudge toward
    optimal performances with small alpha, or risk adverse preference,
    by setting a bigger relaxation.  }  $\Gamma_{\alpha}(\kk)$:
  probability that the cost (thus $\kk$) is $\alpha$-acceptable
  \begin{itemize}
  \item If $\alpha$ known, maximize the probability that $\kk$ gives
    acceptable values:
    \begin{equation}
      \max_{\kk\in\Kspace} \Gamma_{\alpha}(\kk) = \max_{\kk\in\Kspace}\Prob_{\UU}\left[J(\kk, \UU) \leq \alpha J^*(\UU)\right]
    \end{equation}
  \item Set a target probability $1-\eta$, and find the smallest
    $\alpha$.
    \begin{equation}
      \inf\{ \alpha \mid \max_{\kk\in\Kspace}\Gamma_{\alpha}(\kk) \geq 1 - \eta \}
    \end{equation}
  \end{itemize}

  \begin{block}{Relative-regret family of estimators}
    \begin{equation}
      \left\{ \hat{\kk} \mid \hat{\kk} = \argmax_{\kk \in \Kspace} \Gamma_{\alpha}(\kk), \alpha>1 \right\}
    \end{equation}
  \end{block}

\end{frame}

\begin{frame}{Relative or additive regret}
  \pnote{we discussed so far the relative regret, that takes the form
    of a multiplicative relaxation. Why this over the additive regret
    ?

    Relative regret takes better into account the magnitude of the
    cost function, as the region of acceptability grows with alpha AND
    Jstar. When the situation is already bad, we don't want to put
    much effort to stay close to the minimum theta star of u. On the
    other hand, for Jstar close to 0, }

  
  \renewcommand\rmfamily{\sffamily}
  \begin{center}
    \resizebox{.6\textwidth}{!}{\input{\manupath
        Chapter3/img/illustration_region_regret.pgf}}
  \end{center}
  \begin{itemize}
  \item Relative regret
    \begin{itemize}
    \item $\alpha$-acceptability regions large for flat and bad
      situations ($J^*(\uu)$ large)
    \item Conversely, puts high confidence when $J^*(\uu)$ is small
    \item No units $\rightarrow$ ratio of costs
    \end{itemize}
  \end{itemize}
\end{frame}



\section{Adaptive strategies using Gaussian Processes}
\begin{frame}{The computational bottleneck}
  \pnote{We now have a family of estimators, but in practice, finding
    them is expensive, as it requires probability estimation, and
    various optimisations, that is why we are going to use surrogates
    to tackle this problem}
  In general, getting estimates can be very expensive:
  \begin{itemize}
  \item \alert{Estimate} statistical quantities ($\Ex_{\UU}$, $\Prob_{\UU}$)
    \begin{itemize}
    \item[$\rightarrow$] Sufficient exploration of $\Uspace$ with
      respect to $\Prob_{\UU}$ (Monte-Carlo methods, numerical
      integration of integrals)
 \end{itemize}
\item \alert{Optimize} those quantities with respect to $\kk\in\Kspace$
  \begin{itemize}
  \item[$\rightarrow$] Focus on regions of interest of $\Kspace$
  \item[$\rightarrow$] Take into account the uncertainty on the estimation
  \end{itemize}
    \end{itemize}
  $\Rightarrow$ require a lot of computational effort (\emph{i.e.} extensive number of calls to $J$)
\end{frame}

\begin{frame}{Surrogates and cost function}

  
  \begin{itemize}
  \item Replace expensive model by a computationally cheap metamodel
    ($\sim$ plug-in approach)
  \item Adapted sequential procedures e.g. EGO
  \end{itemize}
  $\rightarrow$ Kriging (Gaussian Process Regression)~
  % \item Polynomial Chaos
  %   Expansion~\cite{xiu_wiener--askey_2002,sudret_polynomial_2015}
  % \end{itemize}
  \pnote{We choose Gaussian Process regression as a metamodel, as it
    can replace cheaply the computer code}
\end{frame}

\begin{frame}{Gaussian Process Modelling}
  \begin{columns}
    \begin{column}{0.5\textwidth}   
  Let $x = (\kk, \uu) \in \Kspace \times \Uspace = \Xspace$, $f(x)= f((\kk, \uu)) = J(\kk, \uu)$
  $\mathcal{X} = \left\{ (x_i, f(x_i)) \right\}_{1 \leq i \leq N}$ initial design of experiments ($\sim$ training points)
  \begin{block}{GP regression \citep{matheron_traite_1962,krige_statistical_1951}}
    $Z \sim \GP\left(m_Z,C_Z\right)$ is the GP constructed on $\mathcal{X}$
    with $m_Z: \Xspace \rightarrow \mathbb{R}$ and $C_Z: \Xspace^2 \rightarrow \mathbb{R}$
    \begin{itemize}
    \item $m_Z$: GP (or kriging) regression
    \item $C_Z$: covariance function
    \item  $\sigma_Z^2: x \mapsto C_Z(x, x)$ variance function
    \item \alert<2>{$Z(x)\sim \mathcal{N}\left(m_Z(x), \sigma^2_Z(x)\right)$}
    \end{itemize}
  \end{block}
  \end{column}
    \begin{column}{0.5\textwidth}
    \begin{center}
      \inputpgf{\manupath Chapter4/img/example_GP.pgf}
  \end{center}
  \onslide<2>\begin{itemize}
  \item[$\rightarrow$] Information on $J$ using $m_Z$
  \item[$\rightarrow$] Information on prediction error with $\sigma_Z^2$
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Adaptive strategies}
    Adding and evaluating points to the design iteratively, with
    respect to some particular objective
  \begin{itemize}
  \item Explore the input space
  \item Optimize the unknown function $f$
  \item Level-sets or excursion sets: $\{x \in \Xspace \mid f(x) \gtreqless T\}$
  \end{itemize}
  From a design comprising $n$ points $\mathcal{X}_n = \{(x_i, f(x_i))\}_{1 \leq i \leq n}$
  \begin{itemize}
  \item Construct GP of interest using $\mathcal{X}_n$
  \item Define a criterion $\kappa:\Xspace \rightarrow \mathbb{R}$
    which quantify the interest of evaluating a specific point
  \begin{itemize}
  \item Optimize the criterion and add the optimizer to the design
  \item Sample and cluster according to select a batch of points to evaluate
  \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{1-step criteria}
  \begin{align}
    x_{n+1} &= \argmax_{x \in \Xspace} \kappa(x) \\
    \mathcal{X}_{n+1} &= \mathcal{X}_n \cup \{(x_{n+1},f(x_{n+1})) \}
  \end{align}
  \begin{itemize}
  \item Optimization: PEI, EGO
  \item Exploration: prediction variance, aIMSE
  \item Contour/levelsets estimation: reliability index
  \item \alert{${PEI}$}
  \end{itemize}
\end{frame}
\begin{frame}{Estimation of $\kk^*$, $J^*(\uu)$}
  \pnote{One of the first problem we encountered is the computation of
    the conditional minimum and minimisers.  Using Gaussian processes,
    and an enrichment criterion called the Profile Expected
    Improvement, we can reconstruct the Jstar and theta star function
    iteratively, and then use the kriging prediction as a surrogate
    since Jstar is well approximated}
  
  \begin{columns}
    \begin{column}{0.45\textwidth}
  Estimation of $J^*(\uu)$ and $\kk^*(\uu)$: Enrich the design
  according to PEI criterion~\cite{ginsbourger_bayesian_2014}.
\end{column}
\begin{column}{0.45\textwidth}
  \begin{center}
    \inputpgf{\manupath Chapter4/img/PEI_example.pgf}
  \end{center}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{AK-MCS: Sampling-based methods}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item<1-> Define sampling criterion (margin of uncertainty)
      \item<2-> Get Samples 
      \item<3-> Cluster the samples
      \item<4-> Find closest sample to cluster centers, and add those
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \foreach\y in{1,...,7}{
        \includegraphics<\y>[width=\textwidth]{AKMCS_\y.png}
      }
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}{GP of the ``penalized'' cost function}
  \pnote{What about the estimation of the probability of being alpha
    acceptable ?  As it is a linear combination of GP, we still have a
    Gaussian distribution at the end, and we can rewrite the
    probability of being acceptable as a probability of failure of the
    resulting GP, that we could consider to estimate using common
    techniques, level set, contour estimation.

    One interesting thing is in the decomposition of the variance of
    Delta alpha: two main sources of uncertainties are present: the
    uncertainty on the true value of the function at theta,u; and the
    true value of the minimizer.}
  % \begin{itemize}
  % \item Optimization of the $1-\eta$ quantile of $J/J^*$, or
  %   $J - J^*$
  %   \cite{razaaly_quantile-based_2020,quagliarella_optimization_2014}
  % \end{itemize}
  With
  \begin{equation}
    Z \sim \GP\left(m_Z; C_Z\right) \text{ on } \Kspace \times \Uspace
  \end{equation}
  We define $Z^*(\uu)=Z(\kk^*_Z(\uu), \uu)$, with $\kk^*_Z(\uu) = \min_{\kk} m_Z(\kk, \uu)$,
  and $\Delta_{\alpha} = Z - \alpha Z^*$:
  \begin{block}{The GP $\Delta_\alpha$}
    As a linear combination of GP, $\Delta_{\alpha}$ is a GP as well:
  \begin{align}
    \Delta_{\alpha}(\kk, \uu) &\sim \GP\left(m_{\Delta_{\alpha}};C_{\Delta_\alpha}\right) \\
    m_{\Delta_\alpha}(\kk,\uu)   &= m_Z(\kk, \uu) - \alpha m_Z^*(\uu) \\
    \sigma^2_{\Delta_\alpha}(\kk, \uu) &= \sigma^2_{Z}(\kk, \uu) +  \alert<2>{\alpha^2\sigma^2_{Z^*}(\uu)}- 2\alpha C_Z\left((\kk, \uu), ({\kk}_Z^ *(\uu), \uu)\right)
  \end{align}
\end{block}
\end{frame}


\begin{frame}{Approximation of the ratio}
  Let us assume that $Z^*>0$ with high enough probability:
  $\Xi(\kk, \uu) = \log \frac{Z(\kk,\uu)}{Z^*(\uu)}$ is approximately normal
  \begin{block}{Log-normal approximation of the ratio of GP}
    \begin{align}
      \Xi(\kk, \uu) &\sim \mathcal{N}\left(m_\Xi(\kk,\uu), \sigma^2_\Xi(\kk, \uu) \right) \\
      m_\Xi(\kk, \uu) &= \log \frac{m_Z(\kk, \uu)}{m_Z^*(\uu)} \\
      \sigma^2_\Xi(\kk, \uu) &= \frac{\sigma^2_{Z}(\kk, \uu)}{m_{Z}(\kk,\uu)^2} + \frac{\sigma^2_{Z^*}(\uu)}{m_{Z^*}(\uu)^2} - 2 \frac{\Cov[Z(\kk, \uu), Z^*(\uu)]}{m_Z(\kk, \uu)m_{Z^*}(\uu)} 
    \end{align}
  \end{block}
\end{frame}

\begin{frame}{Computations using $\Delta_{\alpha}$ or $\Xi$}
  \begin{block}{Plug-in approximation}
    Instead of using directly $\Delta_{\alpha}$ or $\Xi$, $m_\Delta$ and $m_{\Xi}$ used instead
  \end{block}

  \begin{align}
      \Gamma_{\alpha}(\kk) &= \Prob_{\UU}\left[J(\kk, \UU) - \alpha J^*(\UU) \leq 0\right] \\
                              &\approx \Prob_{\UU}\left[m_{\Delta_{\alpha}}(\kk, \UU) \leq 0 \right] \\
                              &\approx \Prob_{\UU}\left[m_\Xi(\kk, \UU) \leq \alpha \right]=F_{m_{\Xi}(\kk,\UU)}(\alpha)  \\
  \end{align}
  
  Estimate the ``probability of
  failure''~\cite{bect_sequential_2012,echard_ak-mcs_2011}
  $\Prob_{\UU}\left[J(\kk, \UU) - \alpha J^*(\UU) \leq 0\right]
  \approx \Prob_{\UU}\left[\Prob_Z\left[\Delta_{\alpha} \leq
      0\right]\right]$
\end{frame}

\begin{frame}{Improving the plug-in estimation}
  Use sequential strategies to improve the estimation of $\Delta_{\alpha}$ or $\Xi$
\end{frame}

\begin{frame}{Joint space or objective-oriented exploration}
  Because of $J^*(\uu)$, it is often not enough to select the point
  where the uncertainty is high.  Generally, two main approaches can
  be considered
  \begin{itemize}
  \item Estimate the region
    $\{(\kk, \uu) \mid J(\kk,\uu) \leq \alpha J^*(\uu)\}$, then use
    the surrogate as a plug-in estimate to compute and maximize
    $\Gamma_{\alpha}$

    $\rightarrow$ reduce uncertainty on the whole space
  \item Select a candidate $\tilde{\kk}$, such that uncertainty on the
    estimation of $\Gamma_{\alpha}(\tilde{\kk})$ is reduced

    $\rightarrow$ reduce uncertainty on
    $\{\tilde{\kk}\}\times\Uspace$, with $\tilde{\kk}$ well-chosen.
  \end{itemize}
\end{frame}
% \begin{frame}
%   \frametitle{Surrogates and dimension reduction}
%   \begin{itemize}
%   \item Sensitivity
%     analysis~\cite{sudret_global_2008,le_gratiet_metamodel-based_2016}:
%     Based on intensive computation of the metamodel, or analytic
%     computation based on coefficients of the expansion computed
%   \item Isotropic by groups
%     kernels~\cite{blanchet-scalliet_specific_2017,ribaud_krigeage_2018-1}:
%     Group variables to have a few isotropic kernels
%   \end{itemize}

% \end{frame}
\section{Calibration of a numerical model: Application to CROCO}
\begin{frame}{The numerical model CROCO}
  \begin{columns}
    \begin{column}{0.55\textwidth}
     {
      \textbf{C}oastal and \textbf{R}egional \textbf{O}cean \textbf{CO}mmunity model
      \begin{itemize}
      \item Based on ROMS and AGRIF
      \item Solves the Shallow Water equations
      \item Grid resolution of 1/\ang{14} (\SI{5.5}{\kilo\metre})
      \item \num{15684} cells located in the ocean
      \end{itemize}
    }
\end{column}
\begin{column}{0.45\textwidth}
      \includegraphics<1>[width=\textwidth]{\manupath Chapter5/img/depth_maps_log_sserif.png}
      \includegraphics<2>[width=\textwidth]{\manupath Chapter5/img/sediments_reduced.png}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Modelling of the bottom friction}
  \begin{block}{Quadratic friction coefficient}
    \begin{equation}
    \tau_b = -C_d \|v_b \| v_b, \quad\text{ with }\quad C_d = \left(\frac{\kappa}{\log\left(\frac{H}{\alert{z_b}}\right) - 1 }\right)^2
  \end{equation}
  and we define the control parameter $\kk = \log z_b$
  \end{block}
\end{frame}

\begin{frame}{SA on sediments}
\inputpgf[0.5\textwidth]{\manupath Chapter5/img/SA_sediments.pgf}
\end{frame}


% \begin{frame}
%   \frametitle{Application to CROCO: Dimension reduction}
%   \begin{center}
%     \renewcommand\rmfamily{\sffamily}
%     \begin{columns}
%       \begin{column}{.5\textwidth}
%         \resizebox{\textwidth}{!}{\includegraphics{\manupath
%             Chapter5/img/depth_repartition.pdf}}
%       \end{column}
%       \begin{column}{.5\textwidth}
%         \resizebox{\textwidth}{!}{\input{\manupath
%             Chapter5/img/SA_croco.pgf}}
%       \end{column}
%     \end{columns}
%   \end{center}
%   Ad-hoc segmentation according to the depth, and sensitivity
%   analysis: only the shallow coastal regions seem to have an
%   influence.
% \end{frame}
\begin{frame}
  \frametitle{Robust optimization}
  \begin{center}
    \begin{columns}
      \begin{column}{.5\textwidth}
        % \includegraphics[width=\textwidth]{\manupath
        % Chapter5/img/gaussian_english_channel.png}
      \end{column}
      \begin{column}{.5\textwidth}
        \includegraphics[width=\textwidth]{croco.png}
      \end{column}
    \end{columns}
  \end{center}

  \begin{itemize}
  \item $\UU\sim \mathsf{U}[-1, 1]$ uniform r.v.\ that models the
    percentage of error on the amplitude of the M2 component of the
    tide
  \item The ``truth'' ranges from $8$mm to $13$mm.
  \item $11.0$mm leads to a cost which deviates less than $1\%$ from
    the optimal value with probability $0.77$
  \end{itemize}
\end{frame}



\section{Conclusion}

\begin{frame}[label=conclusion]{Conclusion}
  
  \begin{block}{Wrapping up}
    \begin{itemize}
    \item Problem of a \emph{good} definition of robustness
    \item Tuning $\alpha$ or $\eta$ reflects risk-seeking or
      risk-adverse strategies
    \item Strategies rely heavily on surrogate models, to embed
      aleatoric uncertainties directly in the modelling
    \end{itemize}
  \end{block}


  \begin{block}{Perspectives}
    \begin{itemize}
    \item Cost of computer evaluations $\rightarrow$ limited number of
      runs?
    \item In low dimension, CROCO very well-behaved.
    \item Dimensionality of the input space $\rightarrow$ reduction of
      the input space?
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}[allowframebreaks]{References}
  \renewcommand{\bibsection}{}
\bibliographystyle{authordate1}
  \bibliography{/home/victor/acadwriting/bibzotero.bib}
\end{frame}

\appendix

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
