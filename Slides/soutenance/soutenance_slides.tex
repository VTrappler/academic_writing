\documentclass[10pt,aspectratio=169,usepdftitle=false]{beamer}


\usetheme{metropolis}
\makeatletter
\setbeamertemplate{headline}{%
  \begin{beamercolorbox}[colsep=1.5pt]{upper separation line head}
  \end{beamercolorbox}
  \begin{beamercolorbox}{section in head/foot}
    \vskip2pt\insertnavigation{\paperwidth}\vskip2pt
  \end{beamercolorbox}%
  \begin{beamercolorbox}[colsep=1.5pt]{lower separation line head}
  \end{beamercolorbox}
}
\makeatother
\setbeamercolor{section in head/foot}{fg=normal text.bg, bg=structure.fg}

\AtBeginSection{
  \frame{
    \sectionpage
    \tableofcontents[currentsection]
  }
}

\setcounter{tocdepth}{2}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\newcommand\hmmax{0} % default 3
\newcommand\bmmax{0} % default 4
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{siunitx}
%% Fig mgmt ---------------------------------------
\graphicspath{{../Figures/}}
\usepackage{adjustbox}
\usetikzlibrary{positioning}
\usepackage{subfig}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\newcommand\manupath{/home/victor/acadwriting/Manuscrit/Text/}


%% Bib mgmt ---------------------------------------
\usepackage[authoryear,square]{natbib}
% \setcitestyle{square,aysep={},yysep={;}}

%% New commands --------------------------------------- 
\newcommand{\Uspace}{\mathbb{U}}
\newcommand{\Kspace}{\Theta}
\newcommand{\Xspace}{\mathbb{X}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\GP}{\mathsf{GP}}
\newcommand{\Prob}{\mathbb{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Cov}{\textsf{Cov}}
\newcommand{\tra}{\mathrm{tr}}
\newcommand{\yobs}{y}
% \newcommand{\yobs}{\bm{y}^{\mathrm{obs}}}
\newcommand{\kest}{\hat{\bm{k}}}
\newcommand{\kk}{\theta}
\newcommand{\uu}{u}
\newcommand{\UU}{U}
\DeclareMathOperator*{\KL}{\textsf{KL}}

\newcommand{\inputpgf}[2][\textwidth]{
  \renewcommand\rmfamily{\sffamily}
  \resizebox{#1}{!}{\input{#2}}}



%% Pdfpc notes ---------------------------------------
\usepackage[duration=45, lastminutes=5]{pdfpcnotes}



\definecolor{blkcol}{HTML}{E1E1EA}
\definecolor{blkcol2}{RGB}{209, 224, 224}

\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}
\definecolor{halfgray}{gray}{0.55}
\definecolor{webgreen}{rgb}{0,.5,0}
\definecolor{webbrown}{rgb}{.6,0,0}
\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\definecolor{Black}{cmyk}{0, 0, 0, 0}
\definecolor{blueGreen}{HTML}{4CA6A7}
\definecolor{blueGreenN2}{HTML}{3964B4}
% Using colorbrewer, 1 is lightest colour, 4 is darkest
\definecolor{brewsuperlight}{HTML}{F0F9E8}
\definecolor{brewlight}{HTML}{BAE4BC}
\definecolor{brewdark}{HTML}{7BCCC4}
\definecolor{brewsuperdark}{HTML}{2B8CBE}


%%% VICTOR COLORS ----------------------------------------
\colorlet{cfgHeaderBoxColor}{brewsuperdark} %{green!50!blue!70}
\colorlet{cfgCiteColor}{RoyalBlue} % vLionel : RoyalBlue
\colorlet{cfgUrlColor}{RoyalBlue}  % vLionel : RoyalBlue
\colorlet{cfgLinkColor}{RoyalBlue} % vLionel : blueGreenN2

\setbeamercolor{block title}{bg = blkcol}
\setbeamercolor{block body}{bg = blkcol!50}
\setbeamerfont{author}{size=\footnotesize}

\usepackage{appendixnumberbeamer}
\metroset{progressbar=foot,
  numbering=fraction, subsectionpage=none}




\title{Parameter control in the presence of uncertainties}
\subtitle{PhD Defense, June 11th 2021}
\author[Victor Trappler]{%
  % \begin{center}%
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{center}
        {\Large \textbf{Victor Trappler} \vspace{.0cm}}%
      \end{center}
    \end{column} 
    \begin{column}{0.5\textwidth}
      \begin{center}
    \begin{tabular}{rrr}
      \textit{Advisors}: & Élise Arnaud  & Univ. Grenoble Alpes\\
                & Laurent Debreu & Inria Grenoble\\
                & Arthur Vidard & Inria Grenoble\\
          \textit{Jury}: & Youssef Marzouk & MIT\\
                & Pietro Congedo & Inria Paris Saclay\\
                & Olivier Roustant & INSA Toulouse\\
                & Rémy Baraille & SHOM
    \end{tabular}
  \end{center}
  \end{column} 
\end{columns}
% \end{center}%
}


\institute{%
  % \hrulefill
  \begin{center}
      \includegraphics[height=30pt]{INRIA_SCIENTIFIQUE_UK_CMJN}
    \hspace{1cm}
        \includegraphics[height=30pt]{ljk}
        \hspace{1cm}
        \includegraphics[height=30pt]{logo_UGA.pdf}
      \end{center}
      % \hrulefill%
}

% \date{\textbf{PhD Defence, Grenoble, June 11th 2021}}
%   % {\bf } \today
% }
\date{}
\setcounter{tocdepth}{3}

\begin{document}
\AtBeginSection{}
\begin{frame}
  \pnote{Thank you for the introduction, and thank you to the jury for
    being here today.

    My name is Victor Trappler, and I'm going to defend my PhD, which is named ``Parameter control in the
    presence of uncertainties''

    This work has been realized in the AIRSEA team, here in Grenoble,
    under the supervision of Élise Arnaud, Arthur Vidard and Laurent
    Debreu}
  \maketitle
\end{frame}

\section{Introduction}

\begin{frame}{Why do we need models ?}
  \pnote{the Reality is often very complex to understand, so it is
    even more difficult to be able to predict, and take decisions
    which have environmental, societal or economic impact. A telling
    recent example is the propagation of a well-known virus.

    
    As you cannot take into account and track the behaviour of every
    single person, a lot of things have to simplified. This is where
    mathematics comes into play, allowing to express and to study the
    relationships between observable quantities

    Based on those imperfect representations, but simpler
    representation, we can then simulate natural phenomena, and
    forecast them }

  The ability to understand is
  essential in order to forecast, and to take decisions.
  \begin{itemize}
  \item Natural phenomenon are often very complex to understand in their entirety
  \item Mathematical models are \alert{simplified} versions, which
    allow to study the relationships between some observed (or not) quantities
  \item Mathematical models can be used to construct numerical
    models, which are used for forecasts
  \end{itemize}
  % \includegraphics[scale=0.015]{france_satellite.jpg}
\end{frame}

% \begin{frame}{Parametrization}

%   \begin{itemize}
%     \item Physical parameters, which represent some physical properties
%     that influence the model. 
%   \item Additional parameters, introduced for numerical reasons
%   \end{itemize}

%   % Does reducing the error on the parameters leads to the compensation
%   % of the unaccounted natural variability of the physical processes ?

  
%   \pnote{During the whole process of the modelling of a physical
%     system, that is from the observation of a natural phenomenon, to
%     the simulation using numerical methods, we introduce
%     uncertainties.  Those uncertainties take the form of errors
%     introduced by the simplifications, discretizations and
%     parametrizations needed to represent things numerically.

%     In the end, we have a set of parameters, that we need to
%     calibrate, but during this phase of calibration, how can we be
%     sure that we try to act only the error due to the parametrization,
%     and are not compensating errors coming from others sources.}
  
% \end{frame}

\begin{frame}{An example of simplification: parametrization}
  A lot of choices/simplifications are required to construct models, which depends on
  the scale we wish to represent: 

  \begin{block}{Example: the bottom friction}
    \begin{itemize}
    \item Bottom of the ocean is not completely smooth
    \item Energy is dissipated through turbulences because of the asperities
    \item The water current at the surface is affected
    \end{itemize}
  \end{block}
  \begin{center}
    \scalebox{.9}{\input{../Figures/hauteureau.tikz}}
  \end{center}
\end{frame}

\begin{frame}{The modelling process}
  \begin{center}
    \begin{adjustbox}{clip,
        trim=33cm 0cm 0cm 0cm,
        max width=0.9\textwidth, center}
    \input{\manupath Chapter3/img/modelling_uncertainties.pgf}
    \end{adjustbox}
  \end{center}

  $\rightarrow$ How well can we tune the parameters, so that the model depicts \emph{accurately} the reality ?
\end{frame}



\AtBeginSection{
  \frame{
    \sectionpage
    \tableofcontents[currentsection,currentsubsection]% ,hideothersubsections, 
    % sectionstyle=show/hide, 
    % subsectionstyle=show/shaded]
  }
}

\begin{frame}[t]{Computer code and inverse problem}
  \begin{itemize}
  \item[Input]
    \begin{itemize}
    \item $\kk$: Control
      parameter%Bottom friction (spatially distributed)
    \item $\uu$: Environmental variables (fixed and known)
    \end{itemize}
  \item[Output] \begin{itemize}
    \item $\mathcal{M}(\kk,\uu)$: Quantity to be compared to
      observations%Sea surface height, at predetermined time of the
                  %simulation and at certain location
    \end{itemize}
  \end{itemize}
  \vfill
  % \only<1>{\input{../Figures/comp_code.pgf}}
  \only<1>{
    \begin{center}
      \input{../Figures/inv_prob.pgf}
    \end{center}
  } \pnote{In quite a classical setting, we assume that we have a
    model M, that takes two inputs: Θ the control variable, that we
    aim at calibrating, and u some environmental variables, that we
    consider fixed and knowns.  We wish to calibrate the model wrt to
    some observations yobs }
\end{frame}

\begin{frame}{Data assimilation framework}
  % We have
  % $\yobs = \mathcal{M}(\kk_{\mathrm{obs}},\uu_{\mathrm{obs}})$ with
  % $\uu_{\mathrm{obs}} = \uu$
  Let $\uu \in \Uspace$, assumed fixed and known
  \begin{block}{Objective function}
    We define $J$ as the squared difference between the output of the
    model and the observations
    \begin{equation}
      J(\kk, \uu) =  \frac12 \|\mathcal{M}(\kk,\uu) - \yobs \|^2
    \end{equation}
    \alert<1>{$\rightarrow$ the smaller $J$ is, the better the fit is}
  \end{block}
\onslide<2->{We can get an estimate by solving an optimisation problem:
  \begin{equation}
    \min_{\kk\in\Kspace} J(\kk, \uu) = J(\hat{\kk},\uu)
  \end{equation}
  % \begin{enumerate}[$\rightarrow$]
  % \item Deterministic optimization problem
  % \item Possibly add regularization
  % \item Classical methods: Adjoint gradient and Gradient-descent
  % \end{enumerate}
  \begin{itemize} %A: Mettre en valeur
  \item \alert{$\hat{\kk}$ depends inherently on $\uu$}
  \item What if $\uu$ does not reflect accurately the observations?
  \item Does $\hat{\kk}$ then compensate the errors brought by this random
    misspecification? ($\sim$overfitting)
    % \item How well will $\bm{\hat{k}}$ perform under other
    %   conditions?
  \end{itemize}
}
  \pnote{ In practice, we select a likely value of the environmental
    parameters u, and Using the least square approach, we define J, a
    cost function, as the sum of the squares of the difference between
    the observations and the output of the numerical model.


    This is a deterministic optimisation problem, that we can solve
    using classical methods such as adjoint gradient.

    But what if the u we chose is not quite the same as the one of the
    observations. The minimisation procedure is supposed to correct
    the error on theta but will it try to compensate too much ? }
\end{frame}
\begin{frame}{Misspecification of $u$: twin experiment setup }
  
  Minimization performed on $\log z_b = \kk\mapsto J\left(\kk,\uu\right)$, for
  different $\uu$:
  $\uu=\only<1>{(0.0, 0.0)}%
  \only<2>{(0.0, 0.5)}%
  \only<3>{(0.0, 1.0)}%
  \only<4>{(0.5, 0.0)}%
  \only<5>{(0.5, 0.5)}%
  \only<6>{(0.5, 1.0)}%
  \only<7>{(1.0, 0.0)}%
  \only<8>{(1.0, 0.5)}%
  \only<9>{(1.0, 1.0)}%
  $
  \vfill % \only<2>{Well-specified model} \only<3>{1\%
  %   error on the amplitude of the M2 tide} \only<4>{1\% error on the
  %   amplitude of the M2 tide}
  % % \only<5>{0.5\% error on the amplitude of the M2 tide, starting at
  % % the truth}
  \begin{center}
    \includegraphics<1>[scale=.9]{/home/victor/optimisation_dahu/1b/map_lognorm_200.png}%
    \includegraphics<2>[scale=.9]{/home/victor/optimisation_dahu/2b/map_lognorm_200.png}%
    \includegraphics<3>[scale=.9]{/home/victor/optimisation_dahu/3b/map_lognorm_200.png}%
    \includegraphics<4>[scale=.9]{/home/victor/optimisation_dahu/4b/map_lognorm_200.png}%
    \includegraphics<5>[scale=.9]{/home/victor/optimisation_dahu/optim_sediments/map_lognorm_200.png}%
    \includegraphics<6>[scale=.9]{/home/victor/optimisation_dahu/6b/map_lognorm_200.png}%
    \includegraphics<7>[scale=.9]{/home/victor/optimisation_dahu/7b/map_lognorm_200.png}%
    \includegraphics<8>[scale=.9]{/home/victor/optimisation_dahu/8b/map_lognorm_200.png}%
    \includegraphics<9>[scale=.9]{/home/victor/optimisation_dahu/9b/map_lognorm_200.png}%
  \end{center}
  \pnote{ To have a first look at the problem of misspecification, we
    try to calibrate the bottom friction on this domain in a twin
    experiment setup, with or without misspecification. This figures
    shows the truth value of the bottom friction.

    
    When optimizing without misspecification, we can see that the
    region of the english channel is able to retrieve the truth value,
    while the regions farther from the coast, in the bay of biscay,
    stay quite unaffected by the procedure. So far so good.

    When we introduce a 1\% error in the amplitude of the M2 tide, we
    can see more variations, especially in the english channel, as it
    compensate the misspecification of the tide.

    The estimation of theta is sensitive to the value of the
    environmental parameter. So the question that arise is ``how to
    get a value of theta, which shows robust properties when the
    environmental parameter varies ?''  }
\end{frame}

\begin{frame}{Robustness}
  \pnote{ So that is how we define robustness in this work: So
    basically, we have two main objectives: - First to find some
    criteria of robustness to estimate theta - Be able to compute
    those estimates quickly }
  
  \begin{block}{Robustness}
    $\hat{\kk}$ can be considered ``robust'', if $J(\hat{\kk}, \uu)$
    gives ``good enough'' performances when $\uu$ varies
  \end{block}

  \textbf{Main objectives:}
  \begin{itemize}
  \item Define quantitative criteria of robustness
  \item Develop methods in order to compute robust estimates efficiently
  \item Apply those methods to the robust calibration of CROCO 
  \end{itemize}
\end{frame}


%%%%

\begin{frame}{Outline}
  \tableofcontents
  \pnote{We are first going to see the general setting of calibration
    problems, and then how to define robustness in this
    context. Finally, we are going to see how to tackle this problem
    in practice using surrogate models }
\end{frame}

\section{Robustness in calibration}
\begin{frame}{Different types of uncertainties}
  \begin{block}{Epistemic or aleatoric
      uncertainties?~\citep{walker_defining_2003}}
    \begin{itemize}
    \item Epistemic uncertainties: From a lack of knowledge, that can
      be reduced with more research/exploration
    \item Aleatoric uncertainties: Inherent variability of the system
      studied, operating conditions that we cannot afford to research
    \end{itemize}
  \end{block}
  $\rightarrow$ But where to draw the line?

  Our goal is to take into account the aleatoric uncertainties in the
  estimation of our parameter.  \pnote{ We evoked earlier that the
    uncertainty on theta or on u is not the same, and we can make a
    rough distinction between two types:

    - First, the epistemic uncertainties that result from a lack of
    knowledge, but can be reduced. An example is the uncertainty
    during the estimation of the mean value. The more samples you
    take, the less uncertainty there is on your estimation

    - Secondly, there is the aleatoric uncertainty, that comes from
    the inherent variability of the system studied. Think of the
    different values that a random variable takes.

    Our goal, is then to be able to reduce the epistemic uncertainty
    on the value of theta, while taking into account the aleatoric
    uncertainty.  }
\end{frame}

\begin{frame}[t]{Aleatoric uncertainties}
  Instead of considering $\uu$ fixed, we consider that $\uu\sim \UU$  r.v.\ (with known pdf $\pi(\uu)$), and the output of the model depends on its realization. \\
  \vfill \only<1>{
    \begin{center}
      \input{../Figures/inv_prob.pgf}%
    \end{center}
  }%
  \only<2>{
    \begin{center}
      \input{../Figures/comp_code_unc_inv.pgf}
    \end{center}
  } \vfill


  \pnote{As hinted before, we are going to model the aleatoric
    uncertainty on u by a random variable.  The output of the model
    becomes a random variable }
\end{frame}

\begin{frame}{The cost function as a random variable}
  \begin{itemize}
  \item The computer code is deterministic, and takes $\kk$ and $\uu$
    as inputs (from the user):
    \begin{equation*}
      \mathcal{M}(\kk,\alert{\uu})
    \end{equation*}
  \item Due to the previous assumptions, the quadratic error $J$ is
    now considered as a random variable, indexed by $\kk$
    \begin{equation*}
      J(\kk,\alert{\UU}) =  \frac12\|\mathcal{M}(\kk,\alert{\UU}) - \yobs\|^2
    \end{equation*}
  \end{itemize}

\pnote{ In our study, the model is completely deterministic, so we can
  control its inputs The cost function, becomes then a function of two
  theta and u. We still wish to minimise with respect to theta, but
  what can we do for u ?  }
\end{frame}




% \subsection{Definitions of robustness}
\begin{frame}{Robust objectives of the objective function}
  \pnote{ I'm going to present very quickly some estimates that can be
    considered robust, but will focus mainly on the last one.  First
    we can think about minimising in the worst case sense. This
    usually leads to overly conservative estimates as we are
    maximizing over the whole space U. We can also think about
    minimising the moments, such as the mean or the variance, or even
    combine them in a multiobjective setting by looking for the pareto
    front.


    Every choice of environmental variable gives a distinct situation
    The aspect we are going to focus on is based on the regret, so it
    implies a comparison with the best performance attainable for each
    u.  }
  \begin{itemize}
    % \item Global Optimum: $ \min_{(\kk,\uu)} J(\uu,\kk)$
    %   $ \longrightarrow $ EGO
  \item Worst case~\citep{marzat_worst-case_2013}:
    $$ \min_{\kk \in \Kspace} \left\{\max_{\uu \in \Uspace}
      J(\kk,\uu)\right\}$$
  \item M-robustness, V-robustness~\citep{lehman_designing_2004}:
    $$\min_{\kk\in\Kspace} \Ex_{\UU}\left[J(\kk,\UU)\right], \min_{\kk\in\Kspace} \Var_{\UU}\left[J(\kk,\UU)\right]$$
  \item Multiobjective~\citep{baudoui_optimisation_2012}:
    $$\text{Pareto frontier of } (\Ex_{\UU}\left[J(\kk,\UU)\right],\Var_{\UU}\left[J(\kk,\UU)\right])$$
    \item Reliability
      $$ \min_{\kk\in\Kspace} Q_{\UU}(J(\kk, \UU); p) $$
  % \item Region of failure given by
  %   $J(\kk,\uu)>T$~\cite{bect_sequential_2012}:
  %   $$\max_{\kk \in \Kspace} R(\kk) = \max_{\kk\in \Kspace}
  %   \Prob_{\uu}\left[J(\kk,\uu) \leq T \right]$$
\item \alert{Best performance achievable given $\uu \sim \UU$}
\end{itemize}
\end{frame}


\begin{frame}{Introducing the regret}%
  \pnote{ The main idea is that we want to consider individually all
    situations induced by the value of the environmental variable.
    Basically, once a value u is sampled, the problem is
    deterministic, so under some assumption, we have a minimiser theta
    star that is a function of u


    Keeping in mind the random nature of U, we can define the random
    variable thetastar, and its density (if it is defined), can be
    seen as the frequency of which a value theta is optimal.


    That is an interesting information, but we can have a little more
    than that. We may want to include theta that yield values of the
    cost function close to a minimum. To do that, we introduce a
    relaxation of the equality constraint with alpha, so that for a
    given u, we consider acceptable the theta that give values of the
    cost function between Jstar, the optimal value and alpha times
    Jstar So finally, we compute the probability that this given theta
    is acceptable with respect to the level alpha }

  Given $\uu \sim \UU$, the optimal value is $J^*(\uu)$, attained at
  $\kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk,\uu)$.  \pause
  % \begin{columns}
  %   \begin{column}{0.6\textwidth}

  
  The minimizer can be seen as a random variable:
  \begin{equation*}
    \kk^*(\UU) = \argmin_{\kk\in\Kspace} J(\kk,\alert<1>{\UU})
  \end{equation*}
  $\longrightarrow$ estimate its density (how often is the value $\kk$
  a minimizer)
  \begin{align*}
    p_{\kk^*}(\kk) &= "\Prob_{\UU}\left[J(\kk,\UU)= J^*(\UU) \right]"
  \end{align*}
  \pause In order to take into account values \alert{nearly} optimal

  $\longrightarrow$ relaxation of the equality with
  $\alpha> 1$:
  \begin{equation*}
    \Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk,\UU) \leq \alpha J^*(\UU) \right]
  \end{equation*}
\end{frame}

 
\begin{frame}{Construction of regions of acceptability}
  \pnote{ What does it look like on a concrete example. We have the
    plot of a cost function, where theta is the x axis, and u is on
    the y axis.

    As said earlier, for each horizontal cross section, so for u
    fixed, we compute the minimiser, theta star of u.

    We can then compute the whole set of the conditional minimisers

    Now, we set alpha: inside the yellow lines, we are between the
    minimum and alpha times the minimum

    Finally, we construct and measure for each theta the probability
    to be within this acceptable region.  Great, now we just have to
    know how to choose
    alpha. % Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile.
  }
  \begin{columns}
    \begin{column}{0.45\textwidth}
      \vfill
      \only<1>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_1.pgf}}}%
      \only<2>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_2.pgf}}}%
      \only<3>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_3.pgf}}}
      \only<4>{\resizebox{\linewidth}{!}{\input{/home/victor/Bureau/tmp/relaxation_tuto_4.pgf}}}%
      \vfill
    \end{column}
    \begin{column}{.6\textwidth}
      \begin{itemize}
      \item<1-> Sample $\uu\sim\UU$, and solve
        $\kk^*(\uu) = \argmin_{\kk\in\Kspace} J(\kk,\uu)$
      \item<2->Set of conditional minimisers:
        $\{(\kk^*(\uu), \uu) \mid \uu \in \Uspace\}$
      \item<3-> Set $\alpha \geq 1$
      \item<4>
        $R_{\alpha}(\kk) = \{\uu \mid J(\kk,\uu) \leq \alpha J^*(\uu)
        \}$
      \item<4>
        $\Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[\UU\in
          R_{\alpha}(\kk) \right]$
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}{Regret-based estimates}
  \pnote{Great so now we have Gamma(theta) which is the probability
    that theta gives a cost alpha acceptable If we have an idea of a
    threshold we don't want to exceed, so if alpha is known we can
    maximize the probability of being alpha acceptable

    Or, on the other hand, as gamma is a probability, we can look for
    the smallest relaxation, where the probability of acceptability
    reaches a certain confidence 1-eta.

    We can then define the family of relative-regret estimators, which
    are the maximizers of such a probability of being alpha
    acceptable.  Depending on the approach, we can nudge toward
    optimal performances with small alpha, or risk adverse preference,
    by setting a bigger relaxation.  }  $\Gamma_{\alpha}(\kk)$:
  probability that the cost (thus $\kk$) is $\alpha$-acceptable
  \begin{itemize}
  \item If $\alpha$ known, \alert<1>{maximize the probability that $\kk$ gives
    acceptable values}:
    \begin{equation}
      \max_{\kk\in\Kspace} \Gamma_{\alpha}(\kk) = \max_{\kk\in\Kspace}\Prob_{\UU}\left[J(\kk, \UU) \leq \alpha J^*(\UU)\right]
    \end{equation}
  \item Set a target probability $p$, \alert<2>{find the smallest $\alpha$ such the probability $p$ is reached}
    \begin{equation}
      \inf\{ \alpha \mid \max_{\kk\in\Kspace}\Gamma_{\alpha}(\kk) \geq p \}
    \end{equation}
  \end{itemize}

  \begin{block}{Relative-regret family of estimators \citep{trappler_robust_2020}}
    \begin{equation}
      \left\{ \hat{\kk} \mid \hat{\kk} = \argmax_{\kk \in \Kspace} \Gamma_{\alpha}(\kk), \alpha>1 \right\}
    \end{equation}
    Depending on the formulation:
    \begin{itemize}
    \item Maximization of a probability if $\alpha$ fixed:
      $\Prob_{\UU}\left[J(\kk, \UU) \leq \alpha J^*(\UU)\right]$
    \item Minimization of a quantile of order $p$:
      $Q_{\UU}(J(\kk, \UU)/J^*(\UU);p)$
    \end{itemize}
  \end{block}

\end{frame}


\begin{frame}{Choice of $\alpha$}
  
\end{frame}



\section{Adaptive strategies using Gaussian Processes}
\begin{frame}{The computational bottleneck}
  \pnote{We now have a family of estimators, but in practice, finding
    them is expensive, as it requires probability estimation, and
    various optimisations, that is why we are going to use surrogates
    to tackle this problem}
  In general, getting estimates can be very expensive:
  \begin{itemize}
  \item \alert{Estimate} statistical quantities ($\Ex_{\UU}$, $\Prob_{\UU}$)
    \begin{itemize}
    \item[$\rightarrow$] Sufficient exploration of $\Uspace$ with
      respect to $\Prob_{\UU}$ (Monte-Carlo methods, numerical
      integration of integrals)
 \end{itemize}
\item \alert{Optimize} those quantities with respect to $\kk\in\Kspace$
  \begin{itemize}
  \item[$\rightarrow$] Focus on regions of interest of $\Kspace$
  \item[$\rightarrow$] Take into account the uncertainty on the estimation
  \end{itemize}
    \end{itemize}
  $\Rightarrow$ requires a lot of computational effort (\emph{i.e.} extensive number of calls to $J$)
\end{frame}

\begin{frame}{Surrogates and cost function}
  Given a set comprising points and their evaluations
  $\mathcal{X} = \{(x_i,f(x_i))\}_{1 \leq i \leq n}$ (\textit{training set}), we can construct
  an \alert{approximation} of the expensive function $J$

  
  $\rightarrow$ Polynomial interpolation, \alert<2>{Gaussian Process Regression (Kriging)}, Polynomial Chaos Expansion
\onslide<2>{\begin{itemize}
  \item It replaces the expensive original function $J$ by a computationally cheap surrogate
    ($\sim$ plug-in approach)
  \item It can be adapted for sequential strategies
  \end{itemize}
}  % \item Polynomial Chaos
  %   Expansion~\cite{xiu_wiener--askey_2002,sudret_polynomial_2015}
  % \end{itemize}
  \pnote{We choose Gaussian Process regression as a metamodel, as it
    can replace cheaply the computer code}
\end{frame}

\begin{frame}{Gaussian Process Modelling}
  \begin{columns}
    \begin{column}{0.5\textwidth}   
  Let $x = (\kk, \uu) \in \Kspace \times \Uspace = \Xspace$, $f(x)= f((\kk, \uu)) = J(\kk, \uu)$
  $\mathcal{X} = \left\{ (x_i, f(x_i)) \right\}_{1 \leq i \leq N}$ initial design of experiments ($\sim$ training points)
  \begin{block}{GP regression \citep{matheron_traite_1962,krige_statistical_1951}}
    $Z \sim \GP\left(m_Z,C_Z\right)$ is the GP constructed on $\mathcal{X}$
    with $m_Z: \Xspace \rightarrow \mathbb{R}$ and $C_Z: \Xspace^2 \rightarrow \mathbb{R}$
    \begin{itemize}
    \item $m_Z$: GP (or kriging) regression
    \item $C_Z$: covariance function
    \item  $\sigma_Z^2: x \mapsto C_Z(x, x)$ variance function
    \item \alert<2>{$Z(x)\sim \mathcal{N}\left(m_Z(x), \sigma^2_Z(x)\right)$}
    \end{itemize}
  \end{block}
  \end{column}
    \begin{column}{0.5\textwidth}
    \begin{center}
      \inputpgf{\manupath Chapter4/img/example_GP.pgf}
  \end{center}
  \onslide<2>\begin{itemize}
  \item[$\rightarrow$] Information on $J$ using $m_Z$
  \item[$\rightarrow$] Information on prediction error with $\sigma_Z^2$
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Adaptive strategies}
    Adding and evaluating points to the design iteratively, with
    respect to some particular objective
  \begin{itemize}
  \item Explore the input space
  \item Optimize the unknown function $f$
  \item Level-sets or excursion sets: $\{x \in \Xspace \mid f(x) \gtreqless T\}$
  \end{itemize}
  From a design comprising $n$ points $\mathcal{X}_n = \{(x_i, f(x_i))\}_{1 \leq i \leq n}$
  \begin{itemize}
  \item Construct GP of interest using $\mathcal{X}_n$
  \item Define a criterion $\kappa:\Xspace \rightarrow \mathbb{R}$
    which quantify the interest of evaluating a specific point
    \begin{itemize}
    \item Choose the next point through optimization (1-step)
    \item Choose the next point\alert{s} by sampling and clustering (AK-MCS)
    \end{itemize}
  % \begin{itemize}
  % \item Optimize the criterion and add the optimizer to the design
  % \item Sample and cluster according to select a batch of points to evaluate
  % \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{1-step criteria}
  Let $\kappa$ be such a criterion:
  \begin{itemize}
  \item $\kappa$ is constructed using the properties of the GP
  \item $\kappa(x)$ measures how \emph{interesting} would be the
    evaluation of $x$
  \end{itemize}
  \begin{block}{Selection of the next point and update of the design}
      Given a GP $Z$, constructed using
      $\mathcal{X}_n = \{(x_i, f(x_i))\}_{1\leq i \leq n}$
      \begin{align}
    x_{n+1} &= \argmax_{x \in \Xspace} \kappa(x) = \argmax_{x \in \Xspace} \kappa(x; \alert{Z; \mathcal{X}_n}) \\
    \mathcal{X}_{n+1} &= \mathcal{X}_n \cup \{(x_{n+1},f(x_{n+1})) \}
  \end{align}
\end{block}
\pause
  Different criteria caters to different objectives
  \begin{itemize}
  \item Optimization: PI, EGO \citep{jones_efficient_1998,hernandez-lobato_predictive_2014}
  \item Exploration: prediction variance, aIMSE
  \item Contour/levelsets estimation: reliability index \citep{bect_sequential_2012,picheny_adaptive_2010}
  \end{itemize}
\end{frame}

\begin{frame}{Example: Estimation of $\kk^*(\uu)$, $J^*(\uu)$ with the PEI}
  \pnote{One of the first problem we encountered is the computation of
    the conditional minimum and minimisers.  Using Gaussian processes,
    and an enrichment criterion called the Profile Expected
    Improvement, we can reconstruct the Jstar and theta star function
    iteratively, and then use the kriging prediction as a surrogate
    since Jstar is well approximated}
  Recalling that: $\Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk, \UU) \leq \alert{J^*(\UU)}\right]$
  
  $\rightarrow$ Need to have a good approximation of the conditional minimum and minimisers:
  $\left\{
  \begin{array}{l}
    J^*(\uu) = \min_{\kk\in\Kspace}J(\kk, \uu)\\
    \kk^*(\uu) = \argmin_{\kk\in \Kspace}J(\kk, \uu)
  \end{array}
  \right.$ 
  \begin{columns}
    \begin{column}{0.55\textwidth}
      \begin{block}{PEI~\citep{ginsbourger_bayesian_2014}}
        Profile Expected Improvement:
        $\kappa(\kk, \uu) = \Ex_{Z(\kk, \uu)}\left[
          \left(f_{\min}(\uu) - Z(\kk, \uu)\right)_+\right]$ and close
        form available due to the GP properties of $Z$
      \end{block}
The design can be enriched iteratively using the PEI criterion, one point at a time
\end{column}
\begin{column}{0.45\textwidth}
  \begin{center}
    \inputpgf{\manupath Chapter4/img/PEI_example.pgf}
  \end{center}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Sampling-based methods} %
    Given $g$ $(= \kappa / \int \kappa)$ a sampling density on  $\Xspace$ 
  \begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{block}{Selection of the next $K$ points and update of the design}
      \begin{itemize}
      \item<2-> Sample points according to the sampling density $g$
      \item<3-> Cluster the samples into $K$ clusters (KMeans)
      \item<4-> For each cluster, select the sample which is closest to the cluster center
      \item<5-> Evaluate the chosen samples and update the design
      \end{itemize}
    \end{block}
    \end{column}
    \begin{column}{0.5\textwidth}
      Example: estimation of the level-set $\{J = 2\}$:
      $\kappa(x) =
      \left\{\begin{array}{l}
              1 \text{ if } \Prob_{Z(x)}[Z(x) < 2] \in [\eta, 1-\eta] \\
              0 \text{ elsewhere}
      \end{array} \right.$
      \foreach\y in{1,...,7}{
        \includegraphics<\y>[width=\textwidth]{AKMCS_\y.png}
      }
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}{GP of the ``penalized'' cost function}
  \pnote{What about the estimation of the probability of being alpha
    acceptable ?  As it is a linear combination of GP, we still have a
    Gaussian distribution at the end, and we can rewrite the
    probability of being acceptable as a probability of failure of the
    resulting GP, that we could consider to estimate using common
    techniques, level set, contour estimation.

    One interesting thing is in the decomposition of the variance of
    Delta alpha: two main sources of uncertainties are present: the
    uncertainty on the true value of the function at theta,u; and the
    true value of the minimizer.}
  % \begin{itemize}
  % \item Optimization of the $1-\eta$ quantile of $J/J^*$, or
  %   $J - J^*$
  %   \cite{razaaly_quantile-based_2020,quagliarella_optimization_2014}
  % \end{itemize}
  Let
  $Z \sim \GP\left(m_Z; C_Z\right) \text{ on } \Kspace \times \Uspace$, constructed using $\left\{\left(\kk_i,\uu_i), J(\kk_i, \uu_i)\right)\right\}$

  We define $Z^*(\uu)=Z(\kk^*_Z(\uu), \uu)$, with $\kk^*_Z(\uu) = \min_{\kk} m_Z(\kk, \uu)$,
  and $\Delta_{\alpha} = Z - \alpha Z^*$:
  \begin{block}{The GP $\Delta_\alpha$}
    As a linear combination of GP, $\Delta_{\alpha}$ is a GP as well:
  \begin{align}
    \Delta_{\alpha}(\kk, \uu) &\sim \GP\left(m_{\Delta_{\alpha}};C_{\Delta_\alpha}\right) \\
    m_{\Delta_\alpha}(\kk,\uu)   &= m_Z(\kk, \uu) - \alpha m_Z^*(\uu) \\
    \sigma^2_{\Delta_\alpha}(\kk, \uu) &= \sigma^2_{Z}(\kk, \uu) +  \alert<2>{\alpha^2\sigma^2_{Z^*}(\uu)}- 2\alpha C_Z\left((\kk, \uu), ({\kk}_Z^ *(\uu), \uu)\right)
  \end{align}
\end{block}
\end{frame}


\begin{frame}{Approximation of the ratio}
  Let us assume that $Z^*>0$ with high enough probability:
  $\Xi(\kk, \uu) = \log \frac{Z(\kk,\uu)}{Z^*(\uu)}$ is approximately normal
  \begin{block}{Log-normal approximation of the ratio of GP}
    \begin{align}
      \Xi(\kk, \uu) &\sim \mathcal{N}\left(m_\Xi(\kk,\uu), \sigma^2_\Xi(\kk, \uu) \right) \\
      m_\Xi(\kk, \uu) &= \log \frac{m_Z(\kk, \uu)}{m_Z^*(\uu)} \\
      \sigma^2_\Xi(\kk, \uu) &= \frac{\sigma^2_{Z}(\kk, \uu)}{m_{Z}(\kk,\uu)^2} + \alert<2>{\frac{\sigma^2_{Z^*}(\uu)}{m_{Z^*}(\uu)^2}} - 2 \frac{\Cov[Z(\kk, \uu), Z^*(\uu)]}{m_Z(\kk, \uu)m_{Z^*}(\uu)} 
    \end{align}
  \end{block}
\end{frame}

\begin{frame}{Computations using $\Delta_{\alpha}$ or $\Xi$}
  Recalling that $\Gamma_{\alpha}(\kk) = \Prob_{\UU}\left[J(\kk, \UU) - \alpha J^*(\UU) \leq 0\right] = \Prob_{\UU}\left[\frac{J(\kk, \UU)}{J^*(\UU)} \leq \alpha\right]$

  \begin{block}{Plug-in approximation}    
  \begin{itemize}
  \item $J$ is replaced by $m_Z$
  \item $J - \alpha J^*$ is replaced by $m_{\Delta_\alpha}$
    $\rightarrow$ $\alpha$ fixed, estimation of probability
  \item $\frac{J}{J^*}$ is replaced by $m_{\Xi}$ $\rightarrow$ $p$
    fixed, estimation of quantile
  \end{itemize}
    \begin{align}
      {\Gamma}% ^{\mathsf{PI}}
      _{\alpha}(\kk) % &\approx \Prob_{\UU}\left[m_Z(\kk, \UU) - \alpha m_{Z^*}(\UU) \leq 0\right] \\
                                           &\approx \Prob_{\UU}\left[m_{\Delta_{\alpha}}(\kk, \UU) \leq 0\right] \\
                                           &\approx \Prob_{\UU}\left[m_{\Xi}(\kk,\UU) \leq \log\alpha \right]
    \end{align}
    The outer probability can be approximated using Monte-Carlo since
    $m_{\Delta}$ and $m_\Xi$ are cheaper to evaluate than $J$.
  \end{block}
\onslide<2>{$\rightarrow$ We can use \alert{sequential strategies} to improve the estimation of $\Delta_{\alpha}$ or $\Xi$ and then improve the estimation of $\Gamma_{\alpha}(\kk)$}

  % Estimate the ``probability of
  % failure''~\cite{bect_sequential_2012,echard_ak-mcs_2011}
  % $\Prob_{\UU}\left[J(\kk, \UU) - \alpha J^*(\UU) \leq 0\right]
  % \approx \Prob_{\UU}\left[\Prob_Z\left[\Delta_{\alpha} \leq
  %     0\right]\right]$
\end{frame}

\begin{frame}{Improving the plug-in estimation}
  Let $\Phi \sim \GP(m_\Phi, C_\Phi)$, and associated variance $\sigma^2_{\Phi}$ ($\Phi= \Delta_{\alpha}$ or $\Xi$)

  
  Use sequential strategies to improve \alert{globally} the accuracy of $m_\Phi$.
  \begin{block}{Integrated Mean Square Error}
    $\mathsf{IMSE}(\mathcal{X}_n) = \int_{\Xspace}
    \sigma^2_{\Phi}(x)\,\mathrm{d}x$ is a measure of the uncertainty
    associated with the design $\mathcal{X}_n$ and the GP $\Phi$
  \end{block}
  After having chosen \emph{and evaluated} the next point $x_{+}$, we want the $\mathsf{IMSE}$ to be as small as possible
  \begin{equation}
    \min_{x_{+}} \onslide<3->{\Ex_{Z(x_+)}\bigg[}\mathsf{IMSE}\left(\mathcal{X}_n \cup \{(x_{+}, \only<1>{\underbrace{f(x_{+})}_{\text{unknown}}} \only<2->{\underbrace{Z(x_+)}_{\text{r.v.}}})\}\right) \onslide<3->{\bigg]} \onslide<3>{= \min_{x_+} \kappa(x_{+})}
  \end{equation}
  \onslide<3>{$x_{n+1}$ presents the smallest IMSE on average once evaluated}
\end{frame}

\begin{frame}{Objective-oriented exploration: 2-stages methods}
  
  Instead of reducing \emph{globally} the uncertainty, we can look directly to optimize $\Gamma_{\alpha}$
  \begin{itemize}
  \item Select a candidate $\tilde{\kk}$ with ``high potential'' to optimize $\Gamma_{\alpha}$ 
  \item $(\kk_{n+1},\uu_{n+1}) = \argmin \kappa((\kk, \uu); \tilde{\kk})$
  \end{itemize}
\end{frame}
% \begin{frame}
%   \frametitle{Surrogates and dimension reduction}
%   \begin{itemize}
%   \item Sensitivity
%     analysis~\cite{sudret_global_2008,le_gratiet_metamodel-based_2016}:
%     Based on intensive computation of the metamodel, or analytic
%     computation based on coefficients of the expansion computed
%   \item Isotropic by groups
%     kernels~\cite{blanchet-scalliet_specific_2017,ribaud_krigeage_2018-1}:
%     Group variables to have a few isotropic kernels
%   \end{itemize}

% \end{frame}
\section{Robust calibration: Application to CROCO}
\begin{frame}{The numerical model CROCO}
  \begin{columns}
    \begin{column}{0.50\textwidth}
     {
      \textbf{C}oastal and \textbf{R}egional \textbf{O}cean \textbf{CO}mmunity model
      \begin{itemize}
      \item Based on ROMS and AGRIF
      \item Solves the Shallow Water equations
      \item Grid resolution of 1/\ang{14} (\SI{5.5}{\kilo\metre})
      \item \num{15684} cells located in the ocean
      \item Twin experiment setup:
        \begin{itemize}
        \item Observation $y$ generated using the numerical model $\mathcal{M}$
        \item $J(\kk, \uu) = \frac{1}{2}\|\mathcal{M}(\kk, \uu) - y \|^2$
        \end{itemize}
      \end{itemize}
    }
\end{column}
\begin{column}{0.50\textwidth}
      \includegraphics<1>[width=\textwidth]{\manupath Chapter5/img/depth_maps_log_sserif.png}
      % \includegraphics<2>[width=\textwidth]{\manupath Chapter5/img/sediments_reduced_SA.png}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Modelling of the bottom friction}
  \begin{block}{Quadratic friction coefficient}
    \begin{equation}
    \tau_b = -C_d \|v_b \| v_b, \quad\text{ with }\quad C_d = \left(\frac{\kappa}{\log\left(\frac{H}{\alert{z_b}}\right) - 1 }\right)^2
  \end{equation}
  and we define the control parameter as $\alert{\kk = \log z_b}$
\end{block}
\begin{itemize}
\item The effect on the water circulation (through $\tau_b$) depends
  also on the water height $H$
\item We assume that the friction is uniform and constant for each of
  the sediment type
\end{itemize}
Due to the water height, influence of each sediment type is not the same $\Rightarrow$ Sensitivity analysis
\end{frame}

\begin{frame}{Sensitivity analysis on the friction associated with the sediments}
  Global sensitivity Analysis: Sobol'
  indices~\citep{sobol_global_2001,sobol_sensitivity_1993}: quantify
  the influence of each input variable on $J$
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \inputpgf[\textwidth]{\manupath Chapter5/img/SA_sediments_slides.pgf}
    \end{column}
    % \begin{overlayarea}{\textwidth}{\textheight}
    \begin{column}{0.5\textwidth}
      \only<1>{\includegraphics[width=\textwidth]{\manupath Chapter5/img/sediments_reduced_sserif.png}}%
      \only<2>{\includegraphics[width=\textwidth]{\manupath Chapter5/img/sediments_reduced_SA_sserif.png}}%
    \end{column}
% \end{overlayarea}
  \end{columns}
  \onslide<2>{
    The control variable is then $\kk = (\kk^{(1)}, \kk^{(2)}, \kk^{(3)})$ where
    \begin{itemize}
      \item $\kk^{(1)}$: Pebbles
      \item $\kk^{(2)}$: Gravels
      \item $\kk^{(3)}$: Other sediments (in deeper water)
      \end{itemize}}
\end{frame}

\begin{frame}{Environmental variables}
  \begin{itemize}
  \item In this case, we assume that
    $\uu \sim \mathcal{U}\left([0,1]^2\right)$
\item $\uu$ parametrizes the error on the amplitude of the $M_2$ and
  $S_2$ tide components.
\item $\uu^{\mathsf{truth}} = (0.5, 0.5)$ is the value used to generate the observations
\end{itemize}
In order to ensure strict positivity of $J^*$, design first enriched using the PEI criterion
\begin{center}
 \includegraphics[scale=0.45]{\manupath Chapter5/img/contour_Jstar_800_again.png}
\end{center}
Global minimum not attained at the truth value of the environmental parameter
\end{frame}



\begin{frame}{Numerical results}
  \begin{center}
  \begin{tabular}{rrr}\toprule
    Method & augmented IMSE & Qe-AK MCS \\ \midrule
    % Quantity of interest & $\Ex_{Z}[\int_{\Kspace\times\Uspace}\sigma^2_{\Delta_\alpha \mid Z}]$ & $\mathbb{M}_{\eta}(\mathfrak{q}_l)$ \\
    Type & 1-step & $K$-step \\
    Main Bottleneck &
                      \begin{tabular}{@{}r@{}}
                        Evaluate and optimise integral in \\
                        $(1 + \dim( \Kspace\times \Uspace))$ dimensions
                      \end{tabular}
                      & \begin{tabular}{@{}r@{}}
                          Sampling in unknown \\
                          regions in $\dim( \Kspace\times \Uspace)$
                        \end{tabular} \\
    Advantage & Optimal criterion of enrichment & $K$ chosen arbitrary \\ \midrule
    $\hat{\kk}_{\alpha}$, $\alpha=1.3$ & $(\num{-3.43},\num{-5.20},\num{-6.48})$  & $(\num{-3.38},\num{-5.05}, \num{-6.61})$ \\
    $\hat{\kk}_{\alpha_p}$, $p=0.95$ & $(\num{-3.39},\num{-5.28},\num{-6.50})$  & $(\num{-3.36},\num{-5.10}, \num{-6.63})$ \\
    $\hat{\kk}_{\mathsf{global}}$ & \multicolumn{2}{c}{$(-3.516, -5.078, -6.346 )$} \\
    ${\kk}^{\mathsf{truth}}$ & \multicolumn{2}{c}{$(-3.689, -4.962, \mathsf{n.a.})$} \\
    \bottomrule
  \end{tabular}
\end{center}
In this case:
\begin{itemize}
\item $\hat{\kk}^{(1)}> \hat{\kk}_{\mathsf{global}}^{(1)} > \kk^{\mathsf{truth}, (1)}$
\item $\hat{\kk}^{(2)} \lesssim \hat{\kk}_{\mathsf{global}}^{(2)} < \kk^{\mathsf{truth}, (2)}$
\item $\hat{\kk}^{(3)}< \hat{\kk}_{\mathsf{global}}^{(3)}$
\end{itemize}

\end{frame}
% \begin{frame}
%   \frametitle{Application to CROCO: Dimension reduction}
%   \begin{center}
%     \renewcommand\rmfamily{\sffamily}
%     \begin{columns}
%       \begin{column}{.5\textwidth}
%         \resizebox{\textwidth}{!}{\includegraphics{\manupath
%             Chapter5/img/depth_repartition.pdf}}
%       \end{column}
%       \begin{column}{.5\textwidth}
%         \resizebox{\textwidth}{!}{\input{\manupath
%             Chapter5/img/SA_croco.pgf}}
%       \end{column}
%     \end{columns}
%   \end{center}
%   Ad-hoc segmentation according to the depth, and sensitivity
%   analysis: only the shallow coastal regions seem to have an
%   influence.
% % \end{frame}
% \begin{frame}
%   \frametitle{Robust optimization}
%   \begin{center}
%     \begin{columns}
%       \begin{column}{.5\textwidth}
%         % \includegraphics[width=\textwidth]{\manupath
%         % Chapter5/img/gaussian_english_channel.png}
%       \end{column}
%       \begin{column}{.5\textwidth}
%         \includegraphics[width=\textwidth]{croco.png}
%       \end{column}
%     \end{columns}
%   \end{center}

%   \begin{itemize}
%   \item $\UU\sim \mathsf{U}[-1, 1]$ uniform r.v.\ that models the
%     percentage of error on the amplitude of the M2 component of the
%     tide
%   \item The ``truth'' ranges from $8$mm to $13$mm.
%   \item $11.0$mm leads to a cost which deviates less than $1\%$ from
%     the optimal value with probability $0.77$
%   \end{itemize}
% \end{frame}



\section{Conclusion}

\begin{frame}[label=conclusion]{Conclusion}
  
  \begin{block}{Wrapping-up}
    \begin{itemize}
    \item Robustness can have a lot of different meaning
    \item Relative-regret estimates control the deviation from the optimal value 
    \item $\alpha$ can reflect risk-adverse or risk-seeking preferences
    \item GP can be used in order to add sequentially points to the design
    \end{itemize}
  \end{block}


  \begin{block}{Perspectives}
    \begin{itemize}
    \item Dimension of the input space $\Kspace \times \Uspace$ can be limiting
    \item Adaptive methods still require expensive or difficult tasks (optimization, sampling, integration)
    \item Continue study of 2-stages criterion
    \item Study of CROCO with different settings
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}[allowframebreaks]{References}
  \renewcommand{\bibsection}{}
\bibliographystyle{authordate1}
  \bibliography{/home/victor/acadwriting/bibzotero.bib}
\end{frame}

\appendix

\begin{frame}{Relative or additive regret}
  \pnote{we discussed so far the relative regret, that takes the form
    of a multiplicative relaxation. Why this over the additive regret
    ?

    Relative regret takes better into account the magnitude of the
    cost function, as the region of acceptability grows with alpha AND
    Jstar. When the situation is already bad, we don't want to put
    much effort to stay close to the minimum theta star of u. On the
    other hand, for Jstar close to 0, }

  
  \renewcommand\rmfamily{\sffamily}
  \begin{center}
    \resizebox{.5\textwidth}{!}{\input{\manupath
        Chapter3/img/illustration_region_regret.pgf}}
  \end{center}
  \begin{itemize}
  \item Relative regret
    \begin{itemize}
    \item $\alpha$-acceptability regions large for flat and bad
      situations ($J^*(\uu)$ large)
    \item Conversely, puts high confidence when $J^*(\uu)$ is small
    \item No units $\rightarrow$ ratio of costs
    \end{itemize}
  \end{itemize}
\end{frame}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
