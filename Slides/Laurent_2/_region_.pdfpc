[duration]
25
[last_minutes]
5
[notes]
### 1
 I'm going to present very quickly some estimates that can be considered robust, but will focus mainly on the last one. First we can think about minimising in the worst case sense. This usually leads to overly conservative estimates as we are maximizing over the whole space U. We can also think about minimising the moments, such as the mean or the variance, or even combine them in a multiobjective setting by looking for the pareto front. \par \par Every choice of environmental variable gives a distinct situation The aspect we are going to focus on is based on the regret, so it implies a comparison with the best performance attainable for each u. 
### 2
 The main idea is that we want to consider individually all situations induced by the value of the environmental variable. Basically, once a value u is sampled, the problem is deterministic, so under some assumption, we have a minimiser theta star that is a function of u \par \par Keeping in mind the random nature of U, we can define the random variable thetastar, and its density (if it is defined), can be seen as the frequency of which a value theta is optimal. \par \par That is an interesting information, but we can have a little more than that. We may want to include theta that yield values of the cost function close to a minimum. To do that, we introduce a relaxation of the equality constraint with alpha, so that for a given u, we consider acceptable the theta that give values of the cost function between Jstar, the optimal value and alpha times Jstar So finally, we compute the probability that this given theta is acceptable with respect to the level alpha 
### 2
 The main idea is that we want to consider individually all situations induced by the value of the environmental variable. Basically, once a value u is sampled, the problem is deterministic, so under some assumption, we have a minimiser theta star that is a function of u \par \par Keeping in mind the random nature of U, we can define the random variable thetastar, and its density (if it is defined), can be seen as the frequency of which a value theta is optimal. \par \par That is an interesting information, but we can have a little more than that. We may want to include theta that yield values of the cost function close to a minimum. To do that, we introduce a relaxation of the equality constraint with alpha, so that for a given u, we consider acceptable the theta that give values of the cost function between Jstar, the optimal value and alpha times Jstar So finally, we compute the probability that this given theta is acceptable with respect to the level alpha 
### 2
 The main idea is that we want to consider individually all situations induced by the value of the environmental variable. Basically, once a value u is sampled, the problem is deterministic, so under some assumption, we have a minimiser theta star that is a function of u \par \par Keeping in mind the random nature of U, we can define the random variable thetastar, and its density (if it is defined), can be seen as the frequency of which a value theta is optimal. \par \par That is an interesting information, but we can have a little more than that. We may want to include theta that yield values of the cost function close to a minimum. To do that, we introduce a relaxation of the equality constraint with alpha, so that for a given u, we consider acceptable the theta that give values of the cost function between Jstar, the optimal value and alpha times Jstar So finally, we compute the probability that this given theta is acceptable with respect to the level alpha 
### 3
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 3
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 3
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 3
 What does it look like on a concrete example. We have the plot of a cost function, where theta is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for u fixed, we compute the minimiser, theta star of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each theta the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. 
### 4
Great so now we have Gamma(theta) which is the probability that theta gives a cost alpha acceptable If we have an idea of a threshold we don't want to exceed, so if alpha is known we can maximize the probability of being alpha acceptable \par Or, on the other hand, as gamma is a probability, we can look for the smallest relaxation, where the probability of acceptability reaches a certain confidence 1-eta. \par We can then define the family of relative-regret estimators, which are the maximizers of such a probability of being alpha acceptable. Depending on the approach, we can nudge toward optimal performances with small alpha, or risk adverse preference, by setting a bigger relaxation. 
### 6
we discussed so far the relative regret, that takes the form of a multiplicative relaxation. Why this over the additive regret ? \par Relative regret takes better into account the magnitude of the cost function, as the region of acceptability grows with alpha AND Jstar. When the situation is already bad, we don't want to put much effort to stay close to the minimum theta star of u. On the other hand, for Jstar close to 0, 
