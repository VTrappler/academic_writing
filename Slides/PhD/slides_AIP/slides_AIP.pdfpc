[duration]
25
[last_minutes]
5
[notes]
### 1
Hello everyone, my name is Victor Trappler, I'm a PhD student in the AIRSEA team in Grenoble, under the supervision of Ã‰lise Arnaud, Arthur Vidard and Laurent Debreu. \par \par I'm here to present some of the work entitled ``Parameter Control in the Presence of uncertainties'' 
### 2
During the whole process of the modelling of a physical system, that is from the observation of a natural phenomenon, to the simulation using numerical methods, we introduce uncertainties. Those uncertainties take the form of errors introduced by the simplifications, discretizations and parametrizations needed to represent things numerically. \par In the end, we have a set of parameters, that we need to calibrate, but during this phase of calibration, how can we be sure that we try to correct only the error due to the parametrization, and are not compensating errors coming from others sources.
### 3
We are first going to see what problems arise when considering the deterministic case. This will lead us to introduce uncertainties, and then how to define robustness. Finally, we are going to see quickly how surrogate models can help us tackle these issues, especially those linked to the curse of dimensionality
### 4
In quite a classical setting, we assume that we have a model M, that takes two inputs: k the control variable, that we aim at calibrating, and u some environmental variables, that we consider fixed and knowns. \par \par Now, to have an inverse problem, we have some observations, that are kinda linked to the model, and from those observations we wish to get back the parameter k from the start. 
### 4
In quite a classical setting, we assume that we have a model M, that takes two inputs: k the control variable, that we aim at calibrating, and u some environmental variables, that we consider fixed and knowns. \par \par Now, to have an inverse problem, we have some observations, that are kinda linked to the model, and from those observations we wish to get back the parameter k from the start. 
### 5
 In practice: we assume that the observations have been generated in a twin experiment framework, using the model and two references values kobs and uobs, uobs that is known. Using the least square approach, we define J, a cost function, as the sum of the squares of the difference. \par \par This is a deterministic optimisation problem, that we can solve using classical methods such as adjoint gradient. Notice that the u used in the optimisation procedure is known. \par But what if it is not, to say that there is a difference between uobs and u. The minimisation procedure is supposed to correct the error of k but how about the error due to uobs and u used in the minimisation ?
### 6
 The problem here at stakes is the estimation of the bottom friction. \par The bottom friction k has an influence on the oceanic circulation, as it dissipates some energy by turbulences. This friction parameter depends on the type of sediments, and more particularly, on the characteristic length of the asperities. Something that is hard to observe directly. In oceans modelling, this is a subgrid phenomenon. \par The environmental variable u parametrizes the BC, for instance the relative amplitude of tidal components. 
### 7
 We said earlier that there are a lot of uncertainties everywhere basically. But we can make a rough distinction between two types: \par - First, the epistemic uncertainties that result from a lack of knowledge, but can be reduce. An example is the uncertainty during the estimation of the mean value. The more samples you take, the less uncertainty there is on your estimation \par - Secondly, there is the aleatoric uncertainty, that comes from the inherent variability of the system studied. Think of the different values that a random variable takes. \par Our goal, is then to be able to reduce the epistemic uncertainty on the value of k to use, while taking into account the aleatoric uncertainty. 
### 8
 As hinted before, we are going to model the aleatoric uncertainty on u by a random variable. The inverse problem considered before becomes now the following, and the output of the model becomes a random variable 
### 8
 As hinted before, we are going to model the aleatoric uncertainty on u by a random variable. The inverse problem considered before becomes now the following, and the output of the model becomes a random variable 
### 9
 In our study, the models are deterministic, so we can control the inputs The cost function, becomes then a function of two inputs. We still wish to minimise with respect to k, but what can we do for u ? A first solution would be to set it to a fixed value, such as the mean of the random variable 
### 10
 To look first into this solution, we applied this to a toy problem based on the SWE. We set different observations using different uobs The real friction is the dashed sine curve at the bottom. When there is no difference, that is uobs equals the mean, the solution seems satisfying. But when there are differences, the estimations are not good at all. We can see that this solution is not really robust. Here finding a value k robust has to be understood as the ability for this k to perform reasonaly well under different operating conditions u. 
### 11
So basically, we have two main objectives: - First to find some criteria of robustness to estimate k - Be able to compute those estimates quickly This objectives requires some reflexion on first, being able to explore the U space quite efficiently, using design of experiments. Also, as k may be defined on every points of the mesh, we may want to be able to reduce the dimension of this to keep the computation tractable. 
### 12
 I'm going to present very quickly some estimates that can be considered robust, but will focus mainly on the last one. First we can think about minimising in the worst case sense. This usually leads to overly conservative estimates. We can also think about minimising the moments, such as the mean or the variance, or even combine them in a multiobjective setting by looking for the pareto front. \par The main thing we've been working on is to see how can we get the best performance attainable, for each configuration u sampled 
### 13
 Basically, once a value u is sampled, the problem is deterministic, so under some assumption, we have a minimiser kstar that is a function of u \par \par Keeping in mind the random nature of U, we can define the random variable Kstar, and its density (if it is defined), can be seen as the frequency of which a value k is optimal. \par \par That is an interesting information, but we can have a little more than that. We may want to include k that yield values of the cost function close to a minimum. To do that, we introduce a relaxation of the equality constraint with alpha, so that for a given u, we consider acceptable the k that give values of the cost function between Jstar, the optimal value and alpha times Jstar So finally, we compute the probability that this given k is acceptable with respect to the level alpha 
### 13
 Basically, once a value u is sampled, the problem is deterministic, so under some assumption, we have a minimiser kstar that is a function of u \par \par Keeping in mind the random nature of U, we can define the random variable Kstar, and its density (if it is defined), can be seen as the frequency of which a value k is optimal. \par \par That is an interesting information, but we can have a little more than that. We may want to include k that yield values of the cost function close to a minimum. To do that, we introduce a relaxation of the equality constraint with alpha, so that for a given u, we consider acceptable the k that give values of the cost function between Jstar, the optimal value and alpha times Jstar So finally, we compute the probability that this given k is acceptable with respect to the level alpha 
### 13
 Basically, once a value u is sampled, the problem is deterministic, so under some assumption, we have a minimiser kstar that is a function of u \par \par Keeping in mind the random nature of U, we can define the random variable Kstar, and its density (if it is defined), can be seen as the frequency of which a value k is optimal. \par \par That is an interesting information, but we can have a little more than that. We may want to include k that yield values of the cost function close to a minimum. To do that, we introduce a relaxation of the equality constraint with alpha, so that for a given u, we consider acceptable the k that give values of the cost function between Jstar, the optimal value and alpha times Jstar So finally, we compute the probability that this given k is acceptable with respect to the level alpha 
### 14
 What does it look like on a concrete example. We have the plot of a cost function, where k is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for a u fixed, we compute the minimiser, kstar of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each k the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile. \par 
### 14
 What does it look like on a concrete example. We have the plot of a cost function, where k is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for a u fixed, we compute the minimiser, kstar of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each k the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile. \par 
### 14
 What does it look like on a concrete example. We have the plot of a cost function, where k is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for a u fixed, we compute the minimiser, kstar of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each k the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile. \par 
### 14
 What does it look like on a concrete example. We have the plot of a cost function, where k is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for a u fixed, we compute the minimiser, kstar of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each k the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile. \par 
### 14
 What does it look like on a concrete example. We have the plot of a cost function, where k is the x axis, and u is on the y axis. \par As said earlier, for each horizontal cross section, so for a u fixed, we compute the minimiser, kstar of u. \par We can then compute the whole set of the conditional minimisers \par Now, we set alpha: inside the yellow lines, we are between the minimum and alpha times the minimum \par Finally, we construct and measure for each k the probability to be within this acceptable region. Great, now we just have to know how to choose alpha. Recalling that Gamma here is a probability, we can set levels of interests, such as 1, 0.9 or 0.95 for instance, and take the smallest alpha such that there is a value k where gamma of alpha and k reaches this level, similarly to a quantile. \par 
### 15
 Here we have our problem, and we are increasing alpha. The black curve in the bottom plot is gamma alpha of k. By increasing alpha, we increase the probability of being acceptable, and stop when this probability is 1. This can be seen on the top plot, there is a k, always in the acceptable region. What is interesting is that we have two informations. The value k of the estimation, but also the level alpha, that is controlling in a sense the regret we have relative to the best attainable performance. 
### 15
 Here we have our problem, and we are increasing alpha. The black curve in the bottom plot is gamma alpha of k. By increasing alpha, we increase the probability of being acceptable, and stop when this probability is 1. This can be seen on the top plot, there is a k, always in the acceptable region. What is interesting is that we have two informations. The value k of the estimation, but also the level alpha, that is controlling in a sense the regret we have relative to the best attainable performance. 
### 15
 Here we have our problem, and we are increasing alpha. The black curve in the bottom plot is gamma alpha of k. By increasing alpha, we increase the probability of being acceptable, and stop when this probability is 1. This can be seen on the top plot, there is a k, always in the acceptable region. What is interesting is that we have two informations. The value k of the estimation, but also the level alpha, that is controlling in a sense the regret we have relative to the best attainable performance. 
### 15
 Here we have our problem, and we are increasing alpha. The black curve in the bottom plot is gamma alpha of k. By increasing alpha, we increase the probability of being acceptable, and stop when this probability is 1. This can be seen on the top plot, there is a k, always in the acceptable region. What is interesting is that we have two informations. The value k of the estimation, but also the level alpha, that is controlling in a sense the regret we have relative to the best attainable performance. 
### 16
We now have defined an estimator, but it is really computationally expensive to run
### 16
We now have defined an estimator, but it is really computationally expensive to run
