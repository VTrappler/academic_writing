

@article{walker_defining_2003,
  author =        {Walker, Warren E. and Harremo{\"e}s, Poul and
                   Rotmans, Jan and {van der Sluijs}, Jeroen P. and
                   {van Asselt}, Marjolein BA and Janssen, Peter and
                   {Krayer von Krauss}, Martin P.},
  journal =       {Integrated assessment},
  number =        {1},
  pages =         {5--17},
  title =         {Defining Uncertainty: A Conceptual Basis for
                   Uncertainty Management in Model-Based Decision
                   Support},
  volume =        {4},
  year =          {2003},
}

@article{marzat_worst-case_2013,
  author =        {Marzat, Julien and Walter, Eric and
                   {Piet-Lahanier}, H{\'e}l{\`e}ne},
  journal =       {Journal of Global Optimization},
  month =         apr,
  number =        {4},
  pages =         {707-727},
  title =         {Worst-Case Global Optimization of Black-Box Functions
                   through {{Kriging}} and Relaxation},
  volume =        {55},
  year =          {2013},
  abstract =      {A new algorithm is proposed to deal with the
                   worst-case optimization of black-box functions
                   evaluated through costly computer simulations. The
                   input variables of these computer experiments are
                   assumed to be of two types. Control variables must be
                   tuned while environmental variables have an
                   undesirable effect, to which the design of the
                   control variables should be robust. The algorithm to
                   be proposed searches for a minimax solution, i.e.,
                   values of the control variables that minimize the
                   maximum of the objective function with respect to the
                   environmental variables. The problem is particularly
                   difficult when the control and environmental
                   variables live in continuous spaces. Combining a
                   relaxation procedure with Krigingbased optimization
                   makes it possible to deal with the continuity of the
                   variables and the fact that no analytical expression
                   of the objective function is available in most
                   real-case problems. Numerical experiments are
                   conducted to assess the accuracy and efficiency of
                   the algorithm, both on analytical test functions with
                   known results and on an engineering application.},
  doi =           {10.1007/s10898-012-9899-y},
  issn =          {0925-5001, 1573-2916},
  language =      {en},
}

@article{lehman_designing_2004,
  author =        {Lehman, Jeffrey S. and Santner, Thomas J. and
                   Notz, William I.},
  journal =       {Statistica Sinica},
  pages =         {571--590},
  title =         {Designing Computer Experiments to Determine Robust
                   Control Variables},
  year =          {2004},
}

@phdthesis{baudoui_optimisation_2012,
  author =        {Baudoui, Vincent},
  school =        {Toulouse, ISAE},
  title =         {Optimisation Robuste Multiobjectifs Par Mod{\`e}les
                   de Substitution},
  year =          {2012},
}

@book{matheron_traite_1962,
  author =        {Matheron, Georges},
  publisher =     {{Editions Technip}},
  title =         {Trait{\'e} de G{\'e}ostatistique Appliqu{\'e}e. 1
                   (1962)},
  volume =        {1},
  year =          {1962},
}

@article{xiu_wiener--askey_2002,
  author =        {Xiu, D. and Karniadakis, G.},
  journal =       {SIAM Journal on Scientific Computing},
  month =         jan,
  number =        {2},
  pages =         {619-644},
  title =         {The {{Wiener}}--{{Askey Polynomial Chaos}} for
                   {{Stochastic Differential Equations}}},
  volume =        {24},
  year =          {2002},
  abstract =      {We present a new method for solving stochastic
                   differential equations based on Galerkin projections
                   and extensions of Wiener's polynomial chaos.
                   Specifically, we represent the stochastic processes
                   with an optimum trial basis from the Askey family of
                   orthogonal polynomials that reduces the
                   dimensionality of the system and leads to exponential
                   convergence of the error. Several continuous and
                   discrete processes are treated, and numerical
                   examples show substantial speed-up compared to Monte
                   Carlo simulations for low dimensional stochastic
                   inputs.},
  doi =           {10.1137/S1064827501387826},
  issn =          {1064-8275},
}

@incollection{sudret_polynomial_2015,
  author =        {Sudret, Bruno},
  booktitle =     {Risk and {{Reliability}} in {{Geotechnical
                   Engineering}}},
  editor =        {{Kok-Kwang Phoon}, Jianye Ching},
  pages =         {265-300},
  publisher =     {{CRC Press}},
  title =         {Polynomial Chaos Expansions and Stochastic Finite
                   Element Methods},
  year =          {2015},
  abstract =      {This paper is a state-of-the art review on sparse
                   polynomial chaos expansions (PCE) for engineering
                   applications. It contains a step-by-step presentation
                   of PCEs and their use in moment-, sensitivity- and
                   reliability analysis. Error estimators and sparse
                   expansions for addressing high-dimensional problems
                   are discussed. Applications in geotechnical
                   engineering showcase the efficiency of sparse PCEs.},
}

@article{ginsbourger_bayesian_2014,
  author =        {Ginsbourger, David and Baccou, Jean and
                   Chevalier, Cl{\'e}ment and Perales, Fr{\'e}d{\'e}ric and
                   Garland, Nicolas and Monerie, Yann},
  journal =       {SIAM/ASA Journal on Uncertainty Quantification},
  month =         jan,
  number =        {1},
  pages =         {490-510},
  title =         {Bayesian {{Adaptive Reconstruction}} of {{Profile
                   Optima}} and {{Optimizers}}},
  volume =        {2},
  year =          {2014},
  abstract =      {Given a function depending both on decision
                   parameters and nuisance variables, we consider the
                   issue of estimating and quantifying uncertainty on
                   profile optima and/or optimal points as functions of
                   the nuisance variables. The proposed methods base on
                   interpolations of the objective function constructed
                   from a finite set of evaluations. Here the functions
                   of interest are reconstructed relying on a kriging
                   model, but also using Gaussian field conditional
                   simulations, that allow a quantification of
                   uncertainties in the Bayesian framework. Besides, we
                   elaborate a variant of the Expected Improvement
                   criterion, that proves efficient for adaptively
                   learning the set of profile optima and optimizers.
                   The results are illustrated on a toy example and
                   through a physics case study on the optimal packing
                   of polydisperse frictionless spheres.},
  doi =           {10.1137/130949555},
  issn =          {2166-2525},
  language =      {en},
}

@article{sudret_global_2008,
  author =        {Sudret, Bruno},
  journal =       {Reliability Engineering \& System Safety},
  month =         jul,
  pages =         {964-979},
  title =         {Global Sensitivity Analysis Using Polynomial Chaos
                   Expansion},
  volume =        {93},
  year =          {2008},
  abstract =      {Global sensitivity analysis (SA) aims at quantifying
                   the respective effects of input random variables (or
                   combinations thereof) onto the variance of the
                   response of a physical or mathematical model. Among
                   the abundant literature on sensitivity measures, the
                   Sobol' indices have received much attention since
                   they provide accurate information for most models.
                   The paper introduces generalized polynomial chaos
                   expansions (PCE) to build surrogate models that allow
                   one to compute the Sobol' indices analytically as a
                   post-processing of the PCE coefficients. Thus the
                   computational cost of the sensitivity indices
                   practically reduces to that of estimating the PCE
                   coefficients. An original non intrusive
                   regression-based approach is proposed, together with
                   an experimental design of minimal size. Various
                   application examples illustrate the approach, both
                   from the field of global SA (i.e. well-known
                   benchmark problems) and from the field of stochastic
                   mechanics. The proposed method gives accurate results
                   for various examples that involve up to eight input
                   random variables, at a computational cost which is
                   2\textendash{}3 orders of magnitude smaller than the
                   traditional Monte Carlo-based evaluation of the
                   Sobol' indices.},
  doi =           {10.1016/j.ress.2007.04.002},
}

@incollection{le_gratiet_metamodel-based_2016,
  author =        {Le Gratiet, Loic and Marelli, Stefano and
                   Sudret, Bruno},
  booktitle =     {Handbook of {{Uncertainty Quantification}} - {{Part
                   III}}: {{Sensitivity}} Analysis},
  title =         {Metamodel-Based Sensitivity Analysis: Polynomial
                   Chaos Expansions and {{Gaussian}} Processes},
  year =          {2016},
  abstract =      {Global sensitivity analysis is now established as a
                   powerful approach for determining the key random
                   input parameters that drive the uncertainty of model
                   output predictions. Yet the classical computation of
                   the so-called Sobol' indices is based on Monte Carlo
                   simulation, which is not af- fordable when
                   computationally expensive models are used, as it is
                   the case in most applications in engineering and
                   applied sciences. In this respect metamodels such as
                   polynomial chaos expansions (PCE) and Gaussian
                   processes (GP) have received tremendous attention in
                   the last few years, as they allow one to replace the
                   original, taxing model by a surrogate which is built
                   from an experimental design of limited size. Then the
                   surrogate can be used to compute the sensitivity
                   indices in negligible time. In this chapter an
                   introduction to each technique is given, with an
                   emphasis on their strengths and limitations in the
                   context of global sensitivity analysis. In
                   particular, Sobol' (resp. total Sobol') indices can
                   be computed analytically from the PCE coefficients.
                   In contrast, confidence intervals on sensitivity
                   indices can be derived straightforwardly from the
                   properties of GPs. The performance of the two
                   techniques is finally compared on three well-known
                   analytical benchmarks (Ishigami, G-Sobol and Morris
                   functions) as well as on a realistic engineering
                   application (deflection of a truss structure).},
}

@misc{blanchet-scalliet_specific_2017,
  author =        {{Blanchet-Scalliet}, Christophette and
                   Helbert, C{\'e}line and Ribaud, M{\'e}lina and
                   Vial, C{\'e}line},
  month =         mar,
  title =         {A Specific Kriging Kernel for Dimensionality
                   Reduction: {{Isotropic}} by Group Kernel},
  year =          {2017},
  abstract =      {In the context of computer experiments, metamodels
                   are largely used to represent the output of computer
                   codes. Among these models, Gaussian process
                   regression (kriging) is very efficient see e.g
                   Snelson (2008). In high dimension that is with a
                   large number of input variables , but with few
                   observations the classical anisotropic kriging
                   becomes inefficient and sometimes completely wrong.
                   One way to overcome this drawback is to use the
                   isotropic kernel which is more robust because it
                   estimates not as many parameters. However this model
                   is too restrictive. The aim of this paper is to
                   construct a model between these two, that is at the
                   same time a robust and a flexible model. These two
                   skills are necessary for a model in high dimension.
                   We propose a kernel which is an answer to these
                   requests and that we call isotropic by group kernel.
                   This kernel is a tensor product of few isotropic
                   kernels built on well-chosen subgroup of variables.
                   The number and the composition of the groups are
                   found by an algorithm which explores different
                   structures. The choice of the best model is based on
                   the quality of prediction.},
  language =      {en},
}

@phdthesis{ribaud_krigeage_2018-1,
  author =        {Ribaud, M{\'e}lina},
  month =         oct,
  school =        {Lyon},
  type =          {Thesis},
  title =         {Krigeage Pour La Conception de Turbomachines : Grande
                   Dimension et Optimisation Multi-Objectif Robuste},
  year =          {2018},
  abstract =      {Dans le secteur de l'automobile, les turbomachines
                   sont des machines tournantes participant au
                   refroidissement des moteurs des voitures. Leur
                   performance d{\'e}pend de multiples param{\`e}tres
                   g{\'e}om{\'e}triques qui d{\'e}terminent leur forme.
                   Cette th{\`e}se s'inscrit dans le projet ANR PEPITO
                   r{\'e}unissant industriels et acad{\'e}miques autour
                   de l'optimisation de ces turbomachines. L'objectif du
                   projet est de trouver la forme du ventilateur
                   maximisant le rendement en certains points de
                   fonctionnement. Dans ce but, les industriels ont
                   d{\'e}velopp{\'e} des codes CFD (computational fluid
                   dynamics) simulant le fonctionnement de la machine.
                   Ces codes sont tr{\`e}s co{\^u}teux en temps de
                   calcul. Il est donc impossible d'utiliser directement
                   le r{\'e}sultat de ces simulations pour conduire une
                   optimisation.Par ailleurs, lors de la construction
                   des turbomachines, on observe des perturbations sur
                   les param{\`e}tres d'entr{\'e}e. Elles sont le reflet
                   de fluctuations des machines de production. Les
                   {\'e}carts observ{\'e}s sur la forme
                   g{\'e}om{\'e}trique finale de la turbomachine peuvent
                   provoquer une perte de performance cons{\'e}quente.
                   Il est donc n{\'e}cessaire de prendre en compte ces
                   perturbations et de proc{\'e}der {\`a} une
                   optimisation robuste {\`a} ces fluctuations. Dans ce
                   travail de th{\`e}se, nous proposons des m{\'e}thodes
                   bas{\'e}es sur du krigeage r{\'e}pondant aux deux
                   principales probl{\'e}matiques li{\'e}es {\`a} ce
                   contexte de simulations co{\^u}teuses :\textbullet{}
                   Comment construire une bonne surface de r{\'e}ponse
                   pour le rendement lorsqu'il y a beaucoup de
                   param{\`e}tres g{\'e}om{\'e}triques ?\textbullet{}
                   Comment proc{\'e}der {\`a} une optimisation du
                   rendement efficace tout en prenant en compte les
                   perturbations des entr{\'e}es ?Nous r{\'e}pondons
                   {\`a} la premi{\`e}re probl{\'e}matique en proposant
                   plusieurs algorithmes permettant de construire un
                   noyau de covariance pour le krigeage adapt{\'e} {\`a}
                   la grande dimension. Ce noyau est un produit
                   tensoriel de noyaux isotropes o{\`u} chacun de ces
                   noyaux est li{\'e} {\`a} un sous groupe de variables
                   d'entr{\'e}e. Ces algorithmes sont test{\'e}s sur des
                   cas simul{\'e}s et sur une fonction r{\'e}elle. Les
                   r{\'e}sultats montrent que l'utilisation de ce noyau
                   permet d'am{\'e}liorer la qualit{\'e} de
                   pr{\'e}diction en grande dimension. Concernant la
                   seconde probl{\'e}matique, nous proposons plusieurs
                   strat{\'e}gies it{\'e}ratives bas{\'e}es sur un
                   co-krigeage avec d{\'e}riv{\'e}es pour conduire
                   l'optimisation robuste. A chaque it{\'e}ration, un
                   front de Pareto est obtenu par la minimisation de
                   deux objectifs calcul{\'e}s {\`a} partir des
                   pr{\'e}dictions de la fonction co{\^u}teuse. Le
                   premier objectif repr{\'e}sente la fonction
                   elle-m{\^e}me et le second la robustesse. Cette
                   robustesse est quantifi{\'e}e par un crit{\`e}re
                   estimant une variance locale et bas{\'e}e sur le
                   d{\'e}veloppement de Taylor. Ces strat{\'e}gies sont
                   compar{\'e}es sur deux cas tests en petite et plus
                   grande dimension. Les r{\'e}sultats montrent que les
                   meilleures strat{\'e}gies permettent bien de trouver
                   l'ensemble des solutions robustes. Enfin, les
                   m{\'e}thodes propos{\'e}es sont appliqu{\'e}es sur
                   les cas industriels propres au projet PEPITO.},
}

