\documentclass[11pt]{beamer}
\usetheme{metropolis}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{subfig}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
%\usepackage{booktabs}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\Prob}{\mathbb{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\graphicspath{{./Figures/}}

\title{Parameter control in the presence of uncertainties}
\author{{\large Victor Trappler } \\ Supervisors: Elise Arnaud, Laurent Debreu, Arthur Vidard}
\institute{AIRSEA (Inria)-- LJK \includegraphics[scale=0.3]{ljk}}
\date{\today}
\setcounter{tocdepth}{1}

\begin{document}
\frame{
\maketitle
}
\section{Introduction}
\frame{
\begin{center}
\input{./Figures/hauteureau.tikz}
\end{center}
}
\frame{
\tableofcontents
}
\section{Deterministic problem}
\frame[t]{
\frametitle{Computer code : the Shallow Water Equations}
\begin{itemize}
	\item[Input] 
	\begin{itemize}
		\item $K$: Bottom friction (spatially distributed)
		\item $\bm{x}_e$: Environmental variables (fixed and known)
	\end{itemize}
	\item[Output] \begin{itemize}
	\item $W(K) = \{W_i^n(K)\}_{i,n}$, where $W_i^n(K) = [h_i^n(K) \quad q_i^n(K)]^T$\\ for $0 \leq i \leq N_x$ and $0 \leq n \leq N_t$
	\end{itemize}
\end{itemize}
\vfill
\only<1>{\input{./Figures/comp_code.pgf}}
\only<2>{\input{./Figures/inv_prob.pgf}}

}
\frame{
\frametitle{Data assimilation framework: Twin experiments}

$K_{\mathrm{ref}}$ and $\mathcal{H}$ observation operator\\
We have $Y = \mathcal{H}W(K_{\mathrm{ref}}) = \{h_i^n(K_{\mathrm{ref}})\}_{i,n}$ 
\begin{equation*}
j(K) = \frac12 \|\mathcal{H}W(K) - Y \|^2
\end{equation*} \pause
\begin{equation*}
\argmin_{K \in \mathcal{K}}j(K) ?
\end{equation*}


\begin{itemize}
\item<2-> Gradient-free: Simulated annealing, Nelder-mead,\dots
$\rightarrow$ High number of runs, \alert<2>{very expensive}

\item<3-> Gradient-based: gradient-descent, (quasi-) Newton method
$\rightarrow$ Less number of runs, but \alert<3>{need the adjoint code}
\end{itemize}
}
\section{Dealing with uncertainties}
\frame[t]{
\frametitle{Introducing the uncertainties}
Instead of considering $\bm{x}_e$ fixed, we consider that $\bm{X}_e$ is a random variable, and the output of the model depends on its realization. \\
\vfill
\only<1>{\input{./Figures/inv_prob.pgf}}
\only<2>{\input{./Figures/comp_code_unc_inv.pgf}}
\vfill
}
\frame{
\frametitle{The cost function as a random variable}
\begin{itemize}
\item Output of the computer code:
\begin{equation*}
    W(K) \quad \text{becomes} \quad W(\bm{x}_e,K)
\end{equation*}
\item The (deterministic) quadratic error is now
\begin{equation*}   
   j(\bm{x}_e,K) =  \frac12\|\mathcal{H}W(\bm{x}_e,K) - Y\|^2
\end{equation*}
\end{itemize}


%and for a given $K\in \mathcal{K}$, $j(\bm{X}_e,K)$ is a random variable
What to do with $j(\bm{X}_e,K)$ (r.v.) ?
\begin{itemize}
\item 	
\end{itemize}
}

\frame{
\frametitle{Strategies to tackle some of the current problems}
%Random variable : $J(\bm{X}_e,K)$
\begin{tabular}{ll}
Influence of $\bm{X}_e$ ? & \onslide<2->{ $\longrightarrow$ Sensitivity analysis}\\
Minimizing $j(\bm{X}_e,K)$ wrt $K$ ? &\onslide<2-> { $\longrightarrow$ Robust optimization}\\
Computational cost ? & \onslide<2->{ $\longrightarrow$ Use of surrogate}
\end{tabular}
}


\frame{
	\frametitle{An illustration}
	$(x_e,K) \mapsto f(x_e,K) = \tilde{f}(x_e+K)$ \\ $X_e \sim \mathcal{N}(0,s^2)$ truncated on $[{-3};3]$. Plot of $f(0,\cdot) = \tilde{f}(\cdot)$ \vfill
	\begin{figure}[!h]
	\centering
	\includegraphics[scale = 0.5]{./Figures/mean_worstcase_robustness1}
	\end{figure}
}
\frame{
	\frametitle{An illustration}
	$(x_e,K) \mapsto f(x_e,K) = \tilde{f}(x_e+K)$ \\ $X_e \sim \mathcal{N}(0,s^2)$ truncated on $[{-3};3]$. {\color{red}	Plot of $\max_{x_e} \{f(x_e,\cdot)\}$} \vfill
	\begin{figure}[!h]
	\centering
	\includegraphics[scale = 0.5]{./Figures/mean_worstcase_robustness2}
	\end{figure}
}
\frame{
	\frametitle{An illustration}
	$(x_e,K) \mapsto f(x_e,K) = \tilde{f}(x_e+K)$ \\ $X_e \sim \mathcal{N}(0,s^2)$ truncated on $[{-3};3]$. {\color{green} Plot of $\Ex_{x_e}[f(x_e,\cdot)]$} \vfill
	\begin{figure}[!h]
	\centering
	\includegraphics[scale = 0.5]{./Figures/mean_worstcase_robustness3}
	\end{figure}
}
\subsection{Concepts of robustness}
\frame{
\frametitle{Different Notions of robustness}
\begin{itemize}
\item<1-> Global Optimum: $ \min j(\bm{x}_e,K)$ $ \longrightarrow $ \alert<7->{EGO}
\item<2-> Worst case: $ \min_K \max_{\bm{x}_e} j(\bm{x}_e,K)$ $ \longrightarrow $ \alert<7->{Explorative EGO}
\item<3-> M-robustness: $\min \mu(K),\quad \text{constraint on } \sigma^2(K)$ $ \longrightarrow $ \alert<7->{iterated LHS}
\item<4-> V-robustness: $\min \sigma^2(K),\quad\text{constraint on } \mu(K)$ $ \longrightarrow $ gradient-descent with PCE
\item<5-> $\rho$-robustness: $\min \rho(\mu(K),\sigma^2(K))$ $\longrightarrow$ gradient-descent with PCE
\item<6-> Multiobjective: choice within Pareto frontier $\longrightarrow$ 1L/2L kriging
\end{itemize}
}

\frame[t]{
\frametitle{Why surrogates ?}
\begin{itemize}
\item Model $\longrightarrow$ expensive to run
\item High dimensional problem + taking into account uncertainties ?
\end{itemize}
\vfill
\begin{center}
\only<1>{\input{./Figures/comp_code_surro.pgf}}
\only<2>{\input{./Figures/surrogate.pgf}}
\end{center}
}
\frame{
\frametitle{Main issues raised}
\begin{itemize}
\item<1-> Cost of computer evaluations $\rightarrow$ number of runs limited

\item<2-> Dimensionality of the input space $\rightarrow$ control parameter spatially distributed
\end{itemize}
}
\end{document}