\documentclass[11pt]{beamer}
\usetheme{metropolis}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{subfig}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{multimedia}
%\usepackage{booktabs}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\Prob}{\mathbb{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Cov}{\textsf{Cov}}
\newcommand{\tra}{\mathrm{tr}}
\newcommand{\yobs}{\bm{y}^{\mathrm{obs}}}
\newcommand{\kest}{\hat{\bm{k}}}
\usetikzlibrary{positioning}
\DeclareMathOperator*{\KL}{\textsf{KL}}
\graphicspath{{../Figures/}}

\definecolor{blkcol}{HTML}{E1E1EA}
\definecolor{blkcol2}{RGB}{209, 224, 224}

% \definecolor{BlueTOL}{HTML}{222255}
% \definecolor{BrownTOL}{HTML}{666633}
% \definecolor{GreenTOL}{HTML}{225522}
% \setbeamercolor{normal text}{fg=BlueTOL,bg=white}
% \setbeamercolor{alerted text}{fg=BrownTOL}
% \setbeamercolor{example text}{fg=GreenTOL}

\setbeamercolor{block title}{bg = blkcol}
\setbeamercolor{block body}{bg = blkcol!50}

\setbeamerfont{author}{size=\footnotesize}

\title{Parameter control in the presence of uncertainties}
\subtitle{Robust Estimation of Bottom friction}
\author{{\large Victor Trappler} \hfill \texttt{victor.trappler@univ-grenoble-alpes.fr} \\
  É. Arnaud, L. Debreu, A. Vidard \\
  AIRSEA Research team (Inria)\hfill \texttt{team.inria.fr/airsea/en/}\\
  Laboratoire Jean Kuntzmann}
%     % \end{column}
    
%     % \begin{column}{0.45\linewidth}
%       \texttt{victor.trappler@univ-grenoble-alpes.fr}
%   %  \end{column}
%   % \end{columns}
% }%  \\[0.05cm]}

\institute{\begin{center}
\includegraphics[scale=0.20]{INRIA_SCIENTIFIQUE_UK_CMJN}
\includegraphics[scale=0.20]{ljk}
\end{center}}

\date{\textbf{Demi journée des doctorants de DATA, Grenoble 2018}}
\setcounter{tocdepth}{1}

\begin{document}
\frame{
  \maketitle
}
\metroset{sectionpage=none, subsectionpage=none}
\section{Introduction}

% \frame{
%   \frametitle{Principles of Computer experiments}
%   \scalebox{0.7}{\input{../Figures/schema_bloc}}
% }
\frame{
  \frametitle{Processus of modelling of physical systems}
 %  \begin{center}
%   \scalebox{1}{\input{../Figures/flowchart_modelling_uncertainties_vert}}
% \end{center}  
  Uncertainties and errors are introduced at each stage of the modelling, by simplifications, parametrizations...

  In the end, we have a set of parameters we want to calibrate, but how can we be sure that this calibration is acting upon the errors of the modelling, and does not compensate the effect of the natural variability of the physical system?
}
\frame{
\frametitle{Outline}
\tableofcontents
}
\metroset{sectionpage=progressbar, subsectionpage=none}

\section{Deterministic problem}
\frame[t]{
\frametitle{Computer code and inverse problem}
\begin{itemize}
	\item[Input] 
	\begin{itemize}
		\item $\bm{k}$: Control parameter%Bottom friction (spatially distributed)
		\item $\bm{u}$: Environmental variables (fixed and known)
	\end{itemize}
	\item[Output] \begin{itemize}
          \item $\mathcal{M}(\bm{k},\bm{u})$: Quantity to be compared to observations%Sea surface height, at predetermined time of the simulation and at certain location
	\end{itemize}
      \end{itemize}
      
\vfill
\only<1>{\input{../Figures/comp_code.pgf}}
\only<2>{\input{../Figures/inv_prob.pgf}}
}
\frame{
\frametitle{Data assimilation framework}
We have $\yobs = \mathcal{M}(\bm{k}_{\mathrm{ref}},\bm{u}_{\mathrm{ref}})$ with $\bm{u}_{\mathrm{ref}} = \bm{u}$
\begin{equation*}
\hat{\bm{k}} = \argmin_{\bm{k}\in\mathcal{K}} J(\bm{k}) = \argmin_{\bm{k}\in\mathcal{K}}\frac12 \|\mathcal{M}(\bm{k},\bm{u}) - \bm{y}^{\mathrm{obs}} \|^2
\end{equation*}
\begin{itemize}
\item[$\rightarrow$] Deterministic optimization problem
\item[$\rightarrow$] Possibly add regularization
\item[$\rightarrow$] Classical methods: Adjoint gradient and Gradient-descent
\end{itemize}
BUT
\begin{itemize}
\item What if $\bm{u} \neq \bm{u}_{\mathrm{ref}}$?
\item Does $\hat{\bm{k}}$ compensates the errors brought by this misspecification?
% \item How well will $\bm{\hat{k}}$ perform under other conditions?
\end{itemize}
%\begin{itemize}
%\item<2-> Gradient-free: Simulated annealing, Nelder-mead,\dots
%$\rightarrow$ High number of runs, \alert<2>{very expensive}
%
%\item<3-> Gradient-based: gradient-descent, (quasi-) Newton method
%$\rightarrow$ Less number of runs, but \alert<3>{need the adjoint code}
%\end{itemize}
}
\frame{
\frametitle{Context}
\begin{itemize}
\item The friction $\bm{k}$ of the ocean bed has an influence on the water circulation
\item Depends on the type and/or characteristic length of the asperities 
\item Subgrid phenomenon
\item $\bm{u}$ parametrizes the BC
\end{itemize}
\begin{center}
\scalebox{.9}{\input{../Figures/hauteureau.tikz}}
\end{center}
}


\section{Dealing with uncertainties}
\frame{
  \frametitle{Different types of uncertainties}
  \begin{block}{Epistemic or aleatoric uncertainties?~\cite{walker_defining_2003}}
\begin{itemize}
\item Epistemic uncertainties: From a lack of knowledge, that can be reduced with more research/exploration
\item Aleatoric uncertainties: From the inherent variability of the system studied, operating conditions
\end{itemize}
\end{block}
$\rightarrow$ But where to draw the line?

Our goal is to take into account the aleatoric uncertainties in the estimation of our parameter.
}
\frame[t]{
\frametitle{Aleatoric uncertainties}
Instead of considering $\bm{u}$ fixed, we consider that $\bm{U}$ is a random variable (pdf $\pi(\bm{u})$), and the output of the model depends on its realization. \\
\vfill
\only<1>{\input{../Figures/inv_prob.pgf}}
\only<2>{\input{../Figures/comp_code_unc_inv.pgf}}
\vfill
}
\frame{
\frametitle{The cost function as a random variable}
\begin{itemize}
\item Output of the computer code ($\bm{u}$ is an input):
\begin{equation*}
    \mathcal{M}(\bm{k},\alert{\bm{u}})
\end{equation*}
\item The (deterministic) quadratic error is now
\begin{equation*}   
  J(\bm{k},\alert{\bm{u}}) =  \frac12\|\mathcal{M}(\bm{k},\alert{\bm{u}}) - \yobs\|^2
\end{equation*}
\end{itemize}

\begin{equation*}
  "\hat{\bm{k}} = \argmin_{\bm{k}\in\mathcal{K}} J(\bm{k},\alert{\bm{u}})" \text{ but what can we do about } \bm{u} ?
\end{equation*}
}

\frame{
  \frametitle{Toy Problem: Influence of misspecification of $\bm{u}_{\mathrm{ref}}$}
  Minimization performed on $\bm{k}\mapsto J(\bm{k},\Ex[\bm{U}])$, for different $\bm{u}_{\mathrm{ref}}$: Naïve approach
  \begin{center}
    \includegraphics[width=.8\textwidth]{optimization_using_misspecified_uref_4d}
  \end{center}
% \input{../Figures/optimization_using_misspecified_uref_4d.pgf}
}

\frame{
  \frametitle{Robust Estimation of parameters}
  \begin{itemize}
    \item Main objectives:
    \begin{itemize}
    \item[I.] Define criteria of robustness, based on $J(\bm{k},\bm{u})$, that will depend on the final application
    \item[II.] For each criterion, be able to compute an estimate $\hat{\bm{k}}$ in a reasonable time
    \end{itemize}
  \item Questions to be answered along the way:
    \begin{itemize}
    % \item Bayesian paradigm: $p(\bm{K} | \yobs)$ and $p(\bm{K} | \yobs, \bm{U} = \bm{u})$
    \item Suitable prior distribution on $\bm{K} \sim \pi(\bm{k})$
    \item Good exploration of $\mathcal{U}$, based on the density of $\bm{U}$ (Design of Experiment: LHS, Monte-Carlo, OA,\dots ?)
    \end{itemize}

  \end{itemize}

 }
% \frame{
% \frametitle{Variational approach or Bayesian approach ?}
% \begin{itemize}
% \item<1-> \textbf{Variational}: Minimize a function of $j(\bm{k},\bm{U})$, \\ e.g. Minimize $\Ex[j(\bm{K},\bm{U})|\bm{K} = \bm{k}]$.
% \\ $\longrightarrow$ Precise objective
% \item<2-> \textbf{Bayesian}: let us assume $e^{-j(\bm{k},\bm{u})} \propto p(\yobs|\bm{k},\bm{u})$  \\ Work around the likelihood and posterior distributions $p(\bm{k}|\yobs)$  \\
% $\longrightarrow$ More general method
% \end{itemize}
% \onslide<3-> But
% \begin{itemize}
% \item Dependent on the efficiency of the statistical estimators
% \item Knowledge of $\bm{U}$ ? Assumptions on error ?
% \item Computational cost ?
% \end{itemize}
% }
%\frame{
%\frametitle{Issues raised}
%%Random variable : $J(\bm{X}_e,K)$
%\begin{tabular}{ll}
%Influence of $\bm{X}_e$ ? & \onslide<2->{ $\longrightarrow$ Sensitivity analysis}\\
%Computational cost ? & \onslide<2->{ $\longrightarrow$ Use of surrogate}
%\end{tabular}
%}

\metroset{sectionpage=none, subsectionpage=progressbar}

\section{Robust minimization}

\subsection{Criteria of robustness}
% \frame{
% 	\frametitle{An illustration}
% 	$(\bm{u},\bm{k}) \mapsto f(\bm{u},\bm{k}) = \tilde{f}(\bm{u}+\bm{k})$ \\ $\bm{U} \sim \mathcal{N}(0,s^2)$ truncated on $[{-3};3]$. Plot of $f(0,\cdot) = \tilde{f}(\cdot)$ \vfill
% 	\begin{figure}[!h]
% 	\centering
% 	\includegraphics[scale = 0.5]{../Figures/mean_worstcase_robustness1}
% 	\end{figure}
% }
% \frame{
% 	\frametitle{An illustration}
% 	$(\bm{u},\bm{k}) \mapsto f(\bm{u},\bm{k}) = \tilde{f}(\bm{u}+\bm{k})$ \\ $\bm{U} \sim \mathcal{N}(0,s^2)$ truncated on $[{-3};3]$. {\color{red}	Plot of $\max_{\bm{u}} \{f(\bm{u},\cdot)\}$} \vfill
% 	\begin{figure}[!h]
% 	\centering
% 	\includegraphics[scale = 0.5]{../Figures/mean_worstcase_robustness2}
% 	\end{figure}
% }
% \frame{
% 	\frametitle{An illustration}
% 	$(\bm{u},\bm{k}) \mapsto f(\bm{u},\bm{k}) = \tilde{f}(\bm{u}+\bm{k})$ \\ $\bm{U} \sim \mathcal{N}(0,s^2)$ truncated on $[{-3};3]$. {\color{green} Plot of $\Ex_{\bm{u}}[f(\bm{u},\cdot)]$} \vfill
% 	\begin{figure}[!h]
% 	\centering
% 	\includegraphics[scale = 0.5]{../Figures/mean_worstcase_robustness3}
% 	\end{figure}
% }
\frame{
  \frametitle{Non-exhaustive list of ``Robust'' Objectives }
\begin{itemize}
% \item Global Optimum: $ \min_{(\bm{k},\bm{u})} J(\bm{u},\bm{k})$ $ \longrightarrow $ EGO
\item Worst case: $$ \min_{\bm{k} \in \mathcal{K}} \left\{\max_{\bm{u} \in \mathcal{U}} J(\bm{k},\bm{u})\right\} % \longrightarrow \text{Explorative EGO} 
  $$
\item M-robustness~\cite{lehman_designing_2004}: $$\min_{\bm{k}\in\mathcal{K}} \Ex_U\left[J(\bm{k},\bm{U})\right]% \longrightarrow  \text{iterated LHS}
  $$
\item V-robustness~\cite{lehman_designing_2004}: $$\min_{\bm{k}\in\mathcal{K}} \Var_U\left[J(\bm{k},\bm{U})\right] % \longrightarrow  \text{gradient-descent with PCE}
  $$
% \item $\rho$-robustness: $\min \rho(J(\bm{U},\bm{k}))$ $\longrightarrow$ gradient-descent with PCE
\item Multiobjective~\cite{baudoui_optimisation_2012}: $$ \text{Pareto frontier} % \longrightarrow \text{1L/2L kriging}
  $$
\item Region of failure given by $J(\bm{k},\bm{u})>T$~\cite{bect_sequential_2012}: $$\max_{\bm{k} \in \mathcal{K}} R(\bm{k}) = \max_{\bm{k}\in \mathcal{K}} \Prob_U\left[J(\bm{k},\bm{U}) \leq T \right]$$
\end{itemize}
}


% \subsection{Bayesian inference}
% \frame{
%   \frametitle{Bayesian approach}
%   Let us suppose $\bm{K} \sim \pi(\bm{k})$.
  
%   Having observed $\yobs$, joint distribution of $(\bm{K},\bm{U})$: $p(\bm{k},\bm{u}|\yobs)$ ? 
% \begin{block}{Bayes' Theorem}<2->
% \begin{align*}
% p(\bm{k},\bm{u} | \yobs) &\propto p(\yobs| \bm{k},\bm{u})\pi(\bm{k},\bm{u}) \\
% 					& \propto \alert{L(\bm{k},\bm{u}; \yobs)} \pi(\bm{k})\pi(\bm{u})
% \end{align*}
% \end{block}
% \onslide<3->
% Link with cost function $J$ : Squared error $\leftrightarrow$ Gaussian errors
% \begin{align*}
%   L(\bm{k},\bm{u}; \yobs) &\propto \exp\left[- \tfrac{1}{2}\|M(\bm{k},\bm{u}) - \yobs \|^2_{\bm{\Sigma}^{-1}} \right] = \exp\left[-j(\bm{k},\bm{u})\right]
% \end{align*}
% }
% \frame{
%   \frametitle{Bayesian Quantities of interest}
% % \newline
%   \begin{block}{Bayes' theorem}
%     $p(\bm{k},\bm{u}|\yobs) \propto L(\bm{k},\bm{u}; \yobs) \pi(\bm{k})\pi(\bm{u})\propto \alert{p(\bm{k}|\yobs,\bm{u})}\pi(\bm{u})$
%     \end{block}
%     \begin{align*}
%     \text{ML :}   & \quad\argmax_{(\bm{k},\bm{u})} L(\bm{k},\bm{u}; \yobs) \\
%     \text{MAP :}  & \quad \argmax_{(\bm{k},\bm{u})} p(\bm{k},\bm{u}| \yobs)= L(\bm{k},\bm{u}; \yobs)\pi(\bm{k})\pi(\bm{u})\\
%     \alert<2>{\text{MMAP :}} & \quad \argmax_{\bm{k}} p(\bm{k}|\yobs) = \int_{\mathcal{U}} p(\bm{k},\bm{u}| \yobs) \,\mathrm{d}\bm{u} \\
%     \alert<2>{\text{Min of variance :}} & \quad \argmin_{\bm{k}} \Var_{U}\left[p(\bm{k}| \yobs, \bm{U})\right] \\
%       \alert<2>{\text{Worst Case:}} & \quad \argmax_{\bm{k}} \{\min_{\bm{u}} p(\bm{k}|\yobs,\bm{u}) \} \\
%           \alert<2>{\text{MPE :}} & \quad \text{Mode of } \bm{K}_{\argmax}= \argmax_{\bm{k}} p(\bm{k} | \yobs, \bm{U})
%   \end{align*}
% }
% \frame{
% \frametitle{Reliability analysis}
% Let us define the region of failure given by $\phi(\bm{k},\bm{u})<0$
% \begin{equation*}
%   R_{\phi}(\bm{k}) =  \Prob_U\left[\phi(\bm{k},\bm{U})<0\right]
% \end{equation*}
% that we wish to minimize

% }
\frame{
  \frametitle{Toy Problem: Minimization of mean value}
  \vspace{-5ex}
   \begin{center}
    {\includegraphics[height = .8\textheight, width = \linewidth]{mean_minimization_illustration}}
  \end{center}
  \vspace{-3ex}
   $\longrightarrow$ Quite different from the value $\bm{k} \approx 0.1$ obtained by optimization knowing the true value of $\bm{u}_{\mathrm{ref}}$ 
}
\frame{
  \frametitle{``Most Probable Estimate'', and relaxation}%
    \begin{columns}
      \begin{column}{0.6\textwidth}
      The minimizer as a random variable:
      \begin{equation*}
        \bm{K}_{\argmin}= \argmin_{\bm{k}\in\mathcal{K}} J(\bm{k},\alert<1>{\bm{U}})
      \end{equation*}
      $\longrightarrow$ estimate its density (how often is the value $\bm{k}$ a minimizer)
      \begin{align*}
        R(\bm{k}) & = \Prob_U\left[\bm{k} = \argmin_{\tilde{\bm{k}}} J(\tilde{\bm{k}},\bm{U}) \right] \\
        \only<1>{\phantom{R_{\alpha}(\bm{k})} & = \Prob_U\left[J(\bm{k},\bm{U}) \leq \phantom{\alpha}\min_{\tilde{\bm{k}}} J(\tilde{\bm{k}},\bm{U})\right]}
                                                \only<2>{R_{\alert{\alpha}}(\bm{k}) & = \Prob_U\left[J(\bm{k},\bm{U}) \leq \alert{\alpha}\min_{\tilde{\bm{k}}} J(\tilde{\bm{k}},\bm{U})\right]}
               \end{align*}%
             % \begin{align*}
             %            R(\bm{k}) &=\Prob_U\left[\bm{k} = \argmin_{\tilde{\bm{k}}} J(\tilde{\bm{k}},\bm{U}) \right] \\
             %            R_{\alpha}(\bm{k}) & = \Prob_U\left[J(\bm{k},\bm{U}) \leq \alpha\min_{\tilde{\bm{k}}} J(\tilde{\bm{k}},\bm{U})\right]
             %          \end{align*}
             %        }
               \onslide<2>{$\longrightarrow$ Relaxation of the constraint with $\alpha\geq1$}
                  \end{column}%
                  \begin{column}{0.5\textwidth}
                    \begin{center}
                      \includegraphics[scale=0.3]{summary_criteria}
                    \end{center}
                    % \pause
                    % Idea: Relaxation of the constraint:
                    % \begin{equation*}
                    %   R_{\alpha}(\bm{k}) = \Prob\left[J(\bm{k},\bm{U}) \leq \alpha \min_{\tilde{\bm{k}}} J(\tilde{\bm{k}},\bm{U})\right],\quad \alpha \geq 1
                    % \end{equation*}
                    % and increase $\alpha$ until $\max_{\bm{k}} R_{\alpha}(\bm{k})$ reaches a level of confidence.
                  \end{column}
                \end{columns}
              }
\frame{
\frametitle{Illustration of the relaxation}
\begin{center}
\scalebox{0.45}{%
\input{../Figures/regions_relax_alpha2.pgf}}
\end{center}
}
              
% traightforward algorithm:
%   \begin{itemize}
% \item For $i=1\dots N$:
%     \begin{itemize}
%   \item Sample $\bm{u}^{(i)}$ from $\pi(\bm{u})$ / Adapted space-filling designs
%   \item Maximize $p(\bm{k}|\yobs,\bm{u}^{(i)})$ yielding $\bm{k}_{\argmax}^{(i)}$ (adjoint method)
%   \end{itemize}
%   \item Estimate density (KDE) / Mode
%   \end{itemize}
%  }
% \frame{
%   \frametitle{Illustration of MPE}
%   \movie[width = \linewidth, height= \textheight, poster, showcontrols]{}{animation.mpg}

% \frame{
%   \frametitle{Illustration on the SWE}
%   \begin{centering}
%     Family of densities: $\{ p(\bm{k}|\yobs,\bm{u});\bm{u} \in \mathcal{U}\}$
%     \end{centering}
% \begin{columns}
%   \begin{column}{0.38\linewidth}

%     \emph{MMAP}:
%      $ \argmax_{\bm{k}} p(\bm{k}|\yobs)$ \\
%     \emph{Min Var}:
%       $\argmin_{\bm{k}} \Var_{U}\left[p(\bm{k}| \yobs, \bm{U})\right]$ \\
%     \emph{Worst case}:
%       $\argmax_{\bm{k}} \{\min_{\bm{u}} p(\bm{k}|\yobs,\bm{u}) \}$ \\
% \end{column}
% \begin{column}{0.62\linewidth}
% \begin{center}
%  \includegraphics[width = \linewidth, height = .9\textheight]{MMAP_minvariance}
% \end{center}
% \end{column}
% \end{columns}
% }

\begin{frame}
  \frametitle{MPE and relaxation}
  \begin{center}
    % \includegraphics<1>[height=\textheight]{alpha_check}
    \includegraphics<1>[height=.95\textheight, width = \textwidth]{illustration_alpha0}
    \includegraphics<2>[height=.95\textheight, width = \textwidth]{illustration_alpha1}
    \includegraphics<3>[height=.95\textheight, width = \textwidth]{illustration_alpha2}
    \includegraphics<4>[height=.95\textheight, width = \textwidth]{illustration_alpha3}
    \includegraphics<5>[height=.95\textheight, width = \textwidth]{illustration_alpha4}
    \includegraphics<6>[height=.95\textheight, width = \textwidth]{illustration_alpha5}
    \includegraphics<7>[height=.95\textheight, width = \textwidth]{illustration_alpha6}
    \end{center}
\end{frame}


% \section{Surrogates}
% \subsection{\small{How to compute $\hat{\bm{k}}$ in a reasonable time?}}
% \frame[t]{
% \frametitle{Why surrogates?}
% \begin{itemize}
% \item Computer model: \alert{expensive to run}
% \item $\dim \mathcal{K}$, $\dim \mathcal{U}$ can be very large: \alert{curse of dimensionality}
% \item Uncertainties upon $\bm{u}$ maybe incorporated directly in the surrogate (PCE:~\cite{sudret_polynomial_2015})
% \end{itemize}
% \vfill
% \begin{center}
% 	\only<1>{\scalebox{0.9}{\input{../Figures/comp_code_surro.pgf}}}	
% 	\only<2>{\scalebox{0.9}{\input{../Figures/surrogate.pgf}}}
% \end{center}
% \vfill
% }
% % \frame{
% %   \frametitle{General principles}
% %   Different treatment of $\bm{k}$ and $\bm{u}$
% %   \begin{itemize}
% %   \item $\bm{k}$ must be chosen in an ``optimization'' state of mind
% %   \item $\bm{u}$ must be chosen in an ``exploration'' state of mind
% %   \end{itemize}
% % }

\frame{
  \frametitle{Application on the calibration of the bottom friction}
  \frametitle{MPE and relaxation: SWE toy problem, $\dim \mathcal{K}=1$, $\dim \mathcal{U}=2$}
  \only<1>{Known $\bm{u}_{\mathrm{ref}}=\Ex[\bm{U}]$}
  \only<2>{$\bm{u}_{\mathrm{ref}} = \Ex[\bm{U}] + (-0.2, -0.1) $}
  \only<3>{$\bm{u}_{\mathrm{ref}} = \Ex[\bm{U}] + (-0.1, +0.2) $}
  \only<4>{$\bm{u}_{\mathrm{ref}} = \Ex[\bm{U}] + (+0.2, +0.1) $}
  \only<5>{$\bm{u}_{\mathrm{ref}} = \Ex[\bm{U}] + (+0.1, -0.2) $}
  \begin{center}
    % \includegraphics<1>[height=\textheight]{alpha_check}
    \includegraphics<1>[height=.8\textheight, width = \textwidth]{alpha_checkref__previous_eval}
    \includegraphics<2>[height=.8\textheight, width = \textwidth]{alpha_check_AmPm_previous_eval}
    \includegraphics<3>[height=.8\textheight, width = \textwidth]{alpha_check_AmPp_previous_eval}
    \includegraphics<4>[height=.8\textheight, width = \textwidth]{alpha_check_ApPp_previous_eval}
    \includegraphics<5>[height=.8\textheight, width = \textwidth]{alpha_check_ApPm_previous_eval}
    \end{center}
  }

% \frame{
% \frametitle{Using surrogates for optimization : adaptative sampling}
% Based on kriging model (=Gaussian Process Regression)  $\longrightarrow$ mean and variance \\
% How to choose a new point to evaluate?~\cite{jones_efficient_1998,lehman_designing_2004,janusevskis_simultaneous_2010}
% Criterion $\kappa(\bm{x}) \longrightarrow$ ``potential'' of the point
% \begin{equation*}
% \bm{x}_{\mathrm{new}} = \argmax \kappa(\bm{x})
% \end{equation*}
% \includegraphics[width = \textwidth]{example_EccGO}
% }
\metroset{sectionpage=progressbar, subsectionpage=none}

\section{Conclusion}
\frame{
\frametitle{Conclusion}
\begin{block}{Wrapping up}
\begin{itemize}
\item Problem of a ``good'' definition of robustness
\item Strategies rely heavily on surrogate models, to embed aleatoric uncertainties directly in the modelling
\end{itemize}
\end{block}


\begin{block}{Perspective and future work}
\begin{itemize}
\item Cost of computer evaluations $\rightarrow$ limited number of runs?
\item Dimensionality of the input space $\rightarrow$ reduction of the input space?
\item How to deal with uncontrollable errors $\rightarrow$ realism of the model?
\end{itemize}
\end{block}
}
\begin{frame}[allowframebreaks]
  \frametitle{References}
\bibliographystyle{alpha}
\bibliography{../../Documents/bibzotero}
\end{frame}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
