@article{abdar_review_2021,
  title = {A {{Review}} of {{Uncertainty Quantification}} in {{Deep Learning}}: {{Techniques}}, {{Applications}} and {{Challenges}}},
  shorttitle = {A {{Review}} of {{Uncertainty Quantification}} in {{Deep Learning}}},
  author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
  year = {2021},
  month = dec,
  journal = {Information Fusion},
  volume = {76},
  eprint = {2011.06225},
  eprinttype = {arxiv},
  pages = {243--297},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.05.008},
  abstract = {Uncertainty quantification (UQ) plays a pivotal role in the reduction of uncertainties during both optimization and decision making, applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two of the most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlight the fundamental research challenges and directions associated with the UQ field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FS22G6XJ\\Abdar et al. - 2021 - A Review of Uncertainty Quantification in Deep Lea.pdf}
}

@article{ablin_super-efficiency_2020,
  title = {Super-Efficiency of Automatic Differentiation for Functions Defined as a Minimum},
  author = {Ablin, Pierre and Peyr{\'e}, Gabriel and Moreau, Thomas},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.03722 [cs, stat]},
  eprint = {2002.03722},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In min-min optimization or max-min optimization, one has to compute the gradient of a function defined as a minimum. In most cases, the minimum has no closed-form, and an approximation is obtained via an iterative algorithm. There are two usual ways of estimating the gradient of the function: using either an analytic formula obtained by assuming exactness of the approximation, or automatic differentiation through the algorithm. In this paper, we study the asymptotic error made by these estimators as a function of the optimization error. We find that the error of the automatic estimator is close to the square of the error of the analytic estimator, reflecting a super-efficiency phenomenon. The convergence of the automatic estimator greatly depends on the convergence of the Jacobian of the algorithm. We analyze it for gradient descent and stochastic gradient descent and derive convergence rates for the estimators in these cases. Our analysis is backed by numerical experiments on toy problems and on Wasserstein barycenter computation. Finally, we discuss the computational complexity of these estimators and give practical guidelines to chose between them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\99B3HQZC\\Ablin et al. - 2020 - Super-efficiency of automatic differentiation for .pdf}
}

@article{abramovich_wavelet_1998,
  title = {Wavelet Decomposition Approaches to Statistical Inverse Problems},
  author = {u Abramovich, F. and Silverman, B. W.},
  year = {1998},
  journal = {Biometrika},
  volume = {85},
  number = {1},
  pages = {115--129},
  keywords = {stochastic inverse problem,wavelet},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NEFQHAGV\\biometrika1998.pdf}
}

@article{afshar_probabilistic_nodate,
  title = {Probabilistic {{Inference}} in {{Piecewise Graphical Models}}},
  author = {Afshar, Hadi Mohasel},
  pages = {165},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PR9NVEI4\\Afshar - Probabilistic Inference in Piecewise Graphical Mod.pdf}
}

@article{akaike_new_1974,
  title = {A New Look at the Statistical Model Identification},
  author = {Akaike, Hirotugu},
  year = {1974},
  journal = {IEEE transactions on automatic control},
  volume = {19},
  number = {6},
  pages = {716--723},
  publisher = {{Ieee}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\S35Q2T58\\1100705.html}
}

@inproceedings{amri_bayesian_2019,
  title = {Bayesian {{Optimization Under Uncertainty}} for {{Chance Constrained Problems}}},
  booktitle = {{{PGMO Days}} 2019},
  author = {Amri, Mohamed Reda El and {Blanchet-Scalliet}, Christophette and Helbert, Celine and Riche, Rodolphe Le},
  year = {2019},
  month = dec,
  abstract = {Chance constraint is an important tool for modeling the reliability on decision making in the presence of uncertainties. Indeed, the chance constraint enforces that the constraint is satisfied with probability 1 - {$\alpha$} ( 0 {$<$} {$\alpha$} {$<$} 1 ) at least. In addition, we consider that the objective func- tion is affected by uncertainties. This problem is challenging since modeling a complex system under uncertainty can be expensive and for most real-world stochastic optimization will not be computationally viable. In this talk, we propose a Bayesian methodology to efficiently solve such class of problems. The central idea is to use Gaussian Process (GP) models [1] together with appropriate acquisi- tion functions to guide the search for an optimal solution. We first show that by specifying a GP prior to the objective function, the loss function becomes tractable [2]. Similarly, using GP models for the constraints, the probability satisfaction can be efficiently approximated. Sub- sequently, we introduce new acquisition functions to iteratively select the points to query the expensive objective and constraint functions. Finally, we present numerical examples to validate our approach compared to benchmark results.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ESL4A8EM\\emse-02351011.html}
}

@article{andreassian_all_2012,
  title = {All That Glitters Is Not Gold: The Case of Calibrating Hydrological Models: {{Invited Commentary}}},
  shorttitle = {All That Glitters Is Not Gold},
  author = {Andr{\'e}assian, Vazken and Le Moine, Nicolas and Perrin, Charles and Ramos, Maria-Helena and Oudin, Ludovic and Mathevet, Thibault and Lerat, Julien and Berthet, Lionel},
  year = {2012},
  month = jul,
  journal = {Hydrological Processes},
  volume = {26},
  number = {14},
  pages = {2206--2210},
  issn = {08856087},
  doi = {10.1002/hyp.9264},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UDKEPQMP\\AndrÃ©assian et al. - 2012 - All that glitters is not gold the case of calibra.pdf}
}

@book{andrieu_introduction_2003,
  title = {An {{Introduction}} to {{MCMC}} for {{Machine Learning}}},
  author = {Andrieu, Christophe and Freitas, Nando De and {al}, et},
  year = {2003},
  abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WJ26GD3I\\Andrieu et al. - 2003 - An Introduction to MCMC for Machine Learning.pdf;C\:\\Users\\a846735\\Zotero\\storage\\FVZ5A9HC\\summary.html}
}

@article{andrieu_introduction_nodate,
  title = {An {{Introduction}} to {{MCMC}} for {{Machine Learning}}},
  author = {Andrieu, Christophe and Andrieu, C},
  pages = {39},
  abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8DZ4KN77\\Andrieu et Andrieu - An Introduction to MCMC for Machine Learning.pdf}
}

@article{andrieu_pseudo-marginal_2009,
  title = {The Pseudo-Marginal Approach for Efficient {{Monte Carlo}} Computations},
  author = {Andrieu, Christophe and Roberts, Gareth O.},
  year = {2009},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {37},
  number = {2},
  pages = {697--725},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/07-AOS574},
  abstract = {We introduce a powerful and flexible MCMC algorithm for stochastic simulation. The method builds on a pseudo-marginal method originally introduced in [Genetics 164 (2003) 1139\textendash 1160], showing how algorithms which are approximations to an idealized marginal algorithm, can share the same marginal stationary distribution as the idealized method. Theoretical results are given describing the convergence properties of the proposed method, and simple numerical examples are given to illustrate the promising empirical characteristics of the technique. Interesting comparisons with a more obvious, but inexact, Monte Carlo approximation to the marginal algorithm, are also given.},
  keywords = {60J22,60K35,auxiliary variable,convergence,marginal,Markov chain Monte Carlo},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BA8TBHYQ\\Andrieu et Roberts - 2009 - The pseudo-marginal approach for efficient Monte C.pdf;C\:\\Users\\a846735\\Zotero\\storage\\SV4FFWKF\\07-AOS574.html}
}

@article{angelikopoulos_x-tmcmc:_2015,
  title = {X-{{TMCMC}}: {{Adaptive}} Kriging for {{Bayesian}} Inverse Modeling},
  shorttitle = {X-{{TMCMC}}},
  author = {Angelikopoulos, Panagiotis and Papadimitriou, Costas and Koumoutsakos, Petros},
  year = {2015},
  month = jun,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {289},
  pages = {409--428},
  issn = {00457825},
  doi = {10.1016/j.cma.2015.01.015},
  langid = {english},
  keywords = {Adaptative Kriging,Bayesian inference,Kriging},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YZ3W8HHH\\angelikopoulos2015a.pdf}
}

@article{apley_understanding_2006,
  title = {Understanding the {{Effects}} of {{Model Uncertainty}} in {{Robust Design With Computer Experiments}}},
  author = {Apley, Daniel W. and Liu, Jun and Chen, Wei},
  year = {2006},
  month = jul,
  journal = {Journal of Mechanical Design},
  volume = {128},
  number = {4},
  pages = {945--958},
  issn = {1050-0472, 1528-9001},
  doi = {10.1115/1.2204974},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6JYD98AU\\Apley et al. - 2006 - Understanding the Effects of Model Uncertainty in .pdf}
}

@article{apley_understanding_2006-1,
  title = {Understanding the {{Effects}} of {{Model Uncertainty}} in {{Robust Design With Computer Experiments}}},
  author = {Apley, Daniel W. and Liu, Jun and Chen, Wei},
  year = {2006},
  journal = {Journal of Mechanical Design},
  volume = {128},
  number = {4},
  pages = {945},
  issn = {10500472},
  doi = {10.1115/1.2204974},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B344XSHR\\Apley et al. - 2006 - Understanding the Effects of Model Uncertainty in .pdf}
}

@article{aravkin_estimating_2012,
  title = {Estimating {{Nuisance Parameters}} in {{Inverse Problems}}},
  author = {Aravkin, Aleksandr Y. and {van Leeuwen}, Tristan},
  year = {2012},
  month = nov,
  journal = {Inverse Problems},
  volume = {28},
  number = {11},
  eprint = {1206.6532},
  eprinttype = {arxiv},
  pages = {115016},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/28/11/115016},
  abstract = {Many inverse problems include nuisance parameters which, while not of direct interest, are required to recover primary parameters. Structure present in these problems allows efficient optimization strategies \textemdash{} a well known example is variable projection, where nonlinear least squares problems which are linear in some parameters can be very efficiently optimized. In this paper, we extend the idea of projecting out a subset over the variables to a broad class of maximum likelihood (ML) and maximum a posteriori likelihood (MAP) problems with nuisance parameters, such as variance or degrees of freedom. As a result, we are able to incorporate nuisance parameter estimation into large-scale constrained and unconstrained inverse problem formulations. We apply the approach to a variety of problems, including estimation of unknown variance parameters in the Gaussian model, degree of freedom (d.o.f.) parameter estimation in the context of robust inverse problems, automatic calibration, and optimal experimental design. Using numerical examples, we demonstrate improvement in recovery of primary parameters for several largescale inverse problems. The proposed approach is compatible with a wide variety of algorithms and formulations, and its implementation requires only minor modifications to existing algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {65K05; 65K10; 86-08,Mathematics - Numerical Analysis,Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\72YYDB72\\Aravkin et van Leeuwen - 2012 - Estimating Nuisance Parameters in Inverse Problems.pdf}
}

@article{arcucci_deep_2021,
  title = {Deep {{Data Assimilation}}: {{Integrating Deep Learning}} with {{Data Assimilation}}},
  shorttitle = {Deep {{Data Assimilation}}},
  author = {Arcucci, Rossella and Zhu, Jiangcheng and Hu, Shuang and Guo, Yi-Ke},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {3},
  pages = {1114},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/app11031114},
  abstract = {In this paper, we propose Deep Data Assimilation (DDA), an integration of Data Assimilation (DA) with Machine Learning (ML). DA is the Bayesian approximation of the true state of some physical system at a given time by combining time-distributed observations with a dynamic model in an optimal way. We use a ML model in order to learn the assimilation process. In particular, a recurrent neural network, trained with the state of the dynamical system and the results of the DA process, is applied for this purpose. At each iteration, we learn a function that accumulates the misfit between the results of the forecasting model and the results of the DA. Subsequently, we compose this function with the dynamic model. This resulting composition is a dynamic model that includes the features of the DA process and that can be used for future prediction without the necessity of the DA. In fact, we prove that the DDA approach implies a reduction of the model error, which decreases at each iteration; this is achieved thanks to the use of DA in the training process. DDA is very useful in that cases when observations are not available for some time steps and DA cannot be applied to reduce the model error. The effectiveness of this method is validated by examples and a sensitivity study. In this paper, the DDA technology is applied to two different applications: the Double integral mass dot system and the Lorenz system. However, the algorithm and numerical methods that are proposed in this work can be applied to other physics problems that involve other equations and/or state variables.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {data assimilation,deep learning,neural network},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QRU9UXIX\\Arcucci et al. - 2021 - Deep Data Assimilation Integrating Deep Learning .pdf;C\:\\Users\\a846735\\Zotero\\storage\\IRB22X9P\\1114.html}
}

@article{ardizzone_analyzing_2019,
  title = {Analyzing {{Inverse Problems}} with {{Invertible Neural Networks}}},
  author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and {Maier-Hein}, Lena and Rother, Carsten and K{\"o}the, Ullrich},
  year = {2019},
  month = feb,
  journal = {arXiv:1808.04730 [cs, stat]},
  eprint = {1808.04730},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task \textendash{} so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T01,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MNMAFIAR\\Ardizzone et al. - 2019 - Analyzing Inverse Problems with Invertible Neural .pdf}
}

@book{armagan_bayesian_2010,
  title = {Bayesian Generalized Double {{Pareto}} Shrinkage},
  author = {Armagan, A. and {al}, et},
  year = {2010},
  abstract = {We propose a generalized double Pareto prior for shrinkage estimation in linear models. The prior can be obtained via a scale mixture of Laplace or normal distributions, while forming a bridge between the Laplace and Normal-Jeffreys ' priors. While it has a spike at zero like the Laplace density, it also has a Student-t-like tail behavior. We show strong consistency of the posterior in regression models with a diverging number of parameters, providing a template to be used for other priors in similar settings. Bayesian computation is straightforward via a simple Gibbs sampling algorithm. We also investigate the properties of the maximum a posteriori estimator and reveal connections with some well-established regularization procedures. The performance of the new prior is tested through simulations.},
  keywords = {Bayesian inference},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W6N3EVXB\\Armagan et al - 2010 - Bayesian generalized double Pareto shrinkage.pdf;C\:\\Users\\a846735\\Zotero\\storage\\2UH922I9\\summary.html}
}

@inproceedings{arnaud_evaluation_2010,
  title = {{\'Evaluation d'un risque d'inondation fluviale par planification s\'equentielle d'exp\'eriences}},
  booktitle = {{42\`emes Journ\'ees de Statistique}},
  author = {Arnaud, Aur{\'e}lie and Bect, Julien and Couplet, Mathieu and Pasanisi, Alberto and Vazquez, Emmanuel},
  year = {2010},
  abstract = {Nous nous int\'eressons au risque d'inondation d'une zone habitable ou industrielle, situ\'ee \`a proximit\'e d'un fleuve. Le risque est \'evalu\'e \`a partir d'un mod\`ele de la ligne d'eau du fleuve en pr\'esence d'incertitudes sur le d\'ebit et les caract\'eristiques du lit fluvial. Comme l'\'evaluation du mod\`ele de la hauteur d'eau, pour un d\'ebit et des caract\'eristiques du lit fix\'es, est potentiellement co\^uteux en temps de calcul, l'estimation d'une probabilit\'e de d\'epassement de seuil ou d'un quantile de la hauteur d'eau doit en pratique \^etre conduite avec un budget r\'eduit de simulations. Dans cet article, nous nous int\'eressons sp\'ecifiquement \`a l'estimation d'un quantile et nous proposons une m\'ethode de planification d'exp\'eriences s\'equentielle qui construit une approximation du mod\`ele par krigeage en choisissant les points d'\'evaluation du mod\`ele de mani\`ere \`a r\'eduire la variance d'estimation du quantile.},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6LTDBWZX\\Arnaud et al. - 2010 - Ãvaluation d'un risque d'inondation fluviale par p.pdf;C\:\\Users\\a846735\\Zotero\\storage\\BNT2E33P\\inria-00494767.html}
}

@misc{arnaud_inverse_2018,
  title = {Inverse {{Methods}} and Data Assimilation},
  author = {Arnaud, {\'E}lise},
  year = {2018},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QNKQRSNL\\Poly_InvMet-M2.pdf}
}

@article{artzner_coherent_1999,
  title = {Coherent {{Measures}} of {{Risk}}},
  author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean-Marc and Heath, David},
  year = {1999},
  month = jul,
  journal = {Mathematical Finance},
  volume = {9},
  number = {3},
  pages = {203--228},
  issn = {0960-1627, 1467-9965},
  doi = {10.1111/1467-9965.00068},
  abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties ``coherent.'' We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2U9VWDRP\\Artzner et al. - 1999 - Coherent Measures of Risk.pdf}
}

@phdthesis{asri_etude_2014,
  title = {{\'Etude des M-estimateurs et leurs versions pond\'er\'ees pour des donn\'ees clusteris\'ees}},
  author = {Asri, Mohamed El},
  year = {2014},
  month = dec,
  abstract = {La classe des M-estimateurs engendre des estimateurs classiques d'un param\`etre de localisation multidimensionnel tels que l'estimateur du maximum de vraisemblance, la moyenne empirique et la m\'ediane spatiale. Huber (1964) introduit les M-estimateurs dans le cadre de l'\'etude des estimateurs robustes. Parmi la litt\'erature d\'edi\'ee \`a ces estimateurs, on trouve en particulier les ouvrages de Huber (1981) et de Hampel et al. (1986) sur le comportement asymptotique et la robustesse via le point de rupture et la fonction d'influence (voir Ruiz-Gazen (2012) pour une synth\`ese sur ces notions). Plus r\'ecemment, des r\'esultats sur la convergence et la normalit\'e asymptotique sont \'etablis par Van der Vaart (2000) dans le cadre multidimensionnel. Nevalainen et al. (2006, 2007) \'etudient le cas particulier de la m\'ediane spatiale pond\'er\'ee et non-pond\'er\'ee dans le cas clusteris\'e. Nous g\'en\'eralisons ces r\'esultats aux M-estimateurs pond\'er\'es. Nous \'etudions leur convergence presque s\^ure, leur normalit\'e asymptotique ainsi que leur robustesse dans le cas de donn\'ees clusteris\'ees.},
  langid = {french},
  school = {Universit\'e d'Avignon},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7NP8Z3XT\\Asri - 2014 - Ãtude des M-estimateurs et leurs versions pondÃ©rÃ©e.pdf;C\:\\Users\\a846735\\Zotero\\storage\\EJV47BT2\\tel-01202540.html}
}

@article{atkins_implicit_2012,
  title = {Implicit {{Particle Methods}} and {{Their Connection}} with {{Variational Data Assimilation}}},
  author = {Atkins, Ethan and Morzfeld, Matthias and Chorin, Alexandre J.},
  year = {2012},
  month = nov,
  journal = {Monthly Weather Review},
  volume = {141},
  number = {6},
  pages = {1786--1803},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-12-00145.1},
  abstract = {The implicit particle filter is a sequential Monte Carlo method for data assimilation that guides the particles to the high-probability regions via a sequence of steps that includes minimizations. A new and more general derivation of this approach is presented and the method is extended to particle smoothing as well as to data assimilation for perfect models. Minimizations required by implicit particle methods are shown to be similar to those that one encounters in variational data assimilation, and the connection of implicit particle methods with variational data assimilation is explored. In particular, it is argued that existing variational codes can be converted into implicit particle methods at a low additional cost, often yielding better estimates that are also equipped with quantitative measures of the uncertainty. A detailed example is presented.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PXUNNLB6\\Atkins et al. - 2012 - Implicit Particle Methods and Their Connection wit.pdf;C\:\\Users\\a846735\\Zotero\\storage\\WLSWGZPR\\MWR-D-12-00145.html}
}

@article{au_estimation_2001,
  title = {Estimation of Small Failure Probabilities in High Dimensions by Subset Simulation},
  author = {Au, Siu-Kui and Beck, James L.},
  year = {2001},
  month = oct,
  journal = {Probabilistic Engineering Mechanics},
  volume = {16},
  number = {4},
  pages = {263--277},
  issn = {02668920},
  doi = {10.1016/S0266-8920(01)00019-4},
  abstract = {A new simulation approach, called `subset simulation', is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made suf\textregistered ciently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and ef\textregistered ciently estimated by means of simulation. The conditional probabilities cannot be estimated ef\textregistered ciently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and ef\textregistered cient in computing small probabilities. The ef\textregistered ciency of the method is demonstrated by calculating the \textregistered rst-excursion probabilities for a linear oscillator subjected to white noise excitation and for a \textregistered ve-story nonlinear hysteretic shear building under uncertain seismic excitation. q 2001 Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9JP6H7FP\\Au et Beck - 2001 - Estimation of small failure probabilities in high .pdf}
}

@article{augustin_trust-region_2017,
  title = {A Trust-Region Method for Derivative-Free Nonlinear Constrained Stochastic Optimization},
  author = {Augustin, F. and Marzouk, Y. M.},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.04156 [math]},
  eprint = {1703.04156},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {In this work we introduce the algorithm (S)NOWPAC (Stochastic Nonlinear Optimization With Path-Augmented Constraints) for stochastic nonlinear constrained derivative-free optimization. The algorithm extends the derivative-free optimizer NOWPAC to be applicable to nonlinear stochastic programming. It is based on a trust region framework, utilizing local fully linear surrogate models combined with Gaussian process surrogates to mitigate the noise in the objective function and constraint evaluations. We show several benchmark results that demonstrate (S)NOWPAC's efficiency and highlight the accuracy of the optimal solutions found.},
  archiveprefix = {arXiv},
  keywords = {9080; 90C15; 90C30; 90C56; 65K05; 60G15,G.1.6,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LXXZ4ZT5\\Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PVWW83A3\\Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;C\:\\Users\\a846735\\Zotero\\storage\\WYTGF3KZ\\Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;C\:\\Users\\a846735\\Zotero\\storage\\5W93B5GQ\\1703.html}
}

@article{auroux_back_2005,
  title = {Back and Forth Nudging Algorithm for Data Assimilation Problems},
  author = {Auroux, Didier and Blum, Jacques},
  year = {2005},
  month = jun,
  journal = {Comptes Rendus Mathematique},
  volume = {340},
  number = {12},
  pages = {873--878},
  issn = {1631-073X},
  doi = {10.1016/j.crma.2005.05.006},
  abstract = {In this Note, we introduce a new algorithm for data assimilation problems, called the back and forth nudging (BFN) algorithm. The standard forward nudging algorithm is first studied for a linear ODE model. The backward nudging algorithm is then introduced in order to reconstruct the initial state of the system. These two algorithms are combined in the new BFN algorithm. The mathematical proof of its convergence is given for a linear ODE system. To cite this article: D. Auroux, J. Blum, C. R. Acad. Sci. Paris, Ser. I 340 (2005). R\'esum\'e Dans cette Note, nous introduisons un nouvel algorithme pour les probl\`emes d'assimilation de donn\'ees, l'algorithme du nudging direct et r\'etrograde (back and forth nudging ou BFN en anglais). Nous rappelons tout d'abord la m\'ethode du nudging appliqu\'ee \`a un syst\`eme d'EDO lin\'eaires, avant d'introduire le nudging sur le probl\`eme r\'etrograde en temps afin de reconstruire la condition initiale du syst\`eme. En combinant ces deux m\'ethodes, on obtient le nouvel algorithme BFN. Nous montrons la convergence de cet algorithme pour un syst\`eme d'EDO lin\'eaires. Pour citer cet article : D. Auroux, J. Blum, C. R. Acad. Sci. Paris, Ser. I 340 (2005).},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JXUVYE6E\\Auroux et Blum - 2005 - Back and forth nudging algorithm for data assimila.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZIKMXVS8\\S1631073X05002074.html}
}

@article{ayed_learning_2019,
  title = {Learning {{Dynamical Systems}} from {{Partial Observations}}},
  author = {Ayed, Ibrahim and {de B{\'e}zenac}, Emmanuel and Pajot, Arthur and Brajard, Julien and Gallinari, Patrick},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.11136 [physics]},
  eprint = {1902.11136},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {We consider the problem of forecasting complex, nonlinear space-time processes when observations provide only partial information of on the system's state. We propose a natural data-driven framework, where the system's dynamics are modelled by an unknown time-varying differential equation, and the evolution term is estimated from the data, using a neural network. Any future state can then be computed by placing the associated differential equation in an ODE solver. We first evaluate our approach on shallow water and Euler simulations. We find that our method not only demonstrates high quality long-term forecasts, but also learns to produce hidden states closely resembling the true states of the system, without direct supervision on the latter. Additional experiments conducted on challenging, state of the art ocean simulations further validate our findings, while exhibiting notable improvements over classical baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Dynamical Systems,Physics - Atmospheric and Oceanic Physics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CFTBV7PL\\Ayed et al. - 2019 - Learning Dynamical Systems from Partial Observatio.pdf;C\:\\Users\\a846735\\Zotero\\storage\\FIV4IGP4\\Ayed et al. - 2019 - Learning Dynamical Systems from Partial Observatio.pdf;C\:\\Users\\a846735\\Zotero\\storage\\IQFWC7EN\\1902.html}
}

@article{azais_distribution_2005,
  title = {On the Distribution of the Maximum of a Gaussian Field with d Parameters},
  author = {Azais, Jean-Marc and Wschebor, Mario},
  year = {2005},
  month = feb,
  journal = {The Annals of Applied Probability},
  volume = {15},
  number = {1A},
  eprint = {math/0503475},
  eprinttype = {arxiv},
  pages = {254--278},
  issn = {1050-5164},
  doi = {10.1214/105051604000000602},
  abstract = {Let I be a compact d-dimensional manifold, let X:I\textbackslash to R be a Gaussian process with regular paths and let F\_I(u), u\textbackslash in R, be the probability distribution function of sup\_\{t\textbackslash in I\}X(t). We prove that under certain regularity and nondegeneracy conditions, F\_I is a C\^1-function and satisfies a certain implicit equation that permits to give bounds for its values and to compute its asymptotic behavior as u\textbackslash to +\textbackslash infty. This is a partial extension of previous results by the authors in the case d=1. Our methods use strongly the so-called Rice formulae for the moments of the number of roots of an equation of the form Z(t)=x, where Z:I\textbackslash to R\^d is a random field and x is a fixed point in R\^d. We also give proofs for this kind of formulae, which have their own interest beyond the present application.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\AMLGSN9K\\Azais et Wschebor - 2005 - On the distribution of the maximum of a gaussian f.pdf}
}

@book{azais_level_2009,
  title = {Level Sets and Extrema of Random Processes and Fields},
  author = {Aza{\"i}s, Jean-Marc and Wschebor, Mario},
  year = {2009},
  publisher = {{Wiley \& Sons}},
  doi = {10.1002/9780470434642},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XMVNYBKM\\hal-00982795.html}
}

@book{azais_level_2009-1,
  title = {Level {{Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}: {{Aza\"is}}/{{Level Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}},
  shorttitle = {Level {{Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}},
  author = {Az{\"a}is, Jean-Marc and Wschebor, Mario},
  year = {2009},
  month = feb,
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{Hoboken, NJ, USA}},
  doi = {10.1002/9780470434642},
  isbn = {978-0-470-43464-2 978-0-470-40933-6},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DY8I85EE\\AzÃ¤is et Wschebor - 2009 - Level Sets and Extrema of Random Processes and Fie.pdf}
}

@incollection{azema_lectures_2003,
  title = {Lectures on {{Logarithmic Sobolev Inequalities}}},
  booktitle = {S\'eminaire de {{Probabilit\'es XXXVI}}},
  author = {Guionnet, A. and Zegarlinksi, B.},
  editor = {Az{\'e}ma, Jacques and {\'E}mery, Michel and Ledoux, Michel and Yor, Marc},
  year = {2003},
  volume = {1801},
  pages = {1--134},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-36107-7_1},
  isbn = {978-3-540-00072-3 978-3-540-36107-7},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8R2MU6SD\\Guionnet et Zegarlinksi - 2003 - Lectures on Logarithmic Sobolev Inequalities.pdf}
}

@article{azzimonti_quantifying_2016,
  title = {Quantifying {{Uncertainties}} on {{Excursion Sets Under}} a {{Gaussian Random Field Prior}}},
  author = {Azzimonti, Dario and Bect, Julien and Chevalier, Cl{\'e}ment and Ginsbourger, David},
  year = {2016},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {4},
  number = {1},
  pages = {850--874},
  issn = {2166-2525},
  doi = {10.1137/141000749},
  abstract = {We focus on the problem of estimating and quantifying uncertainties on the excursion set of a function under a limited evaluation budget. We adopt a Bayesian approach where the objective function is assumed to be a realization of a Gaussian random field. In this setting, the posterior distribution on the objective function gives rise to a posterior distribution on excursion sets. Several approaches exist to summarize the distribution of such sets based on random closed set theory. While the recently proposed Vorob'ev approach exploits analytical formulae, further notions of variability require Monte Carlo estimators relying on Gaussian random field conditional simulations. In the present work we propose a method to choose Monte Carlo simulation points and obtain quasi-realizations of the conditional field at fine designs through affine predictors. The points are chosen optimally in the sense that they minimize the posterior expected distance in measure between the excursion set and its reconstruction. The proposed method reduces the computational costs due to Monte Carlo simulations and enables the computation of quasi-realizations on fine designs in large dimensions. We apply this reconstruction approach to obtain realizations of an excursion set on a fine grid which allow us to give a new measure of uncertainty based on the distance transform of the excursion set. Finally we present a safety engineering test case where the simulation method is employed to compute a Monte Carlo estimate of a contour line.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2G4NSBMC\\Azzimonti et al. - 2016 - Quantifying Uncertainties on Excursion Sets Under .pdf}
}

@article{baccou_development_2019,
  title = {Development of Good Practice Guidance for Quantification of Thermal-Hydraulic Code Model Input Uncertainty},
  author = {Baccou, Jean and Zhang, Jinzhao and Fillion, Philippe and Damblin, Guillaume and Petruzzi, Alessandro and Mendiz{\'a}bal, Rafael and Revent{\'o}s, Francesc and Skorek, Tomasz and Couplet, Mathieu and Iooss, Bertrand and Oh, Deog-Yeon and Takeda, Takeshi},
  year = {2019},
  month = dec,
  journal = {Nuclear Engineering and Design},
  series = {Special {{Issue}} on {{TRENDS AND PERSPECTIVES IN NUCLEAR THERMAL-HYDRAULICS}}},
  volume = {354},
  pages = {110173},
  issn = {0029-5493},
  doi = {10.1016/j.nucengdes.2019.110173},
  abstract = {Taking into account uncertainties is a key issue in nuclear power plant safety analysis using best estimate plus uncertainty methodologies. It involves two main types of treatment depending on the variables of interest: input parameters or system response quantity. The OECD/NEA PREMIUM project devoted to the first type of variables has shown that inverse methods for input uncertainty quantification can exhibit strong user-effect. One of the main reasons was the lack of a clear guidance to perform a reliable analysis. This work is precisely devoted to the development of a first good practice guidance document for quantification of thermal-hydraulic code model input uncertainty. The developments have been done in the framework of the OECD/NEA SAPIUM project (January 2017\textendash September 2019). This paper provides a summary of the main project outcome. Recommendations and open issues for future developments are also given.},
  keywords = {Good practice guidance,Inverse quantification of uncertainty,Model input uncertainty quantification,System approach,Thermal hydraulic code,Validation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SAMJNAM2\\Baccou et al. - 2019 - Development of good practice guidance for quantifi.pdf;C\:\\Users\\a846735\\Zotero\\storage\\NKNHW23N\\S0029549319301839.html}
}

@misc{baes_low-rank_2021,
  title = {Low-{{Rank}} plus {{Sparse Decomposition}} of {{Covariance Matrices}} Using {{Neural Network Parametrization}}},
  author = {Baes, Michel and Herrera, Calypso and Neufeld, Ariel and Ruyssen, Pierre},
  year = {2021},
  month = jun,
  number = {arXiv:1908.00461},
  eprint = {1908.00461},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  abstract = {This paper revisits the problem of decomposing a positive semidefinite matrix as a sum of a matrix with a given rank plus a sparse matrix. An immediate application can be found in portfolio optimization, when the matrix to be decomposed is the covariance between the different assets in the portfolio. Our approach consists in representing the low-rank part of the solution as the product M M T , where M is a rectangular matrix of appropriate size, parametrized by the coefficients of a deep neural network. We then use a gradient descent algorithm to minimize an appropriate loss function over the parameters of the network. We deduce its convergence rate to a local optimum from the Lipschitz smoothness of our loss function. We show that the rate of convergence grows polynomially in the dimensions of the input, output, and the size of each of the hidden layers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y94XBVC2\\Baes et al. - 2021 - Low-Rank plus Sparse Decomposition of Covariance M.pdf}
}

@article{baptista_adaptive_2020,
  title = {An Adaptive Transport Framework for Joint and Conditional Density Estimation},
  author = {Baptista, Ricardo and Zahm, Olivier and Marzouk, Youssef},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.10303 [cs, stat]},
  eprint = {2009.10303},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a general framework to robustly characterize joint and conditional probability distributions via transport maps. Transport maps or ``flows'' deterministically couple two distributions via an expressive monotone transformation. Yet, learning the parameters of such transformations in high dimensions is challenging given few samples from the unknown target distribution, and structural choices for these transformations can have a significant impact on performance. Here we formulate a systematic framework for representing and learning monotone maps, via invertible transformations of smooth functions, and demonstrate that the associated minimization problem has a unique global optimum. Given a hierarchical basis for the appropriate function space, we propose a sample-efficient adaptive algorithm that estimates a sparse approximation for the map. We demonstrate how this framework can learn densities with stable generalization performance across a wide range of sample sizes on real-world datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7CN5DPPL\\Baptista et al. - 2020 - An adaptive transport framework for joint and cond.pdf}
}

@inproceedings{barron_bayesian_2014,
  title = {Bayesian Properties of Normalized Maximum Likelihood and Its Fast Computation},
  booktitle = {2014 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Barron, Andrew and Roos, Teemu and Watanabe, Kazuho},
  year = {2014},
  month = jun,
  pages = {1667--1671},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/ISIT.2014.6875117},
  isbn = {978-1-4799-5186-4},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RGVPKKBE\\Barron et al. - 2014 - Bayesian properties of normalized maximum likeliho.pdf}
}

@article{bassett_maximum_2019,
  title = {Maximum a {{Posteriori Estimators}} as a {{Limit}} of {{Bayes Estimators}}},
  author = {Bassett, Robert and Deride, Julio},
  year = {2019},
  month = mar,
  journal = {Mathematical Programming},
  volume = {174},
  number = {1-2},
  eprint = {1611.05917},
  eprinttype = {arxiv},
  pages = {129--144},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/s10107-018-1241-0},
  abstract = {Maximum a posteriori and Bayes estimators are two common methods of point estimation in Bayesian Statistics. It is commonly accepted that maximum a posteriori estimators are a limiting case of Bayes estimators with 0-1 loss. In this paper, we provide a counterexample which shows that in general this claim is false. We then correct the claim that by providing a levelset condition for posterior densities such that the result holds. Since both estimators are defined in terms of optimization problems, the tools of variational analysis find a natural application to Bayesian point estimation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62C10; 62F10; 62F15; 65K10,Mathematics - Optimization and Control,Mathematics - Statistics Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FWDNEZYJ\\Bassett et Deride - 2019 - Maximum a Posteriori Estimators as a Limit of Baye.pdf}
}

@article{basu_analysis_2017,
  title = {Analysis of {{Thompson Sampling}} for {{Gaussian Process Optimization}} in the {{Bandit Setting}}},
  author = {Basu, Kinjal and Ghosh, Souvik},
  year = {2017},
  month = may,
  journal = {arXiv:1705.06808 [stat]},
  eprint = {1705.06808},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We consider the global optimization of a function over a continuous domain. At every evaluation attempt, we can observe the function at a chosen point in the domain and we reap the reward of the value observed. We assume that drawing these observations are expensive and noisy. We frame it as a continuum-armed bandit problem with a Gaussian Process prior on the function. In this regime, most algorithms have been developed to minimize some form of regret. Contrary to this popular norm, in this paper, we study the convergence of the sequential point \$\textbackslash boldsymbol\{x\}\^t\$ to the global optimizer \$\textbackslash boldsymbol\{x\}\^*\$ for the Thompson Sampling approach. Under some assumptions and regularity conditions, we show an exponential rate of convergence to the true optimal.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VCV7B7YA\\Basu et Ghosh - 2017 - Analysis of Thompson Sampling for Gaussian Process.pdf}
}

@article{basu_robust_1998,
  title = {Robust and Efficient Estimation by Minimising a Density Power Divergence},
  author = {Basu, Ayanendranath and Harris, Ian R. and Hjort, Nils L. and Jones, M. C.},
  year = {1998},
  journal = {Biometrika},
  volume = {85},
  number = {3},
  pages = {549--559},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IMF8SAEK\\1997-7.pdf}
}

@phdthesis{baudoui_optimisation_2012,
  title = {Optimisation Robuste Multiobjectifs Par Mod\`eles de Substitution},
  author = {Baudoui, Vincent},
  year = {2012},
  school = {Toulouse, ISAE},
  keywords = {Optim Robuste,ThÃ¨se},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DIHXGHU4\\2012_Baudoui_Vincent.pdf}
}

@article{bayarri_framework_2007,
  title = {A Framework for Validation of Computer Models},
  author = {Bayarri, Maria J. and Berger, James O. and Paulo, Rui and Sacks, Jerry and Cafeo, John A. and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
  year = {2007},
  journal = {Technometrics},
  volume = {49},
  number = {2},
  pages = {138--154},
  keywords = {Validation,Verification},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6RNCHIIS\\Bayarri et al. - 2007 - A framework for validation of computer models.pdf}
}

@article{beaumont_approximate_2010,
  title = {Approximate {{Bayesian Computation}} in {{Evolution}} and {{Ecology}}},
  author = {Beaumont, Mark A.},
  year = {2010},
  month = dec,
  journal = {Annual Review of Ecology, Evolution, and Systematics},
  volume = {41},
  number = {1},
  pages = {379--406},
  issn = {1543-592X, 1545-2069},
  doi = {10.1146/annurev-ecolsys-102209-144621},
  abstract = {In the past 10 years a statistical technique, approximate Bayesian computation (ABC), has been developed that can be used to infer parameters and choose between models in the complicated scenarios that are often considered in the environmental sciences. For example, based on gene sequence and microsatellite data, the method has been used to choose between competing models of human demographic history as well as to infer growth rates, times of divergence, and other parameters. The method fits naturally in the Bayesian inferential framework, and a brief overview is given of the key concepts. Three main approaches to ABC have been developed, and these are described and compared. Although the method arose in population genetics, ABC is increasingly used in other fields, including epidemiology, systems biology, ecology, and agent-based modeling, and many of these applications are briefly described.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3D67J548\\Beaumont - 2010 - Approximate Bayesian Computation in Evolution and .pdf}
}

@article{bect_bayesian_2017,
  title = {Bayesian Subset Simulation},
  author = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
  year = {2017},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {5},
  number = {1},
  eprint = {1601.02557},
  eprinttype = {arxiv},
  pages = {762--786},
  issn = {2166-2525},
  doi = {10.1137/16M1078276},
  abstract = {We consider the problem of estimating a probability of failure {$\alpha$}, defined as the volume of the excursion set of a function f : X {$\subseteq$} Rd \textrightarrow{} R above a given threshold, under a given probability measure on X. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate {$\alpha$} when the number of evaluations of f is very limited and {$\alpha$} is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of f above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on f is used to define the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of f to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves significant savings in the number of function evaluations with respect to other Monte Carlo approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CRIYDVDY\\Bect et al. - 2017 - Bayesian subset simulation.pdf}
}

@article{bect_sequential_2012,
  title = {Sequential Design of Computer Experiments for the Estimation of a Probability of Failure},
  author = {Bect, Julien and Ginsbourger, David and Li, Ling and Picheny, Victor and Vazquez, Emmanuel},
  year = {2012},
  month = may,
  journal = {Statistics and Computing},
  volume = {22},
  number = {3},
  eprint = {1009.5177},
  eprinttype = {arxiv},
  pages = {773--793},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-011-9241-4},
  abstract = {This paper deals with the problem of estimating the volume of the excursion set of a function f : Rd \textrightarrow{} R above a given threshold, under a probability measure on Rd that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian-theoretic formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of f and aim at performing evaluations of f as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62L05; 62C10; 62P30,Statistics - Applications,Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KMNZSABA\\Bect et al. - 2012 - Sequential design of computer experiments for the .pdf}
}

@article{behrmann_invertible_2019,
  title = {Invertible {{Residual Networks}}},
  author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
  year = {2019},
  month = may,
  journal = {arXiv:1811.00995 [cs, stat]},
  eprint = {1811.00995},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both stateof-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F8GNP9BL\\1811.00995.pdf}
}

@article{beirlant_nonparametric_1997,
  title = {Nonparametric Entropy Estimation: {{An}} Overview},
  shorttitle = {Nonparametric Entropy Estimation},
  author = {Beirlant, Jan and Dudewicz, Edward J. and Gy{\"o}rfi, L{\'a}szl{\'o} and {Van der Meulen}, Edward C.},
  year = {1997},
  journal = {International Journal of Mathematical and Statistical Sciences},
  volume = {6},
  number = {1},
  pages = {17--39}
}

@article{beitler_pie_2021,
  title = {{{PIE}}: {{Pseudo-Invertible Encoder}}},
  shorttitle = {{{PIE}}},
  author = {Beitler, Jan Jetze and Sosnovik, Ivan and Smeulders, Arnold},
  year = {2021},
  month = oct,
  journal = {arXiv:2111.00619 [cs]},
  eprint = {2111.00619},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible transformations, we emphasize the importance of invertible compression. We introduce a new class of likelihood-based autoencoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST, where our model outperforms WAE and VAE in sharpness of the generated images.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TQTTATHN\\Beitler et al. - 2021 - PIE Pseudo-Invertible Encoder.pdf}
}

@article{beland_bayesian_nodate,
  title = {Bayesian {{Optimization Under Uncertainty}}},
  author = {Beland, Justin J and Nair, Prasanth B},
  pages = {5},
  abstract = {We consider the problem of robust optimization, where it is sought to design a system such that it sustains a specified measure of performance under uncertainty. This problem is challenging since modeling a complex system under uncertainty can be expensive and for most real-world problems robust optimization will not be computationally viable. In this paper, we propose a Bayesian methodology to efficiently solve a class of robust optimization problems that arise in engineering design under uncertainty. The central idea is to use Gaussian process models of loss functions (or robustness metrics) together with appropriate acquisition functions to guide the search for a robust optimal solution. Numerical studies on a test problem are presented to demonstrate the efficacy of the proposed approach.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VJ4T9MEE\\Beland et Nair - Bayesian Optimization Under Uncertainty.pdf}
}

@article{belkin_reconciling_2019,
  title = {Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = sep,
  journal = {arXiv:1812.11118 [cs, stat]},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SNYB78MY\\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XN7KICAV\\1812.html}
}

@article{bengio_learning_nodate,
  title = {Learning {{Deep Architectures}} for {{AI}}},
  author = {Bengio, Yoshua},
  pages = {56},
  abstract = {Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TR5RHUJB\\Bengio - Learning Deep Architectures for AI.pdf}
}

@article{benichou_delta_1989,
  title = {A {{Delta Method}} for {{Implicitly Defined Random Variables}}},
  author = {Benichou, Jacques and Gail, Mitchell H.},
  year = {1989},
  month = feb,
  journal = {The American Statistician},
  volume = {43},
  number = {1},
  pages = {41},
  issn = {00031305},
  doi = {10.2307/2685169}
}

@inproceedings{benneyan_probability_2006,
  title = {Probability Distributions and Variances of Quadratic Loss Functions},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Computers}} and {{Industrial Engineering}}},
  author = {Benneyan, James and Aksezer, {\c C}a{\u g}lar},
  year = {2006},
  pages = {2890--2899},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LTKZVHNL\\5db30bc0fe22538899a233d598828eac3e88.pdf}
}

@article{bentzien_decomposition_2014,
  title = {Decomposition and Graphical Portrayal of the Quantile Score},
  author = {Bentzien, Sabrina and Friederichs, Petra},
  year = {2014},
  month = jul,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {140},
  number = {683},
  pages = {1924--1934},
  issn = {1477-870X},
  doi = {10.1002/qj.2284},
  abstract = {This study expands the pool of verification methods for probabilistic weather and climate predictions by a decomposition of the quantile score (QS). The QS is a proper score function and evaluates pr...},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JAV7U7ER\\Bentzien et Friederichs - 2014 - Decomposition and graphical portrayal of the quant.pdf;C\:\\Users\\a846735\\Zotero\\storage\\I3BNU8QJ\\qj.html}
}

@article{bera_mm_2002,
  title = {The {{MM}}, {{ME}}, {{ML}}, {{EL}}, {{EF}} and {{GMM}} Approaches to Estimation: A Synthesis},
  shorttitle = {The {{MM}}, {{ME}}, {{ML}}, {{EL}}, {{EF}} and {{GMM}} Approaches to Estimation},
  author = {Bera, Anil K. and Bilias, Yannis},
  year = {2002},
  month = mar,
  journal = {Journal of Econometrics},
  series = {Information and {{Entropy Econometrics}}},
  volume = {107},
  number = {1},
  pages = {51--86},
  issn = {0304-4076},
  doi = {10.1016/S0304-4076(01)00113-0},
  abstract = {The 20th century began on an auspicious statistical note with the publication of Karl Pearson's (Philos. Mag. Ser. 50 (1900) 157) goodness-of-fit test, which is regarded as one of the most important scientific breakthroughs. The basic motivation behind this test was to see whether an assumed probability model adequately described the data at hand. Pearson (Philos. Trans. Roy. Soc. London Ser. A 185 (1894) 71) also introduced a formal approach to statistical estimation through his method of moments (MM) estimation. Ronald A. Fisher, while he was a third year undergraduate at the Gonville and Caius College, Cambridge, suggested the maximum likelihood estimation (MLE) procedure as an alternative to Pearson's MM approach. In 1922 Fisher published a monumental paper that introduced such basic concepts as consistency, efficiency, sufficiency\textemdash and even the term ``parameter'' with its present meaning. Fisher (Philos. Trans. Roy. Soc. London Ser. A 222 (1922) 309) provided the analytical foundation of MLE and studied its efficiency relative to the MM estimator. Fisher (J. Roy. Statist. Soc. 87 (1924a) 442) established the asymptotic equivalence of minimum {$\chi$}2 and ML estimators and wrote in favor of using minimum {$\chi$}2 method rather than Pearson's MM approach. Recently, econometricians have found working under assumed likelihood functions restrictive, and have suggested using a generalized version of Pearson's MM approach, commonly known as the GMM estimation procedure as advocated in Hansen (Econometrica 50 (1982) 1029). Earlier, Godambe (Ann. Math. Statist. 31 (1960) 1208) and Durbin (J. Roy. Statist. Soc. Ser. B 22 (1960) 139) developed the estimating function (EF) approach to estimation that has been proven very useful for many statistical models. A fundamental result is that score is the optimum EF. Ferguson (Ann. Math. Statist. 29 (1958) 1046) considered an approach very similar to GMM and showed that estimation based on the Pearson {$\chi$}2 statistic is equivalent to efficient GMM. Golan et al. (Maximum Entropy Econometrics: Robust Estimation with Limited Data. Wiley, New York, 1996) developed entropy-based formulation that allowed them to solve a wide range of estimation and inference problems in econometrics. More recently, Imbens et al. (Econometrica 66 (1998) 333), Kitamura and Stutzer (Econometrica 65 (1997) 861) and Mittelhammer et al. (Econometric Foundations. Cambridge University Press, Cambridge, 2000) put GMM within the framework of empirical likelihood (EL) and maximum entropy (ME) estimation. It can be shown that many of these estimation techniques can be obtained as special cases of minimizing Cressie and Read (J. Roy. Statist. Soc. Ser. B 46 (1984) 440) power divergence criterion that comes directly from the Pearson (1900) {$\chi$}2 statistic. In this way we are able to assimilate a number of seemingly unrelated estimation techniques into a unified framework.},
  langid = {english},
  keywords = {Empirical likelihood,Entropy,Estimating function,Generalized method of moments,History of estimation,Karl Pearson's goodness-of-fit statistic,Likelihood,Method of moment,Power divergence criterion},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GJL37QWG\\Bera et Bilias - 2002 - The MM, ME, ML, EL, EF and GMM approaches to estim.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PMR4TSV7\\S0304407601001130.html}
}

@article{berger_deriving_1992,
  title = {Deriving {{Generalized Means}} as {{Least Squares}} and {{Maximum Likelihood Estimates}}},
  author = {Berger, Roger L. and Casella, George},
  year = {1992},
  month = nov,
  journal = {The American Statistician},
  volume = {46},
  number = {4},
  pages = {279--282},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1992.10475904},
  abstract = {Functions called generalized means are of interest in statistics because they are simple to compute, have intuitive appeal, and can serve as reasonable parameter estimates. The well-known arithmetic, geometric, and harmonic means are all examples of generalized means. We show how generalized means can be derived in a unified way, as least squares estimates for a transformed data set. We also investigate models that have generalized means as their maximum likelihood estimates.},
  keywords = {Arithmetic mean,Exponential family,Geometric mean,Harmonic mean},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1992.10475904},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\54YTW8HV\\BU-1133-M.pdf;C\:\\Users\\a846735\\Zotero\\storage\\G8XTTU2P\\Berger et Casella - 1992 - Deriving Generalized Means as Least Squares and Ma.pdf;C\:\\Users\\a846735\\Zotero\\storage\\TQKPYYE7\\00031305.1992.html}
}

@article{berger_formal_2009,
  title = {The Formal Definition of Reference Priors},
  author = {Berger, James O. and Bernardo, Jos{\'e} M. and Sun, Dongchu},
  year = {2009},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {37},
  number = {2},
  eprint = {0904.0156},
  eprinttype = {arxiv},
  pages = {905--938},
  issn = {0090-5364},
  doi = {10.1214/07-AOS587},
  abstract = {Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically.},
  archiveprefix = {arXiv},
  keywords = {62F15 (Primary) 62A01; 62B10 (Secondary),Mathematics - Statistics Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WJGEVP2V\\Berger et al. - 2009 - The formal definition of reference priors.pdf;C\:\\Users\\a846735\\Zotero\\storage\\YC35QS73\\0904.html}
}

@article{berger_integrated_1999,
  title = {Integrated Likelihood Methods for Eliminating Nuisance Parameters},
  author = {Berger, James O. and Liseo, Brunero and Wolpert, Robert L.},
  year = {1999},
  month = feb,
  journal = {Statistical Science},
  volume = {14},
  number = {1},
  pages = {1--28},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1009211804},
  abstract = {Elimination of nuisance parameters is a central problem in statistical inference and has been formally studied in virtually all approaches to inference. Perhaps the least studied approach is elimination of nuisance parameters through integration, in the sense that this is viewed as an almost incidental byproduct of Bayesian analysis and is hence not something which is deemed to require separate study. There is, however, considerable value in considering integrated likelihood on its own, especially versions arising from default or noninformative priors. In this paper, we review such common integrated likelihoods and discuss their strengths and weaknesses relative to other methods.},
  langid = {english},
  mrnumber = {MR1702200},
  zmnumber = {1059.62521},
  keywords = {Marginal likelihood,nuisance parameters,profile likelihood,reference priors},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\K775LQTY\\Berger et al. - 1999 - Integrated likelihood methods for eliminating nuis.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KVZ6CQTS\\1009211804.html}
}

@article{berger_overview_1994,
  title = {An Overview of Robust {{Bayesian}} Analysis},
  author = {Berger, James O. and Moreno, El{\'i}as and Pericchi, Luis Raul and Bayarri, M. Jes{\'u}s and Bernardo, Jos{\'e} M. and Cano, Juan A. and {De la Horra}, Juli{\'a}n and Mart{\'i}n, Jacinto and {R{\'i}os-Ins{\'u}a}, David and Betr{\`o}, Bruno},
  year = {1994},
  journal = {Test},
  volume = {3},
  number = {1},
  pages = {5--124},
  keywords = {Bayesian inference},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8IEPC4A3\\tr93-53c.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KN3BLS4A\\BF02562676.html}
}

@phdthesis{bernard_methodes_2019,
  title = {M\'ethodes Probabilistes Pour l'estimation de Probabilit\'es de D\'efaillance},
  author = {Bernard, Lucie},
  year = {2019},
  month = jun,
  abstract = {Pour \'evaluer la rentabilit\'e d'une production en amont du lancement du processus de fabrication, la plupart des entreprises industrielles ont recours \`a la simulation num\'erique. Cela permet de tester virtuellement plusieurs configurations des param\`etres d'un produit donn\'e et de statuer quant \`a ses performances (i.e. les sp\'ecifications impos\'ees par le cahier des charges).  Afin de mesurer l'impact des fluctuations des proc\'ed\'es industriels sur les performances du produit, nous nous int\'eressons en particulier \`a l'estimation de sa probabilit\'e de d\'efaillance. Chaque simulation exigeant l'ex\'ecution d'un code de calcul complexe et co\^uteux, il n'est pas possible d'effectuer un nombre de tests suffisant pour estimer cette probabilit\'e via, par exemple, une m\'ethode Monte-Carlo. Sous la contrainte d'un nombre limit\'e d'appels au code, nous proposons deux m\'ethodes d'estimation tr\`es diff\'erentes. La premi\`ere s'appuie sur les principes de l'estimation bay\'esienne. Nos observations sont les r\'esultats de simulation num\'erique. La probabilit\'e de d\'efaillance est vue comme une variable al\'eatoire, dont la construction repose sur celle d'un processus al\'eatoire destin\'e \`a mod\'eliser le code de calcul co\^uteux. Pour d\'efinir correctement ce mod\`ele, on utilise la m\'ethode de krigeage. Conditionnellement aux observations, la loi a posteriori de la variable al\'eatoire, qui mod\'elise la probabilit\'e de d\'efaillance, est inaccessible. Pour apprendre sur cette loi, nous construisons des approximations des caract\'eristiques suivantes: esp\'erance, variance, quantiles... On utilise pour cela la th\'eorie des ordres stochastiques pour la comparaison de variables al\'eatoires et, plus particuli\`erement, l'ordre convexe. La construction d'un plan d'exp\'eriences optimal est assur\'ee par la mise en place d'une proc\'edure de planificationd'exp\'eriences s\'equentielle, bas\'ee sur le principe des strat\'egies SUR ("Stepwise Uncertainty Reduction"). La seconde m\'ethode est une proc\'edure it\'erative, particuli\`erement adapt\'ee au cas o\`u la probabilit\'e de d\'efaillance est tr\`es petite, i.e. l'\'ev\'enement redout\'e est rare. Le code de calcul co\^uteux est repr\'esent\'e par une fonction que l'on suppose lipschitzienne. \`A chaque it\'eration, cette hypoth\`ese est utilis\'ee pour construire des approximations, par d\'efaut et par exc\`es, de la probabilit\'e de d\'efaillance. Nous montrons que ces approximations convergent vers la valeur vraie avec le nombre d'it\'erations. En pratique, on les estime gr\^ace \`a la m\'ethode Monte-Carlo dite de "splitting". Les m\'ethodes que l'on propose sont relativement simples \`a mettre en \oe uvre et les r\'esultats qu'elles fournissent peuvent \^etre interpr\'et\'es sans difficult\'e. Nous les testons sur divers exemples, ainsi que sur un cas r\'eel provenant de la soci\'et\'e STMicroelectronics. Full text is available here: https://tel.archives-ouvertes.fr/tel-02279258}
}

@article{bernard_methodes_nodate,
  title = {{M\'ethodes probabilistes pour l'estimation de probabilit\'es de d\'efaillance}},
  author = {Bernard, Lucie},
  pages = {168},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XEWCUKGL\\Bernard - MÃ©thodes probabilistes pour l'estimation de probab.pdf}
}

@article{bertsimas_robust_2014,
  title = {Robust {{Sample Average Approximation}}},
  author = {Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
  year = {2014},
  month = aug,
  journal = {arXiv:1408.4445 [math]},
  eprint = {1408.4445},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Sample average approximation (SAA) is a widely popular approach to data-driven decisionmaking under uncertainty. Under mild assumptions, SAA is both tractable and enjoys strong asymptotic performance guarantees. Similar guarantees, however, do not typically hold in finite samples. In this paper, we propose a modification of SAA, which we term Robust SAA, which retains SAA's tractability and asymptotic properties and, additionally, enjoys strong finite-sample performance guarantees. The key to our method is linking SAA, distributionally robust optimization, and hypothesis testing of goodness-of-fit. Beyond Robust SAA, this connection provides a unified perspective enabling us to characterize the finite sample and asymptotic guarantees of various other data-driven procedures that are based upon distributionally robust optimization. This analysis provides insight into the practical performance of these various methods in real applications. We present examples from inventory management and portfolio allocation, and demonstrate numerically that our approach outperforms other data-driven approaches in these applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FUIBHXQN\\Bertsimas et al. - 2014 - Robust Sample Average Approximation.pdf}
}

@article{bertsimas_theory_2010,
  title = {Theory and {{Applications}} of {{Robust Optimization}}},
  author = {Bertsimas, Dimitris and Brown, David B. and Caramanis, Constantine},
  year = {2010},
  month = oct,
  journal = {arXiv:1010.5445 [cs, math]},
  eprint = {1010.5445},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {In this paper we survey the primary research, both theoretical and applied, in the area of Robust Optimization (RO). Our focus is on the computational attractiveness of RO approaches, as well as the modeling power and broad applicability of the methodology. In addition to surveying prominent theoretical results of RO, we also present some recent results linking RO to adaptable models for multi-stage decision-making problems. Finally, we highlight applications of RO across a wide spectrum of domains, including finance, statistics, learning, and various areas of engineering.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {90C25,Computer Science - Computational Engineering; Finance; and Science,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JRZF7SYP\\Bertsimas et al. - 2010 - Theory and Applications of Robust Optimization.pdf}
}

@article{bertsimas_theory_2010-1,
  title = {Theory and {{Applications}} of {{Robust Optimization}}},
  author = {Bertsimas, Dimitris and Brown, David B. and Caramanis, Constantine},
  year = {2010},
  month = oct,
  journal = {arXiv:1010.5445 [cs, math]},
  eprint = {1010.5445},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {In this paper we survey the primary research, both theoretical and applied, in the area of Robust Optimization (RO). Our focus is on the computational attractiveness of RO approaches, as well as the modeling power and broad applicability of the methodology. In addition to surveying prominent theoretical results of RO, we also present some recent results linking RO to adaptable models for multi-stage decision-making problems. Finally, we highlight applications of RO across a wide spectrum of domains, including finance, statistics, learning, and various areas of engineering.},
  archiveprefix = {arXiv},
  keywords = {90C25,Computer Science - Computational Engineering; Finance; and Science,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\T6QT64CY\\Bertsimas et al. - 2010 - Theory and Applications of Robust Optimization.pdf;C\:\\Users\\a846735\\Zotero\\storage\\6DFX49PU\\1010.html}
}

@misc{besta_parallel_2022,
  title = {Parallel and {{Distributed Graph Neural Networks}}: {{An In-Depth Concurrency Analysis}}},
  shorttitle = {Parallel and {{Distributed Graph Neural Networks}}},
  author = {Besta, Maciej and Hoefler, Torsten},
  year = {2022},
  month = may,
  number = {arXiv:2205.09702},
  eprint = {2205.09702},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in order to understand how to effectively apply techniques such as vectorization. We also formally analyze GNN pipelining, and we generalize the established Message-Passing class of GNN models to cover arbitrary pipeline depths, facilitating future optimizations. Finally, we investigate different forms of asynchronicity, navigating the path for future asynchronous parallel GNN pipelines. The outcomes of our analysis are synthesized in a set of insights that help to maximize GNN performance, and a comprehensive list of challenges and opportunities for further research into efficient GNN computations. Our work will help to advance the design of future GNNs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SYUADJUW\\Besta et Hoefler - 2022 - Parallel and Distributed Graph Neural Networks An.pdf}
}

@article{betancourt_conceptual_2017,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2017},
  month = jan,
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous under- standing of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is con- fined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any ex- haustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\97IXVFQ5\\Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;C\:\\Users\\a846735\\Zotero\\storage\\3VVTEM7T\\1701.html}
}

@phdthesis{bettinger_inversion_2009,
  title = {Inversion d'un Syst\`eme Par Krigeage: Application \`a La Synth\`ese Des Catalyseurs \`a Haut D\'ebit},
  shorttitle = {Inversion d'un Syst\`eme Par Krigeage},
  author = {Bettinger, R{\'e}gis},
  year = {2009},
  school = {Universit\'e de Nice Sophia Antipolis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3RXCDSE8\\Bettinger.pdf}
}

@article{beyer_robust_2007,
  title = {Robust Optimization \textendash{} {{A}} Comprehensive Survey},
  author = {Beyer, Hans-Georg and Sendhoff, Bernhard},
  year = {2007},
  month = jul,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {196},
  number = {33-34},
  pages = {3190--3218},
  issn = {00457825},
  doi = {10.1016/j.cma.2007.03.003},
  abstract = {This paper reviews the state-of-the-art in robust design optimization \textendash{} the search for designs and solutions which are immune with respect to production tolerances, parameter drifts during operation time, model sensitivities and others. Starting with a short glimps of Taguchi's robust design methodology, a detailed survey of approaches to robust optimization is presented. This includes a detailed discussion on how to account for design uncertainties and how to measure robustness (i.e., how to evaluate robustness). The main focus will be on the different approaches to perform robust optimization in practice including the methods of mathematical programming, deterministic nonlinear optimization, and direct search methods such as stochastic approximation and evolutionary computation. It discusses the strengths and weaknesses of the different methods, thus, providing a basis for guiding the engineer to the most appropriate techniques. It also addresses performance aspects and test scenarios for direct robust optimization techniques.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CRUQZVV2\\Beyer et Sendhoff - 2007 - Robust optimization â A comprehensive survey.pdf}
}

@article{bichon_efficient_nodate,
  title = {{{EFFICIENT SURROGATE MODELING FOR RELIABILITY ANALYSIS AND DESIGN}}},
  author = {Bichon, Barron James},
  pages = {169},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PBX8ZJTE\\Bichon - EFFICIENT SURROGATE MODELING FOR RELIABILITY ANALY.pdf}
}

@article{bickel_fast_2005,
  title = {On a {{Fast}}, {{Robust Estimator}} of the {{Mode}}: {{Comparisons}} to {{Other Robust Estimators}} with {{Applications}}},
  shorttitle = {On a {{Fast}}, {{Robust Estimator}} of the {{Mode}}},
  author = {Bickel, David R. and Fruehwirth, Rudolf},
  year = {2005},
  month = may,
  journal = {arXiv:math/0505419},
  eprint = {math/0505419},
  eprinttype = {arxiv},
  abstract = {Advances in computing power enable more widespread use of the mode, which is a natural measure of central tendency since, as the most probable value, it is not influenced by the tails in the distribution. The properties of the half-sample mode, which is a simple and fast estimator of the mode of a continuous distribution, are studied. The half-sample mode is less sensitive to outliers than most other estimators of location, including many other low-bias estimators of the mode. Its breakdown point is one half, equal to that of the median. However, because of its finite rejection point, the half-sample mode is much less sensitive to outliers that are all either greater or less than the other values of the sample. This is confirmed by applying the mode estimator and the median to samples drawn from normal, lognormal, and Pareto distributions contaminated by outliers. It is also shown that the half-sample mode, in combination with a robust scale estimator, is a highly robust starting point for iterative robust location estimators such as Huber's M-estimator. The half-sample mode can easily be generalized to modal intervals containing more or less than half of the sample. An application of such an estimator to the finding of collision points in high-energy proton-proton interactions is presented.},
  archiveprefix = {arXiv},
  keywords = {Mode estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QZFLB585\\Bickel et Fruehwirth - 2005 - On a Fast, Robust Estimator of the Mode Compariso.pdf;C\:\\Users\\a846735\\Zotero\\storage\\M5EJDIBY\\0505419.html}
}

@article{bickel_robust_2003,
  title = {Robust and Efficient Estimation of the Mode of Continuous Data: The Mode as a Viable Measure of Central Tendency},
  shorttitle = {Robust and Efficient Estimation of the Mode of Continuous Data},
  author = {Bickel, David R.},
  year = {2003},
  month = dec,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {73},
  number = {12},
  pages = {899--912},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/0094965031000097809},
  abstract = {Although a natural measure of the central tendency of a sample of continuous data is its mode (the most probable value), the mean and median are the most popular measures of location due to their simplicity and ease of estimation. The median is often used instead of the mean for asymmetric data because it is closer to the mode and is insensitive to extreme values in the sample. However, the mode itself can be reliably estimated by first transforming the data into approximately normal data by raising the values to a real power, and then estimating the mean and standard deviation of the transformed data. With this method, two estimators of the mode of the original data are proposed: a simple estimator based on estimating the mean by the sample mean and the standard deviation by the sample standard deviation, and a more robust estimator based on estimating the mean by the median and the standard deviation by the standardized median absolute deviation.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\54JEYDX5\\Bickel - 2003 - Robust and efficient estimation of the mode of con.pdf}
}

@article{bigoni_efficient_2016,
  title = {Efficient Uncertainty Quantification of a Fully Nonlinear and Dispersive Water Wave Model with Random Inputs},
  author = {Bigoni, Daniele and {Engsig-Karup}, Allan P. and Eskilsson, Claes},
  year = {2016},
  month = dec,
  journal = {Journal of Engineering Mathematics},
  volume = {101},
  number = {1},
  eprint = {1410.6338},
  eprinttype = {arxiv},
  pages = {87--113},
  issn = {0022-0833, 1573-2703},
  doi = {10.1007/s10665-016-9848-8},
  abstract = {A major challenge in next-generation industrial applications is to improve numerical analysis by quantifying uncertainties in predictions. In this work we present a formulation of a fully nonlinear and dispersive potential flow water wave model with random inputs for the probabilistic description of the evolution of waves. The model is analyzed using random sampling techniques and non-intrusive methods based on generalized Polynomial Chaos (PC). These methods allow to accurately and efficiently estimate the probability distribution of the solution and require only the computation of the solution in different points in the parameter space, allowing for the reuse of existing simulation software. The choice of the applied methods is driven by the number of uncertain input parameters and by the fact that finding the solution of the considered model is computationally intensive. We revisit experimental benchmarks often used for validation of deterministic water wave models. Based on numerical experiments and assumed uncertainties in boundary data, our analysis reveals that some of the known discrepancies from deterministic simulation in comparison with experimental measurements could be partially explained by the variability in the model input. We finally present a synthetic experiment studying the variance based sensitivity of the wave load on an offshore structure to a number of input uncertainties. In the numerical examples presented the PC methods have exhibited fast convergence, suggesting that the problem is amenable to being analyzed with such methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {65C99; 76B15,Physics - Computational Physics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SMSSFESF\\Bigoni et al. - 2016 - Efficient uncertainty quantification of a fully no.pdf}
}

@article{bilionis_solution_2013,
  title = {Solution of Inverse Problems with Limited Forward Solver Evaluations: A {{Bayesian}} Perspective},
  shorttitle = {Solution of Inverse Problems with Limited Forward Solver Evaluations},
  author = {Bilionis, I. and Zabaras, N.},
  year = {2013},
  journal = {Inverse Problems},
  volume = {30},
  number = {1},
  pages = {015004},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SUA66K2E\\Bilionis et Zabaras - 2013 - Solution of inverse problems with limited forward .pdf;C\:\\Users\\a846735\\Zotero\\storage\\TMEH4W3B\\Bilionis et Zabaras - 2013 - Solution of inverse problems with limited forward}
}

@book{billingsley_probability_2008,
  title = {Probability and Measure},
  author = {Billingsley, Patrick},
  year = {2008},
  publisher = {{John Wiley \& Sons}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7U9W5HAJ\\Billingsley - 2008 - Probability and measure.pdf}
}

@article{binois_replication_2019,
  title = {Replication or {{Exploration}}? {{Sequential Design}} for {{Stochastic Simulation Experiments}}},
  shorttitle = {Replication or {{Exploration}}?},
  author = {Binois, Micka{\"e}l and Huang, Jiangeng and Gramacy, Robert B. and Ludkovski, Mike},
  year = {2019},
  month = jan,
  journal = {Technometrics},
  volume = {61},
  number = {1},
  pages = {7--23},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2018.1469433},
  abstract = {We investigate the merits of replication, and provide methods for optimal design (including replicates), with the goal of obtaining globally accurate emulation of noisy computer simulation experiments. We first show that replication can be beneficial from both design and computational perspectives, in the context of Gaussian process surrogate modeling. We then develop a lookahead-based sequential design scheme that can determine if a new run should be at an existing input location (i.e., replicate) or at a new one (explore). When paired with a newly developed heteroscedastic Gaussian process model, our dynamic design scheme facilitates learning of signal and noise relationships which can vary throughout the input space. We show that it does so efficiently, on both computational and statistical grounds. In addition to illustrative synthetic examples, we demonstrate performance on two challenging real-data simulation experiments, from inventory management and epidemiology. Supplementary materials for the article are available online.},
  keywords = {Computer experiment,Gaussian process,Input-dependent noise,Lookahead,Replicated observations,Surrogate model},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2018.1469433},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WX3AA43G\\Binois et al. - 2019 - Replication or Exploration Sequential Design for .pdf;C\:\\Users\\a846735\\Zotero\\storage\\PVDQMSZ5\\00401706.2018.html}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {{springer}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W2MZ8V2M\\Bishop - Pattern Recognition And Machine Learning - Springer 2006.pdf}
}

@article{bissiri_general_2016,
  title = {A General Framework for Updating Belief Distributions},
  author = {Bissiri, P. G. and Holmes, C. C. and Walker, S. G.},
  year = {2016},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {78},
  number = {5},
  pages = {1103--1130},
  issn = {1467-9868},
  doi = {10.1111/rssb.12158},
  abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered as a special case. Modern application areas make it increasingly challenging for Bayesians to attempt to model the true data-generating mechanism. For instance, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our framework uses loss functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.},
  langid = {english},
  keywords = {Decision theory,General Bayesian updating,Generalized estimating equations,Gibbs posteriors,Information,Loss function,Maximum entropy,Provably approximately correct Bayes methods,Self-information loss function},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\77RV5X3W\\Bissiri et al. - 2016 - A general framework for updating belief distributi.pdf;C\:\\Users\\a846735\\Zotero\\storage\\53IJDAIQ\\rssb.html}
}

@inproceedings{blanchard_polynomial_nodate,
  title = {A {{Polynomial Chaos}} Based {{Bayesian Approach}} for {{Estimating Uncertain Parameters}} of {{Mechanical Systems}}},
  author = {Blanchard, Emmanuel and Sandu, Corina and Sandu, Adrian},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WPFGV2WC\\Blanchard_DETC2007_34600.pdf}
}

@misc{blanchet-scalliet_specific_2017,
  title = {A Specific Kriging Kernel for Dimensionality Reduction: {{Isotropic}} by Group Kernel},
  shorttitle = {A Specific Kriging Kernel for Dimensionality Reduction},
  author = {{Blanchet-Scalliet}, Christophette and Helbert, C{\'e}line and Ribaud, M{\'e}lina and Vial, C{\'e}line},
  year = {2017},
  month = mar,
  abstract = {In the context of computer experiments, metamodels are largely used to represent the output of computer codes. Among these models, Gaussian process regression (kriging) is very efficient see e.g Snelson (2008). In high dimension that is with a large number of input variables , but with few observations the classical anisotropic kriging becomes inefficient and sometimes completely wrong. One way to overcome this drawback is to use the isotropic kernel which is more robust because it estimates not as many parameters. However this model is too restrictive. The aim of this paper is to construct a model between these two, that is at the same time a robust and a flexible model. These two skills are necessary for a model in high dimension. We propose a kernel which is an answer to these requests and that we call isotropic by group kernel. This kernel is a tensor product of few isotropic kernels built on well-chosen subgroup of variables. The number and the composition of the groups are found by an algorithm which explores different structures. The choice of the best model is based on the quality of prediction.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EVJFQIXZ\\Blanchet-Scalliet et al. - 2017 - A specific kriging kernel for dimensionality reduc.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GSUA7RYB\\hal-01496521v1.html}
}

@article{blomker_predictability_2006,
  title = {Predictability of the {{Burgers}} Dynamics under Model Uncertainty},
  author = {Bl{\"o}mker, Dirk and Duan, Jinqiao},
  year = {2006},
  month = jul,
  journal = {arXiv:math/0607357},
  eprint = {math/0607357},
  eprinttype = {arxiv},
  abstract = {Complex systems may be subject to various uncertainties. A great effort has been concentrated on predicting the dynamics under uncertainty in initial conditions. In the present work, we consider the well-known Burgers equation with random boundary forcing or with random body forcing. Our goal is to attempt to understand the stochastic Burgers dynamics by predicting or estimating the solution processes in various diagnostic metrics, such as mean length scale, correlation function and mean energy. First, for the linearized model, we observe that the important statistical quantities like mean energy or correlation functions are the same for the two types of random forcing, even though the solutions behave very differently. Second, for the full nonlinear model, we estimate the mean energy for various types of random body forcing, highlighting the different impact on the overall dynamics of space-time white noises, trace class white-in-time and colored-in-space noises, point noises, additive noises or multiplicative noises.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Classical Analysis and ODEs,Mathematics - Probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MKLWLZCZ\\BlÃ¶mker et Duan - 2006 - Predictability of the Burgers dynamics under model.pdf}
}

@techreport{blonigan_machine-learned_2019,
  title = {Machine-Learned Reduced-Order Modeling.},
  author = {Blonigan, Patrick Joseph},
  year = {2019},
  month = aug,
  number = {SAND2019-9726PE},
  institution = {{Sandia National Lab. (SNL-CA), Livermore, CA (United States)}},
  abstract = {Abstract not provided.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\825QUDKT\\Blonigan - 2019 - Machine-learned reduced-order modeling..pdf;C\:\\Users\\a846735\\Zotero\\storage\\KB6NW45Y\\1645802.html}
}

@techreport{bocquet_data_2019,
  type = {Preprint},
  title = {Data Assimilation as a Deep Learning Tool to Infer {{ODE}} Representations of Dynamical Models},
  author = {Bocquet, Marc and Brajard, Julien and Carrassi, Alberto and Bertino, Laurent},
  year = {2019},
  month = feb,
  institution = {{Predictability, Data Assimilation/Climate, Atmosphere, Ocean, Hydrology, Cryosphere, Biosphere}},
  doi = {10.5194/npg-2019-7},
  abstract = {Recent progress in machine learning has shown how to forecast and, to some extent, learn the dynamics of a model from its output, resorting in particular to neural networks and deep learning techniques. We will show how the same goal can be directly achieved using data assimilation techniques without leveraging on machine learning software libraries, with a view to high-dimensional models. The dynamics of a model are learned from its observation and an ordinary differential equation 5 (ODE) representation of this model is inferred using a recursive nonlinear regression. Because the method is embedded in a Bayesian data assimilation framework, it can learn from partial and noisy observations of a state trajectory of the physical model. Moreover, a space-wise local representation of the ODE system is introduced and is key to cope with high-dimensional models.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\C65ZCNLV\\Bocquet et al. - 2019 - Data assimilation as a deep learning tool to infer.pdf}
}

@article{bocquet_degenerate_2017,
  title = {Degenerate {{Kalman}} Filter Error Covariances and Their Convergence onto the Unstable Subspace},
  author = {Bocquet, Marc and Gurumoorthy, Karthik S. and Apte, Amit and Carrassi, Alberto and Grudzien, Colin and Jones, Christopher K. R. T.},
  year = {2017},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {5},
  number = {1},
  eprint = {1604.02578},
  eprinttype = {arxiv},
  pages = {304--333},
  issn = {2166-2525},
  doi = {10.1137/16M1068712},
  abstract = {The characteristics of the model dynamics are critical in the performance of (ensemble) Kalman filters. In particular, as emphasized in the seminal work of Anna Trevisan and co-authors, the error covariance matrix is asymptotically supported by the unstable-neutral subspace only, i.e., it is spanned by the backward Lyapunov vectors with non-negative exponents. This behavior is at the core of algorithms known as Assimilation in the Unstable Subspace, although a formal proof was still missing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {93E11; 93C05; 93B07; 60G35; 15A03,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\A4ICGM5G\\Bocquet et al. - 2017 - Degenerate Kalman filter error covariances and the.pdf}
}

@article{bocquet_introduction_nodate,
  title = {Introduction to the Principles and Methods of Data Assimilation in the Geosciences},
  author = {Bocquet, Marc},
  pages = {89},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RQ6M3IV6\\Bocquet - Introduction to the principles and methods of data.pdf}
}

@article{bocquet_lecture_nodate,
  title = {Lecture 3 on Data Assimilation:   {{Hybrid}}/Ensemble Variational Methods   and Perspectives},
  author = {Bocquet, Marc},
  pages = {47},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\L9DK2SWY\\Bocquet - Lecture 3 on data assimilation   Hybridensemble .pdf}
}

@article{bocquet_online_2021,
  title = {Online Learning of Both State and Dynamics Using Ensemble {{Kalman}} Filters},
  author = {Bocquet, Marc and Farchi, Alban and Malartic, Quentin},
  year = {2021},
  journal = {Foundations of Data Science},
  volume = {3},
  number = {3},
  eprint = {2006.03859},
  eprinttype = {arxiv},
  pages = {305},
  issn = {2639-8001},
  doi = {10.3934/fods.2020015},
  abstract = {The reconstruction of the dynamics of an observed physical system as a surrogate model has been brought to the fore by recent advances in machine learning. To deal with partial and noisy observations in that endeavor, machine learning representations of the surrogate model can be used within a Bayesian data assimilation framework. However, these approaches require to consider long time series of observational data, meant to be assimilated all together. This paper investigates the possibility to learn both the dynamics and the state online, i.e. to update their estimates at any time, in particular when new observations are acquired. The estimation is based on the ensemble Kalman filter (EnKF) family of algorithms using a rather simple representation for the surrogate model and state augmentation. We consider the implication of learning dynamics online through (i) a global EnKF, (i) a local EnKF and (iii) an iterative EnKF and we discuss in each case issues and algorithmic solutions. We then demonstrate numerically the efficiency and assess the accuracy of these methods using one-dimensional, one-scale and two-scale chaotic Lorenz models.},
  archiveprefix = {arXiv},
  keywords = {62M20; 49M41; 86-08,Computer Science - Machine Learning,Nonlinear Sciences - Chaotic Dynamics,Physics - Atmospheric and Oceanic Physics,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\G5YU5P2A\\Bocquet et al. - 2021 - Online learning of both state and dynamics using e.pdf;C\:\\Users\\a846735\\Zotero\\storage\\9XFXQM6A\\2006.html}
}

@article{bogunovic_robust_nodate,
  title = {Robust {{Adaptive Decision Making}}: {{Bayesian Optimization}} and {{Beyond}}},
  author = {Bogunovic, Ilija},
  pages = {200},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RAMIJHXE\\Bogunovic - Robust Adaptive Decision Making Bayesian Optimiza.pdf}
}

@article{boink_learned_2020,
  title = {Learned {{SVD}}: Solving Inverse Problems via Hybrid Autoencoding},
  shorttitle = {Learned {{SVD}}},
  author = {Boink, Yoeri E. and Brune, Christoph},
  year = {2020},
  month = sep,
  journal = {arXiv:1912.10840 [cs, eess]},
  eprint = {1912.10840},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Our world is full of physics-driven data where effective mappings between data manifolds are desired. There is an increasing demand for understanding combined model-based and data-driven methods. We propose a nonlinear, learned singular value decomposition (L-SVD), which combines autoencoders that simultaneously learn and connect latent codes for desired signals and given measurements. We provide a convergence analysis for a specifically structured L-SVD that acts as a regularisation method. In a more general setting, we investigate the topic of model reduction via data dimensionality reduction to obtain a regularised inversion. We present a promising direction for solving inverse problems in cases where the underlying physics are not fully understood or have very complex behaviour. We show that the building blocks of learned inversion maps can be obtained automatically, with improved performance upon classical methods and better interpretability than black-box methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GL9KZAU8\\Boink et Brune - 2020 - Learned SVD solving inverse problems via hybrid a.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7J8EIWWM\\1912.html}
}

@article{bolton_applications_2019,
  title = {Applications of {{Deep Learning}} to {{Ocean Data Inference}} and {{Sub-Grid Parameterisation}}},
  author = {Bolton, Thomas and Zanna, Laure},
  year = {2019},
  month = jan,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {0},
  number = {ja},
  issn = {1942-2466},
  doi = {10.1029/2018MS001472},
  abstract = {Abstract Oceanographic observations are limited by sampling rates, while ocean models are limited by finite resolution and high viscosity and diffusion coefficients. Therefore both data from observations and ocean models lack information at small- and fast-scales. Methods are needed to either extract information, extrapolate, or up-scale existing oceanographic datasets, to account for or represent unresolved physical processes. Here we use machine learning to leverage observations and model data by predicting unresolved turbulent processes and sub-surface flow fields. As a proof-of-concept, we train convolutional neural networks on degraded-data from a high-resolution quasi-geostrophic ocean model. We demonstrate that convolutional neural networks successfully replicate the spatio-temporal variability of the sub-grid eddy momentum forcing, are capable of generalising to a range of dynamical behaviours, and can be forced to respect global momentum conservation. The training data of our convolutional neural networks can be sub-sampled to 10-20\% of the original size without a significant decrease in accuracy. We also show that the sub-surface flow field can be predicted using only information at the surface (e.g., using only satellite altimetry data). Our study indicates that data-driven approaches can be exploited to predict both sub-grid and large-scale processes, while respecting physical principles, even when data is limited to a particular region or external forcing. Our in-depth study presents evidence for the successful design of ocean eddy parameterisations for implementation in coarse-resolution climate models.},
  keywords = {Data Inference,Eddies,Machine learning,Oceanography,Turbulence},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\D36QTPGZ\\Bolton et Zanna - 2019 - Applications of Deep Learning to Ocean Data Infere.pdf;C\:\\Users\\a846735\\Zotero\\storage\\2EJ763U5\\2018MS001472.html}
}

@article{bonavita_nonlinear_2018,
  title = {Nonlinear Effects in {{4D-Var}}},
  author = {Bonavita, Massimo and Lean, Peter and Holm, Elias},
  year = {2018},
  month = sep,
  journal = {Nonlinear Processes in Geophysics},
  volume = {25},
  number = {3},
  pages = {713--729},
  publisher = {{Copernicus GmbH}},
  issn = {1023-5809},
  doi = {10.5194/npg-25-713-2018},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} The ability of a data assimilation system to deal effectively with nonlinearities arising from the prognostic model or the relationship between the control variables and the available observations has received a lot of attention in theoretical studies based on very simplified test models. Less work has been done to quantify the importance of nonlinearities in operational, state-of-the-art global data assimilation systems. In this paper we analyse the nonlinear effects present in ECMWF 4D-Var and evaluate the ability of the incremental formulation to solve the nonlinear assimilation problem in a realistic NWP environment. We find that nonlinearities have increased over the years due to a combination of increased model resolution and the ever-growing importance of observations that are nonlinearly related to the state. Incremental 4D-Var is well suited for dealing with these nonlinear effects, but at the cost of increasing the number of outer loop relinearisations. We then discuss strategies for accommodating the increasing number of sequential outer loops in the tight schedules of operational global NWP.{$<$}/p{$>$}},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7CFTGVUJ\\Bonavita et al. - 2018 - Nonlinear effects in 4D-Var.pdf;C\:\\Users\\a846735\\Zotero\\storage\\THPIM9CD\\2018.html}
}

@inproceedings{bond_parameterized_2005,
  title = {Parameterized Model Order Reduction of Nonlinear Dynamical Systems},
  booktitle = {Proceedings of the 2005 {{IEEE}}/{{ACM International}} Conference on {{Computer-aided}} Design},
  author = {Bond, Brad and Daniel, Luca},
  year = {2005},
  pages = {487--494},
  publisher = {{IEEE Computer Society}},
  keywords = {Moment matching,MOR methods},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SFL2S7Y6\\pub219_000.pdf}
}

@article{bonhomme_minimizing_nodate,
  title = {Minimizing {{Sensitivity}} to {{Model Misspecification}}},
  author = {Bonhomme, Stephane and Weidner, Martin},
  pages = {35},
  abstract = {We propose a framework to compute predictions based on an economic model when the model may be misspecified. Our approach relies on minimizing sensitivity of the estimates to the type of misspecification that is most influential for the parameter of interest. We rely on a local asymptotic approach where the degree of misspecification is indexed by the sample size. This results in simple rules to adjust the predictions from the reference model. We calibrate the degree of misspecification using a detection error probability approach, which allows us to perform systematic sensitivity analysis in both point-identified and partially-identified settings. We study three examples: demand analysis, treatment effects estimation under selection on observables, and panel data models where the distribution of individual effects may be misspecified and the number of time periods is small.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P5MMJB99\\Bonhomme et Weidner - Minimizing Sensitivity to Model Misspeciï¬cation.pdf}
}

@book{booker_rigorous_1998,
  title = {A {{Rigorous Framework}} for {{Optimization}} of {{Expensive Functions}} by {{Surrogates}}},
  author = {Booker, Andrew J. and Jr, J. E. Dennis and Frank, Paul D. and Serafini, David B. and Torczon, Virginia and Trosset, Michael W.},
  year = {1998},
  abstract = {The goal of the research reported here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical. This paper presents and analyzes a framework for generating a sequence of approximations to the objective function and managing the use of these approximations as surrogates for optimization. The result is to obtain convergence to a minimizer of an expensive objective function subject to simple constraints. The approach is widely applicable because it does not require, or even explicitly approximate, derivatives of the objective. Numerical results are presented for a 31-variable helicopter rotor blade design example and for a standard optimization test example.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\A23KJ6LH\\Booker et al. - 1998 - A Rigorous Framework for Optimization of Expensive.pdf;C\:\\Users\\a846735\\Zotero\\storage\\YPKX7WD3\\summary.html}
}

@article{borman_expectation_2004,
  title = {The Expectation Maximization Algorithm-a Short Tutorial},
  author = {Borman, Sean},
  year = {2004},
  journal = {Submitted for publication},
  pages = {1--9},
  keywords = {EM algorithm},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HGJ62YFV\\EM_algorithm.pdf}
}

@inproceedings{bossek_learning_2015,
  title = {Learning {{Feature-Parameter Mappings}} for {{Parameter Tuning}} via the {{Profile Expected Improvement}}},
  booktitle = {Proceedings of the 2015 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Bossek, Jakob and Bischl, Bernd and Wagner, Tobias and Rudolph, G{\"u}nter},
  year = {2015},
  series = {{{GECCO}} '15},
  pages = {1319--1326},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2739480.2754673},
  abstract = {The majority of algorithms can be controlled or adjusted by parameters. Their values can substantially affect the algorithms' performance. Since the manual exploration of the parameter space is tedious -- even for few parameters -- several automatic procedures for parameter tuning have been proposed. Recent approaches also take into account some characteristic properties of the problem instances, frequently termed instance features. Our contribution is the proposal of a novel concept for feature-based algorithm parameter tuning, which applies an approximating surrogate model for learning the continuous feature-parameter mapping. To accomplish this, we learn a joint model of the algorithm performance based on both the algorithm parameters and the instance features. The required data is gathered using a recently proposed acquisition function for model refinement in surrogate-based optimization: the profile expected improvement. This function provides an avenue for maximizing the information required for the feature-parameter mapping, i.e., the mapping from instance features to the corresponding optimal algorithm parameters. The approach is validated by applying the tuner to exemplary evolutionary algorithms and problems, for which theoretically grounded or heuristically determined feature-parameter mappings are available.},
  isbn = {978-1-4503-3472-3},
  keywords = {evolutionary algorithms,model-based optimization,parameter tuning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CSL4V53D\\Bossek et al. - 2015 - Learning Feature-Parameter Mappings for Parameter .pdf}
}

@book{boucheron_concentration_2013,
  title = {Concentration {{Inequalities}}: {{A Nonasymptotic Theory}} of {{Independence}}},
  shorttitle = {Concentration {{Inequalities}}},
  author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year = {2013},
  month = feb,
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199535255.001.0001},
  isbn = {978-0-19-953525-5},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7CQ8ZP24\\Boucheron et al. - 2013 - Concentration Inequalities A Nonasymptotic Theory.pdf}
}

@article{boudier_dan_2020,
  title = {{{DAN}} -- {{An}} Optimal {{Data Assimilation}} Framework Based on Machine Learning {{Recurrent Networks}}},
  author = {Boudier, Pierre and Fillion, Anthony and Gratton, Serge and G{\"u}rol, Selime},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.09694 [cs, eess]},
  eprint = {2010.09694},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Data assimilation algorithms aim at forecasting the state of a dynamical system by combining a mathematical representation of the system with noisy observations thereof. We propose a fully data driven deep learning architecture generalizing recurrent Elman networks and data assimilation algorithms which provably reaches the same prediction goals as the latter. On numerical experiments based on the well-known Lorenz system and when suitably trained using snapshots of the system trajectory (i.e. batches of state trajectories) and observations, our architecture successfully reconstructs both the analysis and the propagation of probability density functions of the system state at a given time conditioned to past observations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {93B07; 93E11; 60G35; 68T07; 94A17,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DRIIHVSK\\Boudier et al. - 2020 - DAN -- An optimal Data Assimilation framework base.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GC4TXPEP\\2010.html}
}

@article{bouezmarni_nonparametric_2010,
  title = {Nonparametric Density Estimation for Multivariate Bounded Data},
  author = {Bouezmarni, Taoufik and Rombouts, Jeroen V. K.},
  year = {2010},
  month = jan,
  journal = {Journal of Statistical Planning and Inference},
  volume = {140},
  number = {1},
  pages = {139--152},
  issn = {0378-3758},
  doi = {10.1016/j.jspi.2009.07.013},
  abstract = {We propose a new nonparametric estimator for the density function of multivariate bounded data. As frequently observed in practice, the variables may be partially bounded (e.g. nonnegative) or completely bounded (e.g. in the unit interval). In addition, the variables may have a point mass. We reduce the conditions on the underlying density to a minimum by proposing a nonparametric approach. By using a gamma, a beta, or a local linear kernel (also called boundary kernels), in a product kernel, the suggested estimator becomes simple in implementation and robust to the well known boundary bias problem. We investigate the mean integrated squared error properties, including the rate of convergence, uniform strong consistency and asymptotic normality. We establish consistency of the least squares cross-validation method to select optimal bandwidth parameters. A detailed simulation study investigates the performance of the estimators. Applications using lottery and corporate finance data are provided.},
  keywords = {Asymmetric kernels,Asymptotic properties,Bandwidth selection,Density estimation,Least squares cross-validation,Multivariate boundary bias,Nonparametric multivariate density estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3Z77C6KS\\Bouezmarni et Rombouts - 2010 - Nonparametric density estimation for multivariate .pdf;C\:\\Users\\a846735\\Zotero\\storage\\UANRQMPP\\S0378375809002249.html}
}

@article{bouhlel_gradient-enhanced_2017,
  title = {Gradient-Enhanced Kriging for High-Dimensional Problems},
  author = {Bouhlel, Mohamed Amine and Martins, Joaquim R. R. A.},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.02663 [cs, stat]},
  eprint = {1708.02663},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Surrogate models provide a low computational cost alternative to evaluating expensive functions. The construction of accurate surrogate models with large numbers of independent variables is currently prohibitive because it requires a large number of function evaluations. Gradient-enhanced kriging has the potential to reduce the number of function evaluations for the desired accuracy when efficient gradient computation, such as an adjoint method, is available. However, current gradient-enhanced kriging methods do not scale well with the number of sampling points due to the rapid growth in the size of the correlation matrix where new information are added for each sampling point in each direction of the design space. They do not scale well with the number of independent variables either due to the increase in the number of hyperparameters that needs to be estimated. To address this issue, we develop a new gradient-enhanced surrogate model approach that drastically reduced the number of hyperparameters through the use of the partial-least squares method that maintains accuracy. In addition, this method is able to control the size of the correlation matrix by adding only relevant points defined through the information provided by the partial-least squares method. To validate our method, we compare the global accuracy of the proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions, as well as engineering problems of varied complexity with up to 15 dimensions. We show that the proposed method requires fewer sampling points than conventional methods to obtain a desired accuracy, or provides more accuracy for a fixed budget of sampling points. In some cases, we get over 3 times more accurate models than a bench of surrogate models from the literature, and also over 3200 times faster than standard gradient-enhanced kriging models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\L7NWKUIQ\\Bouhlel et Martins - 2017 - Gradient-enhanced kriging for high-dimensional pro.pdf}
}

@article{bouhlel_gradient-enhanced_2019,
  title = {Gradient-Enhanced Kriging for High-Dimensional Problems},
  author = {Bouhlel, Mohamed A. and Martins, Joaquim R. R. A.},
  year = {2019},
  month = jan,
  journal = {Engineering with Computers},
  volume = {35},
  number = {1},
  pages = {157--173},
  issn = {1435-5663},
  doi = {10.1007/s00366-018-0590-x},
  abstract = {Surrogate models provide an affordable alternative to the evaluation of expensive deterministic functions. However, the construction of accurate surrogate models with many independent variables is currently prohibitive because they require a large number of function evaluations for the desired accuracy. Gradient-enhanced kriging has the potential to reduce the number of evaluations when efficient gradient computation, such as an adjoint method, is available. However, current gradient-enhanced kriging methods do not scale well with the number of sampling points because of the rapid growth in the size of the correlation matrix, where new information is added for each sampling point in each direction of the design space. Furthermore, they do not scale well with the number of independent variables because of the increase in the number of hyperparameters that must be estimated. To address this issue, we develop a new gradient-enhanced surrogate model approach that drastically reduces the number of hyperparameters through the use of the partial least squares method to maintain accuracy. In addition, this method is able to control the size of the correlation matrix by adding only relevant points defined by the information provided by the partial least squares method. To validate our method, we compare the global accuracy of the proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions, as well as engineering problems of varied complexity with up to 15 dimensions. We show that the proposed method requires fewer sampling points than conventional methods to obtain the desired accuracy, or it provides more accuracy for a fixed budget of sampling points. In some cases, we get models that are over three times more accurate than previously developed surrogate models for the same computational time, and over 3200 times faster than standard gradient-enhanced kriging models for the same accuracy.},
  langid = {english},
  keywords = {GP,High dimension},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CEZELLGQ\\Bouhlel et Martins - 2019 - Gradient-enhanced kriging for high-dimensional pro.pdf}
}

@article{bouhlel_improving_2016,
  title = {Improving Kriging Surrogates of High-Dimensional Design Models by {{Partial Least Squares}} Dimension Reduction},
  author = {Bouhlel, Mohamed Amine and Bartoli, Nathalie and Otsmane, Abdelkader and Morlier, Joseph},
  year = {2016},
  month = may,
  journal = {Structural and Multidisciplinary Optimization},
  volume = {53},
  number = {5},
  pages = {935--952},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-015-1395-9},
  abstract = {Engineering computer codes are often computationally expensive. To lighten this load, we exploit new covariance kernels to replace computationally expensive codes with surrogate models. For input spaces with large dimensions, using the kriging model in the standard way is computationally expensive because a large covariance matrix must be inverted several times to estimate the parameters of the model. We address this issue herein by constructing a covariance kernel that depends on only a few parameters. The new kernel is constructed based on information obtained from the Partial Least Squares method. Promising results are obtained for numerical examples with up to 100 dimensions, and significant computational gain is obtained while maintaining sufficient accuracy.},
  langid = {english},
  keywords = {GP,High dimension},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\94JUCVF2\\Bouhlel et al. - 2016 - Improving kriging surrogates of high-dimensional d.pdf}
}

@article{boutet_barotropic_nodate,
  title = {Barotropic Tidal Modelling : Sensitivity to Bottom Drag},
  author = {Boutet, Martial and Lathuili{\`e}re, Cyril and Baraille, R{\'e}my and Morel, Yves},
  pages = {15},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KLETX4VL\\Boutet et al. - Barotropic tidal modelling  sensitivity to bottom.pdf}
}

@phdthesis{boutet_estimation_2015,
  title = {Estimation Du Frottement Sur Le Fond Pour La Mod\'elisation de La Mar\'ee Barotrope},
  author = {Boutet, Martial},
  year = {2015},
  school = {Universit\'e d'Aix Marseille},
  keywords = {MarÃ©e,ThÃ¨se},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CSL2KG7Y\\these_martial_boutet.pdf}
}

@book{boyd_convex_2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-83378-3},
  langid = {english},
  lccn = {QA402.5 .B69 2004},
  keywords = {Convex functions,Mathematical optimization},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YZQWMY7F\\Boyd et Vandenberghe - 2004 - Convex optimization.pdf}
}

@phdthesis{brachet_schemas_2018,
  type = {These de Doctorat},
  title = {Sch\'emas Compacts Hermitiens Sur La {{Sph\`ere}} : Applications En Climatologie et Oc\'eanographie Num\'erique},
  shorttitle = {Sch\'emas Compacts Hermitiens Sur La {{Sph\`ere}}},
  author = {Brachet, Matthieu},
  year = {2018},
  month = jul,
  abstract = {L'enjeu de la simulation de la dynamique atmosph\'erique et oc\'eanographique a pris ces derni\`eres ann\'ees une importance accrue avec la question du r\'echauffement climatique. Le mod\`ele \`a simuler est complexe. Il combine les \'equations de la m\'ecanique des fluides avec celles de la thermodynamique. Au 19\`eme si\`ecle, le math\'ematicien Adh\'emar Barr\'e de Saint-Venant formule un syst\`eme d'\'equations aux d\'eriv\'ees partielles d\'ecrivant les mouvements d'un fluide soumis \`a la gravit\'e et de faible \'epaisseur. Il s'agit des \'equations Shallow Water. L'objectif de cette th\`ese est de d\'evelopper et d'analyser un algorithme de r\'esolution des \'equations Shallow Water sur une sph\`ere en rotation. Dans un premier temps, j'\'etudie diff\'erents aspects math\'ematiques des op\'erateurs aux diff\'erences finis utilis\'es par la suite en g\'eom\'etrie sph\'erique. Les sch\'emas aux diff\'erences obtenus sont utilis\'es pour r\'esoudre l'\'equation de transport, l'\'equation des ondes et l'\'equation de Burgers. Les propri\'et\'es de stabilit\'e pr\'ecision et conservation sont analys\'ees. Dans un second temps, la grille Cubed-Sphere est introduite et analys\'ee.  La structure de ce maillage est analogue \`a celle d'un cube. L'interpr\'etation de la Cubed-Sphere \`a l'aide de grands cercles permet de construire des op\'erateurs sph\'eriques discrets gradient, divergence et vorticit\'e d'ordre au moins \'egal \`a 3 (en pratique d'ordre 4).  La troisi\`eme partie de la th\`ese est d\'edi\'ee \`a diff\'erents tests pour le syst\`eme d'\'equations Shallow Water ainsi que pour l'\'equation d'advection. Les r\'esultats d\'emontrent une pr\'ecision proche de celle obtenue par les algorithmes conservatifs d'ordre 4 les plus r\'ecents},
  collaborator = {Croisille, Jean-Pierre},
  copyright = {Licence Etalab},
  school = {Universit\'e de Lorraine},
  keywords = {516.362,518,BarrÃ© de Saint-Venant; AdhÃ©mar-Jean-Claude (1797-1886) -- MÃ©thodologie,Compact scheme,Cubed-Sphere,DiscrÃ©tisation (cartographie),DiscrÃ©tisation en temps,Ãquation Shallow Water,Ãquations aux dÃ©rivÃ©es partielles,SchÃ©mas compacts,Shallow Water equation,SphÃ¨re,Time discretisation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CMPS6VLT\\DDOC_T_2018_0111_BRACHET.pdf}
}

@article{brajard_combining_2020,
  title = {Combining Data Assimilation and Machine Learning to Emulate a Dynamical Model from Sparse and Noisy Observations: {{A}} Case Study with the {{Lorenz}} 96 Model},
  shorttitle = {Combining Data Assimilation and Machine Learning to Emulate a Dynamical Model from Sparse and Noisy Observations},
  author = {Brajard, Julien and Carrassi, Alberto and Bocquet, Marc and Bertino, Laurent},
  year = {2020},
  month = jul,
  journal = {Journal of Computational Science},
  volume = {44},
  pages = {101171},
  issn = {1877-7503},
  doi = {10.1016/j.jocs.2020.101171},
  abstract = {A novel method, based on the combination of data assimilation and machine learning is introduced. The new hybrid approach is designed for a two-fold scope: (i) emulating hidden, possibly chaotic, dynamics and (ii) predicting their future states. The method consists in applying iteratively a data assimilation step, here an ensemble Kalman filter, and a neural network. Data assimilation is used to optimally combine a surrogate model with sparse noisy data. The output analysis is spatially complete and is used as a training set by the neural network to update the surrogate model. The two steps are then repeated iteratively. Numerical experiments have been carried out using the chaotic 40-variables Lorenz 96 model, proving both convergence and statistical skill of the proposed hybrid approach. The surrogate model shows short-term forecast skill up to two Lyapunov times, the retrieval of positive Lyapunov exponents as well as the more energetic frequencies of the power density spectrum. The sensitivity of the method to critical setup parameters is also presented: the forecast skill decreases smoothly with increased observational noise but drops abruptly if less than half of the model domain is observed. The successful synergy between data assimilation and machine learning, proven here with a low-dimensional system, encourages further investigation of such hybrids with more sophisticated dynamics.},
  langid = {english},
  keywords = {Data assimilation,Dynamical model,Emulator,Machine learning,Observations},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9U5MZTDD\\Brajard et al. - 2020 - Combining data assimilation and machine learning t.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZX8YX5GR\\S1877750320304725.html}
}

@article{brajard_combining_2021,
  title = {Combining Data Assimilation and Machine Learning to Infer Unresolved Scale Parametrization},
  author = {Brajard, Julien and Carrassi, Alberto and Bocquet, Marc and Bertino, Laurent},
  year = {2021},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2194},
  pages = {20200086},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2020.0086},
  abstract = {In recent years, machine learning (ML) has been proposed to devise data-driven parametrizations of unresolved processes in dynamical numerical models. In most cases, the ML training leverages high-resolution simulations to provide a dense, noiseless target state. Our goal is to go beyond the use of high-resolution simulations and train ML-based parametrization using direct data, in the realistic scenario of noisy and sparse observations. The algorithm proposed in this work is a two-step process. First, data assimilation (DA) techniques are applied to estimate the full state of the system from a truncated model. The unresolved part of the truncated model is viewed as a model error in the DA system. In a second step, ML is used to emulate the unresolved part, a predictor of model error given the state of the system. Finally, the ML-based parametrization model is added to the physical core truncated model to produce a hybrid model. The DA component of the proposed method relies on an ensemble Kalman filter while the ML parametrization is represented by a neural network. The approach is applied to the two-scale Lorenz model and to MAOOAM, a reduced-order coupled ocean-atmosphere model. We show that in both cases, the hybrid model yields forecasts with better skill than the truncated model. Moreover, the attractor of the system is significantly better represented by the hybrid model than by the truncated model. This article is part of the theme issue `Machine learning for weather and climate modelling'.},
  keywords = {data assimilation,machine learning,numerical modelling},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZSIHQWWH\\Brajard et al. - 2021 - Combining data assimilation and machine learning t.pdf}
}

@article{brajard_representing_2019,
  title = {Representing Ill-Known Parts of a Numerical Model Using a Machine Learning Approach},
  author = {Brajard, Julien and Charantonis, Anastase and Sirven, J{\'e}r{\^o}me},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.07358 [physics, stat]},
  eprint = {1903.07358},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {In numerical modeling of the Earth System, many processes remain unknown or ill represented (let us quote sub-grid processes, the dependence to unknown latent variables or the non-inclusion of complex dynamics in numerical models) but sometimes can be observed. This paper proposes a methodology to produce a hybrid model combining a physical-based model (forecasting the well-known processes) with a neural-net model trained from observations (forecasting the remaining processes). The approach is applied to a shallow-water model in which the forcing, dissipative and diffusive terms are assumed to be unknown. We show that the hybrid model is able to reproduce with great accuracy the unknown terms (correlation close to 1). For long term simulations it reproduces with no significant difference the mean state, the kinetic energy, the potential energy and the potential vorticity of the system. Lastly it is able to function with new forcings that were not encountered during the training phase of the neural network.},
  archiveprefix = {arXiv},
  keywords = {Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZYZCCCMR\\Brajard et al. - 2019 - Representing ill-known parts of a numerical model .pdf;C\:\\Users\\a846735\\Zotero\\storage\\C5IM7VMU\\1903.html}
}

@article{brehmer_flows_2020,
  title = {Flows for Simultaneous Manifold Learning and Density Estimation},
  author = {Brehmer, Johann and Cranmer, Kyle},
  year = {2020},
  month = nov,
  journal = {arXiv:2003.13913 [cs, stat]},
  eprint = {2003.13913},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce manifold-learning flows (M-flows), a new class of generative models that simultaneously learn the data manifold as well as a tractable probability density on that manifold. Combining aspects of normalizing flows, GANs, autoencoders, and energy-based models, they have the potential to represent datasets with a manifold structure more faithfully and provide handles on dimensionality reduction, denoising, and out-of-distribution detection. We argue why such models should not be trained by maximum likelihood alone and present a new training algorithm that separates manifold and density updates. In a range of experiments we demonstrate how M-flows learn the data manifold and allow for better inference than standard flows in the ambient data space.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UQGT4CY5\\2003.13913.pdf}
}

@unpublished{brenner_acceleration_2021,
  title = {Acceleration of {{Newton}}'s Method Using Nonlinear {{Jacobi}} Preconditioning},
  author = {Brenner, Konstantin},
  year = {2021},
  month = jul,
  abstract = {For mildly nonlinear systems, involving concave diagonal nonlinearities, semi-global monotone convergence of Newton's method is guarantied provided that the Jacobian of the system is an M-matrix. However, regardless this convergence result, the efficiency of Newton's method becomes poor for stiff nonlinearities. We propose a nonlinear pre-conditioning procedure inspired by the Jacobi method and resulting in a new system of equations, which can be solved by Newton's method much more efficiently. The obtained preconditioned method is shown to exhibit semi-global convergence.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VFW835R7\\Brenner - 2021 - Acceleration of Newton's method using nonlinear Ja.pdf}
}

@article{bretin_approximation_2020,
  title = {Approximation of Surface Diffusion Flow: A Second Order Variational {{Cahn--Hilliard}} Model with Degenerate Mobilities},
  shorttitle = {Approximation of Surface Diffusion Flow},
  author = {Bretin, Elie and Masnou, Simon and Sengers, Arnaud and Terii, Garry},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.03793 [cs, math]},
  eprint = {2007.03793},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {This paper tackles the approximation of surface diffusion flow using a Cahn--Hilliard-type model. We introduce and analyze a new second order variational phase field model which associates the classical Cahn--Hilliard energy with two degenerate mobilities. This association allows to gain an order of approximation of the sharp limit. In a second part, we propose some simple and efficient numerical schemes to approximate the solutions, and we provide numerical 2D and 3D experiments that illustrate the interest of our model in comparison with other Cahn--Hilliard models.},
  archiveprefix = {arXiv},
  keywords = {74N20; 35A35; 53E10; 53E40; 65M32; 35A15,Mathematics - Analysis of PDEs,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6BG937U3\\Bretin et al. - 2020 - Approximation of surface diffusion flow a second .pdf;C\:\\Users\\a846735\\Zotero\\storage\\LCE3A7VB\\2007.html}
}

@phdthesis{brevault_contributions_2015,
  type = {Theses},
  title = {Contributions to Multidisciplinary Design Optimization under Uncertainty, Application to Launch Vehicle Design.},
  author = {Brevault, L.},
  year = {2015},
  month = oct,
  school = {Ecole Nationale Sup\'erieure des Mines de Saint-\'Etienne},
  keywords = {ANALYSE FIABILITE,CONCEPTION,INCERTITUDE,LANCEUR,LAUNCH VEHICLE DESIGN,METAMODELE,MULTIDISCIPLINARY DESIGN OPTIMIZATION,OPTIMISATION MULTIDISCIPLINAIRE,RELIABILITY ANALYSIS,SURROGATE MODEL,UNCERTAINTY},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UND24GRQ\\Brevault - 2015 - Contributions to multidisciplinary design optimiza.pdf}
}

@article{brevault_contributions_nodate,
  title = {Contributions to Multidisciplinary Design Optimization under Uncertainty, Application to Launch Vehicle Design.},
  author = {Brevault, L},
  pages = {361},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ANSZ62N8\\Brevault - Contributions to multidisciplinary design optimiza.pdf}
}

@inbook{brevault_multi-fidelity_2020,
  title = {Multi-{{Fidelity}} for {{MDO Using Gaussian Processes}}},
  booktitle = {Aerospace {{System Analysis}} and {{Optimization}} in {{Uncertainty}}},
  author = {Garland, Nicolas and Le Riche, Rodolphe and Richet, Yann and Durrande, Nicolas},
  year = {2020},
  volume = {156},
  pages = {295--320},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-39126-3_8},
  collaborator = {Brevault, Lo{\"i}c and Balesdent, Mathieu and Morio, J{\'e}r{\^o}me},
  isbn = {978-3-030-39125-6 978-3-030-39126-3},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8RE5IS7S\\Garland et al. - 2020 - Multi-Fidelity for MDO Using Gaussian Processes.pdf}
}

@article{briec_mean-variance-skewness_2007,
  title = {Mean-{{Variance-Skewness Portfolio Performance Gauging}}: {{A General Shortage Function}} and {{Dual Approach}}},
  shorttitle = {Mean-{{Variance-Skewness Portfolio Performance Gauging}}},
  author = {Briec, Walter and Kerstens, Kristiaan and Jokung, Octave},
  year = {2007},
  month = jan,
  journal = {Management Science},
  volume = {53},
  number = {1},
  pages = {135--149},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.1060.0596},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2L2HPZDM\\Briec et al. - 2007 - Mean-Variance-Skewness Portfolio Performance Gaugi.pdf}
}

@article{brookes_comparison_nodate,
  title = {A Comparison of {{Fuzzy}}, {{Bayesian}} and {{Weighted Average}} Formulations of an in-Stream Habitat Suitability Model.},
  author = {Brookes, C J and Kumar, Vikas and Lane, S N},
  pages = {8},
  abstract = {Three variations of a simple in-stream habitat suitability model were implemented and the effect on output for one organism at 22 sites on one short section of a river was examined. The model uses only two factors, depth and velocity, to calculate quality and a third factor, width to quantify utility. The implementations were based on 3 multi-criteria evaluation approaches: Weighted Average, Fuzzy Sets and Bayesian probability. There was broad agreement between the formulations but important differences in detail. The model outputs indicate that uncertainty arising from model formulation is significant and can have a bearing on planning decisions. There is a complex interaction between the formulations and the characteristics of the sites. Suitability models should be used thoughtfully and implemented in ways that: facilitate exploratory analysis; present ranges of possible outputs; present indicators of uncertainty; and facilitate back tracking to explain the outputs.},
  keywords = {Bayesian,Fuzzy},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TEAUII5N\\Brookes et al. - A comparison of Fuzzy, Bayesian and Weighted Avera.pdf}
}

@article{brynjarsdottir_learning_2014,
  title = {Learning about Physical Parameters: The Importance of Model Discrepancy},
  shorttitle = {Learning about Physical Parameters},
  author = {Brynjarsd{\'o}ttir, Jenn{\'y} and O'Hagan, Anthony},
  year = {2014},
  month = nov,
  journal = {Inverse Problems},
  volume = {30},
  number = {11},
  pages = {114007},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/30/11/114007},
  keywords = {Discrepancy,Model inadequacy},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\97P352BQ\\Learning-about-physical-parameters-The-importance-of-model-discrepancy.pdf}
}

@article{buchholz_high_nodate,
  title = {High Dimensional {{Bayesian}} Computation},
  author = {Buchholz, Alexander},
  pages = {168},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9IIL2VQM\\Buchholz - High dimensional Bayesian computation.pdf}
}

@book{bucholtz_handbook_2013,
  title = {Handbook of {{Differential Entropy}}},
  author = {Bucholtz, Frank},
  year = {2013},
  month = nov,
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/b15991},
  isbn = {978-1-4665-8316-0 978-1-4665-8317-7},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6UFL6ZZI\\Bucholtz - 2013 - Handbook of Differential Entropy.pdf}
}

@phdthesis{buessler_construction_2019,
  type = {These de Doctorat},
  title = {Construction et Validation d'un Atlas Statistique {{3D}} Des Os Carpiens Humains},
  author = {Buessler, Emilie},
  year = {2019},
  month = dec,
  abstract = {Le poignet humain est une articulation essentielle, car il est \`a l'origine de la grande amplitude de mouvement de la main. C'est \'egalement une articulation complexe, compos\'ee de huit petits os carpiens, qui sont connect\'es aux cinq m\'etacarpes et aux deux os de l'avant-bras. La complexit\'e de l'articulation est non seulement due \`a ce grand nombre d'os, mais \'egalement \`a la petite taille des os carpiens et \`a leurs formes \'elabor\'ees, qui rendent le mouvement des os les uns autour des autres \'egalement complexe. Dans cette th\`ese, nous nous sommes int\'eress\'es \`a la mod\'elisation 3D de la forme des os. Peu de travaux ont \'et\'e men\'es jusqu'\`a pr\'esent sur la mod\'elisation des os du poignet, et peu de donn\'ees exploitables pour des mod\`eles informatiques ont \'et\'e collect\'ees. Or de tels mod\`eles informatiques peuvent avoir de nombreuses applications : ils peuvent servir de base pour la cr\'eation d'outils informatiques automatis\'es ou encore \^etre int\'egr\'es dans des logiciels servant de support au diagnostique. La qualit\'e des r\'esultats de telles applications d\'epend de la qualit\'e du mod\`ele utilis\'e. C'est pourquoi nous attachons une attention particuli\`ere \`a la validation de notre travail, alors m\^eme qu'il n'existe pas de mesure directes pour l'\'evaluation, et qu'il faut utiliser des m\'etriques indirectes.Nous nous sommes int\'eress\'es \`a des outils pour la mod\'elisation 3D, particuli\`erement aux techniques de correspondance entre maillages. Nous pr\'esentons une m\'ethode pour transformer des maillages bruts directement cr\'e\'es \`a partir de tomodensitogrammes en nouveaux maillages repr\'esentant les m\^emes os tout en d\'efinissant des relations de correspondance. \%Une telle proc\'edure de remaillage n'est pas triviale \`a d\'efinir ni \`a valider, c'est pourquoi nous y portons une attention particuli\`ere.Une fois d\'efinies, ces relations rendent possible de nombreuses applications, qui permettent une validation suppl\'ementaire des correspondances. Nous pr\'esentons plusieurs applications. La variabilit\'e de la forme des os est analys\'ee \`a l'aide d'outils statistiques tels que l'Analyse en Composantes Principales (ACP) ainsi qu'un outil bas\'e sur les Processus Gaussiens.Les capacit\'es d'adaptation du mod\`ele ACP \`a de nouvelles formes sont utilis\'ees pour d\'efinir des relations de correspondance avec une seconde base de donn\'ees. Nous proposons \'egalement une m\'ethode pour transf\'erer simplement des syst\`emes de coordonn\'ees ou tout autre point remarquable d\'efinis pour quelques exemples vers le reste de la base de donn\'ees. Une telle application est utile pour des \'etudes biom\'ecaniques de mouvement du poignet. Finalement, dans une derni\`ere \'etape, nous nous sommes int\'eress\'es \`a la mod\'elisation des mouvements des os du poignet \`a l'aide d'un mod\`ele param\'etrique bas\'e sur des pr\'edicteurs significatifs et facilement mesurables.},
  collaborator = {Reveret, Lionel and Quaine, Franck},
  copyright = {Licence Etalab},
  school = {Universit\'e Grenoble Alpes (ComUE)},
  keywords = {510,Anatomical atlas,Articulation radio-carpienne,Atlas anatomique,BiomÃ©canique,Biomechanics,Calcul adaptatif,Carpal Bones,Carpe (anatomie),Correspondance,Correspondence,Maillage,Mesh,Os carpiens,Poignet,Wrist}
}

@inproceedings{buhmann_robust_2013,
  title = {Robust Optimization in the Presence of Uncertainty},
  booktitle = {Proceedings of the 4th Conference on {{Innovations}} in {{Theoretical Computer Science}} - {{ITCS}} '13},
  author = {Buhmann, Joachim M. and Mihalak, Matus and Sramek, Rastislav and Widmayer, Peter},
  year = {2013},
  pages = {505},
  publisher = {{ACM Press}},
  address = {{Berkeley, California, USA}},
  doi = {10.1145/2422436.2422491},
  abstract = {We study optimization in the presence of uncertainty such as noise in measurements, and advocate a novel approach of tackling it. The main difference to any existing approach is that we do not assume any knowledge about the nature of the uncertainty (such as for instance a probability distribution). Instead, we are given several instances of the same optimization problem as input, and, assuming they are typical w.r.t. the uncertainty, we make use of it in order to compute a solution that is good for the sample instances as well as for future (unknown) typical instances.},
  isbn = {978-1-4503-1859-4},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NI3RV4WM\\Buhmann et al. - 2013 - Robust optimization in the presence of uncertainty.pdf}
}

@article{buhmann_robust_2018,
  title = {Robust Optimization in the Presence of Uncertainty: {{A}} Generic Approach},
  shorttitle = {Robust Optimization in the Presence of Uncertainty},
  author = {Buhmann, J. M. and Gronskiy, A. Y. and Mihal{\'a}k, M. and Pr{\"o}ger, T. and {\v S}r{\'a}mek, R. and Widmayer, P.},
  year = {2018},
  month = jun,
  journal = {Journal of Computer and System Sciences},
  series = {Journal of {{Computer}} and {{System Science}}: 50 Years of Celebration. {{In}} Memory of {{Professor Edward Blum}}.},
  volume = {94},
  pages = {135--166},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2017.10.004},
  abstract = {We propose a novel approach for optimization under uncertainty. Our approach does not assume any particular noise model behind the measurements, and only requires two typical instances. We first propose a measure of similarity of instances (with respect to a given objective). Based on this measure, we then choose a solution randomly among all solutions that are near-optimum for both instances. The exact notion of near-optimum is intertwined with the proposed similarity measure. Our similarity measure also allows us to derive formal statements about the expected quality of the computed solution. Furthermore, we apply our approach to various optimization problems.},
  keywords = {Instance similarity,Noise,Optimization,Robustness,Uncertainty},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PBY9I6I9\\Buhmann et al. - 2018 - Robust optimization in the presence of uncertainty.pdf;C\:\\Users\\a846735\\Zotero\\storage\\J2ZPYUX7\\S002200001730212X.html}
}

@article{bui-thanh_model_2008,
  title = {Model Reduction for Large-Scale Systems with High-Dimensional Parametric Input Space},
  author = {{Bui-Thanh}, Tan and Willcox, Karen and Ghattas, Omar},
  year = {2008},
  journal = {SIAM Journal on Scientific Computing},
  volume = {30},
  number = {6},
  pages = {3270--3288},
  keywords = {MOR methods,reduction parameter space},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\U7L8XC6G\\bwg08.pdf}
}

@article{bujanovic_norm_2020,
  title = {Norm and Trace Estimation with Random Rank-One Vectors},
  author = {Bujanovi{\'c}, Zvonimir and Kressner, Daniel},
  year = {2020},
  month = aug,
  journal = {arXiv:2004.06433 [cs, math, stat]},
  eprint = {2004.06433},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {A few matrix-vector multiplications with random vectors are often sufficient to obtain reasonably good estimates for the norm of a general matrix or the trace of a symmetric positive semi-definite matrix. Several such probabilistic estimators have been proposed and analyzed for standard Gaussian and Rademacher random vectors. In this work, we consider the use of rankone random vectors, that is, Kronecker products of (smaller) Gaussian or Rademacher vectors. It is not only cheaper to sample such vectors but it can sometimes also be much cheaper to multiply a matrix with a rank-one vector instead of a general vector. In this work, theoretical and numerical evidence is given that the use of rank-one instead of unstructured random vectors still leads to good estimates. In particular, it is shown that our rank-one estimators multiplied with a modest constant constitute, with high probability, upper bounds of the quantity of interest. Partial results are provided for the case of lower bounds. The application of our techniques to condition number estimation for matrix functions is illustrated.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {65F35 (Primary) 60E15 (Secondary),Mathematics - Numerical Analysis,Mathematics - Statistics Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WRZFUL7Y\\BujanoviÄ et Kressner - 2020 - Norm and trace estimation with random rank-one vec.pdf}
}

@book{bujanovic_norm_2020-1,
  title = {Norm and Trace Estimation with Random Rank-One Vectors},
  author = {Bujanovi{\'c}, Zvonimir and Kressner, Daniel},
  year = {2020},
  month = apr,
  abstract = {A few matrix-vector multiplications with random vectors are often sufficient to obtain reasonably good estimates for the norm of a general matrix or the trace of a symmetric positive semi-definite matrix. Several such probabilistic estimators have been proposed and analyzed for standard Gaussian and Rademacher random vectors. In this work, we consider the use of rank-one random vectors, that is, Kronecker products of (smaller) Gaussian or Rademacher vectors. It is not only cheaper to sample such vectors but it can sometimes also be much cheaper to multiply a matrix with a rank-one vector instead of a general vector. In this work, theoretical and numerical evidence is given that the use of rank-one instead of unstructured random vectors still leads to good estimates. In particular, it is shown that our rank-one estimators multiplied with a modest constant constitute, with high probability, upper bounds of the quantity of interest. Partial results are provided for the case of lower bounds. The application of our techniques to condition number estimation for matrix functions is illustrated.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\D8QLCXQY\\BujanoviÄ et Kressner - 2020 - Norm and trace estimation with random rank-one vec.pdf}
}

@article{bull_convergence_nodate,
  title = {Convergence {{Rates}} of {{Efficient Global Optimization Algorithms}}},
  author = {Bull, Adam D and Bull, A},
  pages = {26},
  abstract = {In the efficient global optimization problem, we minimize an unknown function f , using as few observations f (x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HTC3Q7X6\\Bull et Bull - Convergence Rates of Efï¬cient Global Optimization .pdf}
}

@article{burnham_multimodel_2004,
  title = {Multimodel {{Inference}}: {{Understanding AIC}} and {{BIC}} in {{Model Selection}}},
  shorttitle = {Multimodel {{Inference}}},
  author = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  month = nov,
  journal = {Sociological Methods \& Research},
  volume = {33},
  number = {2},
  pages = {261--304},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124104268644},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LJDVJXE3\\Burnham et Anderson - 2004 - Multimodel Inference Understanding AIC and BIC in.pdf}
}

@article{byrd_use_2011,
  title = {On the {{Use}} of {{Stochastic Hessian Information}} in {{Optimization Methods}} for {{Machine Learning}}},
  author = {Byrd, Richard and Chin, Gillian and Neveitt, Will and Nocedal, Jorge},
  year = {2011},
  month = jul,
  journal = {SIAM Journal on Optimization},
  volume = {21},
  pages = {977--995},
  doi = {10.1137/10079923X},
  abstract = {This paper describes how to incorporate sampled curvature information in a Newton-CG method and in a limited memory quasi-Newton method for statistical learning. The motivation for this work stems from supervised machine learning applications involving a very large number of training points. We follow a batch approach, also known in the stochastic optimization literature as a sample average approximation (SAA) approach. Curvature information is incorporated in two sub-sampled Hessian algorithms, one based on a matrix-free inexact Newton iteration and one on a preconditioned limited memory BFGS iteration. A crucial feature of our technique is that Hessian-vector multiplications are carried out with a significantly smaller sample size than is used for the function and gradient. The efficiency of the proposed methods is illustrated using a machine learning application involving speech recognition.}
}

@article{calafiore_scenario_2006,
  title = {The {{Scenario Approach}} to {{Robust Control Design}}},
  author = {Calafiore, G.C. and Campi, M.C.},
  year = {2006},
  month = may,
  journal = {IEEE Transactions on Automatic Control},
  volume = {51},
  number = {5},
  pages = {742--753},
  issn = {0018-9286},
  doi = {10.1109/TAC.2006.875041},
  abstract = {We propose a new probabilistic solution framework for robust control analysis and synthesis problems that can be expressed in the form of minimization of a linear objective subject to convex constraints parameterized by uncertainty terms. This includes for instance the wide class of NP-hard control problems representable by means of parameter-dependent linear matrix inequalities (LMIs).},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8GWTGJ7K\\Calafiore et Campi - 2006 - The Scenario Approach to Robust Control Design.pdf}
}

@article{calafiore_scenario_2006-1,
  title = {The {{Scenario Approach}} to {{Robust Control Design}}},
  author = {Calafiore, G.C. and Campi, M.C.},
  year = {2006},
  month = may,
  journal = {IEEE Transactions on Automatic Control},
  volume = {51},
  number = {5},
  pages = {742--753},
  issn = {0018-9286},
  doi = {10.1109/TAC.2006.875041},
  abstract = {We propose a new probabilistic solution framework for robust control analysis and synthesis problems that can be expressed in the form of minimization of a linear objective subject to convex constraints parameterized by uncertainty terms. This includes for instance the wide class of NP-hard control problems representable by means of parameter-dependent linear matrix inequalities (LMIs).},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UCVEUFN8\\Calafiore et Campi - 2006 - The Scenario Approach to Robust Control Design.pdf}
}

@article{capolei_waterflooding_2013,
  title = {Waterflooding Optimization in Uncertain Geological Scenarios},
  author = {Capolei, Andrea and Suwartadi, Eka and Foss, Bjarne and J{\o}rgensen, John Bagterp},
  year = {2013},
  month = dec,
  journal = {Computational Geosciences},
  volume = {17},
  number = {6},
  pages = {991--1013},
  issn = {1420-0597, 1573-1499},
  doi = {10.1007/s10596-013-9371-1},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WH6UF2IS\\Capolei et al. - 2013 - Waterflooding optimization in uncertain geological.pdf}
}

@book{casella_statistical_2002,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  year = {2002},
  volume = {2},
  publisher = {{Duxbury Pacific Grove, CA}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SQK9PBTE\\Casella et Berger - 2002 - Statistical inference.pdf;C\:\\Users\\a846735\\Zotero\\storage\\X9I3J2BZ\\Casella et Berger - 2002 - Statistical inference.pdf}
}

@article{celeux_stochastic_1992,
  title = {On {{Stochastic Versions}} of the {{EM Algorithm}}},
  author = {Celeux, Gilles and Chauveau, Didier and Diebolt, Jean},
  year = {1992},
  month = dec,
  journal = {Bulletin of Sociological Methodology/Bulletin de M\'ethodologie Sociologique},
  volume = {37},
  number = {1},
  pages = {55--57},
  issn = {0759-1063, 2070-2779},
  doi = {10.1177/075910639203700105},
  abstract = {We compare three different stochastic versions of the EM algorithm: The SEM algorithm, the SAEM algorithm and the MCEM algorithm. We suggest that the most relevant contribution of the MCEM methodology is what we call the simulated annealing MCEM algorithm, which turns out to be very close to SAEM. We focus particularly on the mixture of distributions problem. In this context, we review the available theoretical results on the convergence of these algorithms and on the behavior of SEM as the sample size tends to infinity. The second part is devoted to intensive Monte Carlo numerical simulations and a real data study. We show that, for some particular mixture situations, the SEM algorithm is almost always preferable to the EM and simulated annealing versions SAEM and MCEM. For some very intricate mixtures, however, none of these algorithms can be confidently used. Then, SEM can be used as an efficient data exploratory tool for locating significant maxima of the likelihood function. In the real data case, we show that the SEM stationary distribution provides a contrasted view of the loglikelihood by emphasizing sensible maxima.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BIUEYSS9\\1992 - Institut national de recherche en informatique et .pdf}
}

@article{chapelle_empirical_nodate,
  title = {An {{Empirical Evaluation}} of {{Thompson Sampling}}},
  author = {Chapelle, Olivier and Li, Lihong},
  pages = {9},
  abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TVUBSS3S\\Chapelle et Li - An Empirical Evaluation of Thompson Sampling.pdf}
}

@article{chen_beta_1999,
  title = {Beta Kernel Estimators for Density Functions},
  author = {Chen, Song Xi},
  year = {1999},
  month = aug,
  journal = {Computational Statistics \& Data Analysis},
  volume = {31},
  number = {2},
  pages = {131--145},
  issn = {0167-9473},
  doi = {10.1016/S0167-9473(99)00010-9},
  abstract = {Kernel estimators using non-negative kernels are considered to estimate probability density functions with compact supports. The kernels are chosen from a family of beta densities. The beta kernel estimators are free of boundary bias, non-negative and achieve the optimal rate of convergence for the mean integrated squared error. The proposed beta kernel estimators have two features. One is that the different amount of smoothing is allocated by naturally varying kernel shape without explicitly changing the value of the smoothing bandwidth. Another feature is that the support of the beta kernels can match the support of the density function; this leads to larger effective sample sizes used in the density estimation and can produce density estimates that have smaller finite-sample variance than some other estimators.},
  keywords = {Beta kernels,Boundary bias,Density estimation,Local linear estimators,Variable kernels},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7GIVZIWE\\Chen - 1999 - Beta kernel estimators for density functions.pdf;C\:\\Users\\a846735\\Zotero\\storage\\EHYXW4JH\\S0167947399000109.html}
}

@incollection{chen_estimating_2000,
  title = {Estimating {{Marginal Posterior Densities}}},
  booktitle = {Monte {{Carlo Methods}} in {{Bayesian Computation}}},
  author = {Chen, Ming-Hui and Shao, Qi-Man and Ibrahim, Joseph G.},
  year = {2000},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {94--123},
  publisher = {{Springer, New York, NY}},
  doi = {10.1007/978-1-4612-1276-8_4},
  abstract = {In Bayesian inference, a joint posterior distribution is available through the likelihood function and a prior distribution. One purpose of Bayesian inference is to calculate and display marginal posterior densities because the marginal posterior densities provide complete information about parameters of interest. As shown in Chapter 2, a Markov chain Monte Carlo (MCMC) sampling algorithm, such as the Gibbs sampler or a Metropolis-Hastings algorithm, can be used to draw MCMC samples from the posterior distribution. Chapter 3 also demonstrates how we can easily obtain posterior quantities such as posterior means, posterior standard deviations, and other posterior quantities from MCMC samples. However, when a Bayesian model becomes complicated, it may be difficult to obtain a reliable estimator of a marginal posterior density based on the MCMC sample. A traditional method for estimating marginal posterior densities is kernel density estimation. Since the kernel density estimator is nonparametric, it may not be efficient. On the other hand, the kernel density estimator may not be applicable for some complicated Bayesian models. In the context of Bayesian inference, the joint posterior density is typically known up to a normalizing constant. Using the structure of a posterior density, a number of authors (e.g., Gelfand, Smith, and Lee 1992; Johnson 1992; Chen 1993 and 1994; Chen and Shao 1997c; Chib 1995; Verdinelli and Wasserman 1995) propose parametric marginal posterior density estimators based on the MCMC sample. In this chapter, we present several available Monte Carlo (MC) methods for computing marginal posterior density estimators, and we also discuss how well marginal posterior density estimation works using the Kullback\textemdash Leibler (K\textemdash L) divergence as a performance measure.},
  isbn = {978-1-4612-7074-4 978-1-4612-1276-8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\56KWF5P8\\Chen et al. - 2000 - Estimating Marginal Posterior Densities.pdf;C\:\\Users\\a846735\\Zotero\\storage\\YRAALCCA\\978-1-4612-1276-8_4.html}
}

@article{chen_hessian_2011,
  title = {Hessian {{Matrix}} vs. {{Gauss}}\textendash{{Newton Hessian Matrix}}},
  author = {Chen, Pei},
  year = {2011},
  month = jan,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {49},
  number = {4},
  pages = {1417--1435},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1429},
  doi = {10.1137/100799988},
  abstract = {In this paper, we investigate how the Gauss\textendash Newton Hessian matrix affects the basin of convergence in Newton-type methods. Although the Newton algorithm is theoretically superior to the Gauss\textendash Newton algorithm and the Levenberg\textendash Marquardt (LM) method as far as their asymptotic convergence rate is concerned, the LM method is often preferred in nonlinear least squares problems in practice. This paper presents a theoretical analysis of the advantage of the Gauss\textendash Newton Hessian matrix. It is proved that the Gauss\textendash Newton approximation function is the only nonnegative convex quadratic approximation that retains a critical property of the original objective function: taking the minimal value of zero on an \$(n-1)\$-dimensional manifold (or affine subspace). Due to this property, the Gauss\textendash Newton approximation does not change the zero-on-\$(n-1)\$-D ``structure'' of the original problem, explaining the reason why the Gauss\textendash Newton Hessian matrix is preferred for nonlinear least squares problems, especially when the initial point is far from the solution.},
  keywords = {65Y10,65Y20,GaussâNewton Hessian matrix,Hessian matrix,least squares problem},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4P8YFZYR\\Chen - 2011 - Hessian Matrix vs. Gauss-Newton Hessian Matrix.pdf}
}

@article{chen_recurrent_2013,
  title = {Recurrent Implicit Dynamics for Online Matrix Inversion},
  author = {Chen, Ke},
  year = {2013},
  month = jun,
  journal = {Applied Mathematics and Computation},
  volume = {219},
  number = {20},
  pages = {10218--10224},
  issn = {0096-3003},
  doi = {10.1016/j.amc.2013.03.117},
  abstract = {A novel kind of recurrent implicit dynamics together with its electronic realization is proposed and exploited for real-time matrix inversion. Compared to conventional explicit neural dynamics, our proposed model in the form of implicit dynamics has the following advantages: (a) can coincide better with systems in practice; and (b) has higher abilities in representing dynamic systems. More importantly, our model can achieve superior convergence performance in comparison with the existing dynamic systems, specifically Gradient-based dynamics (GD) and recently-proposed Zhang dynamics (ZD). Theoretical analysis and computer simulation results substantiate the effectiveness and superior efficiency of our model for online matrix inversion.},
  keywords = {Explicit dynamics,Global exponential convergence,Implicit dynamics,Lyapunov stability theory,Online matrix inversion}
}

@article{chen_reduced_nodate,
  title = {Reduced {{Collocation Methods}}: {{Reduced Basis Methods}} in the {{Collocation Framework}}},
  shorttitle = {Reduced {{Collocation Methods}}},
  author = {Chen, Yanlai and Gottlieb, Sigal},
  journal = {Journal of Scientific Computing},
  volume = {55},
  number = {3},
  pages = {718--737},
  issn = {0885-7474},
  abstract = {Abstract: In this paper, we present \{\textbackslash{} em the first\} reduced basis method well-suited for the collocation framework. Two fundamentally different algorithms are presented: the so-called Least Squares Reduced Collocation Method (LSRCM) and Empirical},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BXLVDHK5\\Reduced_Collocation_Methods_Reduced_Basis_Methods_in_the_Collocation_Framework.html}
}

@article{cheng_general_2008,
  title = {General Frequentist Properties of the Posterior Profile Distribution},
  author = {Cheng, Guang and Kosorok, Michael R.},
  year = {2008},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {36},
  number = {4},
  eprint = {math/0612191},
  eprinttype = {arxiv},
  pages = {1819--1853},
  issn = {0090-5364},
  doi = {10.1214/07-AOS536},
  abstract = {In this paper, inference for the parametric component of a semiparametric model based on sampling from the posterior profile distribution is thoroughly investigated from the frequentist viewpoint. The higher-order validity of the profile sampler obtained in Cheng and Kosorok [Ann. Statist. 36 (2008)] is extended to semiparametric models in which the infinite dimensional nuisance parameter may not have a root-\$n\$ convergence rate. This is a nontrivial extension because it requires a delicate analysis of the entropy of the semiparametric models involved. We find that the accuracy of inferences based on the profile sampler improves as the convergence rate of the nuisance parameter increases. Simulation studies are used to verify this theoretical result. We also establish that an exact frequentist confidence interval obtained by inverting the profile log-likelihood ratio can be estimated with higher-order accuracy by the credible set of the same type obtained from the posterior profile distribution. Our theory is verified for several specific examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62G20; 62F25 (Primary) 62F15; 62F12 (Secondary),Mathematics - Statistics Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MAKXMJ3I\\Cheng et Kosorok - 2008 - General frequentist properties of the posterior pr.pdf}
}

@book{cheng_generalised_2022,
  title = {Generalised {{Latent Assimilation}} in {{Heterogeneous Reduced Spaces}} with {{Machine Learning Surrogate Models}}},
  author = {Cheng, Sibo and Chen, Jianhua and Anastasiou, Charitos and Angeli, Panagiota and Matar, Omar and Guo, Yi-Ke and Pain, Christopher and Arcucci, R.},
  year = {2022},
  month = apr,
  abstract = {Reduced-order modelling and low-dimensional surrogate models generated using machine learning algorithms have been widely applied in high-dimensional dynamical systems to improve the algorithmic efficiency. In this paper, we develop a system which combines reduced-order surrogate models with a novel data assimilation (DA) technique used to incorporate real-time observations from different physical spaces. We make use of local smooth surrogate functions which link the space of encoded system variables and the one of current observations to perform variational DA with a low computational cost. The new system, named Generalised Latent Assimilation can benefit both the efficiency provided by the reduced-order modelling and the accuracy of data assimilation. A theoretical analysis of the difference between surrogate and original assimilation cost function is also provided in this paper where an upper bound, depending on the size of the local training set, is given. The new approach is tested on a high-dimensional CFD application of a two-phase liquid flow with non-linear observation operators that current Latent Assimilation methods can not handle. Numerical results demonstrate that the proposed assimilation approach can significantly improve the reconstruction and prediction accuracy of the deep learning surrogate model which is nearly 1000 times faster than the CFD simulation.}
}

@article{cheng_observation_2021,
  title = {Observation Data Compression for Variational Assimilation of Dynamical Systems},
  author = {Cheng, Sibo and Lucor, Didier and Argaud, Jean-Philippe},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.05427 [cs, math]},
  eprint = {2106.05427},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Accurate estimation of error covariances (both background and observation) is crucial for efficient observation compression approaches in data assimilation of large-scale dynamical problems. We propose a new combination of a covariance tuning algorithm with existing PCA-type data compression approaches, either observation- or information-based, with the aim of reducing the computational cost of real-time updating at each assimilation step. Relying on a local assumption of flow-independent error covariances, dynamical assimilation residuals are used to adjust the covariance in each assimilation window. The estimated covariances then contribute to better specify the principal components of either the observation dynamics or the state-observation sensitivity. The proposed approaches are first validated on a shallow water twin experiment with correlated and non-homogeneous observation error. Proper selection of flow-independent assimilation windows, together with sampling density for background error estimation, and sensitivity of the approaches to the observations error covariance knowledge, are also discussed and illustrated with various numerical tests and results. The method is then applied to a more challenging industrial hydrological model with real-world data and non-linear transformation operator provided by an operational precipitation-flow simulation software.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MB9AC7YF\\Cheng et al. - 2021 - Observation data compression for variational assim.pdf}
}

@inproceedings{cheng_robust_2009,
  title = {Robust Reliability Optimization of Mechanical Components Using Non-Probabilistic Interval Model},
  booktitle = {2009 {{IEEE}} 10th {{International Conference}} on {{Computer-Aided Industrial Design Conceptual Design}}},
  author = {Cheng, X.},
  year = {2009},
  month = nov,
  pages = {821--825},
  doi = {10.1109/CAIDCD.2009.5375184},
  abstract = {Mechanical components design is subjected to uncertainties in material and geometrical properties, loads and other variables. For reliability optimization design with uncertain parameters, based on the non-probabilistic reliability theory, robust design and optimal design method, the uncertain parameters of mechanical components are expressed by non-probabilistic interval variables, and a non-probabilistic measure and procedure for robust reliability computation is presented. Compared with the conventional probabilistic reliability optimization approach, the proposed method does not require a presumed probability distribution of the uncertain parameters and only the bounds or ranges of their variations are required. The optimal design for non-probabilistic robust reliability is formulated as a two level optimization problem, in which the first level minimizes the original robust optimal objective with the constraints of non-probabilistic reliability index, and the secondary level is used to identify the reliability index. The purpose of it is to get a tradeoff between the design objective and the robustness to uncertainties and satisfy the requirements for reliability. For instance, the robust reliability optimization of unidirectional wedge-typed overrunning clutch is proposed, and the results show that the proposed method is useful for mechanical components design and quality improvement.},
  keywords = {Constraint optimization,design engineering,Design methodology,Design optimization,geometrical properties,mechanical components design,Mechanical Components Design,Mechanical factors,mechanical products,Mechanical variables measurement,Non-probabilistic Interval Model,nonprobabilistic interval model,nonprobabilistic interval variables,nonprobabilistic measure,nonprobabilistic reliability index,nonprobabilistic reliability theory,nonprobabilistic robust reliability,optimal design,optimisation,Optimization methods,probabilistic reliability optimization,probability,probability distribution,Probability distribution,reliability,Reliability Optimization,reliability optimization design,Reliability theory,robust design,Robust Design,robust reliability optimization,Robustness,two level optimization problem,uncertain parameters,Uncertainty,unidirectional wedge-typed overrunning clutch},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H9PVKPM9\\5375184.html}
}

@article{chennault_adjoint-matching_2021,
  title = {Adjoint-{{Matching Neural Network Surrogates}} for {{Fast 4D-Var Data Assimilation}}},
  author = {Chennault, Austin and Popov, Andrey A. and Subrahmanya, Amit N. and Cooper, Rachel and Karpatne, Anuj and Sandu, Adrian},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.08626 [cs, math]},
  eprint = {2111.08626},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {The data assimilation procedures used in many operational numerical weather forecasting systems are based around variants of the 4D-Var algorithm. The cost of solving the 4D-Var problem is dominated by the cost of forward and adjoint evaluations of the physical model. This motivates their substitution by fast, approximate surrogate models. Neural networks offer a promising approach for the data-driven creation of surrogate models. The accuracy of the surrogate 4D-Var problem's solution has been shown to depend explicitly on accurate modeling of the forward and adjoint for other surrogate modeling approaches and in the general nonlinear setting. We formulate and analyze several approaches to incorporating derivative information into the construction of neural network surrogates. The resulting networks are tested on out of training set data and in a sequential data assimilation setting on the Lorenz-63 system. Two methods demonstrate superior performance when compared with a surrogate network trained without adjoint information, showing the benefit of incorporating adjoint information into the training process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z93C7H4L\\Chennault et al. - 2021 - Adjoint-Matching Neural Network Surrogates for Fas.pdf;C\:\\Users\\a846735\\Zotero\\storage\\6QE5TYQQ\\2111.html}
}

@article{chevalier_estimating_nodate,
  title = {Estimating and Quantifying Uncertainties on Level Sets Using the {{Vorob}}'ev Expectation and Deviance with {{Gaussian}} Process Models},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David and Bect, Julien and Molchanov, Ilya},
  pages = {9},
  abstract = {Several methods based on Kriging have been recently proposed for calculating a probability of failure involving costly-to-evaluate functions. A closely related problem is to estimate the set of inputs leading to a response exceeding a given threshold. Now, estimating such level set \textendash{} and not solely its volume \textendash{} and quantifying uncertainties on it are not straightforward. Here we use notions from random set theory to obtain an estimate of the level set, together with a quantification of estimation uncertainty. We give explicit formulae in the Gaussian process set-up, and provide a consistency result. We then illustrate how space-filling versus adaptive design strategies may sequentially reduce level set estimation uncertainty.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CCUH53KS\\Chevalier et al. - Estimating and quantifying uncertainties on level .pdf}
}

@article{chevalier_fast_nodate,
  title = {Fast Uncertainty Reduction Strategies Relying on {{Gaussian}} Process Models},
  author = {Chevalier, Cl{\'e}ment},
  pages = {196},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9U4ENIG7\\Chevalier - Fast uncertainty reduction strategies relying on G.pdf}
}

@article{chevalier_kriginv:_2014,
  title = {{{KrigInv}}: {{An}} Efficient and User-Friendly Implementation of Batch-Sequential Inversion Strategies Based on Kriging},
  shorttitle = {{{KrigInv}}},
  author = {Chevalier, Cl{\'e}ment and Picheny, Victor and Ginsbourger, David},
  year = {2014},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {71},
  pages = {1021--1034},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.03.008},
  abstract = {Several strategies relying on kriging have recently been proposed for adaptively estimating contour lines and excursion sets of functions under severely limited evaluation budget. The recently released R package KrigInv3 is presented and offers a sound implementation of various sampling criteria for those kinds of inverse problems. KrigInv is based on the DiceKriging package, and thus benefits from a number of options concerning the underlying kriging models. Six implemented sampling criteria are detailed in a tutorial and illustrated with graphical examples. Different functionalities of KrigInv are gradually explained. Additionally, two recently proposed criteria for batch-sequential inversion are presented, enabling advanced users to distribute function evaluations in parallel on clusters or clouds of machines. Finally, auxiliary problems are discussed. These include the fine tuning of numerical integration and optimization procedures used within the computation and the optimization of the considered criteria.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DKH97IKK\\Chevalier et al. - 2014 - KrigInv An efficient and user-friendly implementa.pdf}
}

@misc{chung_diffusion_2022,
  title = {Diffusion {{Posterior Sampling}} for {{General Noisy Inverse Problems}}},
  author = {Chung, Hyungjin and Kim, Jeongsol and Mccann, Michael T. and Klasky, Marc L. and Ye, Jong Chul},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14687},
  eprint = {2209.14687},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14687},
  abstract = {Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via the Laplace approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UL5V35ZA\\Chung et al. - 2022 - Diffusion Posterior Sampling for General Noisy Inv.pdf;C\:\\Users\\a846735\\Zotero\\storage\\3C889SC2\\2209.html}
}

@article{chung_efficient_2014,
  title = {An {{Efficient Approach}} for {{Computing Optimal Low-Rank Regularized Inverse Matrices}}},
  author = {Chung, Julianne and Chung, Matthias},
  year = {2014},
  month = nov,
  journal = {Inverse Problems},
  volume = {30},
  number = {11},
  eprint = {1404.1610},
  eprinttype = {arxiv},
  pages = {114009},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/30/11/114009},
  abstract = {Standard regularization methods that are used to compute solutions to ill-posed inverse problems require knowledge of the forward model. In many real-life applications, the forward model is not known, but training data is readily available. In this paper, we develop a new framework that uses training data, as a substitute for knowledge of the forward model, to compute an optimal low-rank regularized inverse matrix directly, allowing for very fast computation of a regularized solution. We consider a statistical framework based on Bayes and empirical Bayes risk minimization to analyze theoretical properties of the problem. We propose an efficient rank update approach for computing an optimal low-rank regularized inverse matrix for various error measures. Numerical experiments demonstrate the benefits and potential applications of our approach to problems in signal and image processing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZGN3J3U9\\Chung et Chung - 2014 - An Efficient Approach for Computing Optimal Low-Ra.pdf}
}

@article{chung_optimal_2015,
  title = {Optimal Regularized Low Rank Inverse Approximation},
  author = {Chung, Julianne and Chung, Matthias and O'Leary, Dianne P.},
  year = {2015},
  month = mar,
  journal = {Linear Algebra and its Applications},
  series = {18th {{ILAS Conference}}},
  volume = {468},
  pages = {260--269},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2014.07.024},
  abstract = {In this paper, we consider the problem of finding approximate inverses of a given matrix. We give an explicit solution to the rank-constrained regularized inverse approximation problem and obtain an inverse-regularized Eckart\textendash Young-like theorem.},
  langid = {english},
  keywords = {Approximate pseudoinverse,EckartâYoung theorem,Ill-posed problems,Inverse problems,Low rank approximation,Regularization,Tikhonov regularization,TSVD},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GVX4GERN\\Chung et al. - 2015 - Optimal regularized low rank inverse approximation.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GNC7YQG5\\S0024379514004741.html}
}

@article{chunlin_sample_2012,
  title = {Sample {{Average Approximation Method}} for {{Chance Constrained Stochastic Programming}} in {{Transportation Model}} of {{Emergency Management}}},
  author = {Chunlin, Deng and Liu, Yang},
  year = {2012},
  month = jan,
  journal = {Systems Engineering Procedia},
  series = {Safety and {{Emergency Systems Engineering}}},
  volume = {5},
  pages = {137--143},
  issn = {2211-3819},
  doi = {10.1016/j.sepro.2012.04.022},
  abstract = {In this paper, an optimal model for the transportation of emergency resource is established on chance constrained stochastic programming. We use Conditional Value at Risk (CVaR) to approximate the chance constraint, and we get the approximation problem of the chance constrained stochastic programming by using the sample average approximation (SAA) method. For a given sample, the SAA problem is a deterministic nonlinear programming (NLP) and any appropriate NLP code can be applied to solve the problem. The model and method provide a new way for the emergency logistics management engineering.},
  langid = {english},
  keywords = {chance constraint,Conditional Value-at-Risk,emergency managemen engineering,sample average approximation,transportation model},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NDZ9P4NP\\Chunlin et Liu - 2012 - Sample Average Approximation Method for Chance Con.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LKTZ6PTJ\\S2211381912000653.html}
}

@misc{cohen_log-linear-time_2022,
  title = {Log-{{Linear-Time Gaussian Processes Using Binary Tree Kernels}}},
  author = {Cohen, Michael K. and Daulton, Samuel and Osborne, Michael A.},
  year = {2022},
  month = oct,
  number = {arXiv:2210.01633},
  eprint = {2210.01633},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.01633},
  abstract = {Gaussian processes (GPs) produce good probabilistic models of functions, but most GP kernels require \$O((n+m)n\^2)\$ time, where \$n\$ is the number of data points and \$m\$ the number of predictive locations. We present a new kernel that allows for Gaussian process regression in \$O((n+m)\textbackslash log(n+m))\$ time. Our "binary tree" kernel places all data points on the leaves of a binary tree, with the kernel depending only on the depth of the deepest common ancestor. We can store the resulting kernel matrix in \$O(n)\$ space in \$O(n \textbackslash log n)\$ time, as a sum of sparse rank-one matrices, and approximately invert the kernel matrix in \$O(n)\$ time. Sparse GP methods also offer linear run time, but they predict less well than higher dimensional kernels. On a classic suite of regression tasks, we compare our kernel against Mat\textbackslash 'ern, sparse, and sparse variational kernels. The binary tree GP assigns the highest likelihood to the test data on a plurality of datasets, usually achieves lower mean squared error than the sparse methods, and often ties or beats the Mat\textbackslash 'ern GP. On large datasets, the binary tree GP is fastest, and much faster than a Mat\textbackslash 'ern GP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6X43SS6X\\Cohen et al. - 2022 - Log-Linear-Time Gaussian Processes Using Binary Tr.pdf;C\:\\Users\\a846735\\Zotero\\storage\\BC3NGPSM\\2210.html}
}

@article{cohn_introduction_1997,
  title = {An {{Introduction}} to {{Estimation Theory}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}: {{Theory}} and {{Practice}})},
  shorttitle = {An {{Introduction}} to {{Estimation Theory}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}},
  author = {Cohn, Stephen E.},
  year = {1997},
  journal = {Journal of the Meteorological Society of Japan. Ser. II},
  volume = {75},
  number = {1B},
  pages = {257--288},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\488E3CR6\\Cohn - 1997 - An Introduction to Estimation Theory (gtSpecial Is.pdf;C\:\\Users\\a846735\\Zotero\\storage\\3ACGSNZS\\ja.html}
}

@phdthesis{cook_effective_2018,
  type = {Thesis},
  title = {Effective {{Formulations}} of {{Optimization Under Uncertainty}} for {{Aerospace Design}}},
  author = {Cook, Laurence William},
  year = {2018},
  month = jul,
  doi = {10.17863/CAM.23427},
  abstract = {Formulations of optimization under uncertainty (OUU) commonly used in  aerospace design\textemdash those based on treating statistical moments of the quantity  of interest (QOI) as separate objectives\textemdash can result in stochastically dominated  designs. A stochastically dominated design is undesirable, because it is less likely  than another design to achieve a QOI at least as good as a given value, for any  given value.    As a remedy to this limitation for the multi-objective formulation of moments,  a novel OUU formulation is proposed\textemdash dominance optimization. This formulation  seeks a set of solutions and makes use of global optimizers, so is useful for early  stages of the design process when exploration of design space is important.    Similarly, to address this limitation for the single-objective formulation of  moments (combining moments via a weighted sum), a second novel formulation  is proposed\textemdash horsetail matching. This formulation can make use of gradient-  based local optimizers, so is useful for later stages of the design process when  exploitation of a region of design space is important. Additionally, horsetail  matching extends straightforwardly to different representations of uncertainty,  and is flexible enough to emulate several existing OUU formulations.    Existing multi-fidelity methods for OUU are not compatible with these novel  formulations, so one such method\textemdash information reuse\textemdash is generalized to be  compatible with these and other formulations.    The proposed formulations, along with generalized information reuse, are  compared to their most comparable equivalent in the current state-of-the-art  on practical design problems: transonic aerofoil design, coupled aero-structural  wing design, high-fidelity 3D wing design, and acoustic horn shape design.    Finally, the two novel formulations are combined in a two-step design process,  which is used to obtain a robust design in a challenging version of the acoustic horn  design problem. Dominance optimization is given half the computational budget  for exploration; then horsetail matching is given the other half for exploitation.  Using exactly the same computational budget as a moment-based approach, the  design obtained using the novel formulations is 95\% more likely to achieve a  better QOI than the best value achievable by the moment-based design.},
  copyright = {Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)},
  langid = {english},
  school = {University of Cambridge},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8W7UKHCR\\Cook - 2018 - Effective Formulations of Optimization Under Uncer.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RWIX948G\\276146.html}
}

@article{cook_extending_2017,
  title = {Extending {{Horsetail Matching}} for {{Optimization Under Probabilistic}}, {{Interval}}, and {{Mixed Uncertainties}}},
  author = {Cook, Laurence W. and Jarrett, Jerome P. and Willcox, Karen E.},
  year = {2017},
  month = oct,
  journal = {AIAA Journal},
  volume = {56},
  number = {2},
  pages = {849--861},
  issn = {0001-1452},
  doi = {10.2514/1.J056371},
  abstract = {This paper presents a new approach for optimization under uncertainty in the presence of probabilistic, interval, and mixed uncertainties, avoiding the need to specify probability distributions on uncertain parameters when such information is not readily available. Existing approaches for optimization under these types of uncertainty mostly rely on treating combinations of statistical moments as separate objectives, but this can give rise to stochastically dominated designs. Here, horsetail matching is extended for use with these types of uncertainties to overcome some of the limitations of existing approaches. The formulation delivers a single, differentiable metric as the objective function for optimization. It is demonstrated on algebraic test problems, the design of a wing using a low-fidelity coupled aerostructural code, and the aerodynamic shape optimization of a wing using computational fluid dynamics analysis.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZBT9TGQK\\Cook et al. - 2017 - Extending Horsetail Matching for Optimization Unde.pdf;C\:\\Users\\a846735\\Zotero\\storage\\SJNAGEIJ\\1.html}
}

@article{cook_horsetail_2018,
  title = {Horsetail Matching: A Flexible Approach to Optimization under Uncertainty},
  shorttitle = {Horsetail Matching},
  author = {Cook, L. W. and Jarrett, J. P.},
  year = {2018},
  month = apr,
  journal = {Engineering Optimization},
  volume = {50},
  number = {4},
  pages = {549--567},
  issn = {0305-215X, 1029-0273},
  doi = {10.1080/0305215X.2017.1327581},
  abstract = {It is important to design engineering systems to be robust with respect to uncertainties in the design process. Often, this is done by considering statistical moments, but over-reliance on statistical moments when formulating a robust optimization can produce designs that are stochastically dominated by other feasible designs. This article instead proposes a formulation for optimization under uncertainty that minimizes the difference between a design's cumulative distribution function and a target. A standard target is proposed that produces stochastically non-dominated designs, but the formulation also offers enough flexibility to recover existing approaches for robust optimization. A numerical implementation is developed that employs kernels to give a differentiable objective function. The method is applied to algebraic test problems and a robust transonic airfoil design problem where it is compared to multi-objective, weighted-sum and density matching approaches to robust optimization; several advantages over these existing methods are demonstrated.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QZQCNVQ7\\Cook et Jarrett - 2018 - Horsetail matching a flexible approach to optimiz.pdf}
}

@article{cooper_improving_nodate,
  title = {Improving {{Inundation Forecasting}} Using {{Data Assimilation}}},
  author = {Cooper, E S and Dance, S L and Nichols, N K and Smith, P J and {Garcia-Pintado}, J},
  pages = {20},
  abstract = {Fluvial flooding is a costly problem in the UK and worldwide. Real-time accurate inundation forecasting can help to reduce the damage caused by inundation events by alerting people to take necessary mitigating actions. This work is part of an effort to improve inundation forecasting using data assimilation (DA). DA is a method for combining a numerical model of a system with observations in order to best estimate the current state of the system. A river inundation model has been developed using numerical implementation of the shallow water equations with an inflow source term added; the model and implementation of the source term are described here. The model has then been used with idealised river valley topographies in order to investigate the sensitivities of the system to model parameters describing the effect of friction between water and the river channel, and the effect of the topography slope at the downstream boundary. Initial DA experiments using an ensemble Kalman Filter are also described. Identical twin experiments show that the DA as implemented in this domain can correct the water levels at the time of the observations, and that more observations lead to a better correction. However, by the time of the next observations very similar water levels are predicted, regardless of the number of observations used in the assimilation. This implies that the effective time for observations in this system is small compared to the time between observations.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5UJ8F2JJ\\Cooper et al. - Improving Inundation Forecasting using Data Assimi.pdf}
}

@article{cotter_data_2013,
  title = {Data Assimilation on the Exponentially Accurate Slow Manifold},
  author = {Cotter, Colin},
  year = {2013},
  month = may,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {371},
  number = {1991},
  pages = {20120300},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2012.0300},
  abstract = {I describe an approach to data assimilation making use of an explicit map that defines a coordinate system on the slow manifold in the semi-geostrophic scaling in Lagrangian coordinates, and apply the approach to a simple toy system that has previously been proposed as a low-dimensional model for the semi-geostrophic scaling. The method can be extended to Lagrangian particle methods such as Hamiltonian particle\textendash mesh and smooth-particle hydrodynamics applied to the rotating shallow-water equations, and many of the properties will remain for more general Eulerian methods. Making use of Hamiltonian normal-form theory, it has previously been shown that, if initial conditions for the system are chosen as image points of the map, then the fast components of the system have exponentially small magnitude for exponentially long times as {$\epsilon\rightarrow$}0, and this property is preserved if one uses a symplectic integrator for the numerical time stepping. The map may then be used to parametrize initial conditions near the slow manifold, allowing data assimilation to be performed without introducing any fast degrees of motion (more generally, the precise amount of fast motion can be selected).},
  keywords = {data assimilation,exponential estimates,symplectic integrators},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2K644XY4\\Cotter - 2013 - Data assimilation on the exponentially accurate sl.pdf}
}

@phdthesis{couderc_dassfow-shallow_2013,
  title = {Dassfow-Shallow, Variational Data Assimilation for Shallow-Water Models: {{Numerical}} Schemes, User and Developer Guides},
  shorttitle = {Dassfow-Shallow, Variational Data Assimilation for Shallow-Water Models},
  author = {Couderc, Fr{\'e}d{\'e}ric and Madec, Ronan and Monnier, J{\'e}r{\^o}me and Vila, Jean-Paul},
  year = {2013},
  school = {University of Toulouse, CNRS, IMT, INSA, ANR},
  file = {C\:\\Users\\Victor\\Dropbox\\INRIA 2017\\LittÃ©rature\\DassFow-Shallow_Variational_Data_Assimilation_for_.pdf}
}

@article{cox_partial_1975,
  title = {Partial Likelihood},
  author = {Cox, D. R.},
  year = {1975},
  month = aug,
  journal = {Biometrika},
  volume = {62},
  number = {2},
  pages = {269--276},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/62.2.269},
  abstract = {Abstract.  A definition is given of partial likelihood generalizing the ideas of conditional and marginal likelihood. Applications include life tables and infer},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5ASFZ34K\\Cox - 1975 - Partial likelihood.pdf;C\:\\Users\\a846735\\Zotero\\storage\\M6HFBWG2\\337051.html}
}

@article{craig_bayesian_2001,
  title = {Bayesian {{Forecasting}} for {{Complex Systems Using Computer Simulators}}},
  author = {Craig, Peter S and Goldstein, Michael and Rougier, Jonathan C and Seheult, Allan H},
  year = {2001},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {454},
  pages = {717--729},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214501753168370},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4ZGE4AUB\\Craig et al. - 2001 - Bayesian Forecasting for Complex Systems Using Com.pdf}
}

@article{cui_data-free_2021,
  title = {Data-{{Free Likelihood-Informed Dimension Reduction}} of {{Bayesian Inverse Problems}}},
  author = {Cui, Tiangang and Zahm, Olivier},
  year = {2021},
  month = apr,
  journal = {Inverse Problems},
  volume = {37},
  number = {4},
  eprint = {2102.13245},
  eprinttype = {arxiv},
  pages = {045009},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/1361-6420/abeafb},
  abstract = {Identifying a low-dimensional informed parameter subspace offers a viable path to alleviating the dimensionality challenge in the sampled-based solution to large-scale Bayesian inverse problems. This paper introduces a novel gradient-based dimension reduction method in which the informed subspace does not depend on the data. This permits an online-offline computational strategy where the expensive low-dimensional structure of the problem is detected in an offline phase, meaning before observing the data. This strategy is particularly relevant for multiple inversion problems as the same informed subspace can be reused. The proposed approach allows controlling the approximation error (in expectation over the data) of the posterior distribution. We also present sampling strategies that exploit the informed subspace to draw efficiently samples from the exact posterior distribution. The method is successfully illustrated on two numerical examples: a PDE-based inverse problem with a Gaussian process prior and a tomography problem with Poisson data and a Besov-\$\textbackslash mathcal\{B\}\^2\_\{11\}\$ prior.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5SLJ34EG\\Cui et Zahm - 2021 - Data-Free Likelihood-Informed Dimension Reduction .pdf;C\:\\Users\\a846735\\Zotero\\storage\\U2UQPCN4\\Cui et Zahm - 2021 - Data-Free Likelihood-Informed Dimension Reduction .pdf;C\:\\Users\\a846735\\Zotero\\storage\\5Y37GDWF\\2102.html}
}

@article{cui_likelihood-informed_2014,
  title = {Likelihood-Informed Dimension Reduction for Nonlinear Inverse Problems},
  author = {Cui, Tiangang and Martin, James and Marzouk, Youssef M. and Solonen, Antti and Spantini, Alessio},
  year = {2014},
  month = nov,
  journal = {Inverse Problems},
  volume = {30},
  number = {11},
  eprint = {1403.4680},
  eprinttype = {arxiv},
  pages = {114015},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/30/11/114015},
  abstract = {The intrinsic dimensionality of an inverse problem is affected by prior information, the accuracy and number of observations, and the smoothing properties of the forward operator. From a Bayesian perspective, changes from the prior to the posterior may, in many problems, be confined to a relatively low-dimensional subspace of the parameter space. We present a dimension reduction approach that defines and identifies such a subspace, called the "likelihood-informed subspace" (LIS), by characterizing the relative influences of the prior and the likelihood over the support of the posterior distribution. This identification enables new and more efficient computational methods for Bayesian inference with nonlinear forward models and Gaussian priors. In particular, we approximate the posterior distribution as the product of a lower-dimensional posterior defined on the LIS and the prior distribution marginalized onto the complementary subspace. Markov chain Monte Carlo sampling can then proceed in lower dimensions, with significant gains in computational efficiency. We also introduce a Rao-Blackwellization strategy that de-randomizes Monte Carlo estimates of posterior expectations for additional variance reduction. We demonstrate the efficiency of our methods using two numerical examples: inference of permeability in a groundwater system governed by an elliptic PDE, and an atmospheric remote sensing problem based on Global Ozone Monitoring System (GOMOS) observations.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\47LK8WJT\\Cui et al. - 2014 - Likelihood-informed dimension reduction for nonlin.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RNW3P6CB\\1403.html}
}

@article{cui_scalable_2016,
  title = {Scalable Posterior Approximations for Large-Scale {{Bayesian}} Inverse Problems via Likelihood-Informed Parameter and State Reduction},
  author = {Cui, Tiangang and Marzouk, Youssef M. and Willcox, Karen E.},
  year = {2016},
  month = jun,
  journal = {Journal of Computational Physics},
  volume = {315},
  eprint = {1510.06053},
  eprinttype = {arxiv},
  pages = {363--387},
  issn = {00219991},
  doi = {10.1016/j.jcp.2016.03.055},
  abstract = {Two major bottlenecks to the solution of large-scale Bayesian inverse problems are the scaling of posterior sampling algorithms to high-dimensional parameter spaces and the computational cost of forward model evaluations. Yet incomplete or noisy data, the state variation and parameter dependence of the forward model, and correlations in the prior collectively provide useful structure that can be exploited for dimension reduction in this setting--both in the parameter space of the inverse problem and in the state space of the forward model. To this end, we show how to jointly construct low-dimensional subspaces of the parameter space and the state space in order to accelerate the Bayesian solution of the inverse problem. As a byproduct of state dimension reduction, we also show how to identify low-dimensional subspaces of the data in problems with high-dimensional observations. These subspaces enable approximation of the posterior as a product of two factors: (i) a projection of the posterior onto a low-dimensional parameter subspace, wherein the original likelihood is replaced by an approximation involving a reduced model; and (ii) the marginal prior distribution on the high-dimensional complement of the parameter subspace. We present and compare several strategies for constructing these subspaces using only a limited number of forward and adjoint model simulations. The resulting posterior approximations can rapidly be characterized using standard sampling techniques, e.g., Markov chain Monte Carlo. Two numerical examples demonstrate the accuracy and efficiency of our approach: inversion of an integral equation in atmospheric remote sensing, where the data dimension is very high; and the inference of a heterogeneous transmissivity field in a groundwater system, which involves a partial differential equation forward model with high dimensional state and parameters.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6Y8QQ22T\\Cui et al. - 2016 - Scalable posterior approximations for large-scale .pdf;C\:\\Users\\a846735\\Zotero\\storage\\4G4Z2HFF\\1510.html}
}

@article{cui_uncertainty_nodate,
  title = {Uncertainty {{Quantification}} in {{Inverse Problems}}: A {{Subspace Approach}}},
  author = {Cui, Tiangang and Marzouk, Youssef and Willcox, Karen},
  pages = {42},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\S579UVIJ\\Cui et al. - Uncertainty Quantification in Inverse Problems a .pdf}
}

@article{cunningham_principal_2022,
  title = {Principal {{Manifold Flows}}},
  author = {Cunningham, Edmond and Cobb, Adam and Jha, Susmit},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.07037 [cs, stat]},
  eprint = {2202.07037},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Normalizing flows map an independent set of latent variables to their samples using a bijective transformation. Despite the exact correspondence between samples and latent variables, their high level relationship is not well understood. In this paper we characterize the geometric structure of flows using principal manifolds and understand the relationship between latent variables and samples using contours. We introduce a novel class of normalizing flows, called principal manifold flows (PF), whose contours are its principal manifolds, and a variant for injective flows (iPF) that is more efficient to train than regular injective flows. PFs can be constructed using any flow architecture, are trained with a regularized maximum likelihood objective and can perform density estimation on all of their principal manifolds. In our experiments we show that PFs and iPFs are able to learn the principal manifolds over a variety of datasets. Additionally, we show that PFs can perform density estimation on data that lie on a manifold with variable dimensionality, which is not possible with existing normalizing flows.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NGQSIHK7\\Cunningham et al. - 2022 - Principal Manifold Flows.pdf}
}

@incollection{d._bates_uncertainty_2014,
  title = {Uncertainty in {{Flood Inundation Modelling}}},
  author = {D. Bates, Paul and Pappenberger, Florian and Romanowicz, Renata},
  year = {2014},
  month = mar,
  pages = {232--269},
  doi = {10.1142/9781848162716_0010},
  isbn = {978-1-84816-270-9},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BQGH65W8\\D. Bates et al. - 2014 - Uncertainty in Flood Inundation Modelling.pdf}
}

@book{d._morris_bayesian_2012,
  title = {Bayesian {{Design}} and {{Analysis}} of {{Computer Experiments}}: {{Use}} of {{Derivatives}} in {{Surface Prediction}}},
  shorttitle = {Bayesian {{Design}} and {{Analysis}} of {{Computer Experiments}}},
  author = {D. Morris, Max and J. Mitchell, Toby and Ylvisaker, Don},
  year = {2012},
  month = mar,
  volume = {35},
  doi = {10.1080/00401706.1993.10485320},
  abstract = {The work of Currin et al. and others in developing fast predictive approximations'' of computer models is extended for the case in which derivatives of the output variable of interest with respect to input variables are available. In addition to describing the calculations required for the Bayesian analysis, the issue of experimental design is also discussed, and an algorithm is described for constructing maximin distance'' designs. An example is given based on a demonstration model of eight inputs and one output, in which predictions based on a maximin design, a Latin hypercube design, and two compromise'' designs are evaluated and compared. 12 refs., 2 figs., 6 tabs.}
}

@article{daescu_efficiency_2007,
  title = {Efficiency of a {{POD-based}} Reduced Second-Order Adjoint Model in {{4D-Var}} Data Assimilation},
  author = {Daescu, D. N. and Navon, I. M.},
  year = {2007},
  journal = {International Journal for Numerical Methods in Fluids},
  volume = {53},
  number = {6},
  pages = {985--1004},
  issn = {1097-0363},
  doi = {10.1002/fld.1316},
  abstract = {Order reduction strategies aim to alleviate the computational burden of the four-dimensional variational data assimilation by performing the optimization in a low-order control space. The proper orthogonal decomposition (POD) approach to model reduction is used to identify a reduced-order control space for a two-dimensional global shallow water model. A reduced second-order adjoint (SOA) model is developed and used to facilitate the implementation of a Hessian-free truncated-Newton (HFTN) minimization algorithm in the POD-based space. The efficiency of the SOA/HFTN implementation is analysed by comparison with the quasi-Newton BFGS and a nonlinear conjugate gradient algorithm. Several data assimilation experiments that differ only in the optimization algorithm employed are performed in the reduced control space. Numerical results indicate that first-order derivative methods are effective during the initial stages of the assimilation; in the later stages, the use of second-order derivative information is of benefit and HFTN provided significant CPU time savings when compared to the BFGS and CG algorithms. A comparison with data assimilation experiments in the full model space shows that with an appropriate selection of the basis functions the optimization in the POD space is able to provide accurate results at a reduced computational cost. The HFTN algorithm benefited most from the order reduction since computational savings were achieved both in the outer and inner iterations of the method. Further experiments are required to validate the approach for comprehensive global circulation models. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {adjoint model,data assimilation,model reduction,optimization},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/fld.1316},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BBMURKUQ\\fld.html}
}

@techreport{daks_golden_2017,
  type = {{{SSRN Scholarly Paper}}},
  title = {Do the {{Golden State Warriors Have Hot Hands}}?},
  author = {Daks, Alon and Desai, Nishant and Goldberg, Lisa R.},
  year = {2017},
  month = nov,
  number = {ID 2984615},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {Star Golden State Warriors Steph Curry, Klay Thompson, and Kevin Durant are great shooters but they are not streak shooters. Only rarely do they show signs of a hot hand. This conclusion is based on an empirical analysis of field goal and free throw data from the 82 regular season and 17 postseason games played by the Warriors in 2016-2017. Our analysis is inspired by the iconic 1985 hot-hand study by Thomas Gilovitch, Robert Vallone and Amos Tversky, but uses a permutation test to automatically account for Josh Miller and Adam Sanjurjo's recent small sample correction. In this study we show how long standing problems can be reexamined using nonparametric statistics to avoid faulty hypothesis tests due to misspecified distributions.},
  langid = {english},
  keywords = {conditional probability,hot hand,nonparametric test,permutation test,small sample correction,streak shooting},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VAXQU5TJ\\papers.html}
}

@article{damblin_adaptive_2015,
  title = {Adaptive Numerical Designs for the Calibration of Computer Codes},
  author = {Damblin, Guillaume and Barbillon, Pierre and Keller, Merlin and Pasanisi, Alberto and Parent, Eric},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.07252 [stat]},
  eprint = {1502.07252},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Making good predictions of a physical system using a computer code requires the inputs to be carefully specified. Some of these inputs called control variables have to reproduce physical conditions whereas other inputs, called parameters, are specific to the computer code and most often uncertain. The goal of statistical calibration consists in estimating these parameters with the help of a statistical model which links the code outputs with the field measurements. In a Bayesian setting, the posterior distribution of these parameters is normally sampled using MCMC methods. However, they are impractical when the code runs are high time-consuming. A way to circumvent this issue consists of replacing the computer code with a Gaussian process emulator, then sampling a cheap-to-evaluate posterior distribution based on it. Doing so, calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator. We aim at reducing this error by building a proper sequential design by means of the Expected Improvement criterion. Numerical illustrations in several dimensions assess the efficiency of such sequential strategies.},
  archiveprefix = {arXiv},
  keywords = {Adaptive sampling,Calibration,Numerical design},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9YULXTPU\\Damblin et al. - 2015 - Adaptive numerical designs for the calibration of .pdf;C\:\\Users\\a846735\\Zotero\\storage\\LVYNJDWT\\1502.html}
}

@article{damblin_adaptive_2018,
  title = {Adaptive Numerical Designs for the Calibration of Computer Codes},
  author = {Damblin, Guillaume and Barbillon, Pierre and Keller, Merlin and Pasanisi, Alberto and Parent, Eric},
  year = {2018},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {6},
  number = {1},
  eprint = {1502.07252},
  eprinttype = {arxiv},
  pages = {151--179},
  issn = {2166-2525},
  doi = {10.1137/15M1033162},
  abstract = {Making good predictions of a physical system using a computer code requires the inputs to be carefully specified. Some of these inputs, called control variables, reproduce physical conditions whereas other inputs, called parameters, are specific to the computer code and most often uncertain. The goal of statistical calibration consists in reducing their uncertainty with the help of a statistical model which links the code outputs with the field measurements. In a Bayesian setting, the posterior distribution of these parameters is typically sampled using MCMC methods. However, they are impractical when the code runs are highly timeconsuming. A way to circumvent this issue consists of replacing the computer code with a Gaussian process emulator, then sampling a surrogate posterior distribution based on it. Doing so, calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator. Under the assumption that there is no code discrepancy, we aim to reduce this error by constructing a sequential design by means of the Expected Improvement criterion. Numerical illustrations in several dimensions assess the efficiency of such sequential strategies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XCJPZ39M\\Damblin et al. - 2018 - Adaptive numerical designs for the calibration of .pdf}
}

@phdthesis{damblin_contributions_2015,
  title = {Contributions Statistiques Au Calage et \`a La Validation Des Codes de Calcul},
  author = {DAMBLIN, Guillaume},
  year = {2015},
  school = {AgroParisTech},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\I7YJVMRB\\DAMBLIN - 2015 - DE LâUNIVERSITÃ PARIS-SACLAY.pdf}
}

@article{damianou_deep_nodate,
  title = {Deep {{Gaussian Processes}}  and {{Variational Propagation}} of {{Uncertainty}}},
  author = {Damianou, Andreas},
  pages = {224},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZZG36SAF\\Damianou - Deep Gaussian Processes  and Variational Propagati.pdf}
}

@article{dang_exact_nodate,
  title = {Exact Asymptotic Limit for Kernel Estimation of Regression Level Sets},
  author = {Dang, Dau and Lalo{\"e}, Thomas and Servien, R{\'e}mi},
  pages = {15},
  abstract = {The asymptotic behavior of a plug-in kernel estimator of the regression level sets is studied. The exact asymtotic rate in terms of the symmetric difference is derived for a given level. Then, we exhibit the exact asymptotic rate when the level corresponds to a fixed probability and is therefore unknown.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\C4N6JP9Q\\Dang et al. - Exact asymptotic limit for kernel estimation of re.pdf}
}

@article{dangel_vivit_2022,
  title = {{{ViViT}}: {{Curvature}} Access through the Generalized {{Gauss-Newton}}'s Low-Rank Structure},
  shorttitle = {{{ViViT}}},
  author = {Dangel, Felix and Tatzel, Lukas and Hennig, Philipp},
  year = {2022},
  month = feb,
  journal = {arXiv:2106.02624 [cs, stat]},
  eprint = {2106.02624},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Curvature in form of the Hessian or its generalized Gauss-Newton (GGN) approximation is valuable for algorithms that rely on a local model for the loss to train, compress, or explain deep networks. Existing methods based on implicit multiplication via automatic differentiation or Kronecker-factored block diagonal approximations do not consider noise in the mini-batch. We present VIVIT, a curvature model that leverages the GGN's low-rank structure without further approximations. It allows for efficient computation of eigenvalues, eigenvectors, as well as per-sample first- and second-order directional derivatives. The representation is computed in parallel with gradients in one backward pass and offers a fine-grained cost-accuracy trade-off, which allows it to scale. As examples for VIVIT's usefulness, we investigate the directional gradients and curvatures during training, and how noise information can be used to improve the stability of second-order methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VRQ8JRPT\\2106.02624v1.pdf}
}

@article{das_estimation_1991,
  title = {On the Estimation of Parameters of Hydraulic Models by Assimilation of Periodic Tidal Data},
  author = {Das, S. K. and Lardner, R. W.},
  year = {1991},
  journal = {Journal of Geophysical Research},
  volume = {96},
  number = {C8},
  pages = {15187},
  issn = {0148-0227},
  doi = {10.1029/91JC01318},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GP9KM35M\\Das et Lardner - 1991 - On the estimation of parameters of hydraulic model.pdf}
}

@article{das_variational_1992,
  title = {Variational Parameter Estimation for a Two-Dimensional Numerical Tidal Model},
  author = {Das, S. K. and Lardner, R. W.},
  year = {1992},
  month = aug,
  journal = {International Journal for Numerical Methods in Fluids},
  volume = {15},
  number = {3},
  pages = {313--327},
  issn = {1097-0363},
  doi = {10.1002/fld.1650150305},
  abstract = {It is shown that the parameters in a two-dimensional (depth-averaged) numerical tidal model can be estimated accurately by assimilation of data from tide gauges. The tidal model considered is a semi-linearized one in which kinematical non-linearities are neglected but non-linear bottom friction is included. The parameters to be estimated (bottom friction coefficient and water depth) are assumed to be position-dependent and are approximated by piecewise linear interpolations between certain nodal values. The numerical scheme consists of a two-level leapfrog method. The adjoint scheme is constructed on the assumption that a certain norm of the difference between computed and observed elevations at the tide gauges should be minimized. It is shown that a satisfactory numerical minimization can be completed using either the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton algorithm or Nash's truncated Newton algorithm. On the basis of a number of test problems, it is shown that very effective estimation of the nodal values of the parameters can be achieved provided the number of data stations is sufficiently large in relation to the number of nodes.},
  copyright = {Copyright \textcopyright{} 1992 John Wiley \& Sons, Ltd},
  langid = {english},
  keywords = {Data assimilation,Numerical tidal model,Optimal control,Parameter estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IUZLL882\\Das et Lardner - 1992 - Variational parameter estimation for a two-dimensi.pdf;C\:\\Users\\a846735\\Zotero\\storage\\DICBZ35B\\fld.html}
}

@article{dashti_bayesian_2013,
  title = {The {{Bayesian Approach To Inverse Problems}}},
  author = {Dashti, Masoumeh and Stuart, Andrew M.},
  year = {2013},
  month = feb,
  journal = {arXiv:1302.6989 [math]},
  eprint = {1302.6989},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications involving the blending of mathematical models with data.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WJAQBC8F\\Dashti et Stuart - 2013 - The Bayesian Approach To Inverse Problems.pdf;C\:\\Users\\a846735\\Zotero\\storage\\TYUV9KQE\\1302.html}
}

@article{dauzickaite_randomised_2021,
  title = {Randomised Preconditioning for the Forcing Formulation of Weak Constraint {{4D-Var}}},
  author = {Dau{\v z}ickait{\.e}, Ieva and Lawless, Amos S. and Scott, Jennifer A. and {van Leeuwen}, Peter Jan},
  year = {2021},
  month = oct,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {147},
  number = {740},
  eprint = {2101.07249},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  pages = {3719--3734},
  issn = {0035-9009, 1477-870X},
  doi = {10.1002/qj.4151},
  abstract = {There is growing awareness that errors in the model equations cannot be ignored in data assimilation methods such as four-dimensional variational assimilation (4D-Var). If allowed for, more information can be extracted from observations, longer time windows are possible, and the minimisation process is easier, at least in principle. Weak constraint 4D-Var estimates the model error and minimises a series of linear least-squares cost functionsfunctions, which can be achieved using the conjugate gradient (CG) method; minimising each cost function is called an inner loop. CG needs preconditioning to improve its performance. In previous work, limited memory preconditioners (LMPs) have been constructed using approximations of the eigenvalues and eigenvectors of the Hessian in the previous inner loop. If the Hessian changes significantly in consecutive inner loops, the LMP may be of limited usefulness. To circumvent this, we propose using randomised methods for low rank eigenvalue decomposition and use these approximations to cheaply construct LMPs using information from the current inner loop. Three randomised methods are compared. Numerical experiments in idealized systems show that the resulting LMPs perform better than the existing LMPs. Using these methods may allow more efficient and robust implementations of incremental weak constraint 4D-Var.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2KSAPAHL\\DauÅ¾ickaitÄ et al. - 2021 - Randomised preconditioning for the forcing formula.pdf;C\:\\Users\\a846735\\Zotero\\storage\\V3UVVNMG\\2101.html}
}

@article{dawson_data_nodate,
  title = {Data Assimilation for Parameter Estimation in Coastal Ocean Hydrodynamics Modeling},
  author = {Dawson, Clint and Ghattas, Omar and Gonzalez, Oscar and Hoteit, Ibrahim and Tsai, Richard},
  pages = {193},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8RQXINSW\\Dawson et al. - Data assimilation for parameter estimation in coas.pdf}
}

@article{debreu_multigrid_2016,
  title = {Multigrid Solvers and Multigrid Preconditioners for the Solution of Variational Data Assimilation Problems},
  author = {Debreu, Laurent and Neveu, Emilie and Simon, Ehouarn and Le Dimet, Fran{\c c}ois-Xavier and Vidard, Arthur},
  year = {2016},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {142},
  number = {694},
  pages = {515--528},
  issn = {0035-9009, 1477-870X},
  doi = {10.1002/qj.2676},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZDFYRD55\\Debreu et al. - 2016 - Multigrid solvers and multigrid preconditioners fo.pdf}
}

@article{debreu_two-way_2012,
  title = {Two-Way Nesting in Split-Explicit Ocean Models: {{Algorithms}}, Implementation and Validation},
  shorttitle = {Two-Way Nesting in Split-Explicit Ocean Models},
  author = {Debreu, Laurent and Marchesiello, Patrick and Penven, Pierrick and Cambon, Gildas},
  year = {2012},
  month = jun,
  journal = {Ocean Modelling},
  volume = {49--50},
  pages = {1--21},
  issn = {1463-5003},
  doi = {10.1016/j.ocemod.2012.03.003},
  abstract = {A full two-way nesting approach for split-explicit, free surface ocean models is presented. It is novel in three main respects: the treatment of grid refinement at the fast mode (barotropic) level; the use of scale selective update schemes; the conservation of both volume and tracer contents via refluxing. An idealized application to vortex propagation on a {$\beta$} plane shows agreement between nested and high resolution solutions. A realistic application to the California Current System then confirm these results in a complex configuration. The selected algorithm is now part of ROMS\_AGRIF. It is fully consistent with ROMS parallel capabilities on both shared and distributed memory architectures. The nesting implementation authorizes several nesting levels and several grids at any particular level. This operational capability, combined with the inner qualities of our two-way nesting algorithm and generally high-order accuracy of ROMS numerics, allow for realistic simulation of coastal and ocean dynamics at multiple, interacting scales.},
  langid = {english},
  keywords = {Boundary conditions,Coastal upwelling,Finite difference method,Modeling,Two-way nesting},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MAUHCJPW\\S1463500312000480.html}
}

@misc{december_11th_how_2013,
  title = {How {{Academia Resembles}} a {{Drug Gang}}},
  author = {December 11th and communication|75 Comments, 2013|Academic},
  year = {2013},
  month = dec,
  journal = {Impact of Social Sciences},
  abstract = {Academic systems rely on the existence of a~supply of ``outsiders''~ready to forgo wages and employment security in exchange for the prospect of uncertain security, prestige, freedom and reasonably h\ldots},
  langid = {american},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z9GJMRI5\\how-academia-resembles-a-drug-gang.html}
}

@article{dellino_robust_2012,
  title = {Robust {{Optimization}} in {{Simulation}}: {{Taguchi}} and {{Krige Combined}}},
  shorttitle = {Robust {{Optimization}} in {{Simulation}}},
  author = {Dellino, Gabriella and Kleijnen, Jack P. C. and Meloni, Carlo},
  year = {2012},
  month = aug,
  journal = {INFORMS Journal on Computing},
  volume = {24},
  number = {3},
  pages = {471--484},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.1110.0465},
  langid = {english},
  keywords = {Optim Robuste},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6RZ7KGTL\\Dellinoetal_IJOC_24_3_2012.pdf}
}

@article{demortier_p_nodate,
  title = {P {{Values}} and {{Nuisance Parameters}}},
  author = {Demortier, Luc},
  pages = {11},
  abstract = {We review the definition and interpretation of p values, describe methods to incorporate systematic uncertainties in their calculation, and briefly discuss a non-regular but common problem caused by nuisance parameters that are unidentified under the null hypothesis.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JR5P3ZB3\\Demortier - P Values and Nuisance Parameters.pdf}
}

@article{dempster_maximum_1977,
  title = {Maximum Likelihood from Incomplete Data via the {{EM}} Algorithm},
  author = {Dempster, Arthur P. and Laird, Nan M. and Rubin, Donald B.},
  year = {1977},
  journal = {Journal of the royal statistical society. Series B (methodological)},
  pages = {1--38},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\773M4BD9\\Dempster et al. - Maximum Likelihood from Incomplete Data via the EM.pdf;C\:\\Users\\a846735\\Zotero\\storage\\CSLEN38G\\Dempster et al. - 1977 - Maximum likelihood from incomplete data via the EM.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7H9ZL662\\2984875.html}
}

@book{dener_training_2020,
  title = {Training Neural Networks under Physical Constraints Using a Stochastic Augmented {{Lagrangian}} Approach},
  author = {Dener, Alp and Miller, Marco and Churchill, Randy and Munson, Todd and Chang, C.S.},
  year = {2020},
  month = sep,
  abstract = {We investigate the physics-constrained training of an encoder-decoder neural network for approximating the Fokker-Planck-Landau collision operator in the 5-dimensional kinetic fusion simulation in XGC. To train this network, we propose a stochastic augmented Lagrangian approach that utilizes pyTorch's native stochastic gradient descent method to solve the inner unconstrained minimization subproblem, paired with a heuristic update for the penalty factor and Lagrange multipliers in the outer augmented Lagrangian loop. Our training results for a single ion species case, with self-collisions and collision against electrons, show that the proposed stochastic augmented Lagrangian approach can achieve higher model prediction accuracy than training with a fixed penalty method for our application problem, with the accuracy high enough for practical applications in kinetic simulations.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IHTCM8WJ\\Dener et al. - 2020 - Training neural networks under physical constraint.pdf}
}

@article{denker_conditional_2021,
  title = {Conditional {{Invertible Neural Networks}} for {{Medical Imaging}}},
  author = {Denker, Alexander and Schmidt, Maximilian and Leuschner, Johannes and Maass, Peter},
  year = {2021},
  month = nov,
  journal = {Journal of Imaging},
  volume = {7},
  number = {11},
  pages = {243},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2313-433X},
  doi = {10.3390/jimaging7110243},
  abstract = {Over recent years, deep learning methods have become an increasingly popular choice for solving tasks from the field of inverse problems. Many of these new data-driven methods have produced impressive results, although most only give point estimates for the reconstruction. However, especially in the analysis of ill-posed inverse problems, the study of uncertainties is essential. In our work, we apply generative flow-based models based on invertible neural networks to two challenging medical imaging tasks, i.e., low-dose computed tomography and accelerated medical resonance imaging. We test different architectures of invertible neural networks and provide extensive ablation studies. In most applications, a standard Gaussian is used as the base distribution for a flow-based model. Our results show that the choice of a radial distribution can improve the quality of reconstructions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {image reconstruction,invertible neural networks,normalizing flows},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4JUZZKU5\\Denker et al. - 2021 - Conditional Invertible Neural Networks for Medical.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GS382DH9\\htm.html}
}

@incollection{deodatis_robust_2014,
  title = {Robust Minimax Design from Costly Simulations},
  booktitle = {Safety, {{Reliability}}, {{Risk}} and {{Life-Cycle Performance}} of {{Structures}} and {{Infrastructures}}},
  author = {{Piet-Lahanier}, H and Marzat, J and Walter, E},
  editor = {Deodatis, George and Ellingwood, Bruce and Frangopol, Dan},
  year = {2014},
  month = jan,
  pages = {3231--3236},
  publisher = {{CRC Press}},
  doi = {10.1201/b16387-467},
  abstract = {Design of complex physical systems most often relies on numerical simulations that may be extremely costly. In this paper, design is formalized as the optimization of a performance index with respect to control variables. Uncertainty is modeled via a vector of environmental variables that can take any value in a known compact set and may have an adverse effect on performance. In this context, the determination of a robust design requires the continuous minimax optimization of black-box functions. An algorithm combining Kriging-based optimization with relaxation is presented, which makes it possible to find approximate solutions to such problems on a limited computational budget. The design of a vibration absorber is presented as an illustrative example.},
  isbn = {978-1-138-00086-5 978-1-315-88488-2},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\V67NAMCJ\\Piet-Lahanier et al. - 2014 - Robust minimax design from costly simulations.pdf}
}

@article{descombes_generalized_2015,
  title = {Generalized Background Error Covariance Matrix Model ({{GEN}}\_{{BE}} v2.0)},
  author = {Descombes, Gael and Aulign{\'e}, Thomas and Vandenberghe, Francois and Barker, Dale and Barr{\'e}, J{\'e}r{\^o}me},
  year = {2015},
  month = mar,
  journal = {Geoscientific Model Development},
  volume = {8},
  doi = {10.5194/gmd-8-669-2015},
  abstract = {The specification of state background error statistics is a key component of data assimilation since it affects the impact observations will have on the analysis. In the variational data assimilation approach, applied in geophysical sciences, the dimensions of the background error covariance matrix (B) are usually too large to be explicitly determined and B needs to be modeled. Recent efforts to include new variables in the analysis such as cloud parameters and chemical species have required the development of the code to GENerate the Background Errors (GEN\_BE) version 2.0 for the Weather Research and Forecasting (WRF) community model. GEN\_BE allows for a simpler, flexible, robust, and community-oriented framework that gathers methods used by some meteorological operational centers and researchers. We present the advantages of this new design for the data assimilation community by performing benchmarks of different modeling of B and showing some of the new features in data assimilation test cases. As data assimilation for clouds remains a challenge, we present a multivariate approach that includes hydrometeors in the control variables and new correlated errors. In addition, the GEN\_BE v2.0 code is employed to diagnose error parameter statistics for chemical species, which shows that it is a tool flexible enough to implement new control variables. While the generation of the background errors statistics code was first developed for atmospheric research, the new version (GEN\_BE v2.0) can be easily applied to other domains of science and chosen to diagnose and model B. Initially developed for variational data assimilation, the model of the B matrix may be useful for variational ensemble hybrid methods as well.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\U2IFBLU4\\Descombes et al. - 2015 - Generalized background error covariance matrix mod.pdf}
}

@incollection{destercke_maximum_2019,
  title = {A {{Maximum Likelihood Approach}} to {{Inference Under Coarse Data Based}} on {{Minimax Regret}}},
  booktitle = {Uncertainty {{Modelling}} in {{Data Science}}},
  author = {Guillaume, Romain and Dubois, Didier},
  editor = {Destercke, S{\'e}bastien and Denoeux, Thierry and Gil, Mar{\'i}a {\'A}ngeles and Grzegorzewski, Przemyslaw and Hryniewicz, Olgierd},
  year = {2019},
  volume = {832},
  pages = {99--106},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-97547-4_14},
  abstract = {Various methods have been proposed to express and solve maximum likelihood problems with incomplete data. In some of these approaches, the idea is that incompleteness makes the likelihood func-tion imprecise. Two proposals can be found to cope with this situation: maximize the maximal likelihood induced by precise datasets compat-ible with the incomplete observations, or maximize the minimal such likelihood. These approaches prove to be extremist, the maximax ap-proach having a tendency to disambiguate the data, while the maximin approach favors uniform distributions. In this paper we propose an al-ternative approach consisting in minimax relative regret criterion with respect to maximal likelihood solutions obtained for all precise datasets compatible with the coarse data. It uses relative likelihood and seems to achieve a trade-off between the maximax and the maximin methods.},
  isbn = {978-3-319-97546-7 978-3-319-97547-4},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SQGQ8MX7\\Guillaume et Dubois - 2019 - A Maximum Likelihood Approach to Inference Under C.pdf}
}

@article{diez_design-space_2015,
  title = {Design-Space Dimensionality Reduction in Shape Optimization by {{Karhunen}}\textendash{{Lo\`eve}} Expansion},
  author = {Diez, Matteo and Campana, Emilio F. and Stern, Frederick},
  year = {2015},
  month = jan,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {283},
  pages = {1525--1544},
  issn = {00457825},
  doi = {10.1016/j.cma.2014.10.042},
  langid = {english},
  keywords = {Dimensionality reduction,KarhunenâLoÃ¨ve expansion},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8B582P7U\\2015-CMAME-Diez_etal.pdf}
}

@book{dimet_variational_1988,
  title = {Variational and {{Optimization Methods}} in {{Meteorology}}: {{A Review}}},
  shorttitle = {Variational and {{Optimization Methods}} in {{Meteorology}}},
  author = {Dimet, F. X. Le and Navon, I. M.},
  year = {1988},
  abstract = {Recent advances in variational and optimization methods applied to increasingly complex numerical weather prediction models with larger numbers of degrees of freedom mandate to take a perspective view of past and recent developments in this field, and present a view of the state of art in the field. Variational methods attempt to achieve a best fit between data and model subject to some `a priori' criteria \textendash{} in view of resolving the undeterminancy problem between the size of the model and the respective number of data required for its satisfactory solution. This review paper presents in a synthesized way the combined views of the authors as to the state of the art of variational and optimization methods in meteorology. Issues discussed include topics of variational analysis, variational initialization, optimal control techniques, variational methods applied for numerical purposes and constrained adjustment, and finally how some of the variational and optimization methods discussed in the review relate to each other.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W6FNUMJV\\Dimet et Navon - 1988 - Variational and Optimization Methods in Meteorolog.pdf;C\:\\Users\\a846735\\Zotero\\storage\\TW3ZC5FN\\summary.html}
}

@article{ding_model_2018,
  title = {Model {{Selection Techniques}} -- {{An Overview}}},
  author = {Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
  year = {2018},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {6},
  eprint = {1810.09583},
  eprinttype = {arxiv},
  pages = {16--34},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2018.2867638},
  abstract = {In the era of ``big data'', analysts usually explore various statistical models or machine learning methods for observed data in order to facilitate scientific discoveries or gain predictive power. Whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. Model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus central to scientific studies in fields such as ecology, economics, engineering, finance, political science, biology, and epidemiology. There has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. A considerable number of methods have been proposed, following different philosophies and exhibiting varying performances. The purpose of this article is to bring a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. We provide integrated and practically relevant discussions on theoretical properties of state-ofthe-art model selection approaches. We also share our thoughts on some controversial views on the practice of model selection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Economics - Econometrics,Physics - Applied Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XFAJYGC7\\Ding et al. - 2018 - Model Selection Techniques -- An Overview.pdf}
}

@article{dinh_density_2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2017},
  month = feb,
  journal = {arXiv:1605.08803 [cs, stat]},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efficient sampling, exact and efficient inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Normalizing flows,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LIAXCJSQ\\1605.08803.pdf}
}

@article{dinh_nice_2015,
  title = {{{NICE}}: {{Non-linear Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  year = {2015},
  month = apr,
  journal = {arXiv:1410.8516 [cs]},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PNMKQJ5X\\Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf;C\:\\Users\\a846735\\Zotero\\storage\\Y38VZ8BU\\1410.html}
}

@phdthesis{dobre_analyses_2010,
  title = {{Analyses de sensibilit\'e et d'identifiabilit\'e globales. Application \`a l'estimation de param\`etres photophysiques en th\'erapie photodynamique}},
  author = {Dobre, Simona},
  year = {2010},
  month = jun,
  abstract = {La th\'erapie photodynamique (PDT) est un traitement m\'edical destin\'e \`a certains types de cancer. Elle utilise un agent photosensibilisant qui se concentre dans les tissus pathologiques est qui sera. Cet agent est ensuite activ\'e par une lumi\`ere d'une longueur d'onde pr\'ecise produisant, apr\`es une cascade de r\'eactions, des esp\`eces r\'eactives de l'oxyg\`ene qui endommagent les cellules canc\'ereuses. Cette th\`ese aborde les analyses d'identifiabilit\'e et de sensibilit\'e des param\`etres du mod\`ele dynamique non lin\'eaire retenu. Apr\`es avoir pr\'ecis\'e diff\'erents cadres d'analyse d'identifiabilit\'e, nous nous int\'eressons plus particuli\`erement \`a l'identifiabilit\'e a posteriori, pour des conditions exp\'erimentales fix\'ees, puis \`a l'identifiabilit\'e pratique, prenant en plus en compte les bruits de mesure. Pour ce dernier cadre, nous proposons une m\'ethodologie d'analyse locale autour de valeurs particuli\`eres des param\`etres. En ce qui concerne l'identifiabilit\'e des param\`etres du mod\`ele dynamique de la phase photocytotoxique de la PDT, nous montrons que parmi les dix param\`etres localement identifiables a posteriori, seulement l'un d'entre eux l'est en pratique. N\'eanmoins, ces r\'esultats locaux demeurent insuffisants en raison des larges plages de variation possibles des param\`etres du mod\`ele et n\'ecessitent d'\^etre compl\'et\'es par une analyse globale. Le manque de m\'ethode visant \`a tester l'identifiabilit\'e globale a posteriori ou pratique, nous a orient\'es vers l'analyse de sensibilit\'e globale de la sortie du mod\`ele par rapport \`a ses param\`etres. Une m\'ethode d'analyse de sensibilit\'e globale fond\'ee sur l'\'etude de la variance a permis de mettre en \'evidence trois param\`etres sensibilisants. Nous abordons ensuite les liens entre les analyses globales d'identifiabilit\'e et de sensibilit\'e des param\`etres, en employant une d\'ecomposition de Sobol'. Nous montrons alors que les liens suivants existent : une fonction de sensibilit\'e totale nulle implique un param\`etre non-identifiable; deux fonctions de sensibilit\'e colin\'eaires impliquent la non-identifiabilit\'e mutuelle des param\`etres en question ; la non-injectivit\'e de la sortie par rapport \`a un de ses param\`etres peut aussi entrainer la non-identifiabilit\'e du param\`etre en question mais ce dernier point ne peut \^etre d\'etect\'e en analysant les fonctions de sensibilit\'e uniquement. En somme, la d\'etection des param\`etres non globalement identifiables dans un cadre exp\'erimental donn\'e \`a partir de r\'esultats d'analyse de sensibilit\'e globale ne peut \^etre que partielle. Elle permet d'observer deux (sensibilit\'e nulle ou n\'egligeable et sensibilit\'es corr\'el\'ees) des trois causes de la non-identifiabilit\'e.},
  langid = {french},
  school = {Universit\'e Henri Poincar\'e - Nancy I},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3ZJS3E9C\\Dobre - 2010 - Analyses de sensibilitÃ© et d'identifiabilitÃ© globa.pdf;C\:\\Users\\a846735\\Zotero\\storage\\MXQQJMVI\\tel-00550527.html}
}

@inproceedings{dobre_global_2010,
  title = {Global Sensitivity and Identifiability Implications in Systems Biology},
  booktitle = {11th {{IFAC Symposium}} on {{Computer Applications}} in {{Biotechnology}}, {{CAB}} 2010},
  author = {Dobre, Simona and Bastogne, Thierry and Richard, Alain},
  year = {2010},
  month = jul,
  pages = {CDROM},
  address = {{Leuven, Belgium}},
  abstract = {In systems biology, a common approach to model biological processes is to use large systems of differential equations.The associated parameter estimation problem requires to prior handle identifiability and sensitivity issues in a practical biological framework. The lack of method to assess global practical identifiability has leaded us to analyze and establish bridges between global sensitivity and identifiability measures. Specifically, we are interested in deriving conditions of global practical non-identifiability in association with global sensitivity results. Two cases are considered: i) insensitive (or non-observable) parameters ; ii) two (or more) correlated sensitivity measures of the model output with respect to model parameters. Propositions of relationships between sensitivity and identifiability, and their proofs are developped herein. Academic examples are also treated in order to illustrate contents of these propositions.},
  keywords = {identifiability,nonlinear systems,sensitivity analysis,systems biology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\AY3DC9VQ\\Dobre et al. - 2010 - Global sensitivity and identifiability implication.pdf}
}

@article{dobricic_application_2013,
  title = {An {{Application}} of {{Sequential Variational Method}} without {{Tangent Linear}} and {{Adjoint Model Integrations}}},
  author = {Dobricic, Srdjan},
  year = {2013},
  month = jan,
  journal = {Monthly Weather Review},
  volume = {141},
  number = {1},
  pages = {307--323},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-11-00012.1},
  abstract = {Abstract The sequential variational (SVAR) method minimizes the weakly constrained four-dimensional cost function by splitting it into a set of smaller cost functions. This study shows how it is possible to apply SVAR in practice by reducing the computational effort required by the algorithm. A major finding of the study is that, instead of using tangent linear and adjoint models, it is possible to estimate the largest eigenvalues and the corresponding eigenvectors of the evolution of the background error covariances only by applying successive nonlinear model integrations. Another major finding is that the impact of future observations on previous state estimates may be obtained in an accurate and numerically stable way by using suitably defined cost functions and control space transformations without any additional model integrations. The new method is applied in a realistic data assimilation experiment with a primitive equations ocean model.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YZLKZPX2\\Dobricic - 2013 - An Application of Sequential Variational Method wi.pdf;C\:\\Users\\a846735\\Zotero\\storage\\YYSHZ3EQ\\mwr-d-11-00012.1.html}
}

@article{dobrovidov_data-driven_2014,
  title = {Data-Driven Bandwidth Choice for Gamma Kernel Estimates of Density Derivatives on the Positive Semi-Axis},
  author = {Dobrovidov, A. V. and Markovich, L. A.},
  year = {2014},
  month = jan,
  journal = {arXiv:1401.6801 [math, stat]},
  eprint = {1401.6801},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {In some applications it is necessary to estimate derivatives of probability densities defined on the positive semi-axis. The quality of nonparametric estimates of the probability densities and their derivatives are strongly influenced by smoothing parameters (bandwidths). In this paper an expression for the optimal smoothing parameter of the gamma kernel estimate of the density derivative is obtained. For this parameter data-driven estimates based on methods called "rule of thumb" and "cross-validation" are constructed. The quality of the estimates is verified and demonstrated on examples of density derivatives generated by Maxwell and Weibull distributions.},
  archiveprefix = {arXiv},
  keywords = {Bandwidth selection,Density estimation,Gamma kernel},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\APEW2FA4\\Dobrovidov et Markovich - 2014 - Data-driven bandwidth choice for gamma kernel esti.pdf;C\:\\Users\\a846735\\Zotero\\storage\\SDV5GMA9\\1401.html}
}

@article{doucet_marginal_2002,
  title = {Marginal Maximum a Posteriori Estimation Using {{Markov}} Chain {{Monte Carlo}}},
  author = {Doucet, Arnaud and Godsill, Simon J. and Robert, Christian P.},
  year = {2002},
  month = jan,
  journal = {Statistics and Computing},
  volume = {12},
  number = {1},
  pages = {77--84},
  issn = {0960-3174, 1573-1375},
  doi = {10.1023/A:1013172322619},
  abstract = {Markov chain Monte Carlo (MCMC) methods, while facilitating the solution of many complex problems in Bayesian inference, are not currently well adapted to the problem of marginal maximum a posteriori (MMAP) estimation, especially when the number of parameters is large. We present here a simple and novel MCMC strategy, called State-Augmentation for Marginal Estimation (SAME), which leads to MMAP estimates for Bayesian models. We illustrate the simplicity and utility of the approach for missing data interpolation in autoregressive time series and blind deconvolution of impulsive processes.},
  langid = {english},
  keywords = {MCMC,MMAP,SAME},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8DPAVYIP\\Doucet et al. - 2002 - Marginal maximum a posteriori estimation using Mar.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LB5L4CSS\\A1013172322619.html}
}

@incollection{doumpos_performance_2016,
  title = {Performance {{Analysis}} in {{Robust Optimization}}},
  booktitle = {Robustness {{Analysis}} in {{Decision Aiding}}, {{Optimization}}, and {{Analytics}}},
  author = {Chassein, Andr{\'e} and Goerigk, Marc},
  editor = {Doumpos, Michael and Zopounidis, Constantin and Grigoroudis, Evangelos},
  year = {2016},
  volume = {241},
  pages = {145--170},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-33121-8_7},
  abstract = {We discuss the problem of evaluating a robust solution. To this end, we first give a short primer on how to apply robustification approaches to uncertain optimization problems using the assignment problem and the knapsack problem as illustrative examples. As it is not immediately clear in practice which such robustness approach is suitable for the problem at hand, we present current approaches for evaluating and comparing robustness from the literature, and introduce the new concept of a scenario curve. Using the methods presented in this paper, an easy guide is given to the decision maker to find, solve and compare the best robust optimization method for his purposes.},
  isbn = {978-3-319-33119-5 978-3-319-33121-8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EI2VPC8N\\Chassein et Goerigk - 2016 - Performance Analysis in Robust Optimization.pdf}
}

@article{drovandi_ensemble_2019,
  title = {Ensemble {{MCMC}}: {{Accelerating Pseudo-Marginal MCMC}} for {{State Space Models}} Using the {{Ensemble Kalman Filter}}},
  shorttitle = {Ensemble {{MCMC}}},
  author = {Drovandi, Christopher and Everitt, Richard G. and Golightly, Andrew and Prangle, Dennis},
  year = {2019},
  month = aug,
  journal = {arXiv:1906.02014 [stat]},
  eprint = {1906.02014},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Particle Markov chain Monte Carlo (pMCMC) is now a popular method for performing Bayesian statistical inference on challenging state space models (SSMs) with unknown static parameters. It uses a particle filter (PF) at each iteration of an MCMC algorithm to unbiasedly estimate the likelihood for a given static parameter value. However, pMCMC can be computationally intensive when a large number of particles in the PF is required, such as when the data is highly informative, the model is misspecified and/or the time series is long. In this paper we exploit the ensemble Kalman filter (EnKF) developed in the data assimilation literature to speed up pMCMC. We replace the unbiased PF likelihood with the biased EnKF likelihood estimate within MCMC to sample over the space of the static parameter. On a wide class of different non-linear SSM models, we demonstrate that our new ensemble MCMC (eMCMC) method can significantly reduce the computational cost whilst maintaining reasonable accuracy. We also propose several extensions of the vanilla eMCMC algorithm to further improve computational efficiency. Computer code to implement our methods on all the examples can be downloaded from https://github.com/cdrovandi/Ensemble-MCMC.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3IR5SAUX\\Drovandi et al. - 2019 - Ensemble MCMC Accelerating Pseudo-Marginal MCMC f.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ML337G3Y\\1906.html}
}

@article{du_centroidal_1999,
  title = {Centroidal {{Voronoi}} Tessellations: {{Applications}} and Algorithms},
  shorttitle = {Centroidal {{Voronoi}} Tessellations},
  author = {Du, Qiang and Faber, Vance and Gunzburger, Max},
  year = {1999},
  journal = {SIAM review},
  volume = {41},
  number = {4},
  pages = {637--676},
  keywords = {CVT,Voronoi},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P76KIDGT\\dfg99sirv.pdf}
}

@article{dubois_data-driven_2020,
  title = {Data-Driven Predictions of the {{Lorenz}} System},
  author = {Dubois, Pierre and Gomez, Thomas and Planckaert, Laurent and Perret, Laurent},
  year = {2020},
  month = jul,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {408},
  pages = {132495},
  issn = {01672789},
  doi = {10.1016/j.physd.2020.132495},
  abstract = {This paper investigates the use of a data-driven method to model the dynamics of the chaotic Lorenz system. An architecture based on a recurrent neural network with long and short term dependencies predicts multiple time steps ahead the position and velocity of a particle using a sequence of past states as input. To account for modeling errors and make a continuous forecast, a dense artificial neural network assimilates online data to detect and update wrong predictions such as non-relevant switchings between lobes. The data-driven strategy leads to good prediction scores and does not require statistics of errors to be known, thus providing significant benefits compared to a simple Kalman filter update.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3LZ8NEQA\\Dubois et al. - 2020 - Data-driven predictions of the Lorenz system.pdf}
}

@article{dubourg_adaptive_nodate,
  title = {{Adaptive surrogate models for reliability analysis and reliability-based design optimization}},
  author = {Dubourg, Vincent},
  pages = {308},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZSC6EMCA\\Dubourg - Adaptive surrogate models for reliability analysis.pdf}
}

@article{dubourg_reliability-based_2011,
  title = {Reliability-Based Design Optimization Using Kriging Surrogates and Subset Simulation},
  author = {Dubourg, V. and Sudret, B. and Bourinet, J.-M.},
  year = {2011},
  month = nov,
  journal = {Structural and Multidisciplinary Optimization},
  volume = {44},
  number = {5},
  eprint = {1104.3667},
  eprinttype = {arxiv},
  pages = {673--690},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-011-0653-8},
  abstract = {The aim of the present paper is to develop a strategy for solving reliability-based design optimization (RBDO) problems that remains applicable when the performance models are expensive to evaluate. Starting with the premise that simulation-based approaches are not affordable for such problems, and that the most-probablefailure-point-based approaches do not permit to quantify the error on the estimation of the failure probability, an approach based on both metamodels and advanced simulation techniques is explored. The kriging metamodeling technique is chosen in order to surrogate the performance functions because it allows one to genuinely quantify the surrogate error. The surrogate error onto the limit-state surfaces is propagated to the failure probabilities estimates in order to provide an empirical error measure. This error is then sequentially reduced by means of a populationbased adaptive refinement technique until the kriging surrogates are accurate enough for reliability analysis. This original refinement strategy makes it possible to add several observations in the design of experiments at the same time. Reliability and reliability sensitivity analyses are performed by means of the subset simulation technique for the sake of numerical efficiency. The adaptive surrogate-based strategy for reliability estimation is finally involved into a classical gradient-based optimization algorithm in order to solve the RBDO problem. The kriging surrogates are built in a so-called augmented reliability space thus making them reusable from one nested RBDO iteration to the other. The strategy is compared to other approaches available in the literature on three academic examples in the field of structural mechanics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\L9JA7B9K\\Dubourg et al. - 2011 - Reliability-based design optimization using krigin.pdf}
}

@article{dubrule_cross_1983,
  title = {Cross Validation of Kriging in a Unique Neighborhood},
  author = {Dubrule, Olivier},
  year = {1983},
  month = dec,
  journal = {Journal of the International Association for Mathematical Geology},
  volume = {15},
  number = {6},
  pages = {687--699},
  issn = {1573-8868},
  doi = {10.1007/BF01033232},
  abstract = {Cross validation is an appropriate tool for testing interpolation methods: it consists of leaving out one data point at a time, and determining how well this point can be estimated from the other data. Cross validation is often used for testing ``moving neighborhood'' kriging models; in this case, each unknown value is predicted from a small number of surrounding data. In ``unique neighborhood'' kriging algorithms, each estimation uses all the available data; as a result, cross validation would spend much computer time. For instance, with ndata points it would cost at least the resolution of nsystems of n \texttimes{} nlinear equations (each with a different matrix).Here, we present a much faster method for cross validation in a unique neighborhood. Instead of solving nsystems n \texttimes{} n,it only requires the inversion of one n \texttimes{} nmatrix. We also generalized this method to leaving out several points instead of one.},
  langid = {english}
}

@article{dunlop_map_2016,
  title = {{{MAP Estimators}} for {{Piecewise Continuous Inversion}}},
  author = {Dunlop, Matthew M. and Stuart, Andrew M.},
  year = {2016},
  month = oct,
  journal = {Inverse Problems},
  volume = {32},
  number = {10},
  eprint = {1509.03136},
  eprinttype = {arxiv},
  pages = {105003},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/32/10/105003},
  abstract = {We study the inverse problem of estimating a field \$u\$ from data comprising a finite set of nonlinear functionals of \$u\$, subject to additive noise; we denote this observed data by \$y\$. Our interest is in the reconstruction of piecewise continuous fields in which the discontinuity set is described by a finite number of geometric parameters. Natural applications include groundwater flow and electrical impedance tomography. We take a Bayesian approach, placing a prior distribution on \$u\$ and determining the conditional distribution on \$u\$ given the data \$y\$. It is then natural to study maximum a posterior (MAP) estimators. Recently (Dashti et al 2013) it has been shown that MAP estimators can be characterised as minimisers of a generalised Onsager-Machlup functional, in the case where the prior measure is a Gaussian random field. We extend this theory to a more general class of prior distributions which allows for piecewise continuous fields. Specifically, the prior field is assumed to be piecewise Gaussian with random interfaces between the different Gaussians defined by a finite number of parameters. We also make connections with recent work on MAP estimators for linear problems and possibly non-Gaussian priors (Helin, Burger 2015) which employs the notion of Fomin derivative. In showing applicability of our theory we focus on the groundwater flow and EIT models, though the theory holds more generally. Numerical experiments are implemented for the groundwater flow model, demonstrating the feasibility of determining MAP estimators for these piecewise continuous models, but also that the geometric formulation can lead to multiple nearby (local) MAP estimators. We relate these MAP estimators to the behaviour of output from MCMC samples of the posterior, obtained using a state-of-the-art function space Metropolis-Hastings method.},
  archiveprefix = {arXiv},
  keywords = {MAP},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IHB7KHYR\\Dunlop et Stuart - 2016 - MAP Estimators for Piecewise Continuous Inversion.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7CHTAAK8\\1509.html}
}

@article{duraisamy_goal_2011,
  title = {Goal {{Oriented Uncertainty Propagation}} Using {{Stochastic Adjoints}}},
  author = {Duraisamy, Karthik and Chandrashekar, Praveen},
  year = {2011},
  month = jun,
  journal = {Computers \& Fluids},
  volume = {66},
  pages = {10--20},
  doi = {10.1016/j.compfluid.2012.05.013},
  abstract = {We propose a framework based on the use of adjoint equations to formulate an adaptive sampling strategy for uncertainty quantification for problems governed by algebraic or differential equations involving random parameters. The approach is non-intrusive and makes use of discrete sampling based on collocation on simplex elements in stochastic space. Adjoint or dual equations are introduced to estimate errors in statistical moments of random functionals resulting from the inexact reconstruction of the solution within the simplex elements. The approach is demonstrated to be accurate in estimating errors in statistical moments of interest and shown to exhibit super-convergence, in accordance with the underlying theoretical rates. Goal-oriented error indicators are then built using the adjoint solution and exploited to identify regions for adaptive sampling. The error-estimation and adaptive refinement strategy is applied to a range of problems including those governed by algebraic equations as well as scalar and systems of ordinary and partial differential equations. The strategy holds promise as a reliable method to set and achieve error tolerances for efficient aleatory uncertainty quantification in complex problems. Furthermore, the procedure can be combined with numerical error estimates in physical space so as to effectively manage a computational budget to achieve the best possible overall accuracy in the results.}
}

@misc{durasov_debosh_2021,
  title = {{{DEBOSH}}: {{Deep Bayesian Shape Optimization}}},
  shorttitle = {{{DEBOSH}}},
  author = {Durasov, Nikita and Lukoyanov, Artem and Donier, Jonathan and Fua, Pascal},
  year = {2021},
  month = sep,
  number = {arXiv:2109.13337},
  eprint = {2109.13337},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.13337},
  abstract = {Shape optimization is at the heart of many industrial applications, such as aerodynamics, heat transfer, and structural analysis. It has recently been shown that Graph Neural Networks (GNNs) can predict the performance of a shape quickly and accurately and be used to optimize more effectively than traditional techniques that rely on response-surfaces obtained by Kriging. However, GNNs suffer from the fact that they do not evaluate their own accuracy, which is something Bayesian Optimization methods require. Therefore, estimating confidence in generated predictions is necessary to go beyond straight deterministic optimization, which is less effective. In this paper, we demonstrate that we can use Ensembles-based technique to overcome this limitation and outperform the state-of-the-art. Our experiments on diverse aerodynamics and structural analysis tasks prove that adding uncertainty to shape optimization significantly improves the quality of resulting shapes and reduces the time required for the optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MYIV9G6F\\Durasov et al. - 2021 - DEBOSH Deep Bayesian Shape Optimization.pdf;C\:\\Users\\a846735\\Zotero\\storage\\HIA9S9YJ\\2109.html}
}

@phdthesis{durbiano_vecteurs_2001,
  type = {These de Doctorat},
  title = {Vecteurs Caract\'eristiques de Mod\`eles Oc\'eaniques Pour La R\'eduction d'ordre En Assimilation de Donn\'ees},
  author = {Durbiano, Sophie},
  year = {2001},
  month = jan,
  abstract = {Les principales difficult\'es que connaissent les m\'ethodes actuelles d'assimilation de donn\'ees sont la taille des probl\`emes (tr\`es grande), la sp\'ecification des erreurs (mal connues) et les non-lin\'earit\'es (mal prises en compte). C'est pour tenter de rem\'edier \`a ces deux premiers points en particulier que nous nous int\'eressons \`a des m\'ethodes de r\'eduction d'ordre prenant le mieux possible en compte la "variabilit\'e" du syst\`eme consid\'er\'e. Sa connaissance passe par la d\'etection de modes privil\'egi\'es que nous appelons vecteurs caract\'eristiques et qui sont \`a la base de toutes nos exp\'eriences. Il s'agit des vecteurs singuliers, des vecteurs de Liapunov, de ceux obtenus par breeding, des vecteurs singuliers non lin\'eaires et des composantes principales. Au travers de diverses applications, nous abordons successivement trois points : le premier concerne la r\'eduction des co\^uts de calcul de la m\'ethode d'assimilation 4D-Var par minimisation de la fonctionnelle d'\'ecart aux observations dans un espace de dimension plus petite que celle de l'espace d'origine. Il s'agit pour le second de la prise en compte d'un terme d'erreur mod\`ele dans les \'equations de la m\'ethode 4D-Var, terme d\'ecompos\'e sur des bases de vecteurs caract\'eristiques. Le troisi\`eme vise \`a \'etudier la capacit\'e de ces vecteurs \`a corriger les pr\'edictions effectu\'ees par une m\'ethode d'assimilation s\'equentielle variante du filtre de Kalman : le filtre SEEK. Il r\'esulte de toutes ces exp\'eriences que chaque type de vecteurs caract\'eristiques s'adapte plus ou moins bien aux diff\'erents processus d'assimilation, les r\'esultats mettant toutefois globalement en \'evidence le r\^ole des non-lin\'earit\'es - particuli\`erement au travers des modes issus de la technique de breeding - et l'efficacit\'e de la r\'eduction d'ordre en termes de co\^uts de calcul.},
  collaborator = {Le Dimet, Fran{\c c}ois-Xavier},
  copyright = {Licence Etalab},
  school = {Grenoble 1}
}

@article{durkan_neural_2019,
  title = {Neural {{Spline Flows}}},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  year = {2019},
  month = dec,
  journal = {arXiv:1906.04032 [cs, stat]},
  eprint = {1906.04032},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PAQJVUR4\\1906.html}
}

@article{durmus_fast_2015,
  title = {Fast {{Langevin}} Based Algorithm for {{MCMC}} in High Dimensions},
  author = {Durmus, Alain and Roberts, Gareth and Vilmart, Gilles and Zygalakis, Konstantinos},
  year = {2015},
  month = jul,
  journal = {The Annals of Applied Probability},
  volume = {27},
  doi = {10.1214/16-AAP1257},
  abstract = {We introduce new Gaussian proposals to improve the efficiency of the standard Hastings-Metropolis algorithm in Markov chain Monte Carlo (MCMC) methods, used for the sampling from a target distribution in large dimension \$d\$. The improved complexity is \$\textbackslash mathcal\{O\}(d\^\{1/5\})\$ compared to the complexity \$\textbackslash mathcal\{O\}(d\^\{1/3\})\$ of the standard approach. We prove an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterised by its overall acceptance rate (with asymptotical value 0.704), independently of the target distribution. Numerical experiments confirm our theoretical findings.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5B5ISHWC\\Durmus et al. - 2015 - Fast Langevin based algorithm for MCMC in high dim.pdf}
}

@book{duy_khanh_generalized_2021,
  title = {Generalized {{Damped Newton Algorithms}} in {{Nonsmooth Optimization}} via {{Second-Order Subdifferentials}}},
  author = {Duy Khanh, Pham and Mordukhovich, Boris and Phat, Vo and Tran, Dat},
  year = {2021},
  month = jan,
  abstract = {The paper proposes and develops new globally convergent algorithms of the generalized damped Newton type for solving important classes of nonsmooth optimization problems. These algorithms are based on the theory and calculations of second-order subdifferentials of nonsmooth functions with employing the machinery of second-order variational analysis and generalized differentiation. First we develop a globally superlinearly convergent damped Newton-type algorithm for the class of continuously differentiable functions with Lipschitzian gradients, which are nonsmooth of second order. Then we design such a globally convergent algorithm to solve a structured class of nonsmooth quadratic composite problems with extended-real-valued cost functions, which typically arise in machine learning and statistics. Finally, we present the results of numerical experiments and compare the performance of our main algorithm applied to an important class of Lasso problems with those achieved by other first-order and second-order optimization algorithms},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KUM5MJMF\\Duy Khanh et al. - 2021 - Generalized Damped Newton Algorithms in Nonsmooth .pdf}
}

@article{e._randall_optimum_1981,
  title = {Optimum {{Vibration Absorbers}} for {{Linear Damped Systems}}},
  author = {E. Randall, S and M. Halsted, D and L. Taylor, D},
  year = {1981},
  month = oct,
  journal = {Journal of Mechanical Design - J MECH DESIGN},
  volume = {103},
  doi = {10.1115/1.3255005},
  abstract = {This paper presents computational graphs that determine the optimal linear vibration absorber for linear damped primary systems. Considered as independent parameters are the main system damping ratio and the mass ratio examined over the range 0 to 0. 50 and 0. 01 to 0. 40, respectively. The remaining nondimensional parameters were optimized using numerical methods based on minimum-maximum amplitude criteria. This procedure is illustrated in a design example.}
}

@article{echard_ak-mcs_2011,
  title = {{{AK-MCS}}: {{An}} Active Learning Reliability Method Combining {{Kriging}} and {{Monte Carlo Simulation}}},
  shorttitle = {{{AK-MCS}}},
  author = {Echard, B. and Gayton, N. and Lemaire, M.},
  year = {2011},
  month = mar,
  journal = {Structural Safety},
  volume = {33},
  number = {2},
  pages = {145--154},
  issn = {0167-4730},
  doi = {10.1016/j.strusafe.2011.01.002},
  abstract = {An important challenge in structural reliability is to keep to a minimum the number of calls to the numerical models. Engineering problems involve more and more complex computer codes and the evaluation of the probability of failure may require very time-consuming computations. Metamodels are used to reduce these computation times. To assess reliability, the most popular approach remains the numerous variants of response surfaces. Polynomial Chaos [1] and Support Vector Machine [2] are also possibilities and have gained considerations among researchers in the last decades. However, recently, Kriging, originated from geostatistics, have emerged in reliability analysis. Widespread in optimisation, Kriging has just started to appear in uncertainty propagation [3] and reliability [4], [5] studies. It presents interesting characteristics such as exact interpolation and a local index of uncertainty on the prediction which can be used in active learning methods. The aim of this paper is to propose an iterative approach based on Monte Carlo Simulation and Kriging metamodel to assess the reliability of structures in a more efficient way. The method is called AK-MCS for Active learning reliability method combining Kriging and Monte Carlo Simulation. It is shown to be very efficient as the probability of failure obtained with AK-MCS is very accurate and this, for only a small number of calls to the performance function. Several examples from literature are performed to illustrate the methodology and to prove its efficiency particularly for problems dealing with high non-linearity, non-differentiability, non-convex and non-connex domains of failure and high dimensionality.},
  langid = {english},
  keywords = {Active learning,Failure probability,Kriging,Metamodel,Monte Carlo,Reliability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KA8TFZN7\\Echard et al. - 2011 - AK-MCS An active learning reliability method comb.pdf}
}

@article{egbert_efficient_2002,
  title = {Efficient {{Inverse Modeling}} of {{Barotropic Ocean Tides}}},
  author = {Egbert, Gary D. and Erofeeva, Svetlana Y.},
  year = {2002},
  month = feb,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {19},
  number = {2},
  pages = {183--204},
  publisher = {{American Meteorological Society}},
  issn = {0739-0572},
  doi = {10.1175/1520-0426(2002)019<0183:EIMOBO>2.0.CO;2},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\STY3EGHW\\Egbert et Erofeeva - 2002 - Efficient Inverse Modeling of Barotropic Ocean Tid.pdf;C\:\\Users\\a846735\\Zotero\\storage\\L7JJ5XZF\\Efficient-Inverse-Modeling-of-Barotropic-Ocean.html}
}

@article{eguchi_robustifing_2001,
  title = {Robustifing Maximum Likelihood Estimation by Psi-Divergence},
  author = {Eguchi, Shinto},
  year = {2001},
  month = jan,
  abstract = {A new class of "{$\Psi$}-divergence functionals" over the space of probability densities is defined by introduction of a generic function {$\Psi$}. A new estimator defined as a minimiser of the {$\Psi$}-divergence functional based on data is suggested and is shown to be a robustified maximum likelihood estimator under some simple conditions on the {$\Psi$}. When {$\Psi$} is an identity function, the {$\Psi$}-divergence reduces to the Kullback-Leibler divergence and the new estimator reduces to the usual maximum likelihood estimator. The "{$\Psi$}-likelihood" is defined, and minimization of the {$\Psi$}-divergence is shown to be equivalent to maximization of the {$\Psi$}-likelihood. Once a probability model and a generic function are chosen, one can automatically construct such a robustified estimator. Several informative examples are provided, and relationships with existing robust estimation methods are discussed.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\G3ZRZR29\\Eguchi - 2001 - Robustifing maximum likelihood estimation by psi-d.pdf}
}

@phdthesis{el_amri_analyse_2019,
  type = {Thesis},
  title = {Analyse d'incertitudes et de Robustesse Pour Les Mod\`eles \`a Entr\'ees et Sorties Fonctionnelles},
  author = {El Amri, Mohamed},
  year = {2019},
  month = apr,
  abstract = {L'objectif de cette th\`ese est de r\'esoudre un probl\`eme d'inversion sous incertitudes de fonctions co\^uteuses \`a \'evaluer dans le cadre du param\'etrage du contr\^ole d'un syst\`eme de d\'epollution de v\'ehicules.L'effet de ces incertitudes est pris en compte au travers de l'esp\'erance de la grandeur d'int\'er\^et. Une difficult\'e r\'eside dans le fait que l'incertitude est en partie due \`a une entr\'ee fonctionnelle connue \`a travers d'un \'echantillon donn\'e. Nous proposons deux approches bas\'ees sur une approximation du code co\^uteux par processus gaussiens et une r\'eduction de dimension de la variable fonctionnelle par une m\'ethode de Karhunen-Lo\`eve.La premi\`ere approche consiste \`a appliquer une m\'ethode d'inversion de type SUR (Stepwise Uncertainty Reduction) sur l'esp\'erance de la grandeur d'int\'er\^et. En chaque point d'\'evaluation dans l'espace de contr\^ole, l'esp\'erance est estim\'ee par une m\'ethode de quantification fonctionnelle gloutonne qui fournit une repr\'esentation discr\`ete de la variable fonctionnelle et une estimation s\'equentielle efficace \`a partir de l'\'echantillon donn\'e de la variable fonctionnelle.La deuxi\`eme approche consiste \`a appliquer la m\'ethode SUR directement sur la grandeur d'int\'er\^et dans l'espace joint des variables de contr\^ole et des variables incertaines. Une strat\'egie d'enrichissement du plan d'exp\'eriences d\'edi\'ee \`a l'inversion sous incertitudes fonctionnelles et exploitant les propri\'et\'es des processus gaussiens est propos\'ee.Ces deux approches sont compar\'ees sur des fonctions jouets et sont appliqu\'ees \`a un cas industriel de post-traitement des gaz d'\'echappement d'un v\'ehicule. La probl\'ematique est de d\'eterminer les r\'eglages du contr\^ole du syst\`eme permettant le respect des normes de d\'epollution en pr\'esence d'incertitudes, sur le cycle de conduite.},
  school = {Grenoble Alpes},
  keywords = {Automobiles -- Dispositifs antipollution,Inversion,MathÃ©matiques appliquÃ©es,Quantification des incertitudes,Response surfaces,Surfaces de rÃ©ponse (statistique),Surfaces de rÃ©ponses,Uncertainty Quantification},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5UFAZ49I\\El Amri - 2019 - Analyse d'incertitudes et de robustesse pour les m.pdf;C\:\\Users\\a846735\\Zotero\\storage\\6F4IM8BL\\2019GREAM015.html}
}

@article{el_amri_data-driven_2020,
  title = {Data-Driven Stochastic Inversion via Functional Quantization},
  author = {El Amri, Mohamed Reda and Helbert, C{\'e}line and Lepreux, Olivier and Zuniga, Miguel Munoz and Prieur, Cl{\'e}mentine and Sinoquet, Delphine},
  year = {2020},
  month = may,
  journal = {Statistics and Computing},
  volume = {30},
  number = {3},
  pages = {525--541},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-019-09888-8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4284TR9V\\El Amri et al. - 2020 - Data-driven stochastic inversion via functional qu.pdf}
}

@phdthesis{el_mocayd_decomposition_2017,
  type = {Thesis},
  title = {La D\'ecomposition En Polyn\^ome Du Chaos Pour l'am\'elioration de l'assimilation de Donn\'ees Ensembliste En Hydraulique Fluviale},
  author = {El Mo{\c c}ayd, Nabil},
  year = {2017},
  month = mar,
  journal = {http://www.theses.fr},
  abstract = {Ce travail porte sur la construction d'un mod\`ele r\'eduit en hydraulique fluviale avec une m\'ethode de d\'ecomposition en polyn\^ome du chaos. Ce mod\`ele r\'eduit remplace le mod\`ele direct afin de r\'eduire le co\^ut de calcul li\'e aux m\'ethodes ensemblistes en quantification d'incertitudes et assimilation de donn\'ees. Le contexte de l'\'etude est la pr\'evision des crues et la gestion de la ressource en eau. Ce manuscrit est compos\'e de cinq parties, chacune divis\'ee en chapitres. La premi\`ere partie pr\'esente un \'etat de l'art des travaux en quantification des incertitudes et en assimilation de donn\'ees dans le domaine de l'hydraulique ainsi que les objectifs de la th\`ese. On pr\'esente le cadre de la pr\'evision des crues, ses enjeux et les outils dont on dispose pour pr\'evoir la dynamique des rivi\`eres. On pr\'esente notamment la future mission SWOT qui a pour but de mesurer les hauteurs d'eau dans les rivi\`eres avec un couverture globale \`a haute r\'esolution. On pr\'ecise notamment l'apport de ces mesures et leur compl\'ementarit\'e avec les mesures in-situ. La deuxi\`eme partie pr\'esente les \'equations de Saint-Venant, qui d\'ecrivent les \'ecoulements dans les rivi\`eres, ainsi qu'une discr\'etisation num\'erique de ces \'equations, telle qu'impl\'ement\'ee dans le logiciel Mascaret-1D. Le dernier chapitre de cette partie propose des simplifications des \'equations de Saint-Venant. La troisi\`eme partie de ce manuscrit pr\'esente les m\'ethodes de quantification et de r\'eduction des incertitudes. On pr\'esente notamment le contexte probabiliste de la quantification d'incertitudes et d'analyse de sensibilit\'e. On propose ensuite de r\'eduire la dimension d'un probl\`eme stochastique quand on traite de champs al\'eatoires. Les m\'ethodes de d\'ecomposition en polyn\^omes du chaos sont ensuite pr\'esent\'ees. Cette partie d\'edi\'ee \`a la m\'ethodologie s'ach\`eve par un chapitre consacr\'e \`a l'assimilation de donn\'ees ensemblistes et \`a l'utilisation des mod\`eles r\'eduits dans ce cadre. La quatri\`eme partie de ce manuscrit est d\'edi\'ee aux r\'esultats. On commence par identifier les sources d'incertitudes en hydraulique que l'on s'attache \`a quantifier et r\'eduire par la suite. Un article en cours de r\'evision d\'etaille la validation d'un mod\`ele r\'eduit pour les \'equations de Saint-Venant en r\'egime stationnaire lorsque l'incertitude est majoritairement port\'ee par les coefficients de frottement et le d\'ebit \`a l'amont. On montre que les moments statistiques, la densit\'e de probabilit\'e et la matrice de covariances spatiales pour la hauteur d'eau sont efficacement et pr\'ecis\'ement estim\'es \`a l'aide du mod\`ele r\'eduit dont la construction ne n\'ecessite que quelques dizaines d'int\'egrations du mod\`ele direct. On met \`a profit l'utilisation du mod\`ele r\'eduit pour r\'eduire le co\^ut de calcul du filtre de Kalman d'Ensemble dans le cadre d'un exercice d'assimilation de donn\'ees synth\'etiques de type SWOT. On s'int\'eresse pr\'ecis\'ement \`a la repr\'esentation spatiale de la donn\'ee telle que vue par SWOT: couverture globale du r\'eseau, moyennage spatial entre les pixels observ\'es. On montre notamment qu'\`a budget de calcul donn\'e les r\'esultats de l'analyse d'assimilation de donn\'ees qui repose sur l'utilisation du mod\`ele r\'eduit sont meilleurs que ceux obtenus avec le filtre classique. On s'int\'eresse enfin \`a la construction du mod\`ele r\'eduit en r\'egime instationnaire. On suppose ici que l'incertitude est li\'ee aux coefficients de frottement. Il s'agit \`a pr\'esent de juger de la n\'ecessit\'e du recalcul des coefficients polynomiaux au fil du temps et des cycles d'assimilation de donn\'ees. Pour ce travail seul des donn\'ees in-situ ont \'et\'e consid\'er\'ees. On suppose dans un deuxi\`eme temps que l'incertitude est port\'ee par le d\'ebit en amont du r\'eseau, qui est un vecteur temporel. On proc\`ede \`a une d\'ecomposition de type Karhunen-Lo\`eve pour r\'eduire la taille de l'espace incertain aux trois premiers modes. Nous sommes ainsi en mesure de mener \`a bien un exercice d'assimilation de donn\'ees. Pour finir, les conclusions et les perspectives de ce travail sont pr\'esent\'ees en cinqui\`eme partie.},
  school = {Toulouse, INPT},
  keywords = {Assimilation de donnÃ©es,Assimilation de donnÃ©es (gÃ©ophysique) -- ThÃ¨ses et Ã©crits acadÃ©miques,Chaos polynomial expansion,Data assimilation,GÃ©nie fluvial -- ThÃ¨ses et Ã©crits acadÃ©miques,Hydraulics,Hydraulique fluviale,PolynÃ´mes du chaos,PrÃ©vision hydrologique -- ThÃ¨ses et Ã©crits acadÃ©miques,Quantification des incertitudes,Surfaces Interfaces Continentales Hydrologie,Uncertainty quantification},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FGED5FTL\\2017INPT0020.html}
}

@inproceedings{eldred_formulations_2002,
  title = {Formulations for {{Surrogate-Based Optimization Under Uncertainty}}},
  booktitle = {9th {{AIAA}}/{{ISSMO Symposium}} on {{Multidisciplinary Analysis}} and {{Optimization}}},
  author = {Eldred, Michael and Giunta, Anthony and Wojtkiewicz, Steven and Trucano, Timothy},
  year = {2002},
  month = sep,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  address = {{Atlanta, Georgia}},
  doi = {10.2514/6.2002-5585},
  abstract = {In this paper, several formulations for optimization under uncertainty are presented. In addition to the direct nesting of uncertainty quantification within optimization, formulations are presented for surrogate-based optimization under uncertainty in which the surrogate model appears at the optimization level, at the uncertainty quantification level, or at both levels. These surrogate models encompass both data fit and hierarchical surrogates. The DAKOTA software framework is used to provide the foundation for prototyping and initial benchmarking of these formulations. A critical component is the extension of algorithmic techniques for deterministic surrogate-based optimization to these surrogate-based optimization under uncertainty formulations. This involves the use of sequential trust regionbased approaches to manage the extent of the approximations and verify the approximate optima. Two analytic test problems and one engineering problem are solved using the different methodologies in order to compare their relative merits. Results show that surrogate-based optimization under uncertainty formulations show promise both in reducing the number of function evaluations required and in mitigating the effects of nonsmooth response variations.},
  isbn = {978-1-62410-120-5},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5KF3T7LA\\Eldred et al. - 2002 - Formulations for Surrogate-Based Optimization Unde.pdf}
}

@article{elhabr_tony_2021,
  title = {Tony: {{Tired}}: {{PCA}} + Kmeans, {{Wired}}: {{UMAP}} + {{GMM}}},
  shorttitle = {Tony},
  author = {ElHabr, Tony},
  year = {2021},
  month = jun,
  abstract = {An Alternative to the Classic Approach to Dimension Reduction + Clustering},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZAFGX7HL\\dimensionality-reduction-and-clustering.html}
}

@article{emery_kriging_2009,
  title = {The Kriging Update Equations and Their Application to the Selection of Neighboring Data},
  author = {Emery, Xavier},
  year = {2009},
  month = sep,
  journal = {Computational Geosciences},
  volume = {13},
  number = {3},
  pages = {269--280},
  issn = {1420-0597, 1573-1499},
  doi = {10.1007/s10596-008-9116-8},
  langid = {english}
}

@article{erichson_physics-informed_2019,
  title = {Physics-Informed {{Autoencoders}} for {{Lyapunov-stable Fluid Flow Prediction}}},
  author = {Erichson, N. Benjamin and Muehlebach, Michael and Mahoney, Michael W.},
  year = {2019},
  month = may,
  journal = {arXiv:1905.10866 [physics]},
  eprint = {1905.10866},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {In addition to providing high-profile successes in computer vision and natural language processing, neural networks also provide an emerging set of techniques for scientific problems. Such data-driven models, however, typically ignore physical insights from the scientific system under consideration. Among other things, a ``physics-informed'' model formulation should encode some degree of stability or robustness or well-conditioning (in that a small change of the input will not lead to drastic changes in the output), characteristic of the underlying scientific problem. We investigate whether it is possible to include physics-informed prior knowledge for improving the model quality (e.g., generalization performance, sensitivity to parameter tuning, or robustness in the presence of noisy data). To that extent, we focus on the stability of an equilibrium, one of the most basic properties a dynamic system can have, via the lens of Lyapunov analysis. For the prototypical problem of fluid flow prediction, we show that models preserving Lyapunov stability improve the generalization error and reduce the prediction uncertainty.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P5HANGKD\\Erichson et al. - 2019 - Physics-informed Autoencoders for Lyapunov-stable .pdf}
}

@article{erickson_comparison_2017,
  title = {Comparison of {{Gaussian}} Process Modeling Software},
  author = {Erickson, Collin B. and Ankenman, Bruce E. and Sanchez, Susan M.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.03157 [stat]},
  eprint = {1710.03157},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Gaussian process fitting, or kriging, is often used to create a model from a set of data. Many available software packages do this, but we show that very different results can be obtained from different packages even when using the same data and model. We describe the parameterization, features, and optimization used by eight different fitting packages that run on four different platforms. We then compare these eight packages using various data functions and data sets, revealing that there are stark differences between the packages. In addition to comparing the prediction accuracy, the predictive variance\textemdash which is important for evaluating precision of predictions and is often used in stopping criteria\textemdash is also evaluated.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H8N6PEJV\\Erickson et al. - 2017 - Comparison of Gaussian process modeling software.pdf}
}

@article{evans_inferences_2011,
  title = {Inferences from Prior-Based Loss Functions},
  author = {Evans, Michael and Jang, Gun Ho},
  year = {2011},
  month = apr,
  journal = {arXiv:1104.3258 [math, stat]},
  eprint = {1104.3258},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Inferences that arise from loss functions determined by the prior are considered and it is shown that these lead to limiting Bayes rules that are closely connected with likelihood. The procedures obtained via these loss functions are invariant under reparameterizations and are Bayesian unbiased or limits of Bayesian unbiased inferences. These inferences serve as well-supported alternatives to MAP-based inferences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62C10,Mathematics - Statistics Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EFUNJNSR\\Evans et Jang - 2011 - Inferences from prior-based loss functions.pdf}
}

@article{evans_measuring_2016,
  title = {Measuring Statistical Evidence Using Relative Belief},
  author = {Evans, Michael},
  year = {2016},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {14},
  pages = {91--96},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2015.12.001},
  abstract = {A fundamental concern of a theory of statistical inference is how one should measure statistical evidence. Certainly the words ``statistical evidence,'' or perhaps just ``evidence,'' are much used in statistical contexts. It is fair to say, however, that the precise characterization of this concept is somewhat elusive. Our goal here is to provide a definition of how to measure statistical evidence for any particular statistical problem. Since evidence is what causes beliefs to change, it is proposed to measure evidence by the amount beliefs change from a priori to a posteriori. As such, our definition involves prior beliefs and this raises issues of subjectivity versus objectivity in statistical analyses. This is dealt with through a principle requiring the falsifiability of any ingredients to a statistical analysis. These concerns lead to checking for prior-data conflict and measuring the a priori bias in a prior.},
  langid = {english},
  keywords = {Checking for prior-data conflict,Principle of empirical criticism,Relative belief ratios,Statistical evidence},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3TSW3AQB\\Evans - 2016 - Measuring statistical evidence using relative beli.pdf;C\:\\Users\\a846735\\Zotero\\storage\\BV3FCMIX\\S2001037015000549.html}
}

@inproceedings{fablet_bilinear_2018,
  title = {Bilinear Residual Neural Network for the Identification and Forecasting of Geophysical Dynamics},
  booktitle = {2018 26th {{European}} Signal Processing Conference ({{EUSIPCO}})},
  author = {Fablet, Ronan and Ouala, Said and Herzet, Cedric},
  year = {2018},
  pages = {1477--1481},
  publisher = {{IEEE}}
}

@article{fablet_joint_2020,
  title = {Joint Learning of Variational Representations and Solvers for Inverse Problems with Partially-Observed Data},
  author = {Fablet, Ronan and Drumetz, Lucas and Rousseau, Francois},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.03653 [cs, eess, stat]},
  eprint = {2006.03653},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Designing appropriate variational regularization schemes is a crucial part of solving inverse problems, making them better-posed and guaranteeing that the solution of the associated optimization problem satisfies desirable properties. Recently, learning-based strategies have appeared to be very efficient for solving inverse problems, by learning direct inversion schemes or plug-and-play regularizers from available pairs of true states and observations. In this paper, we go a step further and design an end-to-end framework allowing to learn actual variational frameworks for inverse problems in such a supervised setting. The variational cost and the gradient-based solver are both stated as neural networks using automatic differentiation for the latter. We can jointly learn both components to minimize the data reconstruction error on the true states. This leads to a data-driven discovery of variational models. We consider an application to inverse problems with incomplete datasets (image inpainting and multivariate time series interpolation). We experimentally illustrate that this framework can lead to a significant gain in terms of reconstruction performance, including w.r.t. the direct minimization of the variational formulation derived from the known generative model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IR5Q9Z5C\\Fablet et al. - 2020 - Joint learning of variational representations and .pdf;C\:\\Users\\a846735\\Zotero\\storage\\NIM35KC7\\Fablet et al. - 2020 - Joint learning of variational representations and .pdf;C\:\\Users\\a846735\\Zotero\\storage\\MNMWXNA6\\2006.html}
}

@article{fablet_learning_2021,
  title = {Learning {{Variational Data Assimilation Models}} and {{Solvers}}},
  author = {Fablet, R. and Chapron, B. and Drumetz, L. and M{\'e}min, E. and Pannekoucke, O. and Rousseau, F.},
  year = {2021},
  month = oct,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {13},
  number = {10},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2021MS002572},
  abstract = {Data assimilation is a key component of operational systems and scientific studies for the understanding, modeling, forecasting and reconstruction of earth systems informed by observation data. Here, we investigate how physics-informed deep learning may provide new means to revisit data assimilation problems. We develop a so-called end-to-end learning approach, which explicitly relies on a variational data assimilation formulation. Using automatic differentiation embedded in deep learning framework, the key novelty of the proposed physics-informed approach is to allow the joint training of the representation of the dynamical process of interest as well as of the solver of the data assimilation problem. We may perform this joint training using both supervised and unsupervised strategies. Our numerical experiments on Lorenz-63 and Lorenz-96 systems report significant gain w.r.t. a classic gradient-based minimization of the variational cost both in terms of reconstruction performance and optimization complexity. Intriguingly, we also show that the variational models issued from the true Lorenz-63 and Lorenz-96 ODE representations may not lead to the best reconstruction performance. We believe these results may open new research avenues for the specification of assimilation models for earth systems, both to speed-up the inversion problem with trainable solvers but possibly more importantly in the way data assimilation systems are designed, for instance regarding the representation of geophysical dynamics.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NUIKWNMD\\Fablet et al. - 2021 - Learning Variational Data Assimilation Models and .pdf}
}

@article{feinberg_chaospy:_2015,
  title = {Chaospy: {{An}} Open Source Tool for Designing Methods of Uncertainty Quantification},
  shorttitle = {Chaospy},
  author = {Feinberg, Jonathan and Langtangen, Hans Petter},
  year = {2015},
  month = nov,
  journal = {Journal of Computational Science},
  volume = {11},
  pages = {46--57},
  issn = {1877-7503},
  doi = {10.1016/j.jocs.2015.08.008},
  abstract = {The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.},
  keywords = {Monte Carlo simulation,Polynomial chaos expansions,Python package,Rosenblatt transformations,Uncertainty quantification},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F5UT4FSX\\Feinberg et Langtangen - 2015 - Chaospy An open source tool for designing methods.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KZ9YB9SP\\Feinberg et Langtangen - 2015 - Chaospy An open source tool for designing methods.pdf;C\:\\Users\\a846735\\Zotero\\storage\\CFEA9WQX\\S1877750315300119.html;C\:\\Users\\a846735\\Zotero\\storage\\EYK3LJHA\\S1877750315300119.html}
}

@article{feliot_bayesian_2017,
  title = {A {{Bayesian}} Approach to Constrained Single- and Multi-Objective Optimization},
  author = {Feliot, Paul and Bect, Julien and Vazquez, Emmanuel},
  year = {2017},
  month = jan,
  journal = {Journal of Global Optimization},
  volume = {67},
  number = {1-2},
  eprint = {1510.00503},
  eprinttype = {arxiv},
  pages = {97--133},
  issn = {0925-5001, 1573-2916},
  doi = {10.1007/s10898-016-0427-3},
  abstract = {This article addresses the problem of derivative-free (single- or multi-objective) optimization subject to multiple inequality constraints. Both the objective and constraint functions are assumed to be smooth, non-linear and expensive to evaluate. As a consequence, the number of evaluations that can be used to carry out the optimization is very limited, as in complex industrial design optimization problems. The method we propose to overcome this difficulty has its roots in both the Bayesian and the multi-objective optimization literatures. More specifically, an extended domination rule is used to handle objectives and constraints in a unified way, and a corresponding expected hyper-volume improvement sampling criterion is proposed. This new criterion is naturally adapted to the search of a feasible point when none is available, and reduces to existing Bayesian sampling criteria---the classical Expected Improvement (EI) criterion and some of its constrained/multi-objective extensions---as soon as at least one feasible point is available. The calculation and optimization of the criterion are performed using Sequential Monte Carlo techniques. In particular, an algorithm similar to the subset simulation method, which is well known in the field of structural reliability, is used to estimate the criterion. The method, which we call BMOO (for Bayesian Multi-Objective Optimization), is compared to state-of-the-art algorithms for single- and multi-objective constrained optimization.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FT8S2KHS\\Feliot et al. - 2017 - A Bayesian approach to constrained single- and mul.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ACBZ689B\\1510.html}
}

@article{fillion_iterative_2020,
  title = {An {{Iterative Ensemble Kalman Smoother}} in {{Presence}} of {{Additive Model Error}}},
  author = {Fillion, Anthony and Bocquet, Marc and Gratton, Serge and G{\"u}rol, Selime and Sakov, Pavel},
  year = {2020},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {8},
  number = {1},
  pages = {198--228},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/19M1244147},
  abstract = {Ensemble variational methods are being increasingly used in the field of geophysical data assimilation. Their efficiency comes from the combined use of ensembles, which provide statistics estimates, and a variational analysis, which handles nonlinear operators through iterative optimization techniques. Taking model error into account in four-dimensional ensemble variational algorithms is challenging because the state trajectory over the data assimilation window (DAW) is no longer determined by its sole initial condition. In particular, the control variable dimension scales with the DAW length, which yields a high numerical complexity. This is unfortunate since accuracy improvement is expected with longer DAWs. Building upon the work of [P. Sakov and M. Bocquet, Tellus A, 70 (2018), 1414545], this paper discusses how to algorithmically construct and numerically test an iterative ensemble Kalman smoother with additive model error (IEnKS-Q) which is thought to be the natural weak constraint generalization of the IEnKS [M. Bocquet and P. Sakov, Quart. J. Roy. Meteorol. Soc., 140 (2014), pp. 1521--1535], as well as the generalization of IEnKF-Q [P. Sakov, J. Haussaire, and M. Bocquet, Quart. J. Roy. Meteorol. Soc., 144 (2018), pp. 1297--1309] to general DAWs. The number of model evaluations per cycle of the IEnKS-Q is also examined. Solutions based on perturbation decomposition are proposed to dissociate those numerically costly evaluations from the control variable dimension.},
  keywords = {15A03,60G35,93B07,93C05,93E11,data assimilation,ensemble variational methods,iterative ensemble Kalman filter,iterative ensemble Kalman smoother,model error,weak constraint 4DVar}
}

@article{fletcher_data_2006,
  title = {A Data Assimilation Method for Log-Normally Distributed Observational Errors},
  author = {Fletcher, S. J. and Zupanski, M.},
  year = {2006},
  month = oct,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {132},
  number = {621},
  pages = {2505--2519},
  issn = {1477-870X},
  doi = {10.1256/qj.05.222},
  abstract = {In this paper we change the standard assumption made in the Bayesian framework of variational data assimilation to allow for observational errors that are log-normally distributed. We address the question of which statistic best describes the distribution for the univariate and multivariate cases to justify our choice of the mode. From this choice we derive the associated cost function, Jacobian and Hessian with a normal background. We also find the solution to the Jacobian equal to zero in both model and observational space. Given the Hessian that we derive, we define a preconditioner to aid in the minimization of the cost function. We extend this to define a general form for the preconditioner, given a certain type of cost function. Copyright \textcopyright{} 2006 Royal Meteorological Society},
  langid = {english},
  keywords = {Hessian,Jacobian,Preconditioner},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FD53NXLW\\Fletcher et Zupanski - 2006 - A data assimilation method for log-normally distri.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZUS2Z2T2\\abstract.html}
}

@article{fletcher_hybrid_2006,
  title = {A Hybrid Multivariate {{Normal}} and Lognormal Distribution for Data Assimilation},
  author = {Fletcher, Steven J. and Zupanski, Milija},
  year = {2006},
  month = apr,
  journal = {Atmospheric Science Letters},
  volume = {7},
  number = {2},
  pages = {43--46},
  issn = {1530-261X},
  doi = {10.1002/asl.128},
  abstract = {In this article, we define and prove a distribution, which is a combination of a multivariate Normal and lognormal distribution. From this distribution, we apply a Bayesian probability framework to derive a non-linear cost function similar to the one that is in current variational data assimilation (DA) applications. Copyright \textcopyright{} 2006 Royal Meteorological Society},
  langid = {english},
  keywords = {data assimilation,lognormal,non-Normal,probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3SFBYEXR\\Fletcher et Zupanski - 2006 - A hybrid multivariate Normal and lognormal distrib.pdf;C\:\\Users\\a846735\\Zotero\\storage\\5AFA4GSM\\abstract\;jsessionid=7A1211F56DF3620C35EAF009D18E8855.html}
}

@article{fletcher_mixed_2010,
  title = {Mixed {{Gaussian-lognormal}} Four-Dimensional Data Assimilation},
  author = {Fletcher, S. J.},
  year = {2010},
  month = may,
  journal = {Tellus A},
  volume = {62},
  number = {3},
  pages = {266--287},
  issn = {02806495, 16000870},
  doi = {10.1111/j.1600-0870.2010.00439.x},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6J6U69Y3\\~.pdf}
}

@article{forouzandeh_shahraki_reliability-based_2014,
  title = {Reliability-Based Robust Design Optimization: {{A}} General Methodology Using Genetic Algorithm},
  shorttitle = {Reliability-Based Robust Design Optimization},
  author = {Forouzandeh Shahraki, Ameneh and Noorossana, Rassoul},
  year = {2014},
  month = aug,
  journal = {Computers \& Industrial Engineering},
  volume = {74},
  pages = {199--207},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2014.05.013},
  abstract = {In this paper, we present an improved general methodology including four stages to design robust and reliable products under uncertainties. First, as the formulation stage, we consider reliability and robustness simultaneously to propose the new formulation of reliability-based robust design optimization (RBRDO) problems. In order to generate reliable and robust Pareto-optimal solutions, the combination of genetic algorithm with reliability assessment loop based on the performance measure approach is applied as the second stage. Next, we develop two criteria to select a solution from obtained Pareto-optimal set to achieve the best possible implementation. Finally, the result verification is performed with Monte Carlo Simulations and also the quality improvement during manufacturing process is considered by identifying and controlling the critical variables. The effectiveness and applicability of this new proposed methodology is demonstrated through a case study.},
  keywords = {Genetic algorithm,Most probable point,Multi-objective optimization,Process capability index,Reliability-based robust design optimization},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3SXRCR3K\\Forouzandeh Shahraki et Noorossana - 2014 - Reliability-based robust design optimization A ge.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PFKHCTX8\\S0360835214001582.html}
}

@incollection{forsyth_quick_2008,
  title = {Quick {{Shift}} and {{Kernel Methods}} for {{Mode Seeking}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2008},
  author = {Vedaldi, Andrea and Soatto, Stefano},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  year = {2008},
  volume = {5305},
  pages = {705--718},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-88693-8_52},
  abstract = {We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N 2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and overfragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces in a straightforward manner. We also show that the accelerated medoid shift can be used to initialize mean shift for increased efficiency. We illustrate our algorithms to clustering data on manifolds, image segmentation, and the automatic discovery of visual categories.},
  isbn = {978-3-540-88692-1 978-3-540-88693-8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\54BBK8MD\\Vedaldi et Soatto - 2008 - Quick Shift and Kernel Methods for Mode Seeking.pdf}
}

@article{fragoso_bayesian_2018,
  title = {Bayesian Model Averaging: {{A}} Systematic Review and Conceptual Classification},
  shorttitle = {Bayesian Model Averaging},
  author = {Fragoso, Tiago M. and Neto, Francisco Louzada},
  year = {2018},
  month = apr,
  journal = {International Statistical Review},
  volume = {86},
  number = {1},
  eprint = {1509.08864},
  eprinttype = {arxiv},
  pages = {1--28},
  issn = {03067734},
  doi = {10.1111/insr.12243},
  abstract = {Bayesian Model Averaging (BMA) is an application of Bayesian inference to the problems of model selection, combined estimation and prediction that produces a straightforward model choice criteria and less risky predictions. However, the application of BMA is not always straightforward, leading to diverse assumptions and situational choices on its different aspects. Despite the widespread application of BMA in the literature, there were not many accounts of these differences and trends besides a few landmark revisions in the late 1990s and early 2000s, therefore not taking into account any advancements made in the last 15 years. In this work, we present an account of these developments through a careful content analysis of 587 articles in BMA published between 1996 and 2014. We also develop a conceptual classification scheme to better describe this vast literature, understand its trends and future directions and provide guidance for the researcher interested in both the application and development of the methodology. The results of the classification scheme and content review are then used to discuss the present and future of the BMA literature.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\K7NL3ZT3\\Fragoso et Neto - 2018 - Bayesian model averaging A systematic review and .pdf}
}

@article{frangos_surrogate_2010,
  title = {Surrogate and Reduced-Order Modeling: A Comparison of Approaches for Large-Scale Statistical Inverse Problems [{{Chapter}} 7]},
  shorttitle = {Surrogate and Reduced-Order Modeling},
  author = {Frangos, M. and Marzouk, Y. and Willcox, K. and {van Bloemen Waanders}, B.},
  year = {2010},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RQW32K5F\\Frangos et al. - 2010 - Surrogate and reduced-order modeling a comparison.pdf;C\:\\Users\\a846735\\Zotero\\storage\\U8J84R4V\\105500.html}
}

@article{frantar_m-fac_2021,
  title = {M-{{FAC}}: {{Efficient Matrix-Free Approximations}} of {{Second-Order Information}}},
  shorttitle = {M-{{FAC}}},
  author = {Frantar, Elias and Kurtic, Eldar and Alistarh, Dan},
  year = {2021},
  month = nov,
  journal = {arXiv:2107.03356 [cs]},
  eprint = {2107.03356},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Efficiently approximating local curvature information of the loss function is a key tool for optimization and compression of deep neural networks. Yet, most existing methods to approximate second-order information have high computational or storage costs, which limits their practicality. In this work, we investigate matrix-free, linear-time approaches for estimating Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be approximated as a sum of rank-one matrices, as in the classic approximation of the Hessian by the empirical Fisher matrix. We propose two new algorithms: the first is tailored towards network compression and can compute the IHVP for dimension d, if the Hessian is given as a sum of m rank-one matrices, using O(dm2) precomputation, O(dm) cost for computing the IHVP, and query cost O(m) for any single element of the inverse Hessian. The second algorithm targets an optimization setting, where we wish to compute the product between the inverse Hessian, estimated over a sliding window of optimization steps, and a given gradient direction, as required for preconditioned SGD. We give an algorithm with cost O(dm + m2) for computing the IHVP and O(dm + m3) for adding or removing any gradient from the sliding window. These two algorithms yield state-of-the-art results for network pruning and optimization with lower computational overhead relative to existing second-order methods. Implementations are available at [9] and [17].},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZV9ZTHEC\\Frantar et al. - 2021 - M-FAC Efficient Matrix-Free Approximations of Sec.pdf}
}

@article{frazier_knowledge-gradient_2008,
  title = {A {{Knowledge-Gradient Policy}} for {{Sequential Information Collection}}},
  author = {Frazier, Peter I. and Powell, Warren B. and Dayanik, Savas},
  year = {2008},
  month = jan,
  journal = {SIAM Journal on Control and Optimization},
  volume = {47},
  number = {5},
  pages = {2410--2439},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/070693424},
  abstract = {In a sequential Bayesian ranking and selection problem with independent normal populations and common known variance, we study a previously introduced measurement policy which we refer to as the knowledge-gradient policy. This policy myopically maximizes the expected increment in the value of information in each time period, where the value is measured according to the terminal utility function. We show that the knowledge-gradient policy is optimal both when the horizon is a single time period, and in the limit as the horizon extends to infinity. We show furthermore that, in some special cases, the knowledge-gradient policy is optimal regardless of the length of any given fixed total sampling horizon. We bound the knowledge-gradient policy's suboptimality in the remaining cases, and show through simulations that it performs competitively with or significantly better than other policies.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZGFMIHD8\\Frazier et al. - 2008 - A Knowledge-Gradient Policy for Sequential Informa.pdf}
}

@article{freedman_histogram_1981,
  title = {On the Histogram as a Density Estimator:{{L2}} Theory},
  shorttitle = {On the Histogram as a Density Estimator},
  author = {Freedman, David and Diaconis, Persi},
  year = {1981},
  month = dec,
  journal = {Zeitschrift f\"ur Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  volume = {57},
  number = {4},
  pages = {453--476},
  issn = {1432-2064},
  doi = {10.1007/BF01025868},
  langid = {english},
  keywords = {Density Estimator,Mathematical Biology,Probability Theory,Stochastic Process},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FVPC6M7L\\Freedman et Diaconis - 1981 - On the histogram as a density estimatorL2 theory.pdf}
}

@article{frerix_variational_2021,
  title = {Variational {{Data Assimilation}} with a {{Learned Inverse Observation Operator}}},
  author = {Frerix, Thomas and Kochkov, Dmitrii and Smith, Jamie A. and Cremers, Daniel and Brenner, Michael P. and Hoyer, Stephan},
  year = {2021},
  month = may,
  journal = {arXiv:2102.11192 [physics]},
  eprint = {2102.11192},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Variational data assimilation optimizes for an initial state of a dynamical system such that its evolution fits observational data. The physical model can subsequently be evolved into the future to make predictions. This principle is a cornerstone of large scale forecasting applications such as numerical weather prediction. As such, it is implemented in current operational systems of weather forecasting agencies across the globe. However, finding a good initial state poses a difficult optimization problem in part due to the non-invertible relationship between physical states and their corresponding observations. We learn a mapping from observational data to physical states and show how it can be used to improve optimizability. We employ this mapping in two ways: to better initialize the non-convex optimization problem, and to reformulate the objective function in better behaved physics space instead of observation space. Our experimental results for the Lorenz96 model and a two-dimensional turbulent fluid flow demonstrate that this procedure significantly improves forecast quality for chaotic systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Atmospheric and Oceanic Physics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7EDY23P2\\2102.11192.pdf}
}

@article{friederichs_probabilistic_2008,
  title = {A {{Probabilistic Forecast Approach}} for {{Daily Precipitation Totals}}},
  author = {Friederichs, Petra and Hense, Andreas},
  year = {2008},
  month = aug,
  journal = {Weather and Forecasting},
  volume = {23},
  number = {4},
  pages = {659--673},
  issn = {0882-8156, 1520-0434},
  doi = {10.1175/2007WAF2007051.1},
  abstract = {Commonly, postprocessing techniques are employed to calibrate a model forecast. Here, a probabilistic postprocessor is presented that provides calibrated probability and quantile forecasts of precipitation on the local scale. The forecasts are based on large-scale circulation patterns of the 12-h forecast from the NCEP high-resolution Global Forecast System (GFS). The censored quantile regression is used to estimate selected quantiles of the precipitation amount and the probability of the occurrence of precipitation. The approach accounts for the mixed discrete-continuous character of daily precipitation totals. The forecasts are verified using a new verification score for quantile forecasts, namely the censored quantile verification (CQV) score.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\N6IJ5UU4\\Friederichs et Hense - 2008 - A Probabilistic Forecast Approach for Daily Precip.pdf}
}

@article{friel_estimating_2011,
  title = {Estimating the Evidence -- a Review},
  author = {Friel, Nial and Wyse, Jason},
  year = {2011},
  month = nov,
  journal = {arXiv:1111.1957 [stat]},
  eprint = {1111.1957},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The model evidence is a vital quantity in the comparison of statistical models under the Bayesian paradigm. This paper presents a review of commonly used methods. We outline some guidelines and offer some practical advice. The reviewed methods are compared for two examples; non-nested Gaussian linear regression and covariate subset selection in logistic regression.},
  archiveprefix = {arXiv},
  keywords = {Evidence estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W9R587RW\\Friel et Wyse - 2011 - Estimating the evidence -- a review.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZV9QA6PC\\1111.html}
}

@article{frisch_burgulence_2000,
  title = {Burgulence},
  author = {Frisch, U. and Bec, J.},
  year = {2000},
  month = dec,
  journal = {arXiv:nlin/0012033},
  eprint = {nlin/0012033},
  eprinttype = {arxiv},
  abstract = {This is a review of selected work on the one- and multi-dimensional random Burgers equation (burgulence) with emphasis on questions generally asked for incompressible Navier\textendash Stokes turbulence, such as the law of decay of the energy and the pdf of velocity gradients. Most of the material is devoted to decaying (unforced) burgulence. For more details see the Table of Contents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astrophysics,Condensed Matter,Nonlinear Sciences - Chaotic Dynamics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6MSAE4SI\\Frisch et Bec - 2000 - Burgulence.pdf}
}

@article{fu_analysis_2018,
  title = {Analysis of Influence of Observation Operator on Sequential Data Assimilation through Soil Temperature Simulation with Common Land Model},
  author = {Fu, Xiao-lei and Yu, Zhong-bo and Ding, Yong-jian and Tang, Ying and L{\"u}, Hai-shen and Jiang, Xiao-lei and Ju, Qin},
  year = {2018},
  month = jul,
  journal = {Water Science and Engineering},
  volume = {11},
  number = {3},
  pages = {196--204},
  issn = {1674-2370},
  doi = {10.1016/j.wse.2018.09.003},
  abstract = {An observation operator is a bridge linking the system state vector and observations in a data assimilation system. Despite its importance, the degree to which an observation operator influences the performance of data assimilation methods is still poorly understood. This study aimed to analyze the influences of linear and nonlinear observation operators on the sequential data assimilation through soil temperature simulation using the unscented particle filter (UPF) and the common land model. The linear observation operator between unprocessed simulations and observations was first established. To improve the correlation between simulations and observations, both were processed based on a series of equations. This processing essentially resulted in a nonlinear observation operator. The linear and nonlinear observation operators were then used along with the UPF in three assimilation experiments: an hourly in situ soil surface temperature assimilation, a daily in situ soil surface temperature assimilation, and a moderate resolution imaging spectroradiometer (MODIS) land surface temperature (LST) assimilation. The results show that the filter improved the soil temperature simulation significantly with the linear and nonlinear observation operators. The nonlinear observation operator improved the UPF's performance more significantly for the hourly and daily in situ observation assimilations than the linear observation operator did, while the situation was opposite for the MODIS LST assimilation. Because of the high assimilation frequency and data quality, the simulation accuracy was significantly improved in all soil layers for hourly in situ soil surface temperature assimilation, while the significant improvements of the simulation accuracy were limited to the lower soil layers for the assimilation experiments with low assimilation frequency or low data quality.},
  langid = {english},
  keywords = {Data assimilation,MODIS LST,Observation operator,Soil temperature,Unscented particle filter (UPF)}
}

@article{fuhg_state---art_2020,
  title = {State-of-the-{{Art}} and {{Comparative Review}} of {{Adaptive Sampling Methods}} for {{Kriging}}},
  author = {Fuhg, Jan N. and Fau, Am{\'e}lie and Nackenhorst, Udo},
  year = {2020},
  month = aug,
  journal = {Archives of Computational Methods in Engineering},
  issn = {1886-1784},
  doi = {10.1007/s11831-020-09474-6},
  abstract = {Metamodels aim to approximate characteristics of functions or systems from the knowledge extracted on only a finite number of samples. In recent years kriging has emerged as a widely applied metamodeling technique for resource-intensive computational experiments. However its prediction quality is highly dependent on the size and distribution of the given training points. Hence, in order to build proficient kriging models with as few samples as possible adaptive sampling strategies have gained considerable attention. These techniques aim to find pertinent points in an iterative manner based on information extracted from the current metamodel. A review of adaptive schemes for kriging proposed in the literature is presented in this article. The objective is to provide the reader with an overview of the main principles of adaptive techniques, and insightful details to pertinently employ available tools depending on the application at hand. In this context commonly applied strategies are compared with regards to their characteristics and approximation capabilities. In light of these experiments, it is found that the success of a scheme depends on the features of a specific problem and the goal of the analysis. In order to facilitate the entry into adaptive sampling a guide is provided. All experiments described herein are replicable using a provided open source toolbox.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BRKD4UA7\\Fuhg et al. - 2020 - State-of-the-Art and Comparative Review of Adaptiv.pdf}
}

@techreport{fukumizu_dimensionality_2003,
  title = {Dimensionality {{Reduction}} for {{Supervised Learning With Reproducing Kernel Hilbert Spaces}}:},
  shorttitle = {Dimensionality {{Reduction}} for {{Supervised Learning With Reproducing Kernel Hilbert Spaces}}},
  author = {Fukumizu, Kenji and Bach, Francis R. and Jordan, Michael I.},
  year = {2003},
  month = may,
  address = {{Fort Belvoir, VA}},
  institution = {{Defense Technical Information Center}},
  doi = {10.21236/ADA446572},
  abstract = {We propose a novel method of dimensionality reduction for supervised learning problems. Given a regression or classification problem in which we wish to predict a response variable Y from an explanatory variable X, we treat the problem of dimensionality reduction as that of finding a low-dimensional ``effective subspace'' for X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . We present experiments that compare the performance of the method with conventional methods.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\L4WGY8C9\\Fukumizu et al. - 2003 - Dimensionality Reduction for Supervised Learning W.pdf}
}

@article{gabrel_recent_2014,
  title = {Recent Advances in Robust Optimization: {{An}} Overview},
  shorttitle = {Recent Advances in Robust Optimization},
  author = {Gabrel, Virginie and Murat, C{\'e}cile and Thiele, Aur{\'e}lie},
  year = {2014},
  month = jun,
  journal = {European Journal of Operational Research},
  volume = {235},
  number = {3},
  pages = {471--483},
  issn = {03772217},
  doi = {10.1016/j.ejor.2013.09.036},
  abstract = {This paper provides an overview of developments in robust optimization since 2007. It seeks to give a representative picture of the research topics most explored in recent years, highlight common themes in the investigations of independent research teams and highlight the contributions of rising as well as established researchers both to the theory of robust optimization and its practice. With respect to the theory of robust optimization, this paper reviews recent results on the cases without and with recourse, i.e., the static and dynamic settings, as well as the connection with stochastic optimization and risk theory, the concept of distributionally robust optimization, and findings in robust nonlinear optimization. With respect to the practice of robust optimization, we consider a broad spectrum of applications, in particular inventory and logistics, finance, revenue management, but also queueing networks, machine learning, energy systems and the public good. Key developments in the period from 2007 to present include: (i) an extensive body of work on robust decision-making under uncertainty with uncertain distributions, i.e., ``robustifying'' stochastic optimization, (ii) a greater connection with decision sciences by linking uncertainty sets to risk theory, (iii) further results on nonlinear optimization and sequential decision-making and (iv) besides more work on established families of examples such as robust inventory and revenue management, the addition to the robust optimization literature of new application areas, especially energy systems and the public good.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\72QGQQ79\\Gabrel et al. - 2014 - Recent advances in robust optimization An overvie.pdf}
}

@article{gallier_algebra_nodate,
  title = {Algebra, {{Topology}}, {{Differential Calculus}}, and {{Optimization Theory For Computer Science}} and {{Machine Learning}}},
  author = {Gallier, Jean and Quaintance, Jocelyn},
  pages = {2188},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BRZ85FYA\\Gallier et Quaintance - Algebra, Topology, Diï¬erential Calculus, and Optim.pdf}
}

@book{garnett_bayesian_2022,
  title = {Bayesian {{Optimization}}},
  author = {Garnett, Roman},
  year = {2022},
  publisher = {{Cambridge University Press}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WCMWM39Q\\Garnett - BAYESIAN OPTIMIZATION.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZUF6GMRH\\bayesoptbook.com.html}
}

@misc{gassiat_statistiques_nodate,
  title = {Statistiques {{Asymptotiques}} - {{Notes}} de {{Cours M2}}},
  author = {Gassiat, Elisabeth},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LCNS66GE\\notes de cours- M2Stat.pdf}
}

@article{gazzola_krylov_2020,
  title = {Krylov Methods for Inverse Problems: {{Surveying}} Classical, and Introducing New, Algorithmic Approaches},
  shorttitle = {Krylov Methods for Inverse Problems},
  author = {Gazzola, Silvia and Sabat{\'e} Landman, Malena},
  year = {2020},
  journal = {GAMM-Mitteilungen},
  volume = {43},
  number = {4},
  pages = {e202000017},
  issn = {1522-2608},
  doi = {10.1002/gamm.202000017},
  abstract = {Large-scale linear systems coming from suitable discretizations of linear inverse problems are challenging to solve. Indeed, since they are inherently ill-posed, appropriate regularization should be applied; since they are large-scale, well-established direct regularization methods (such as Tikhonov regularization) cannot often be straightforwardly employed, and iterative linear solvers should be exploited. Moreover, every regularization method crucially depends on the choice of one or more regularization parameters, which should be suitably tuned. The aim of this paper is twofold: (a) survey some well-established regularizing projection methods based on Krylov subspace methods (with a particular emphasis on methods based on the Golub-Kahan bidiagonalization algorithm), and the so-called hybrid approaches (which combine Tikhonov regularization and projection onto Krylov subspaces of increasing dimension); (b) introduce a new principled and adaptive algorithmic approach for regularization similar to specific instances of hybrid methods. In particular, the new strategy provides reliable parameter choice rules by leveraging the framework of bilevel optimization, and the links between Gauss quadrature and Golub-Kahan bidiagonalization. Numerical tests modeling inverse problems in imaging illustrate the performance of existing regularizing Krylov methods, and validate the new algorithms.},
  langid = {english},
  keywords = {hybrid methods,imaging problems,Krylov subspace methods,large-scale linear inverse problems,regularization parameter choice rules,Tikhonov regularization},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/gamm.202000017},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JIQKK6K3\\Gazzola et SabatÃ© Landman - 2020 - Krylov methods for inverse problems Surveying cla.pdf;C\:\\Users\\a846735\\Zotero\\storage\\FYQWKHCY\\gamm.html}
}

@article{ge_stochastic_2008,
  title = {Stochastic {{Solution}} for {{Uncertainty Propagation}} in {{Nonlinear Shallow-Water Equations}}},
  author = {Ge, Liang and Cheung, Kwok Fai and Kobayashi, Marcelo H.},
  year = {2008},
  month = dec,
  journal = {Journal of Hydraulic Engineering},
  volume = {134},
  number = {12},
  pages = {1732--1743},
  issn = {0733-9429, 1943-7900},
  doi = {10.1061/(ASCE)0733-9429(2008)134:12(1732)},
  abstract = {This paper presents a stochastic approach to describe input uncertainties and their propagation through the nonlinear shallowwater equations. The formulation builds on a finite-volume model with a Godunov-type scheme for its shock capturing capabilities. Orthogonal polynomials from the Askey scheme provide expansion of the variables in terms of a finite number of modes from which the mean and higher-order moments of the distribution can be derived. The orthogonal property of the polynomials allows the use of a Galerkin projection to derive separate equations for the individual modes. Implementation of the polynomial chaos expansion and its nonintrusive counterpart determines the modal contributions from the resulting system of equations. Examples of long-wave transformation over a submerged hump illustrate the stochastic approach with uncertainties represented by Gaussian distribution. Additional results demonstrate the applicability of the approach with other distributions as well. The stochastic solution agrees well with the results from the Monte Carlo method, but at a small fraction of its computing cost.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QG6NBBQ9\\Ge et al. - 2008 - Stochastic Solution for Uncertainty Propagation in.pdf}
}

@misc{geer_learning_2020,
  title = {Learning Earth System Models from Observations: Machine Learning or Data Assimilation?},
  shorttitle = {Learning Earth System Models from Observations},
  author = {Geer, Alan},
  year = {2020},
  publisher = {{ECMWF}},
  doi = {10.21957/7fyj2811r},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GMJIYWNU\\Geer - 2020 - Learning earth system models from observations ma.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KE7ZQMZE\\19525-learning-earth-system-models-observations-machine-learning-or-data-assimilation.html}
}

@unpublished{gerbeau_moment-matching_2016,
  title = {A Moment-Matching Method to Study the Variability of Phenomena Described by Partial Differential Equations},
  author = {Gerbeau, Jean-Fr{\'e}d{\'e}ric and Lombardi, Damiano and Tixier, Eliott},
  year = {2016},
  month = nov,
  abstract = {Many phenomena are modeled by deterministic differential equations , whereas the observation of these phenomena, in particular in life sciences, exhibits an important variability. This paper addresses the following question: how can the model be adapted to reflect the observed variability? Given an adequate model, it is possible to account for this variability by allowing some parameters to adopt a stochastic behavior. Finding the parameters probability density function that explains the observed variability is a difficult stochastic inverse problem, especially when the computational cost of the forward problem is high. In this paper, a non-parametric and non-intrusive procedure based on offline computations of the forward model is proposed. It infers the probability density function of the uncertain parameters from the matching of the statistical moments of observable degrees of freedom (DOFs) of the model. This inverse procedure is improved by incorporating an algorithm that selects a subset of the model DOFs that both reduces its computational cost and increases its robustness. This algorithm uses the pre-computed model outputs to build an approximation of the local sensitivities. The DOFs are selected so that the maximum information on the sensitivities is conserved. The proposed approach is illustrated with elliptic and parabolic PDEs. In the Appendix, an nonlinear ODE is considered and the strategy is compared with two existing ones.},
  keywords = {backward uncertainty quantification,maximum entropy,Moment matching,stochastic inverse problem},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\M4DPXLDQ\\Gerbeau et al. - 2016 - A moment-matching method to study the variability .pdf}
}

@article{geyer_5601_nodate,
  title = {5601 {{Notes}}: {{The Sandwich Estimator}}},
  author = {Geyer, Charles J},
  pages = {28},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DWXES7GY\\Geyer - 5601 Notes The Sandwich Estimator.pdf}
}

@article{ghate_inexpensive_nodate,
  title = {Inexpensive {{Monte Carlo Uncertainty Analysis}}},
  author = {Ghate, D and Giles, M B},
  pages = {8},
  abstract = {This paper proposes a new methodology for uncertainty propagation through CFD codes using adjoint error correction[7]. The mathematical formulation is presented followed by a proof-ofconcept model example. The method is further demonstrated using artificial geometric uncertainty for a 2D inviscid airfoil code. Reduced order modelling has been used to further reduce the computational costs, and results of approximate Monte Carlo(MC) simulations have been compared with full nonlinear MC simulations.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2SC6RRQI\\Ghate et Giles - Inexpensive Monte Carlo Uncertainty Analysis.pdf}
}

@article{gibbs_choosing_nodate,
  title = {{{ON CHOOSING AND BOUNDING PROBABILITY METRICS}}},
  author = {Gibbs, Alison L and Su, Francis Edward},
  pages = {21},
  abstract = {When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability metrics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SMSLWCWI\\Gibbs et Su - ON CHOOSING AND BOUNDING PROBABILITY METRICS.pdf}
}

@article{gilbert_numerical_1989,
  title = {Some Numerical Experiments with Variable-Storage Quasi-{{Newton}} Algorithms},
  author = {Gilbert, Jean Charles and Lemar{\'e}chal, Claude},
  year = {1989},
  journal = {Mathematical programming},
  volume = {45},
  number = {1-3},
  pages = {407--435},
  publisher = {{Springer}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VKPDTYAZ\\Gilbert et LemarÃ©chal - 1989 - Some numerical experiments with variable-storage q.pdf;C\:\\Users\\a846735\\Zotero\\storage\\FIDU8WKN\\BF01589113.html}
}

@phdthesis{gilquin_echantillonnages_2016,
  title = {{\'Echantillonnages Monte Carlo et quasi-Monte Carlo pour l'estimation des indices de Sobol' : application \`a un mod\`ele transport-urbanisme}},
  shorttitle = {{\'Echantillonnages Monte Carlo et quasi-Monte Carlo pour l'estimation des indices de Sobol'}},
  author = {Gilquin, Laurent},
  year = {2016},
  month = oct,
  abstract = {Le d\'eveloppement et l'utilisation de mod\`eles int\'egr\'es transport-urbanisme sont devenus une norme pour repr\'esenter les interactions entre l'usage des sols et le transport de biens et d'individus sur un territoire. Ces mod\`eles sont souvent utilis\'es comme outils d'aide \`a la d\'ecision pour des politiques de planification urbaine.Les mod\`eles transport-urbanisme, et plus g\'en\'eralement les mod\`eles math\'ematiques, sont pour la majorit\'e con\c{c}us \`a partir de codes num\'eriques complexes. Ces codes impliquent tr\`es souvent des param\`etres dont l'incertitude est peu connue et peut potentiellement avoir un impact important sur les variables de sortie du mod\`ele.Les m\'ethodes d'analyse de sensibilit\'e globales sont des outils performants permettant d'\'etudier l'influence des param\`etres d'un mod\`ele sur ses sorties. En particulier, les m\'ethodes bas\'ees sur le calcul des indices de sensibilit\'e de Sobol' fournissent la possibilit\'e de quantifier l'influence de chaque param\`etre mais \'egalement d'identifier l'existence d'interactions entre ces param\`etres.Dans cette th\`ese, nous privil\'egions la m\'ethode dite \`a base de plans d'exp\'eriences r\'epliqu\'es encore appel\'ee m\'ethode r\'epliqu\'ee. Cette m\'ethode a l'avantage de ne requ\'erir qu'un nombre relativement faible d'\'evaluations du mod\`ele pour calculer les indices de Sobol' d'ordre un et deux.Cette th\`ese se focalise sur des extensions de la m\'ethode r\'epliqu\'ee pour faire face \`a des contraintes issues de notre application sur le mod\`ele transport-urbanisme Tranus, comme la pr\'esence de corr\'elation entre param\`etres et la prise en compte de sorties multivari\'ees.Nos travaux proposent \'egalement une approche r\'ecursive pour l'estimation s\'equentielle des indices de Sobol'. L'approche r\'ecursive repose \`a la fois sur la construction it\'erative d'hypercubes latins et de tableaux orthogonaux stratifi\'es et sur la d\'efinition d'un nouveau crit\`ere d'arr\^et. Cette approche offre une meilleure pr\'ecision sur l'estimation des indices tout en permettant de recycler des premiers jeux d'\'evaluations du mod\`ele. Nous proposons aussi de combiner une telle approche avec un \'echantillonnage quasi-Monte Carlo.Nous pr\'esentons \'egalement une application de nos contributions pour le calage du mod\`ele de transport-urbanisme Tranus.},
  langid = {french},
  school = {Universit\'e Grenoble Alpes},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W44NI63K\\Gilquin - 2016 - Ãchantillonnages Monte Carlo et quasi-Monte Carlo .pdf;C\:\\Users\\a846735\\Zotero\\storage\\RK4DE9LZ\\tel-01680304.html}
}

@article{gilquin_making_2019,
  title = {Making Best Use of Permutations to Compute Sensitivity Indices with Replicated Orthogonal Arrays},
  author = {Gilquin, Laurent and Arnaud, Elise and Prieur, Cl{\'e}mentine and Janon, Alexandre},
  year = {2019},
  month = jul,
  journal = {Reliability Engineering and System Safety},
  volume = {187},
  pages = {28--39},
  doi = {10.1016/j.ress.2018.09.010},
  abstract = {Among practitioners, the importance of inputs to a model output is commonly measured via the computation of Sobol' sensitivity indices. Various estimation strategies exist in the literature, most of them requiring a very high number of model evaluations. Designing methods that compete favorably both in terms of computational cost and accuracy is therefore an issue of crucial importance. In this paper, an efficient replication-based strategy is proposed to estimate the full set of first- and second-order Sobol' indices. It relies on a Sobol' pick-freeze estimation scheme and requires only two replicated designs based on randomized orthogonal arrays of strength two. The precision of this procedure is assessed with bootstrap confidence intervals, presented for the first time in the replication framework. Our developments are compared to known approaches and validated on numerical test cases. A way to estimate the full set of first-, second-order but also total-effect Sobol' indices at a very competitive cost is also described, as a combination of our procedure and the one introduced by Saltelli.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IJF77PWT\\Gilquin et al. - 2019 - Making best use of permutations to compute sensiti.pdf;C\:\\Users\\a846735\\Zotero\\storage\\R2NJWXBE\\hal-01558915v2.html}
}

@article{ginsbourger_bayesian_2014,
  title = {Bayesian {{Adaptive Reconstruction}} of {{Profile Optima}} and {{Optimizers}}},
  author = {Ginsbourger, David and Baccou, Jean and Chevalier, Cl{\'e}ment and Perales, Fr{\'e}d{\'e}ric and Garland, Nicolas and Monerie, Yann},
  year = {2014},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {2},
  number = {1},
  pages = {490--510},
  issn = {2166-2525},
  doi = {10.1137/130949555},
  abstract = {Given a function depending both on decision parameters and nuisance variables, we consider the issue of estimating and quantifying uncertainty on profile optima and/or optimal points as functions of the nuisance variables. The proposed methods base on interpolations of the objective function constructed from a finite set of evaluations. Here the functions of interest are reconstructed relying on a kriging model, but also using Gaussian field conditional simulations, that allow a quantification of uncertainties in the Bayesian framework. Besides, we elaborate a variant of the Expected Improvement criterion, that proves efficient for adaptively learning the set of profile optima and optimizers. The results are illustrated on a toy example and through a physics case study on the optimal packing of polydisperse frictionless spheres.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FBVA9HE4\\Ginsbourger et al. - 2014 - Bayesian Adaptive Reconstruction of Profile Optima.pdf}
}

@incollection{ginsbourger_kriging_2010,
  title = {Kriging Is Well-Suited to Parallelize Optimization},
  booktitle = {Computational {{Intelligence}} in {{Expensive Optimization Problems}}},
  author = {Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  editor = {Tenne, Yoel and Goh, Chi-Keong},
  year = {2010},
  series = {Springer Series in {{Evolutionary Learning}} and {{Optimization}}},
  pages = {131--162},
  publisher = {{springer}},
  doi = {10.1007/978-3-642-10701-6_6},
  abstract = {The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement ({$\mathsl{q}-\mathbb{E}\mathsl{I}$}q-EIq-\{\textbackslash mathbb E\}I), aimed at choosing several points at the same time. An analytical expression of the {$\mathsl{q}-\mathbb{E}\mathsl{I}$}q-EIq-\{\textbackslash mathbb E\}I is given when q\,=\,2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the {$\mathsl{q}-\mathbb{E}\mathsl{I}$}q-EIq-\{\textbackslash mathbb E\}I, and apply them to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q\,{$\in$} [1,10]).},
  keywords = {gaussian process,global optimization,kriging,optimization},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\M92WKPUQ\\emse-00436126.html}
}

@article{ginsbourger_note_2009,
  title = {A Note on the Choice and the Estimation of {{Kriging}} Models for the Analysis of Deterministic Computer Experiments},
  author = {Ginsbourger, David and Dupuy, Delphine and Badea, Anca and Carraro, Laurent and Roustant, Olivier},
  year = {2009},
  month = mar,
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {25},
  number = {2},
  pages = {115--131},
  issn = {15241904, 15264025},
  doi = {10.1002/asmb.741},
  abstract = {Our goal in the present work is to give an insight on some important questions to be asked when choosing a Kriging model for the analysis of numerical experiments. We are especially concerned about the cases where the size of the design of experiments is small relatively to the algebraic dimension of the inputs. We first fix the notations and recall some basic properties of Kriging. Then we expose two experimental studies on subjects that are often skipped in the field of computer simulation analysis: the lack of reliability of likelihood maximization with few data, and the consequences of a trend misspecification. We finally propose an example from a porous media application, with the introduction of an original Kriging method in which a non-linear additive model is used as external trend.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MBJ3DIBV\\Ginsbourger et al. - 2009 - A note on the choice and the estimation of Kriging.pdf}
}

@inproceedings{ginsbourger_towards_2010,
  title = {Towards {{Gaussian Process-based Optimization}} with {{Finite Time Horizon}}},
  booktitle = {{{mODa}} 9 \textendash{} {{Advances}} in {{Model-Oriented Design}} and {{Analysis}}},
  author = {Ginsbourger, David and Le Riche, Rodolphe},
  editor = {Giovagnoli, Alessandra and Atkinson, Anthony C. and Torsney, Bernard and May, Caterina},
  year = {2010},
  series = {Contributions to {{Statistics}}},
  pages = {89--96},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-7908-2410-0_12},
  abstract = {During the last decade, Kriging-based sequential optimization algorithms have become standard methods in computer experiments. These algorithms rely on the iterative maximization of sampling criteria such as the Expected Improvement (EI), which takes advantage of Kriging conditional distributions to make an explicit trade-off between promising and uncertain points in the search space. We have recently worked on a multipoint EI criterion meant to choose simultaneously several points for synchronous parallel computation. The results presented in this article concern sequential procedures with a fixed number of iterations. We show that maximizing the usual EI at each iteration is suboptimal. In essence, the latter amounts to considering the current iteration as the last one. This work formulates the problem of optimal strategy for finite horizon sequential optimization, provides the solution to this problem in terms of a new multipoint EI, and illustrates the suboptimality of maximizing the 1-point EI at each iteration on the basis of a first counter-example.},
  isbn = {978-3-7908-2410-0},
  langid = {english},
  keywords = {Approximate Dynamic Program,Finite Horizon,Finite Time Horizon,Gaussian Process Model,Sequential Strategy}
}

@inproceedings{giunta_perspectives_2004,
  title = {Perspectives in {{Optimization Under Uncertainty}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Perspectives in {{Optimization Under Uncertainty}}},
  booktitle = {10th {{AIAA}}/{{ISSMO Multidisciplinary Analysis}} and {{Optimization Conference}}},
  author = {Giunta, Anthony and Eldred, Michael and Swiler, Laura and Trucano, Timothy and Wojtkiewicz, Steven},
  year = {2004},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2004-4451},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CJN956LS\\Giunta et al. - Perspectives in Optimization Under Uncertainty Al.pdf;C\:\\Users\\a846735\\Zotero\\storage\\IW3DSWB8\\6.html}
}

@book{glasserman_monte_2013,
  title = {Monte {{Carlo}} Methods in Financial Engineering},
  author = {Glasserman, Paul},
  year = {2013},
  volume = {53},
  publisher = {{Springer Science \& Business Media}},
  keywords = {Gibbs,LHS,MCMC,Sampling},
  file = {C\:\\Users\\Victor\\Documents\\[Paul_Glasserman]_Monte_Carlo_Methods_in_Financial_Engineering_Springer_2003.pdf}
}

@article{gneiting_comparing_2011,
  title = {Comparing {{Density Forecasts Using Threshold-}} and {{Quantile-Weighted Scoring Rules}}},
  author = {Gneiting, Tilmann and Ranjan, Roopesh},
  year = {2011},
  month = jul,
  journal = {Journal of Business \& Economic Statistics},
  volume = {29},
  number = {3},
  pages = {411--422},
  issn = {0735-0015, 1537-2707},
  doi = {10.1198/jbes.2010.08110},
  abstract = {We propose a method for comparing density forecasts that is based on weighted versions of the continuous ranked probability score. The weighting emphasizes regions of interest, such as the tails or the center of a variable's range, while retaining propriety, as opposed to a recently developed weighted likelihood ratio test, which can be hedged. Threshold and quantile based decompositions of the continuous ranked probability score can be illustrated graphically and prompt insights into the strengths and deficiencies of a forecasting method. We illustrate the use of the test and graphical tools in case studies on the Bank of England's density forecasts of quarterly inflation rates in the United Kingdom, and probabilistic predictions of wind resources in the Pacific Northwest.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EWQYSHA7\\Gneiting et Ranjan - 2011 - Comparing Density Forecasts Using Threshold- and Q.pdf}
}

@article{gneiting_making_2009,
  title = {Making and {{Evaluating Point Forecasts}}},
  author = {Gneiting, Tilmann},
  year = {2009},
  month = dec,
  journal = {arXiv:0912.0902 [math, stat]},
  eprint = {0912.0902},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, such as the absolute error or the squared error. The individual scores are then averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the (root) mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CHKBUKFR\\Gneiting - 2009 - Making and Evaluating Point Forecasts.pdf;C\:\\Users\\a846735\\Zotero\\storage\\2YUQK24L\\0912.html}
}

@article{gneiting_probabilistic_2014,
  title = {Probabilistic {{Forecasting}}},
  author = {Gneiting, Tilmann and Katzfuss, Matthias},
  year = {2014},
  month = jan,
  journal = {Annual Review of Statistics and Its Application},
  volume = {1},
  number = {1},
  pages = {125--151},
  issn = {2326-8298},
  doi = {10.1146/annurev-statistics-062713-085831},
  abstract = {A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts. We emphasize methodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NL6TV4DL\\Gneiting et Katzfuss - 2014 - Probabilistic Forecasting.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GM53DIYU\\annurev-statistics-062713-085831.html}
}

@misc{goerigk_algorithm_nodate,
  title = {Algorithm {{Engineering}} in {{Robust Optimization}}},
  author = {Goerigk, Mark and Sch{\"o}bel, Anita},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6JX6UK4B\\1505.04901.pdf}
}

@article{goerigk_ranking_2018,
  title = {Ranking Robustness and Its Application to Evacuation Planning},
  author = {Goerigk, Marc and Hamacher, Horst W. and Kinscherff, Anika},
  year = {2018},
  month = feb,
  journal = {European Journal of Operational Research},
  volume = {264},
  number = {3},
  pages = {837--846},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2016.05.037},
  abstract = {We present a new approach to handle uncertain combinatorial optimization problems that uses solution ranking procedures to determine the degree of robustness of a solution. Unlike classic concepts for robust optimization, our approach is not purely based on absolute quantitative performance, but also includes qualitative aspects that are of major importance for the decision maker. We discuss the two variants, solution ranking and objective ranking robustness, in more detail, presenting problem complexities and solution approaches. Using an uncertain shortest path problem as a computational example, the potential of our approach is demonstrated in the context of evacuation planning due to river flooding.},
  keywords = {Combinatorial optimization,Evacuation planning,Robust optimization,Solution ranking},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\M3XSTTJ7\\Goerigk et al. - 2018 - Ranking robustness and its application to evacuati.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PYHEKM95\\S0377221716303745.html}
}

@article{gong_adaptive_2017,
  title = {An Adaptive Surrogate Modeling-Based Sampling Strategy for Parameter Optimization and Distribution Estimation ({{ASMO-PODE}})},
  author = {Gong, Wei and Duan, Qingyun},
  year = {2017},
  month = sep,
  journal = {Environmental Modelling \& Software},
  volume = {95},
  pages = {61--75},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2017.05.005},
  abstract = {Parameter distribution estimation has long been a hot issue for the uncertainty quantification of environmental models. Traditional approaches such as MCMC (Markov Chain Monte Carlo) are prohibitive to be applied to large complex dynamic models because of the high computational cost of computing resources. To reduce the number of model evaluations required, we proposed an adaptive surrogate modeling-based sampling strategy for parameter distribution estimation, named ASMO-PODE (Adaptive Surrogate Modeling-based Optimization \textendash{} Parameter Optimization and Distribution Estimation). The ASMO-PODE can provide an estimation of the parameter distribution using as little as one percent of the model evaluations required by a regular MCMC approach. The effectiveness and efficiency of the ASMO-PODE approach have been evaluated with 2 test problems and one land surface model, the Common Land Model. The results demonstrated that the ASMO-PODE method is an economic way for parameter optimization and distribution estimation.},
  keywords = {Adaptive sampling,Gaussian processes regression,MCMC,Surrogate model},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XLDRYL8N\\Gong et Duan - 2017 - An adaptive surrogate modeling-based sampling stra.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GLLWW75R\\S1364815216310830.html}
}

@article{gong_multi-objective_2015,
  title = {Multi-Objective Parameter Optimization of Common Land Model Using Adaptive Surrogate Modeling},
  author = {Gong, W. and Duan, Q. and Li, J. and Wang, C. and Di, Z. and Dai, Y. and Ye, A. and Miao, C.},
  year = {2015},
  month = may,
  journal = {Hydrology and Earth System Sciences},
  volume = {19},
  number = {5},
  pages = {2409--2425},
  issn = {1607-7938},
  doi = {10.5194/hess-19-2409-2015},
  abstract = {Parameter specification usually has significant influence on the performance of land surface models (LSMs). However, estimating the parameters properly is a challenging task due to the following reasons: (1) LSMs usually have too many adjustable parameters (20 to 100 or even more), leading to the curse of dimensionality in the parameter input space; (2) LSMs usually have many output variables involving water/energy/carbon cycles, so that calibrating LSMs is actually a multi-objective optimization problem; (3) Regional LSMs are expensive to run, while conventional multi-objective optimization methods need a large number of model runs (typically {$\sim$} 105\textendash 106). It makes parameter optimization computationally prohibitive. An uncertainty quantification framework was developed to meet the aforementioned challenges, which include the following steps: (1) using parameter screening to reduce the number of adjustable parameters, (2) using surrogate models to emulate the responses of dynamic models to the variation of adjustable parameters, (3) using an adaptive strategy to improve the efficiency of surrogate modeling-based optimization; (4) using a weighting function to transfer multi-objective optimization to single-objective optimization. In this study, we demonstrate the uncertainty quantification framework on a single column application of a LSM \textendash{} the Common Land Model (CoLM), and evaluate the effectiveness and efficiency of the proposed framework. The result indicate that this framework can efficiently achieve optimal parameters in a more effective way. Moreover, this result implies the possibility of calibrating other large complex dynamic models, such as regionalscale LSMs, atmospheric models and climate models.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YHYSTU8F\\Gong et al. - 2015 - Multi-objective parameter optimization of common l.pdf}
}

@article{gong_wei_multiobjective_2015,
  title = {Multiobjective Adaptive Surrogate Modeling-based Optimization for Parameter Estimation of Large, Complex Geophysical Models},
  author = {{Gong Wei} and {Duan Qingyun} and {Li Jianduo} and {Wang Chen} and {Di Zhenhua} and {Ye Aizhong} and {Miao Chiyuan} and {Dai Yongjiu}},
  year = {2015},
  month = dec,
  journal = {Water Resources Research},
  volume = {52},
  number = {3},
  pages = {1984--2008},
  issn = {0043-1397},
  doi = {10.1002/2015WR018230},
  abstract = {Abstract Parameter specification is an important source of uncertainty in large, complex geophysical models. These models generally have multiple model outputs that require multiobjective optimization algorithms. Although such algorithms have long been available, they usually require a large number of model runs and are therefore computationally expensive for large, complex dynamic models. In this paper, a multiobjective adaptive surrogate modeling?based optimization (MO?ASMO) algorithm is introduced that aims to reduce computational cost while maintaining optimization effectiveness. Geophysical dynamic models usually have a prior parameterization scheme derived from the physical processes involved, and our goal is to improve all of the objectives by parameter calibration. In this study, we developed a method for directing the search processes toward the region that can improve all of the objectives simultaneously. We tested the MO?ASMO algorithm against NSGA?II and SUMO with 13 test functions and a land surface model ? the Common Land Model (CoLM). The results demonstrated the effectiveness and efficiency of MO?ASMO.},
  keywords = {adaptive sampling,Gaussian processes regression,multiobjective optimization,surrogate model},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\G5IIESLL\\Gong Wei et al. - 2015 - Multiobjective adaptive surrogate modelingâbased o.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RBW94KNS\\2015WR018230.html}
}

@article{gorissen_practical_2015,
  title = {A Practical Guide to Robust Optimization},
  author = {Gorissen, Bram L. and Yan{\i}ko{\u g}lu, {\.I}hsan and {den Hertog}, Dick},
  year = {2015},
  month = jun,
  journal = {Omega},
  volume = {53},
  pages = {124--137},
  issn = {03050483},
  doi = {10.1016/j.omega.2014.12.006},
  abstract = {Robust optimization is a young and active research field that has been mainly developed in the last 15 years. Robust optimization is very useful for practice, since it is tailored to the information at hand, and it leads to computationally tractable formulations. It is therefore remarkable that real-life applications of robust optimization are still lagging behind; there is much more potential for real-life applications than has been exploited hitherto. The aim of this paper is to help practitioners to understand robust optimization and to successfully apply it in practice. We provide a brief introduction to robust optimization, and also describe important do's and don'ts for using it in practice. We use many small examples to illustrate our discussions.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7UD53J55\\Gorissen et al. - 2015 - A practical guide to robust optimization.pdf}
}

@article{gotoh_robust_2018,
  title = {Robust Empirical Optimization Is Almost the Same as Mean\textendash Variance Optimization},
  author = {Gotoh, Jun-ya and Kim, Michael Jong and Lim, Andrew E. B.},
  year = {2018},
  month = jul,
  journal = {Operations Research Letters},
  volume = {46},
  number = {4},
  pages = {448--452},
  issn = {0167-6377},
  doi = {10.1016/j.orl.2018.05.005},
  abstract = {We formulate a distributionally robust optimization problem where the deviation of the alternative distribution is controlled by a {$\phi$}-divergence penalty in the objective, and show that a large class of these problems are essentially equivalent to a mean\textendash variance problem. We also show that while a ``small amount of robustness'' always reduces the in-sample expected reward, the reduction in the variance, which is a measure of sensitivity to model misspecification, is an order of magnitude larger.},
  keywords = {-divergence,Biasâvariance trade-off,Data-driven optimization,Meanâvariance optimization,Regularization,Robust empirical optimization},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\57TMEQYI\\Gotoh et al. - 2018 - Robust empirical optimization is almost the same a.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RHUWPUAR\\S016763771730514X.html}
}

@article{gottwald_combining_2021,
  title = {Combining Machine Learning and Data Assimilation to Forecast Dynamical Systems from Noisy Partial Observations},
  author = {Gottwald, Georg A. and Reich, Sebastian},
  year = {2021},
  month = oct,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {10},
  eprint = {2108.03561},
  eprinttype = {arxiv},
  pages = {101103},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0066080},
  abstract = {We present a supervised learning method to learn the propagator map of a dynamical system from partial and noisy observations. In our computationally cheap and easy-to-implement framework a neural network consisting of random feature maps is trained sequentially by incoming observations within a data assimilation procedure. By employing Takens' embedding theorem, the network is trained on delay coordinates. We show that the combination of random feature maps and data assimilation, called RAFDA, outperforms standard random feature maps for which the dynamics is learned using batch data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Physics - Data Analysis; Statistics and Probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IMS9NMDX\\Gottwald et Reich - 2021 - Combining machine learning and data assimilation t.pdf}
}

@article{gould_differentiating_2016,
  title = {On Differentiating Parameterized Argmin and Argmax Problems with Application to Bi-Level Optimization},
  author = {Gould, Stephen and Fernando, Basura and Cherian, Anoop and Anderson, Peter and Cruz, Rodrigo Santa and Guo, Edison},
  year = {2016},
  journal = {arXiv preprint arXiv:1607.05447},
  eprint = {1607.05447},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5TLGQHHW\\argmin-TR-2016.pdf}
}

@article{gower_tracking_2018,
  title = {Tracking the Gradients Using the {{Hessian}}: {{A}} New Look at Variance Reducing Stochastic Methods},
  shorttitle = {Tracking the Gradients Using the {{Hessian}}},
  author = {Gower, Robert M. and Roux, Nicolas Le and Bach, Francis},
  year = {2018},
  month = mar,
  journal = {arXiv:1710.07462 [cs, math, stat]},
  eprint = {1710.07462},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Our goal is to improve variance reducing stochastic methods through better control variates. We first propose a modification of SVRG which uses the Hessian to track gradients over time, rather than to recondition, increasing the correlation of the control variates and leading to faster theoretical convergence close to the optimum. We then propose accurate and computationally efficient approximations to the Hessian, both using a diagonal and a low-rank matrix. Finally, we demonstrate the effectiveness of our method on a wide range of problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {90C15; 90C25; 68W20,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3XULCQR7\\Gower et al. - 2018 - Tracking the gradients using the Hessian A new lo.pdf}
}

@article{gratton_approximate_2007,
  title = {Approximate {{Gauss}}\textendash{{Newton Methods}} for {{Nonlinear Least Squares Problems}}},
  author = {Gratton, S. and Lawless, A. S. and Nichols, N. K.},
  year = {2007},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {18},
  number = {1},
  pages = {106--132},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/050624935},
  abstract = {The Gauss\textendash Newton algorithm is an iterative method regularly used for solving nonlinear least squares problems. It is particularly well suited to the treatment of very large scale variational data assimilation problems that arise in atmosphere and ocean forecasting. The procedure consists of a sequence of linear least squares approximations to the nonlinear problem, each of which is solved by an ``inner'' direct or iterative process. In comparison with Newton's method and its variants, the algorithm is attractive because it does not require the evaluation of second-order derivatives in the Hessian of the objective function. In practice the exact Gauss\textendash Newton method is too expensive to apply operationally in meteorological forecasting, and various approximations are made in order to reduce computational costs and to solve the problems in real time. Here we investigate the effects on the convergence of the Gauss\textendash Newton method of two types of approximation used commonly in data assimilation. First, we examine ``truncated'' Gauss\textendash Newton methods where the inner linear least squares problem is not solved exactly, and second, we examine ``perturbed'' Gauss\textendash Newton methods where the true linearized inner problem is approximated by a simplified, or perturbed, linear least squares problem. We give conditions ensuring that the truncated and perturbed Gauss\textendash Newton methods converge and also derive rates of convergence for the iterations. The results are illustrated by a simple numerical example. A practical application to the problem of data assimilation in a typical meteorological system is presented.},
  keywords = {65H10,65K10,90C06,90C30,approximate GaussâNewton methods,nonlinear least squares problems,variational data assimilation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\598YQPTQ\\Gratton et al. - 2007 - Approximate GaussâNewton Methods for Nonlinear Lea.pdf}
}

@article{gratton_improved_2018,
  title = {Improved {{Bounds}} for {{Small-Sample Estimation}}},
  author = {Gratton, Serge and {Titley-Peloquin}, David},
  year = {2018},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {39},
  number = {2},
  pages = {922--931},
  publisher = {{Society for Industrial and Applied Mathematics}},
  abstract = {We derive improved error bounds for small-sample statistical estimation of the matrixFrobenius norm. The bounds rigorously establish that small-sample estimators provide reliable order-of-magnitude estimates of norms and condition numbers, for matrices of arbitrary rank, even whenvery few random samples are used.},
  keywords = {Condition number estimation,Norm estimation,Small-sample estimation,Statistical estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2EBQE7LJ\\Gratton et Titley-Peloquin - 2018 - Improved Bounds for Small-Sample Estimation.pdf}
}

@article{gratton_preconditioning_nodate,
  title = {Preconditioning of Conjugate-Gradients in Observation Space for {{4D-VAR}}},
  author = {Gratton, S and Gurol, S and Toint, Ph L},
  pages = {26},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F3TYHLSG\\Gratton et al. - Preconditioning of conjugate-gradients in observat.pdf}
}

@article{gratton_reduced_2011,
  title = {A Reduced and Limited-Memory Preconditioned Approach for the {{4D-Var}} Data-Assimilation Problem: {{Reduced 4D-Var}} Using the {{LMP Preconditioner}}},
  shorttitle = {A Reduced and Limited-Memory Preconditioned Approach for the {{4D-Var}} Data-Assimilation Problem},
  author = {Gratton, S. and Laloyaux, P. and Sartenaer, A. and Tshimanga, J.},
  year = {2011},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {137},
  number = {655},
  pages = {452--466},
  issn = {00359009},
  doi = {10.1002/qj.743},
  abstract = {Data assimilation is a concept involving any method which estimates the initial state of a dynamical system by combining both the information from a numerical model and from observations. The computed estimated initial state of the system can then be integrated in time to obtain a forecast. There are two main ways to solve data assimilation problems. The sequential methods are based on statistical estimation theory and regroup the different Kalman filtering approaches. The variational methods are based on optimal control theory and state nonlinear weighted least-squares problems as the four-dimensional variational (4D-Var) formulation. Approximations of these methods have been defined to make them suitable for solving large-scale data assimilation problems. In the first part of this paper, we present a theoretical work on the equivalence between the Kalman filter and the 4D-Var, that we then generalize to the approximate case, for the SEEK filter and the reduced 4D-Var. We next concentrate on the solution of the 4D-Var which is usually computed with a Gauss-Newton algorithm using a preconditioned conjugate-gradient-like (CG) method. Motivated by the equivalences shown in the first part, we explore in a second part the techniques used in the SEEK filter, which are based on relevant information contained in the empirical orthogonal functions (EOFs), as an attempt to further accelerate the Gauss-Newton method. This leads to the development of an appropriate starting point for the CG method together with that of a powerful limited memory preconditioner (LMP), as shown by preliminary numerical experiments performed on a shallow water model.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Q4F5WW2M\\Gratton et al. - 2011 - A reduced and limited-memory preconditioned approa.pdf}
}

@article{grodzevich_normalization_2006,
  title = {Normalization and Other Topics in Multi-Objective Optimization},
  author = {Grodzevich, Oleg and Romanko, Oleksandr},
  year = {2006},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FHRIYBL5\\Grodzevich et Romanko - 2006 - Normalization and other topics in multi-objective .pdf}
}

@article{groenke_climalign_2020,
  title = {{{ClimAlign}}: {{Unsupervised}} Statistical Downscaling of Climate Variables via Normalizing Flows},
  shorttitle = {{{ClimAlign}}},
  author = {Groenke, Brian and Madaus, Luke and Monteleoni, Claire},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.04679 [cs, stat]},
  eprint = {2008.04679},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Downscaling is a landmark task in climate science and meteorology in which the goal is to use coarse scale, spatio-temporal data to infer values at finer scales. Statistical downscaling aims to approximate this task using statistical patterns gleaned from an existing dataset of downscaled values, often obtained from observations or physical models. In this work, we investigate the application of deep latent variable learning to the task of statistical downscaling. We present ClimAlign, a novel method for unsupervised, generative downscaling using adaptations of recent work in normalizing flows for variational inference. We evaluate the viability of our method using several different metrics on two datasets consisting of daily temperature and precipitation values gridded at low (1 degree latitude/longitude) and high (1/4 and 1/8 degree) resolutions. We show that our method achieves comparable predictive performance to existing supervised statistical downscaling methods while simultaneously allowing for both conditional and unconditional sampling from the joint distribution over high and low resolution spatial fields. We provide publicly accessible implementations of our method, as well as the baselines used for comparison, on GitHub.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.5.4,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\G24XFR7N\\Groenke et al. - 2020 - ClimAlign Unsupervised statistical downscaling of.pdf}
}

@article{groetzner_multiobjective_2022,
  title = {Multiobjective Optimization under Uncertainty: {{A}} Multiobjective Robust (Relative) Regret Approach},
  shorttitle = {Multiobjective Optimization under Uncertainty},
  author = {Groetzner, Patrick and Werner, Ralf},
  year = {2022},
  month = jan,
  journal = {European Journal of Operational Research},
  volume = {296},
  number = {1},
  pages = {101--115},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2021.03.068},
  abstract = {Consider a multiobjective decision problem with uncertainty in the objective functions, given as a set of scenarios. In the single-criterion case, robust optimization methodology helps to identify solutions which remain feasible and of good quality for all possible scenarios. A well-known alternative method in the single-objective case is to compare possible decisions under uncertainty with the optimal decision with the benefit of hindsight, i.e. to minimize the (possibly scaled) regret of not having chosen the optimal decision. In this contribution, we extend the concept of regret from the single-objective case to the multiobjective setting and introduce a proper definition of multivariate (robust) (relative) regret. In contrast to the few existing ideas that mix scalarization and optimization, we clearly separate the modelling of multiobjective (robust) regret from its numerical solution. Moreover, our approach is not limited to a finite uncertainty set or interval uncertainty and furthermore, computations or at least approximations remain tractable in several important special cases. We illustrate all approaches based on a biobjective shortest path problem under uncertainty.},
  langid = {english},
  keywords = {Chebyshev scalarization,Multiobjective optimization,Multivariate robust regret,Polytopal approximation,Robust optimization},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UJB9R7FC\\S0377221721003738.html}
}

@article{grooms_analog_2021,
  title = {Analog Ensemble Data Assimilation and a Method for Constructing Analogs with Variational Autoencoders},
  author = {Grooms, Ian},
  year = {2021},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {147},
  number = {734},
  eprint = {2006.01101},
  eprinttype = {arxiv},
  pages = {139--149},
  issn = {0035-9009, 1477-870X},
  doi = {10.1002/qj.3910},
  abstract = {It is proposed to use analogs of the forecast mean to generate an ensemble of perturbations for use in ensemble optimal interpolation (EnOI) or ensemble variational (EnVar) methods. A new method of constructing analogs using variational autoencoders (VAEs; a machine learning method) is proposed. The resulting analog methods using analogs from a catalog (AnEnOI), and using constructed analogs (cAnEnOI), are tested in the context of a multiscale Lorenz-`96 model, with standard EnOI and an ensemble square root filter for comparison. The use of analogs from a modestly-sized catalog is shown to improve the performance of EnOI, with limited marginal improvements resulting from increases in the catalog size. The method using constructed analogs (cAnEnOI) is found to perform as well as a full ensemble square root filter, and to be robust over a wide range of tuning parameters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BIG3VLQE\\Grooms - 2021 - Analog ensemble data assimilation and a method for.pdf}
}

@article{grubisic_efficient_2007,
  title = {Efficient Rank Reduction of Correlation Matrices},
  author = {Grubi{\v s}i{\'c}, Igor and Pietersz, Raoul},
  year = {2007},
  month = apr,
  journal = {Linear Algebra and its Applications},
  volume = {422},
  number = {2},
  pages = {629--653},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2006.11.024},
  abstract = {Geometric optimisation algorithms are developed that efficiently find the nearest low-rank correlation matrix. We show, in numerical tests, that our methods compare favourably with the existing methods in the literature. The connection with the Lagrange multiplier method is established, along with an identification of whether a local minimum is a global minimum. An additional benefit of the geometric approach is that any weighted norm can be applied. The problem of finding the nearest low-rank correlation matrix occurs as part of the calibration of multi-factor interest rate market models to correlation.},
  langid = {english},
  keywords = {Correlation matrix,Geometric optimisation,LIBOR market model,Rank},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2WH2KB2Y\\GrubiÅ¡iÄ et Pietersz - 2007 - Efficient rank reduction of correlation matrices.pdf}
}

@inproceedings{gu_multi-parametric_2015,
  title = {Multi-Parametric Uncertainty Quantification with a Hybrid {{Monte-Carlo}} / Polynomial Chaos Expansion {{FDTD}} Method},
  booktitle = {2015 {{IEEE MTT-S International Microwave Symposium}}},
  author = {Gu, Zixi and Sarris, C. D.},
  year = {2015},
  month = may,
  pages = {1--3},
  doi = {10.1109/MWSYM.2015.7166881},
  abstract = {The increasing complexity of microwave structures necessitates the quantification and analysis of fabrication process uncertainties that directly impact their performance, as a means of ensuring performance robustness. A particular technique that has emerged as a promising alternative to the time-consuming Monte-Carlo method for such analyses, is the polynomial chaos expansion (PCE). However, PCE-based methods suffer from a rapid increase in their computational cost with the number of random variables included in the analysis. This paper demonstrates that a hybrid Monte-Carlo/PCE technique can lead to the dramatic acceleration of its constituent methods, in the context of FDTD, effectively mitigating the ``curse of dimensionality'' and facilitating the multi-parametric uncertainty analysis of electromagnetic geometries.},
  keywords = {chaos,Computational electromagnetics,electromagnetic geometries,FDTD,FDTD method,finite difference time-domain analysis,finite difference time-domain method,hybrid Monte Carlo-PCE technique,hybrid Monte Carlo-polynomial chaos expansion,microwave materials,microwave structures,Monte Carlo method,Monte Carlo methods,multiparametric uncertainty analysis,multiparametric uncertainty quantification,PCE-based methods,polynomials,statistical analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JWLRK5J5\\7166881.html}
}

@article{gudmundsson_small-sample_1995,
  title = {Small-{{Sample Statistical Estimates}} for {{Matrix Norms}}},
  author = {Gudmundsson, T. and Kenney, C. S. and Laub, A. J.},
  year = {1995},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {16},
  number = {3},
  pages = {17},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, United States}},
  issn = {08954798},
  doi = {10.1137/S0895479893243876},
  abstract = {This paper extends a recent statistically based vector-norm estimator to matrices. The new estimator requires only a few matrix-vector multiplications and can be applied when the matrix is not known explicitly. It is useful for efficiently estimating the sensitivity of vector-valued functions and can be applied to many problems where the power method runs into difficulties. Lower bounds for the probability that an estimate is within a given factor of the correct norm are derived. These bounds are straightforward to compute and show that a very inaccurate estimate is extremely unlikely in most cases. A conservative lower bound has been derived and a tighter bound is given in the form of a conjecture. This conjecture is true in some important special cases and the general case is supported by considerable empirical evidence.},
  copyright = {[Copyright] \textcopyright{} 1995 Society for Industrial and Applied Mathematics},
  langid = {english},
  keywords = {Approximation,Estimates,Mathematics,Norms,Random variables},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DKTCLZ98\\Gudmundsson et al. - 1995 - Small-Sample Statistical Estimates for Matrix Norm.pdf}
}

@article{guhaniyogi_divide-and-conquer_2017,
  title = {A {{Divide-and-Conquer Bayesian Approach}} to {{Large-Scale Kriging}}},
  author = {Guhaniyogi, Rajarshi and Li, Cheng and Savitsky, Terrance D. and Srivastava, Sanvesh},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.09767 [stat]},
  eprint = {1712.09767},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Flexible hierarchical Bayesian modeling of massive data is challenging due to poorly scaling computations in large sample size settings. This article is motivated by spatial process models for analyzing geostatistical data, which typically entail computations that become prohibitive as the number of spatial locations becomes large. We propose a three-step divide-and-conquer strategy within the Bayesian paradigm to achieve massive scalability for any spatial process model. We partition the data into a large number of subsets, apply a readily available Bayesian spatial process model on every subset in parallel, and optimally combine the posterior distributions estimated across all the subsets into a pseudo-posterior distribution that conditions on the entire data. The combined pseudo posterior distribution is used for predicting the responses at arbitrary locations and for performing posterior inference on the model parameters and the residual spatial surface. We call this approach "Distributed Kriging" (DISK). It offers significant advantages in applications where the entire data are or can be stored on multiple machines. Under the standard theoretical setup, we show that if the number of subsets is not too large, then the Bayes risk of estimating the true residual spatial surface using the DISK posterior distribution decays to zero at a nearly optimal rate. While DISK is a general approach to distributed nonparametric regression, we focus on its applications in spatial statistics and demonstrate its empirical performance using a stationary full-rank and a nonstationary low-rank model based on Gaussian process (GP) prior. A variety of simulations and a geostatistical analysis of the Pacific Ocean sea surface temperature data validate our theoretical results.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\95I6757C\\Guhaniyogi et al. - 2017 - A Divide-and-Conquer Bayesian Approach to Large-Sc.pdf;C\:\\Users\\a846735\\Zotero\\storage\\2H2UGTP6\\1712.html}
}

@book{guido_introduction_2016,
  title = {{Introduction to Machine Learning with Python}},
  author = {Guido, Sarah and Mueller, Andreas},
  year = {2016},
  month = jan,
  publisher = {{O{${'}$}Reilly}},
  address = {{Sebastopol, CA}},
  abstract = {any Python developers are curious about what machine learning is and how it can be concretely applied to solve issues faced in businesses handling medium to large amount of data. Machine Learning with Python teaches you the basics of machine learning and provides a thorough hands-on understanding of the subject.},
  isbn = {978-1-4493-6941-5},
  langid = {Anglais},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CQTYK4WI\\Guido et Mueller - 2016 - Introduction to Machine Learning with Python.pdf}
}

@article{guillaume_robust_nodate,
  title = {Robust Parameter Estimation of Density Functions under Fuzzy Interval Observations},
  author = {Guillaume, Romain and Dubois, Didier},
  pages = {12},
  abstract = {This paper deals with the derivation of a probabilistic parametric model from interval or fuzzy data using the maximum likelihood principle. In contrast with classical techniques such as the EM algorithm, that define a precise likelihood function by averaging inside each imprecise observations, our approach presupposes that each imprecise observation underlies a precise one, and that the uncertainty that pervades its observation is epistemic, rather than representing noise. We define an interval-valued likelihood function and apply robust optimisation methods to find a safe plausible estimate of the statistical parameters. The resulting density has a standard deviation that is large enough to cover the imprecision of the observations, making a pessimistic assumption on dispersion. This approach is extended to fuzzy data by optimizing the average of lower likelihoods over a collection of data sets obtained from cuts of the fuzzy intervals, as a trade off between optimistic and pessimistic interpretations of fuzzy data. The principles of this method are compared with those of other existing approaches to handle incompleteness of observations, especially the EM technique.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P4R5KWNI\\Guillaume et Dubois - Robust parameter estimation of density functions u.pdf}
}

@article{gunantara_review_2018,
  title = {A Review of Multi-Objective Optimization: {{Methods}} and Its Applications},
  shorttitle = {A Review of Multi-Objective Optimization},
  author = {Gunantara, Nyoman},
  editor = {Ai, Qingsong},
  year = {2018},
  month = jul,
  journal = {Cogent Engineering},
  volume = {5},
  number = {1},
  issn = {2331-1916},
  doi = {10.1080/23311916.2018.1502242},
  abstract = {Several reviews have been made regarding the methods and application of multi-objective optimization (MOO). There are two methods of MOO that do not require complicated mathematical equations, so the problem becomes simple. These two methods are the Pareto and scalarization. In the Pareto method, there is a dominated solution and a non-dominated solution obtained by a continuously updated algorithm. Meanwhile, the scalarization method creates multi-objective functions made into a single solution using weights. There are three types of weights in scalarization which are equal weights, rank order centroid weights, and rank-sum weights. Next, the solution using the Pareto method is a performance indicators component that forms MOO a separate and produces a compromise solution and can be displayed in the form of Pareto optimal front, while the solution using the scalarization method is a performance indicators component that forms a scalar function which is incorporated in the fitness function.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GDAZ9BPL\\Gunantara - 2018 - A review of multi-objective optimization Methods .pdf}
}

@article{gupta_measure_nodate,
  title = {A {{Measure Theory Tutorial}} ({{Measure Theory}} for {{Dummies}})},
  author = {Gupta, Maya R},
  pages = {7},
  abstract = {This tutorial is an informal introduction to measure theory for people who are interested in reading papers that use measure theory. The tutorial assumes one has had at least a year of college-level calculus, some graduate level exposure to random processes, and familiarity with terms like ``closed'' and ``open.'' The focus is on the terms and ideas relevant to applied probability and information theory. There are no proofs and no exercises.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CJIR46KK\\Gupta - A Measure Theory Tutorial (Measure Theory for Dumm.pdf}
}

@incollection{gupta_unified_2016,
  title = {A {{Unified Framework}} for {{Optimization Under Uncertainty}}},
  booktitle = {Optimization {{Challenges}} in {{Complex}}, {{Networked}} and {{Risky Systems}}},
  author = {Powell, Warren B.},
  editor = {Gupta, Aparna and Capponi, Agostino and Smith, J. Cole and Greenberg, Harvey J.},
  year = {2016},
  month = oct,
  pages = {45--83},
  publisher = {{INFORMS}},
  doi = {10.1287/educ.2016.0149},
  isbn = {978-0-9843378-9-7},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B72JCMMX\\Powell - 2016 - A Unified Framework for Optimization Under Uncerta.pdf}
}

@article{gurol_b_2014,
  title = {{\textbf{B}} -Preconditioned Minimization Algorithms for Variational Data Assimilation with the Dual Formulation: {{{\textbf{B}}}} -Preconditioned Minimization Algorithms},
  shorttitle = {{\textbf{B}} -Preconditioned Minimization Algorithms for Variational Data Assimilation with the Dual Formulation},
  author = {G{\"u}rol, S. and Weaver, A. T. and Moore, A. M. and Piacentini, A. and Arango, H. G. and Gratton, S.},
  year = {2014},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {140},
  number = {679},
  pages = {539--556},
  issn = {00359009},
  doi = {10.1002/qj.2150},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\I7XJSBNZ\\GÃ¼rol et al. - 2014 - B -preconditioned minimization algorithms f.pdf}
}

@article{haben_conditioning_2011,
  title = {Conditioning and Preconditioning of the Variational Data Assimilation Problem},
  author = {Haben, S.A. and Lawless, A.S. and Nichols, N.K.},
  year = {2011},
  month = jul,
  journal = {Computers \& Fluids},
  volume = {46},
  number = {1},
  pages = {252--256},
  issn = {00457930},
  doi = {10.1016/j.compfluid.2010.11.025},
  abstract = {Numerical weather prediction (NWP) centres use numerical models of the atmospheric flow to forecast future weather states from an estimate of the current state. Variational data assimilation (VAR) is used commonly to determine an optimal state estimate that miminizes the errors between observations of the dynamical system and model predictions of the flow. The rate of convergence of the VAR scheme and the sensitivity of the solution to errors are dependent on the condition number of the Hessian of the variational least-squares objective function. The traditional formulation of VAR is illconditioned and hence leads to slow convergence and an inaccurate solution. In practice, operational NWP centres precondition the system via a control variable transform to reduce the condition number of the Hessian. In this paper we investigate the conditioning of VAR for a single, periodic, spatiallydistributed state variable. We present theoretical bounds on the condition number of the original and preconditioned Hessians and hence establish the improvement produced by the preconditioning. We also investigate theoretically the effect of observation position and error variance on the preconditioned system and show that the problem becomes more ill-conditioned with increasingly dense and accurate observations. Finally, we confirm the theoretical results in an operational setting by giving experimental results from the Met Office variational system.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GWGDM4KN\\Haben et al. - 2011 - Conditioning and preconditioning of the variationa.pdf}
}

@article{habibi_exact_2011,
  title = {Exact Distribution of Argmax (Argmin)},
  author = {Habibi, Reza},
  year = {2011},
  journal = {Economic Quality Control},
  volume = {26},
  number = {2},
  pages = {155--162},
  keywords = {argmax distribution},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FCTZLU2W\\EQC.2011.015.pdf}
}

@article{hafner_fast_nodate,
  title = {Fast, Cheap, \& Turbulent \textemdash{} {{Global}} Ocean Modelling with {{GPU}} Acceleration in {{Python}}},
  author = {H{\"a}fner, Dion and Nuterman, Roman and Jochum, Markus},
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {n/a},
  number = {n/a},
  pages = {e2021MS002717},
  issn = {1942-2466},
  doi = {10.1029/2021MS002717},
  abstract = {Even to this date, most earth system models are coded in Fortran, especially those used at the largest compute scales. Our ocean model Veros takes a different approach: it is implemented using the high-level programming language Python. Besides numerous usability advantages, this allows us to leverage modern high-performance frameworks that emerged in tandem with the machine learning boom. By interfacing with the JAX library, Veros is able to run high-performance simulations on both CPU and GPU through the same code base, with full support for distributed architectures. On CPU, Veros is able to match the performance of a Fortran reference, both on a single process and on hundreds of CPU cores. On GPU, we find that each device can replace dozens to hundreds of CPU cores, at a fraction of the energy consumption. We demonstrate the viability of using GPUs for earth system modelling by integrating a global 0.1{$\smwhtcircle$} eddy-resolving setup in single precision, where we achieve 1.3 model years per day on a single compute instance with 16 GPUs, comparable to over 2000 Fortran processes.},
  langid = {english},
  keywords = {Benchmarks,GPU,High-performance,Ocean modelling,Python},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002717},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\INCIEYFI\\HÃ¤fner et al. - Fast, cheap, & turbulent â Global ocean modelling .pdf;C\:\\Users\\a846735\\Zotero\\storage\\8HDH3FF4\\2021MS002717.html}
}

@misc{halko_finding_2010,
  title = {Finding Structure with Randomness: {{Probabilistic}} Algorithms for Constructing Approximate Matrix Decompositions},
  shorttitle = {Finding Structure with Randomness},
  author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
  year = {2010},
  month = dec,
  number = {arXiv:0909.4061},
  eprint = {0909.4061},
  eprinttype = {arxiv},
  primaryclass = {math},
  publisher = {{arXiv}},
  abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RH5YKPTY\\0909.4061.pdf}
}

@article{halpern_weighted_2013,
  title = {Weighted Regret-Based Likelihood: A New Approach to Describing Uncertainty},
  shorttitle = {Weighted Regret-Based Likelihood},
  author = {Halpern, Joseph Y.},
  year = {2013},
  month = sep,
  journal = {arXiv:1309.1228 [cs]},
  eprint = {1309.1228},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, Halpern and Leung suggested representing uncertainty by a weighted set of probability measures, and suggested a way of making decisions based on this representation of uncertainty: maximizing weighted regret. Their paper does not answer an apparently simpler question: what it means, according to this representation of uncertainty, for an event E to be more likely than an event E'. In this paper, a notion of comparative likelihood when uncertainty is represented by a weighted set of probability measures is defined. It generalizes the ordering defined by probability (and by lower probability) in a natural way; a generalization of upper probability can also be defined. A complete axiomatic characterization of this notion of regret-based likelihood is given.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VQQ9XGJC\\Halpern - 2013 - Weighted regret-based likelihood a new approach t.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PKZQNW3C\\1309.html}
}

@article{hamill_ensemble-based_nodate,
  title = {Ensemble-{{Based Data Assimilation}}},
  author = {Hamill, Thomas M},
  pages = {30},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MSXRLF22\\Hamill - Ensemble-Based Data Assimilation.pdf}
}

@article{han_simultaneous_2009,
  title = {Simultaneous {{Determination}} of {{Tuning}} and {{Calibration Parameters}} for {{Computer Experiments}}},
  author = {Han, Gang and Santner, Thomas J. and Rawlinson, Jeremy J.},
  year = {2009},
  month = nov,
  journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
  volume = {51},
  number = {4},
  pages = {464--474},
  issn = {0040-1706},
  doi = {10.1198/TECH.2009.08126},
  abstract = {Tuning and calibration are processes for improving the representativeness of a computer simulation code to a physical phenomenon. This article introduces a statistical methodology for simultaneously determining tuning and calibration parameters in settings where data are available from a computer code and the associated physical experiment. Tuning parameters are set by minimizing a discrepancy measure while the distribution of the calibration parameters are determined based on a hierarchical Bayesian model. The proposed Bayesian model views the output as a realization of a Gaussian stochastic process with hyperpriors. Draws from the resulting posterior distribution are obtained by the Markov chain Monte Carlo simulation. Our methodology is compared with an alternative approach in examples and is illustrated in a biomechanical engineering application. Supplemental materials, including the software and a user manual, are available online and can be requested from the first author.},
  pmcid = {PMC2879656},
  pmid = {20523754},
  keywords = {Calibration},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WHDGXHCE\\Han et al. - 2009 - Simultaneous Determination of Tuning and Calibrati.pdf}
}

@techreport{hanson_markov_2001,
  title = {Markov {{Chain Monte Carlo}} Posterior Sampling with the {{Hamiltonian Method}}},
  author = {Hanson, K.},
  year = {2001},
  month = feb,
  number = {LA-UR-01-1016},
  institution = {{Los Alamos National Lab., NM (US)}},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  langid = {english},
  keywords = {Hamiltonian MCMC,MCMC},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H3Y9Y8VD\\Hanson - 2001 - MARKOV CHAIN MONTE CARLO POSTERIOR SAMPLING WITH T.pdf;C\:\\Users\\a846735\\Zotero\\storage\\AMUBRP74\\775292.html}
}

@article{harris_array_2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2649-2},
  abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TW7H46IB\\Harris et al. - 2020 - Array programming with NumPy.pdf;C\:\\Users\\a846735\\Zotero\\storage\\HZRF6DPP\\s41586-020-2649-2.html}
}

@misc{hartnett_what_nodate,
  title = {What {{Makes}} the {{Hardest Equations}} in {{Physics So Difficult}}?},
  author = {Hartnett, ByKevin},
  journal = {Quanta Magazine},
  abstract = {The Navier-Stokes equations describe simple, everyday phenomena, like water flowing from a garden hose, yet they provide a million-dollar mathematical challenge},
  howpublished = {https://www.quantamagazine.org/what-makes-the-hardest-equations-in-physics-so-difficult-20180116/},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XJ89CPQF\\what-makes-the-hardest-equations-in-physics-so-difficult-20180116.html}
}

@article{hascoet_automatic_nodate,
  title = {Automatic {{Differentiation}} with {{Tapenade}}},
  author = {Hasco{\"e}t, Laurent},
  pages = {38},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XZXZ2FYB\\HascoÃ«t - Automatic Differentiation with Tapenade.pdf}
}

@article{hascoet_tapenade_2013,
  title = {The {{Tapenade}} Automatic Differentiation Tool: {{Principles}}, Model, and Specification},
  shorttitle = {The {{Tapenade}} Automatic Differentiation Tool},
  author = {Hascoet, Laurent and Pascual, Val{\'e}rie},
  year = {2013},
  month = apr,
  journal = {ACM Transactions on Mathematical Software},
  volume = {39},
  number = {3},
  pages = {1--43},
  issn = {0098-3500, 1557-7295},
  doi = {10.1145/2450153.2450158},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\85N3W2TF\\Hascoet et Pascual - 2013 - The Tapenade automatic differentiation tool Princ.pdf}
}

@article{hatfield_building_2021,
  title = {Building {{Tangent-Linear}} and {{Adjoint Models}} for {{Data Assimilation With Neural Networks}}},
  author = {Hatfield, Sam and Chantry, Matthew and Dueben, Peter and Lopez, Philippe and Geer, Alan and Palmer, Tim},
  year = {2021},
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {13},
  number = {9},
  pages = {e2021MS002521},
  issn = {1942-2466},
  doi = {10.1029/2021MS002521},
  abstract = {We assess the ability of neural network emulators of physical parametrization schemes in numerical weather prediction models to aid in the construction of linearized models required by four-dimensional variational (4D-Var) data assimilation. Neural networks can be differentiated trivially, and so if a physical parametrization scheme can be accurately emulated by a neural network then its tangent-linear and adjoint versions can be obtained with minimal effort, compared with the standard paradigms of manual or automatic differentiation of the model code. Here we apply this idea by emulating the non-orographic gravity wave drag parametrization scheme in an atmospheric model with a neural network, and deriving its tangent-linear and adjoint models. We demonstrate that these neural network-derived tangent-linear and adjoint models not only pass the standard consistency tests but also can be used successfully to do 4D-Var data assimilation. This technique holds the promise of significantly easing maintenance of tangent-linear and adjoint codes in weather forecasting centers, if accurate neural network emulators can be constructed.},
  langid = {english},
  keywords = {adjoint,data assimilation,neural network,tangent-linear},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002521},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GH2LCEVC\\Hatfield et al. - 2021 - Building Tangent-Linear and Adjoint Models for Dat.pdf;C\:\\Users\\a846735\\Zotero\\storage\\5N4TIKPL\\2021MS002521.html}
}

@article{hatfield_building_2021-1,
  title = {Building {{Tangent-Linear}} and {{Adjoint Models}} for {{Data Assimilation With Neural Networks}}},
  author = {Hatfield, Sam and Chantry, Matthew and Dueben, Peter and Lopez, Philippe and Geer, Alan and Palmer, Tim},
  year = {2021},
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {13},
  number = {9},
  pages = {e2021MS002521},
  issn = {1942-2466},
  doi = {10.1029/2021MS002521},
  abstract = {We assess the ability of neural network emulators of physical parametrization schemes in numerical weather prediction models to aid in the construction of linearized models required by four-dimensional variational (4D-Var) data assimilation. Neural networks can be differentiated trivially, and so if a physical parametrization scheme can be accurately emulated by a neural network then its tangent-linear and adjoint versions can be obtained with minimal effort, compared with the standard paradigms of manual or automatic differentiation of the model code. Here we apply this idea by emulating the non-orographic gravity wave drag parametrization scheme in an atmospheric model with a neural network, and deriving its tangent-linear and adjoint models. We demonstrate that these neural network-derived tangent-linear and adjoint models not only pass the standard consistency tests but also can be used successfully to do 4D-Var data assimilation. This technique holds the promise of significantly easing maintenance of tangent-linear and adjoint codes in weather forecasting centers, if accurate neural network emulators can be constructed.},
  langid = {english},
  keywords = {adjoint,data assimilation,neural network,tangent-linear},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002521},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WQJUUYFS\\Hatfield et al. - 2021 - Building Tangent-Linear and Adjoint Models for Dat.pdf;C\:\\Users\\a846735\\Zotero\\storage\\93TLA2SL\\2021MS002521.html}
}

@article{hatfield_single-precision_2020,
  title = {Single-{{Precision}} in the {{Tangent-Linear}} and {{Adjoint Models}} of {{Incremental 4D-Var}}},
  author = {Hatfield, Sam and McRae, Andrew and Palmer, Tim and D{\"u}ben, Peter},
  year = {2020},
  month = apr,
  journal = {Monthly Weather Review},
  volume = {148},
  number = {4},
  pages = {1541--1552},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-19-0291.1},
  abstract = {Abstract The use of single-precision arithmetic in ECMWF's forecasting model gave a 40\% reduction in wall-clock time over double-precision, with no decrease in forecast quality. However, using reduced-precision in 4D-Var data assimilation is relatively unexplored and there are potential issues with using single-precision in the tangent-linear and adjoint models. Here, we present the results of reducing numerical precision in an incremental 4D-Var data assimilation scheme, with an underlying two-layer quasigeostrophic model. The minimizer used is the conjugate gradient method. We show how reducing precision increases the asymmetry between the tangent-linear and adjoint models. For ill-conditioned problems, this leads to a loss of orthogonality among the residuals of the conjugate gradient algorithm, which slows the convergence of the minimization procedure. However, we also show that a standard technique, reorthogonalization, eliminates these issues and therefore could allow the use of single-precision arithmetic. This work is carried out within ECMWF's data assimilation framework, the Object Oriented Prediction System.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZYVU8HKQ\\Hatfield et al. - 2020 - Single-Precision in the Tangent-Linear and Adjoint.pdf}
}

@article{hayya_note_1975,
  title = {A {{Note}} on the {{Ratio}} of {{Two Normally Distributed Variables}}},
  author = {Hayya, Jack and Armstrong, Donald and Gressis, Nicolas},
  year = {1975},
  month = jul,
  journal = {Management Science},
  volume = {21},
  number = {11},
  pages = {1338--1341},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.21.11.1338},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8VU8U4M8\\Hayya et al. - 1975 - A Note on the Ratio of Two Normally Distributed Va.pdf}
}

@inproceedings{healy_retrospective_1991,
  title = {Retrospective Simulation Response Optimization},
  booktitle = {1991 {{Winter Simulation Conference Proceedings}}.},
  author = {Healy, K. and Schruben, L.W.},
  year = {1991},
  pages = {901--906},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}},
  doi = {10.1109/WSC.1991.185703},
  abstract = {An approach to simulation response optimization is presented where a simulation experiment is run in such a manner as to generate optimal solutions. Once the stochastic sample path of a simulation run has been generated, it is sometimes possible to retrospectively solve a deterministic optimization problem or a closely related problem. As demonstrated with some examples from communications and manufacturing, this approach can greatly simplify both the simulation experiment and the simulation model.},
  isbn = {978-0-7803-0181-8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P97JS8Y7\\Healy et Schruben - 1991 - Retrospective simulation response optimization.pdf}
}

@article{heinrich_level_2012,
  title = {Level Sets Estimation and {{Vorob}}'ev Expectation of Random Compact Sets},
  author = {Heinrich, Philippe and Stoica, Radu S. and Tran, Viet Chi},
  year = {2012},
  month = dec,
  journal = {Spatial Statistics},
  volume = {2},
  pages = {47--61},
  issn = {22116753},
  doi = {10.1016/j.spasta.2012.10.001},
  abstract = {The issue of a ``mean shape'' of a random set X often arises, in particular in image analysis and pattern detection. There is no canonical definition but one possible approach is the so-called Vorob'ev expectation EV (X), which is closely linked to quantile sets. In this paper, we propose a consistent and ready to use estimator of EV (X) built from independent copies of X with spatial discretization. The control of discretization errors is handled with a mild regularity assumption on the boundary of X: a not too large `box counting' dimension. Some examples are developed and an application to cosmological data is presented.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2E7UAPPX\\Heinrich et al. - 2012 - Level sets estimation and Vorobâev expectation of .pdf}
}

@article{hennig_entropy_2011,
  title = {Entropy {{Search}} for {{Information-Efficient Global Optimization}}},
  author = {Hennig, Philipp and Schuler, Christian J.},
  year = {2011},
  month = dec,
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W9KFZ9I4\\Hennig et Schuler - 2011 - Entropy Search for Information-Efficient Global Op.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XTZ68DPE\\1112.html}
}

@article{hennig_fast_nodate,
  title = {Fast {{Probabilistic Optimization}} from {{Noisy Gradients}}},
  author = {Hennig, Philipp},
  pages = {9},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IZHCITWD\\Hennig - Fast Probabilistic Optimization from Noisy Gradien.pdf}
}

@article{hernandez-lobato_predictive_2014,
  title = {Predictive {{Entropy Search}} for {{Efficient Global Optimization}} of {{Black-box Functions}}},
  author = {{Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
  year = {2014},
  month = jun,
  journal = {arXiv:1406.2541 [cs, stat]},
  eprint = {1406.2541},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and realworld applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FF7YVTEY\\HernÃ¡ndez-Lobato et al. - 2014 - Predictive Entropy Search for Efficient Global Opt.pdf}
}

@article{hewston_quantifying_2010,
  title = {Quantifying Uncertainty in Tide, Surge and Wave Modelling during Extreme Storms},
  author = {Hewston, R and Chen, Yanxian and Pan, Shunqi and Zou, Qingping and Reeve, Dominic and Cluckie, Ian},
  year = {2010},
  month = sep,
  doi = {10.7558/bhs.2010.ic74},
  abstract = {Interactions between meteorological and hydrodynamic processes are poorly understood, and may result in large uncertainties when assessing the performance of sea defences in extreme conditions. This study integrates numerical weather prediction models with models of wave generation and propagation, and surge and tide propagation. By using an ensemble methodology, the uncertainty at each stage of the model cascade may be quantified. Subsequently, this information, either as a proxy or appropriately transformed into predictive uncertainty, will be valuable in calculating the likelihood of hydraulic and structural failure in extreme storms. This paper describes results for a domain centred on one of the locations for the proposed Severn Barrage. This barrage will be the focus for the world's largest marine renewable energy scheme and will potentially have a significant impact on the coastal flooding response of this part of the Severn Estuary. Dynamically downscaled, high resolution wind and pressure fields of historic extreme storms are generated using the Weather Research and Forecasting (WRF) modelling system. The state of the art tide and surge model, POLCOMS, in conjunction with a third generation wave model (ProWAM), utilises the meteorological data, producing hydrodynamic parameters such as surge and wave heights at a proposed location for the Severn Barrage. European Centre for Medium range Forecasting (ECMWF) Ensemble Prediction System data are used for boundary conditions in WRF, producing a 50-member ensemble. The variation in storm track and intensity between members allows the uncertainty in the model system to be quantified in terms of wave and surge heights. This work is part of the NERC funded EPIRUS consortium research but is closely allied to the interests of the EPSRC FRMRC project and the HEPEX international network focused on ensemble prediction in the context of hydrological prediction systems.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TCS9SVPG\\Hewston et al. - 2010 - Quantifying uncertainty in tide, surge and wave mo.pdf}
}

@article{higdon_combining_2004,
  title = {Combining Field Data and Computer Simulations for Calibration and Prediction},
  author = {Higdon, Dave and Kennedy, Marc and Cavendish, James C. and Cafeo, John A. and Ryne, Robert D.},
  year = {2004},
  journal = {SIAM Journal on Scientific Computing},
  volume = {26},
  number = {2},
  pages = {448--466},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FTF7QNLB\\Higdon et al. - 2004 - Combining field data and computer simulations for .pdf;C\:\\Users\\a846735\\Zotero\\storage\\SF5KN75N\\S1064827503426693.html}
}

@article{higdon_computer_2008,
  title = {Computer Model Calibration Using High-Dimensional Output},
  author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
  year = {2008},
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {482},
  pages = {570--583},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\J724KP8I\\Higdon et al. - 2008 - Computer model calibration using high-dimensional .pdf;C\:\\Users\\a846735\\Zotero\\storage\\IWJUKBKK\\016214507000000888.html}
}

@article{higdon_posterior_2011,
  title = {Posterior Exploration for Computationally Intensive Forward Models},
  author = {Higdon, David and Reese, C. Shane and Moulton, J. David and Vrugt, Jasper A. and Fox, Colin},
  year = {2011},
  journal = {Handbook of Markov Chain Monte Carlo},
  pages = {401--418},
  keywords = {MCMC},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SBUQHLQH\\Higdon et al. - 2011 - Posterior exploration for computationally intensiv.pdf;C\:\\Users\\a846735\\Zotero\\storage\\9GARKPTV\\books.html}
}

@incollection{hiot_kriging_2010,
  title = {Kriging {{Is Well-Suited}} to {{Parallelize Optimization}}},
  booktitle = {Computational {{Intelligence}} in {{Expensive Optimization Problems}}},
  author = {Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  editor = {Hiot, Lim Meng and Ong, Yew Soon and Tenne, Yoel and Goh, Chi-Keong},
  year = {2010},
  volume = {2},
  pages = {131--162},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-10701-6_6},
  abstract = {The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement (q-EI), aimed at choosing several points at the same time. An analytical expression of the q-EI is given when q = 2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the q-EI, and apply them to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q {$\in$} [1, 10]).},
  isbn = {978-3-642-10700-9 978-3-642-10701-6},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ENBVB5QF\\Ginsbourger et al. - 2010 - Kriging Is Well-Suited to Parallelize Optimization.pdf}
}

@article{hoffman_stochastic_2012,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2012},
  month = jun,
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\M2SKEP2E\\Hoffman et al. - 2012 - Stochastic Variational Inference.pdf;C\:\\Users\\a846735\\Zotero\\storage\\8CAJZT4Q\\1206.html}
}

@article{hoffmann_unified_2015,
  title = {Unified Treatment of the Asymptotics of Asymmetric Kernel Density Estimators},
  author = {Hoffmann, Till and Jones, Nick S.},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03188 [math, stat]},
  eprint = {1512.03188},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We extend balloon and sample-smoothing estimators, two types of variable-bandwidth kernel density estimators, by a shift parameter and derive their asymptotic properties. Our approach facilitates the unified study of a wide range of density estimators which are subsumed under these two general classes of kernel density estimators. We demonstrate our method by deriving the asymptotic bias, variance, and mean (integrated) squared error of density estimators with gamma, log-normal, Birnbaum-Saunders, inverse Gaussian and reciprocal inverse Gaussian kernels. We propose two new density estimators for positive random variables that yield properly-normalised density estimates. Plugin expressions for bandwidth estimation are provided to facilitate easy exploratory data analysis.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LJUJKCBD\\Hoffmann et Jones - 2015 - Unified treatment of the asymptotics of asymmetric.pdf;C\:\\Users\\a846735\\Zotero\\storage\\84RUCHCQ\\1512.html}
}

@article{hogan_point--set_1973,
  title = {Point-to-{{Set Maps}} in {{Mathematical Programming}}},
  author = {Hogan, William W.},
  year = {1973},
  journal = {SIAM Review},
  volume = {15},
  number = {3},
  pages = {591--603},
  issn = {0036-1445},
  abstract = {Properties of point-to-set maps are studied from an elementary viewpoint oriented toward applications in mathematical programming. A number of different definitions and results are compared and integrated. Conditions establishing continuity of extremal value functions and properties of maps determined by inequalities are included.}
}

@article{holmes_assigning_2017,
  title = {Assigning a Value to a Power Likelihood in a General {{Bayesian}} Model},
  author = {Holmes, Chris and Walker, Stephen},
  year = {2017},
  month = jan,
  journal = {arXiv:1701.08515 [stat]},
  eprint = {1701.08515},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Bayesian approaches to data analysis and machine learning are widespread and popular as they provide intuitive yet rigorous axioms for learning from data; see Bernardo \& Smith (2004) and Bishop (2006). However, this rigour comes with a caveat, that the Bayesian model is a precise reflection of Nature. There has been a recent trend to address potential model misspecification by raising the likelihood function to a power, primarily for robustness reasons, though not exclusively. In this paper we provide a coherent specification of the power parameter once the Bayesian model has been specified in the absence of a perfect model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\23ETEW6R\\Holmes et Walker - 2017 - Assigning a value to a power likelihood in a gener.pdf}
}

@article{holt_prospects_2017,
  title = {Prospects for Improving the Representation of Coastal and Shelf Seas in Global Ocean Models},
  author = {Holt, Jason and Hyder, Patrick and Ashworth, Mike and Harle, James and Hewitt, Helene and Liu, Hedong and New, Adrian and Pickles, Stephen and Porter, Andrew and Popova, Ekaterina and Allen, Icarus and Siddorn, John and Wood, Richard},
  year = {2017},
  month = feb,
  journal = {Geoscientific Model Development},
  volume = {10},
  pages = {499--523},
  doi = {10.5194/gmd-10-499-2017},
  abstract = {Accurately representing coastal and shelf seas in global ocean models represents one of the grand challenges of Earth system science. They are regions of immense societal importance through the goods and services they provide, hazards they pose and their role in global-scale processes and cycles, e.g. carbon fluxes and dense water formation. However, they are poorly represented in the current generation of global ocean models. In this contribution, we aim to briefly characterise the problem, and then to identify the important physical processes, and their scales, needed to address this issue in the context of the options available to resolve these scales globally and the evolving computational landscape. We find barotropic and topographic scales are well resolved by the current state-of-the-art model resolutions, e.g. nominal 1/12\textdegree, and still reasonably well resolved at 1/4\textdegree; here, the focus is on process representation. We identify tides, vertical coordinates, river inflows and mixing schemes as four areas where modelling approaches can readily be transferred from regional to global modelling with substantial benefit. In terms of finer-scale processes, we find that a 1/12\textdegree{} global model resolves the first baroclinic Rossby radius for only {$\sim$} 8 \% of regions {$<$} 500 m deep, but this increases to {$\sim$} 70 \% for a 1/72\textdegree{} model, so resolving scales globally requires substantially finer resolution than the current state of the art. We quantify the benefit of improved resolution and process representation using 1/12\textdegree{} global- and basin-scale northern North Atlantic nucleus for a European model of the ocean (NEMO) simulations; the latter includes tides and a k-{$\epsilon$} vertical mixing scheme. These are compared with global stratification observations and 19 models from CMIP5. In terms of correlation and basin-wide rms error, the high-resolution models outperform all these CMIP5 models. The model with tides shows improved seasonal cycles compared to the high-resolution model without tides. The benefits of resolution are particularly apparent in eastern boundary upwelling zones. To explore the balance between the size of a globally refined model and that of multiscale modelling options (e.g. finite element, finite volume or a two-way nesting approach), we consider a simple scale analysis and a conceptual grid refining approach. We put this analysis in the context of evolving computer systems, discussing model turnaround time, scalability and resource costs. Using a simple cost model compared to a reference configuration (taken to be a 1/4\textdegree{} global model in 2011) and the increasing performance of the UK Research Councils' computer facility, we estimate an unstructured mesh multiscale approach, resolving process scales down to 1.5 km, would use a comparable share of the computer resource by 2021, the two-way nested multiscale approach by 2022, and a 1/72\textdegree{} global model by 2026. However, we also note that a 1/12\textdegree{} global model would not have a comparable computational cost to a 1\textdegree{} global model in 2017 until 2027. Hence, we conclude that for computationally expensive models (e.g. for oceanographic research or operational oceanography), resolving scales to {$\sim$} 1.5 km would be routinely practical in about a decade given substantial effort on numerical and computational development. For complex Earth system models, this extends to about 2 decades, suggesting the focus here needs to be on improved process parameterisation to meet these challenges.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MPLA8FYC\\Holt et al. - 2017 - Prospects for improving the representation of coas.pdf}
}

@article{homan_output-space_nodate,
  title = {Output-{{Space Predictive Entropy Search}} for {{Flexible Global Optimization}}},
  author = {Hoffman, Matthew W and Ghahramani, Zoubin},
  pages = {5},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IGLU6SSE\\Hoï¬man et Ghahramani - Output-Space Predictive Entropy Search for Flexibl.pdf}
}

@article{homescu_adjoints_2011,
  title = {Adjoints and {{Automatic}} ({{Algorithmic}}) {{Differentiation}} in {{Computational Finance}}},
  author = {Homescu, Cristian},
  year = {2011},
  month = jul,
  journal = {arXiv:1107.1831 [q-fin]},
  eprint = {1107.1831},
  eprinttype = {arxiv},
  primaryclass = {q-fin},
  abstract = {Two of the most important areas in computational finance: Greeks and, respectively, calibration, are based on efficient and accurate computation of a large number of sensitivities. This paper gives an overview of adjoint and automatic differentiation (AD), also known as algorithmic differentiation, techniques to calculate these sensitivities. When compared to finite difference approximation, this approach can potentially reduce the computational cost by several orders of magnitude, with sensitivities accurate up to machine precision. Examples and a literature survey are also provided.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Finance - Computational Finance},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5H4LWPRB\\Homescu - 2011 - Adjoints and Automatic (Algorithmic) Differentiati.pdf}
}

@article{honnorat_identification_2010,
  title = {Identification of Equivalent Topography in an Open Channel Flow Using {{Lagrangian}} Data Assimilation},
  author = {Honnorat, Marc and Monnier, J{\'e}r{\^o}me and Rivi{\`e}re, Nicolas and Huot, {\'E}tienne and Le Dimet, Fran{\c c}ois-Xavier},
  year = {2010},
  month = mar,
  journal = {Computing and Visualization in Science},
  volume = {13},
  number = {3},
  pages = {111--119},
  issn = {1432-9360, 1433-0369},
  doi = {10.1007/s00791-009-0130-8},
  abstract = {We present a Lagrangian data assimilation experiment in an open channel flow above a broad-crested weir. The observations consist of trajectories of particles transported by the flow and extracted from a video film, in addition to classical water level measurements. However, the presence of vertical recirculations on both sides of the weir actually conducts to the identification of an equivalent topography corresponding to the lower limit of a surface jet. In addition, results on the identification of the Manning coefficient may allow to detect the presence of bottom reciruclations.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QBA5CTZ4\\Honnorat et al. - 2010 - Identification of equivalent topography in an open.pdf}
}

@article{hourdin_art_2017,
  title = {The Art and Science of Climate Model Tuning},
  author = {Hourdin, Frederic and Mauritsen, Thorsten and Gettelman, Andrew and Golaz, Jean-Christophe and Balaji, Venkatramani and Duan, Qingyun and Folini, Doris and Ji, Duoying and Klocke, Daniel and Qian, Yun},
  year = {2017},
  journal = {Bulletin of the American Meteorological Society},
  volume = {98},
  number = {3},
  pages = {589--602},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WM7J3PJU\\Tuning2016.pdf}
}

@article{hu_robust_2011,
  title = {Robust {{Design}} of {{Horizontal Axis Wind Turbines Using Taguchi Method}}},
  author = {Hu, Yi and Rao, Singiresu},
  year = {2011},
  month = nov,
  journal = {Journal of Mechanical Design},
  volume = {133},
  pages = {111009},
  doi = {10.1115/1.4004989},
  abstract = {The robust design of horizontal axis wind turbines, including both parameter design and tolerance design, is presented. A simple way of designing robust horizontal axis wind turbine systems under realistic conditions is outlined with multiple design parameters (variables), multiple objectives, and multiple constraints simultaneously by using the traditional Taguchi method and its extensions. The performance of the turbines is predicted using the axial momentum theory and the blade element momentum theory. In the parameter design stage, the energy output of the turbine is maximized using the Taguchi method and an extended penalty-based Taguchi method is proposed to solve constrained parameter design problems. The results of the unconstrained and constrained parameter design problems, in terms of the objective function and constraints are compared. Using an appropriate set of tolerance settings of the parameters, the tolerance design problem is formulated so as to yield an economical design, while ensuring a minimal variability in the performance of the wind turbine. The resulting multi-objective tolerance design problem is solved using the traditional Taguchi method. The present work provides a simple and economical approach for the robust optimal design of horizontal axis wind turbines.}
}

@incollection{huber_robust_2011,
  title = {Robust Statistics},
  booktitle = {International {{Encyclopedia}} of {{Statistical Science}}},
  author = {Huber, Peter J.},
  year = {2011},
  pages = {1248--1251},
  publisher = {{Springer}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\48JTUEST\\Huber - 2011 - Robust statistics.pdf;C\:\\Users\\a846735\\Zotero\\storage\\SA65GIYR\\10.html}
}

@article{hug_high-dimensional_2013,
  title = {High-Dimensional {{Bayesian}} Parameter Estimation: {{Case}} Study for a Model of {{JAK2}}/{{STAT5}} Signaling},
  shorttitle = {High-Dimensional {{Bayesian}} Parameter Estimation},
  author = {Hug, S. and Raue, A. and Hasenauer, J. and Bachmann, J. and Klingm{\"u}ller, U. and Timmer, J. and Theis, F.J.},
  year = {2013},
  month = dec,
  journal = {Mathematical Biosciences},
  volume = {246},
  number = {2},
  pages = {293--304},
  issn = {00255564},
  doi = {10.1016/j.mbs.2013.04.002},
  abstract = {In this work we present results of a detailed Bayesian parameter estimation for an analysis of ordinary differential equation models. These depend on many unknown parameters that have to be inferred from experimental data. The statistical inference in a high-dimensional parameter space is however conceptually and computationally challenging. To ensure rigorous assessment of model and prediction uncertainties we take advantage of both a profile posterior approach and Markov chain Monte Carlo sampling.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IKC38469\\Hug et al. - 2013 - High-dimensional Bayesian parameter estimation Ca.pdf}
}

@incollection{hutchison_evaluating_2004,
  title = {Evaluating the {{CMA Evolution Strategy}} on {{Multimodal Test Functions}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} - {{PPSN VIII}}},
  author = {Hansen, Nikolaus and Kern, Stefan},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Yao, Xin and Burke, Edmund K. and Lozano, Jos{\'e} A. and Smith, Jim and {Merelo-Guerv{\'o}s}, Juan Juli{\'a}n and Bullinaria, John A. and Rowe, Jonathan E. and Ti{\v n}o, Peter and Kab{\'a}n, Ata and Schwefel, Hans-Paul},
  year = {2004},
  volume = {3242},
  pages = {282--291},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30217-9_29},
  abstract = {In this paper the performance of the CMA evolution strategy with rank-\textmu -update and weighted recombination is empirically investigated on eight multimodal test functions. In particular the effect of the population size {$\lambda$} on the performance is investigated. Increasing the population size remarkably improves the performance on six of the eight test functions. The optimal population size takes a wide range of values, but, with one exception, scales sub-linearly with the problem dimension. The global optimum can be located in all but one function. The performance for locating the global optimum scales between linear and cubic with the problem dimension. In a comparison to state-of-the-art global search strategies the CMA evolution strategy achieves superior performance on multimodal, non-separable test functions without intricate parameter tuning.},
  isbn = {978-3-540-23092-2 978-3-540-30217-9},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GVIYE7T4\\Hansen et Kern - 2004 - Evaluating the CMA Evolution Strategy on Multimoda.pdf}
}

@article{hutter_bayesian_2006,
  title = {Bayesian {{Regression}} of {{Piecewise Constant Functions}}},
  author = {Hutter, Marcus},
  year = {2006},
  month = jun,
  journal = {arXiv:math/0606315},
  eprint = {math/0606315},
  eprinttype = {arxiv},
  abstract = {We derive an exact and efficient Bayesian regression algorithm for piecewise constant functions of unknown segment number, boundary location, and levels. It works for any noise and segment level prior, e.g. Cauchy which can handle outliers. We derive simple but good estimates for the in-segment variance. We also propose a Bayesian regression curve as a better way of smoothing data without blurring boundaries. The Bayesian approach also allows straightforward determination of the evidence, break probabilities and error estimates, useful for model selection and significance and robustness studies. We discuss the performance on synthetic and real-world examples. Many possible extensions will be discussed.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\E9LCVMVB\\Hutter - 2006 - Bayesian Regression of Piecewise Constant Function.pdf;C\:\\Users\\a846735\\Zotero\\storage\\G94863ZU\\0606315.html}
}

@article{huyse_free-form_2001,
  title = {Free-Form Airfoil Shape Optimization under Uncertainty Using Maximum Expected Value and Second-Order Second-Moment Strategies},
  author = {Huyse, Luc and Bushnell, Dennis M.},
  year = {2001},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\N7A2YLMJ\\Huyse et Bushnell - 2001 - Free-form airfoil shape optimization under uncerta.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7GVM9C4N\\search.html}
}

@article{huyse_probabilistic_2002,
  title = {Probabilistic {{Approach}} to {{Free-Form Airfoil Shape Optimization Under Uncertainty}}},
  author = {Huyse, Luc and Padula, Sharon L. and Lewis, R. Michael and Li, Wu},
  year = {2002},
  journal = {AIAA Journal},
  volume = {40},
  number = {9},
  pages = {1764--1772},
  issn = {0001-1452},
  doi = {10.2514/2.1881},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TSHAX373\\2.html}
}

@article{ibrahim_power_2015,
  title = {The {{Power Prior}}: {{Theory}} and {{Applications}}},
  shorttitle = {The {{Power Prior}}},
  author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
  year = {2015},
  month = dec,
  journal = {Statistics in medicine},
  volume = {34},
  number = {28},
  pages = {3724--3749},
  issn = {0277-6715},
  doi = {10.1002/sim.6728},
  abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A to Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Prequentist properties of power priors in posterior inference are established and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials.},
  pmcid = {PMC4626399},
  pmid = {26346180},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7ST5NWJ8\\Ibrahim et al. - 2015 - The Power Prior Theory and Applications.pdf}
}

@article{ibrahimi_robust_2011,
  title = {Robust {{Max-Product Belief Propagation}}},
  author = {Ibrahimi, Morteza and Javanmard, Adel and Kanoria, Yashodhan and Montanari, Andrea},
  year = {2011},
  month = nov,
  journal = {arXiv:1111.6214 [cs, math]},
  eprint = {1111.6214},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We study the problem of optimizing a graph-structured objective function under \textbackslash emph\{adversarial\} uncertainty. This problem can be modeled as a two-persons zero-sum game between an Engineer and Nature. The Engineer controls a subset of the variables (nodes in the graph), and tries to assign their values to maximize an objective function. Nature controls the complementary subset of variables and tries to minimize the same objective. This setting encompasses estimation and optimization problems under model uncertainty, and strategic problems with a graph structure. Von Neumann's minimax theorem guarantees the existence of a (minimax) pair of randomized strategies that provide optimal robustness for each player against its adversary. We prove several structural properties of this strategy pair in the case of graph-structured payoff function. In particular, the randomized minimax strategies (distributions over variable assignments) can be chosen in such a way to satisfy the Markov property with respect to the graph. This significantly reduces the problem dimensionality. Finally we introduce a message passing algorithm to solve this minimax problem. The algorithm generalizes max-product belief propagation to this new domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JXFMY8DC\\Ibrahimi et al. - 2011 - Robust Max-Product Belief Propagation.pdf;C\:\\Users\\a846735\\Zotero\\storage\\E6QC2FXD\\1111.html}
}

@article{ide_unified_1997,
  title = {Unified {{Notation}} for {{Data Assimilation}} : {{Operational}}, {{Sequential}} and {{Variational}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}: {{Theory}} and {{Practice}})},
  shorttitle = {Unified {{Notation}} for {{Data Assimilation}}},
  author = {Ide, Kayo and Courtier, Philippe and Ghil, Michael and Lorenc, Andrew C.},
  year = {1997},
  month = mar,
  journal = {Journal of the Meteorological Society of Japan. Ser. II},
  volume = {75},
  number = {1B},
  pages = {181--189},
  issn = {0026-1165, 2186-9057},
  doi = {10.2151/jmsj1965.75.1B_181},
  abstract = {Japan's largest platform for academic e-journals: J-STAGE is a full text database for reviewed academic papers published by Japanese societies},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\STH2ECYW\\Ide et al. - 1997 - Unified Notation for Data Assimilation  Operation.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GD2IBDM9\\_article.html}
}

@article{iooss_evaluation_nodate,
  title = {{Evaluation de quantiles de codes de calcul}},
  author = {Iooss, B and Devictor, N},
  pages = {19},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y8GS47EU\\Iooss et Devictor - Evaluation de quantiles de codes de calcul.pdf}
}

@article{iooss_revue_2011,
  title = {Revue Sur l'analyse de Sensibilit\'e Globale de Mod\`eles Num\'eriques},
  author = {Iooss, Bertrand},
  year = {2011},
  journal = {Journal de la Societe Fran\c{c}aise de Statistique},
  volume = {152},
  number = {1},
  pages = {1--23},
  publisher = {{Societe Fran\c{c}aise de Statistique et Societe Mathematique de France}},
  abstract = {Cet article a pour objectif d'effectuer un survol rapide, mais dans un cadre m\'ethodologique relativement complet, des diff\'erentes m\'ethodes d'analyse de sensibilit\'e globale d'un mod\`ele num\'erique. Faisant appel \`a de nombreux outils statistiques (r\'egression, lissage, tests, apprentissage, techniques de Monte Carlo, ...), celles-ci permettent de d\'eterminer quelles sont les variables d'entr\'ee d'un mod\`ele qui contribuent le plus \`a une quantit\'e d'int\'er\^et calcul\'ee \`a l'aide de ce mod\`ele (par exemple la variance d'une variable de sortie). Trois grandes classes de m\'ethodes sont ainsi distingu\'ees : le criblage (tri grossier des entr\'ees les plus influentes parmi un grand nombre), les mesures d'importance (indices quantitatifs donnant l'influence de chaque entr\'ee) et les outils d'exploration du mod\`ele (mesurant les effets des entr\'ees sur tout leur domaine de variation). Une m\'ethodologie progressive d'application de ces techniques est illustr\'ee sur une application \`a vocation p\'edagogique. Une synth\`ese est alors formul\'ee afin de situer chaque m\'ethode selon trois axes : co\^ut en nombre d'\'evaluations du mod\`ele, complexit\'e du mod\`ele et type d'information apport\'ee.},
  keywords = {Code de calcul,ExpÃ©rience numÃ©rique,Incertitude,MÃ©tamodÃ¨le,Plan d'expÃ©riences},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PSVUZRMV\\Iooss - 2011 - Revue sur l'analyse de sensibilitÃ© globale de modÃ¨.pdf}
}

@book{j._bichonyyk_reliability-based_2009,
  title = {Reliability-{{Based Design Optimization Using Efficient Global Reliability Analysis}}},
  author = {J. Bichonyyk, Barron and Mahadevanky, Sankaran and Eldred, Michael},
  year = {2009},
  month = may,
  doi = {10.2514/6.2009-2261},
  abstract = {Finding the optimal (lightest, least expensive, etc.) design for an engineered component that meets or exceeds a specified level of reliability is a problem of obvious interest across a wide spectrum of engineering fields. Various methods for this reliability-based design optimization problem have been proposed. Unfortunately, this problem is rarely solved in practice because, regardless of the method used, solving the problem is too expensive or the final solution is too inaccurate to ensure that the reliability constraint is actually satisfied. This is especially true for engineering applications involving expensive, implicit, and possibly nonlinear performance functions (such as large finite element models). The Efficient Global Reliability Analysis method was recently introduced to improve both the accuracy and efficiency of reliability analysis for this type of performance function. This paper explores how this new reliability analysis method can be used in a design optimization context to create a method of sufficient accuracy and efficiency to enable the use of reliability-based design optimization as a practical design tool.}
}

@article{jacobsen_i-revnet_2018,
  title = {I-{{RevNet}}: {{Deep Invertible Networks}}},
  shorttitle = {I-{{RevNet}}},
  author = {Jacobsen, J{\"o}rn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.07088 [cs, stat]},
  eprint = {1802.07088},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EITNZSPD\\Jacobsen et al. - 2018 - i-RevNet Deep Invertible Networks.pdf}
}

@article{jacquier_maximum_nodate,
  title = {Maximum {{Expected Utility}} via {{MCMC}}},
  author = {Jacquier, Eric and Johannes, Michael and Polson, Nicholas},
  pages = {26},
  abstract = {In this paper we provide a new simulation-based approach to maximum expected utility (MEU) portfolio allocation problems. MEU requires computation of expected utility and its optimization over the decision variable. In portfolio problems, the expected utility is generally not analytically available. Traditional methods resort to gradient-based estimates of expected utility with its ensuing derivatives. This leads to computational inefficiencies which are particularly acute in portfolio problems with parameter uncertainty (a.k.a. estimation risk). Our simulation-based method avoids the calculation of derivative and also allows for functional optimization. The algorithm combines Markov Chain Monte Carlo (MCMC) with the insights of simulated annealing and evolutionary Monte Carlo. It can exploit conjugate utility functions and latent variables in the relevant predictive density for efficient simulation. We also show how slice sampling naturally allows for constraints in the portfolio weights. We illustrate our methodology with a portfolio problem with estimation risk and CARA utility.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2DXK3GLL\\Jacquier et al. - Maximum Expected Utility via MCMC.pdf}
}

@phdthesis{janon_analyse_2012,
  type = {These de Doctorat},
  title = {Analyse de Sensibilit\'e et R\'eduction de Dimension. {{Application}} \`a l'oc\'eanographie {{Uncertainties}} Assessment in Global Sensitivity Indices Estimation from Metamodels {{Certified}} Reduced Basis Solutions of Viscous {{Burgers}} Equation Parametrized by Initial and Boundary Values},
  author = {Janon, Alexandre},
  year = {2012},
  month = nov,
  abstract = {Les mod\`eles math\'ematiques ont pour but de d\'ecrire le comportement d'un syst\`eme. Bien souvent, cette description est imparfaite, notamment en raison des incertitudes sur les param\`etres qui d\'efinissent le mod\`ele. Dans le contexte de la mod\'elisation des fluides g\'eophysiques, ces param\`etres peuvent \^etre par exemple la g\'eom\'etrie du domaine, l'\'etat initial, le for\c{c}age par le vent, ou les coefficients de frottement ou de viscosit\'e. L'objet de l'analyse de sensibilit\'e est de mesurer l'impact de l'incertitude attach\'ee \`a chaque param\`etre d'entr\'ee sur la solution du mod\`ele, et, plus particuli\`erement, identifier les param\`etres (ou groupes de param\`etres) og sensibles fg. Parmi les diff\'erentes m\'ethodes d'analyse de sensibilit\'e, nous privil\'egierons la m\'ethode reposant sur le calcul des indices de sensibilit\'e de Sobol. Le calcul num\'erique de ces indices de Sobol n\'ecessite l'obtention des solutions num\'eriques du mod\`ele pour un grand nombre d'instances des param\`etres d'entr\'ee. Cependant, dans de nombreux contextes, dont celui des mod\`eles g\'eophysiques, chaque lancement du mod\`ele peut n\'ecessiter un temps de calcul important, ce qui rend inenvisageable, ou tout au moins peu pratique, d'effectuer le nombre de lancements suffisant pour estimer les indices de Sobol avec la pr\'ecision d\'esir\'ee. Ceci am\`ene \`a remplacer le mod\`ele initial par un emph\{m\'etamod\`ele\} (aussi appel\'e emph\{surface de r\'eponse\} ou emph\{mod\`ele de substitution\}). Il s'agit d'un mod\`ele approchant le mod\`ele num\'erique de d\'epart, qui n\'ecessite un temps de calcul par lancement nettement diminu\'e par rapport au mod\`ele original. Cette th\`ese se centre sur l'utilisation d'un m\'etamod\`ele dans le cadre du calcul des indices de Sobol, plus particuli\`erement sur la quantification de l'impact du remplacement du mod\`ele par un m\'etamod\`ele en terme d'erreur d'estimation des indices de Sobol. Nous nous int\'eressons \'egalement \`a une m\'ethode de construction d'un m\'etamod\`ele efficace et rigoureux pouvant \^etre utilis\'e dans le contexte g\'eophysique.},
  collaborator = {Prieur, Cl{\'e}mentine and Nodet, Ma{\"e}lle},
  copyright = {Licence Etalab},
  school = {Grenoble},
  keywords = {Analyse de sensibilitÃ©,Calcul scientifique,Model reduction,RÃ©duction de dimension,Scientific computing,Sensitivity analysis,Statistics,Statistiques}
}

@techreport{janusevskis_simultaneous_2010,
  title = {Simultaneous Kriging-Based Sampling for Optimization and Uncertainty Propagation},
  author = {Janusevskis, Janis and Le Riche, Rodolphe},
  year = {2010},
  month = jul,
  abstract = {Robust analysis and optimization is typically based on repeated calls to a deterministic simulator that aim at propagating uncertainties and finding optimal design variables. Without loss of generality a double set of simulation parameters can be assumed: x are deterministic optimization variables, u are random parameters of known probability density function and f (x, u) is the objective function attached to the simulator. Most robust optimization methods involve two imbricated tasks, the u's uncertainty propagation (e.g., Monte Carlo simulations, reliability index calculation) which is recurcively performed inside optimization iterations on the x's. In practice, f is often calculated through a computationally expensive software. This makes the computational cost one of the principal obstacle to optimization in the presence of uncertainties. This report proposes a new efficient method for minimizing the mean objective function, min E[f(x, U)]. The efficiency stems from the simultaneous sampling of f for uncertainty propagation and optimization, i.e., the hierarchical imbrication is avoided. Y(x,u) ({$\omega$}), a kriging (Gaussian process conditioned on t past calculations of f) model of f (x, u) is built and the mean process, Z(x) ({$\omega$}) = E[Y(x,U)({$\omega$})], is analytically derived from it. The sampling criterion that yields both x and u is the one-step ahead minimum variance of the mean process Z at the maximizer of the expected improvement. The method is compared with Monte Carlo and kriging-based approaches on analytical test functions in two, four and six dimensions.},
  keywords = {Efficient Global Optimization,EGO,Expected improvement,Gaussian process,Kriging,Optimization under uncertainty,Robust optimization,Uncertainty Propagation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B2SDTQSK\\Janusevskis et Le Riche - 2010 - Simultaneous kriging-based sampling for optimizati.pdf}
}

@article{jaynes_rationale_1982,
  title = {On the Rationale of Maximum-Entropy Methods},
  author = {Jaynes, E.T.},
  year = {1982},
  journal = {Proceedings of the IEEE},
  volume = {70},
  number = {9},
  pages = {939--952},
  issn = {0018-9219},
  doi = {10.1109/PROC.1982.12425},
  abstract = {I summarize a Bayesian perspective of machine learning. We view Bayes as an optimization problem whose solutions use the information-geometry of the posterior. Using this perspective, we can show that many machine-learning methods have a (more general) Bayesian side to them. I believe this perspective to be essential for bridging the gap between `artificial' and `natural' learning systems.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4ENB5QQG\\Jaynes - 1982 - On the rationale of maximum-entropy methods.pdf}
}

@article{jewson_principles_2018,
  title = {Principles of {{Bayesian}} Inference Using General Divergence Criteria},
  author = {Jewson, Jack and Smith, Jim Q. and Holmes, Chris},
  year = {2018},
  journal = {Entropy},
  volume = {20},
  number = {6},
  pages = {442},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\98E54BDF\\Jewson et al. - 2018 - Principles of Bayesian inference using general div.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ME3I52WJ\\442.html}
}

@article{jiang_gibbs_2008,
  title = {Gibbs Posterior for Variable Selection in High-Dimensional Classification and Data Mining},
  author = {Jiang, Wenxin and Tanner, Martin A.},
  year = {2008},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {36},
  number = {5},
  eprint = {0810.5655},
  eprinttype = {arxiv},
  pages = {2207--2231},
  issn = {0090-5364},
  doi = {10.1214/07-AOS547},
  abstract = {In the popular approach of "Bayesian variable selection" (BVS), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. A completely new direction will be considered here to study BVS with a Gibbs posterior originating in statistical mechanics. The Gibbs posterior is constructed from a risk function of practical interest (such as the classification error) and aims at minimizing a risk function without modeling the data probabilistically. This can improve the performance over the usual Bayesian approach, which depends on a probability model which may be misspecified. Conditions will be provided to achieve good risk performance, even in the presence of high dimensionality, when the number of candidate variables "\$K\$" can be much larger than the sample size "\$n\$." In addition, we develop a convenient Markov chain Monte Carlo algorithm to implement BVS with the Gibbs posterior.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62F99 (Primary),82-08 (Secondary),Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6XQSG29X\\Jiang et Tanner - 2008 - Gibbs posterior for variable selection in high-dim.pdf}
}

@book{jolliffe_principal_2002,
  title = {Principal {{Component Analysis}}},
  author = {Jolliffe, I. T.},
  year = {2002},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98835},
  abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques, it continues to be the subject of much research, ranging from new model-based approaches to algorithmic ideas from neural networks. It is extremely versatile, with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
  isbn = {978-0-387-95442-4},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WNMLCMD7\\9780387954424.html}
}

@article{jones_efficient_1998,
  title = {Efficient Global Optimization of Expensive Black-Box Functions},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  year = {1998},
  journal = {Journal of Global optimization},
  volume = {13},
  number = {4},
  pages = {455--492},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8QF4EUL8\\Jones et al. - 1998 - Efficient global optimization of expensive black-b.pdf;C\:\\Users\\a846735\\Zotero\\storage\\DSISWVYM\\A1008306431147.html}
}

@article{jones_kernel-type_nodate,
  title = {Kernel-Type Density Estimation on the Unit Interval},
  author = {JONES, M C and HENDERSON, D A},
  pages = {35},
  abstract = {We consider kernel-type methods for estimation of a density on [0, 1] which eschew explicit boundary correction. Our starting point is the successful implementation of beta kernel density estimators of Chen (1999). We propose and investigate two alternatives. For the first, we reverse the roles of estimation point x and datapoint Xi in each summand of the estimator. For the second, we provide kernels that are symmetric in x and X; these kernels are conditional densities of bivariate copulas. We develop asymptotic theory for the new estimators and compare them with Chen's in a substantial simulation study. We also develop automatic bandwidth selection in the form of `rule-of-thumb' bandwidths for all three estimators. We find that our second proposal, that based on `copula kernels', seems particularly competitive with the beta kernel method of Chen in integrated squared error performance terms. Advantages include its greater range of possible values at 0 and 1, the fact that it is a bona fide density, and that the individual kernels and resulting estimator are comprehensible in terms of a direct single picture (as is ordinary kernel density estimation on the line).},
  keywords = {Beta kernels,Density estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YSHMWFLM\\JONES et HENDERSON - Kernel-type density estimation on the unit interva.pdf}
}

@article{jones_taxonomy_nodate,
  title = {A {{Taxonomy}} of {{Global Optimization Methods Based}} on {{Response Surfaces}}},
  author = {JONES, DONALD R},
  pages = {39},
  abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KNZITF9Q\\JONES - A Taxonomy of Global Optimization Methods Based on.pdf}
}

@article{journee_low-rank_2010,
  title = {Low-{{Rank Optimization}} on the {{Cone}} of {{Positive Semidefinite Matrices}}},
  author = {Journ{\'e}e, M. and Bach, F. and Absil, P.-A. and Sepulchre, R.},
  year = {2010},
  month = may,
  journal = {SIAM Journal on Optimization},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/080731359},
  abstract = {We propose an algorithm for solving optimization problems defined on a subset of the cone of symmetric positive semidefinite matrices. This algorithm relies on the factorization \$X=YY\^T\$, where the number of columns of Y fixes an upper bound on the rank of the positive semidefinite matrix X. It is thus very effective for solving problems that have a low-rank solution. The factorization \$X=YY\^T\$ leads to a reformulation of the original problem as an optimization on a particular quotient manifold. The present paper discusses the geometry of that manifold and derives a second-order optimization method with guaranteed quadratic convergence. It furthermore provides some conditions on the rank of the factorization to ensure equivalence with the original problem. In contrast to existing methods, the proposed algorithm converges monotonically to the sought solution. Its numerical efficiency is evaluated on two applications: the maximal cut of a graph and the problem of sparse principal component analysis.},
  copyright = {Copyright \textcopyright{} 2010 Society for Industrial and Applied Mathematics},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NMPMSXIU\\JournÃ©e et al. - 2010 - Low-Rank Optimization on the Cone of Positive Semi.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RLCQUXQ7\\080731359.html}
}

@inproceedings{juditsky_stochastic_2009,
  title = {Stochastic {{Approximation Approach}} to {{Stochastic Programming}}},
  booktitle = {{{ISMP}} 2009 - 20th {{International Symposium}} of {{Mathematical Programming}}},
  author = {Juditsky, Anatoli and Nemirovski, Arkadii S. and Lan, Guanghui and Shapiro, Alexander},
  year = {2009},
  month = aug,
  abstract = {A basic difficulty with solving stochastic programming problems is that it requires computation of expectations given by multidimensional integrals. One approach, based on Monte Carlo sampling techniques, is to generate a reasonably large random sample and consequently to solve the constructed so-called Sample Average Approximation (SAA) problem. The other classical approach is based on Stochastic Approximation (SA) techniques. In this talk we discuss some recent advances in development of SA type numerical algorithms for solving convex stochastic programming problems. Numerical experiments show that for some classes of problems the so-called Mirror Descent SA Method can significantly outperform the SAA approach.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\D3M8UC7U\\Juditsky et al. - 2009 - Stochastic Approximation Approach to Stochastic Pr.pdf;C\:\\Users\\a846735\\Zotero\\storage\\MAKLLQFI\\hal-00981931.html}
}

@article{jullien_technical_nodate,
  title = {Technical and Numerical Doc (Croco)},
  author = {Jullien, S and Caillaud, M and Benshila, R and Bordois, L and Cambon, G and Dumas, F and Gentil, S Le and Lemari{\'e}, F and Marchesiello, P and Theetten, S},
  pages = {142},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DAQBZJV7\\Jullien et al. - Technical and numerical doc.pdf}
}

@article{kalai_lexicographic_2010,
  title = {Lexicographic {\emph{{$\alpha$}}} -Robustness: An Application to the 1-Median Problem},
  shorttitle = {Lexicographic {\emph{{$\alpha$}}} -Robustness},
  author = {Kala{\"i}, R. and Aloulou, M. A. and Vallin, Ph. and Vanderpooten, D.},
  year = {2010},
  month = apr,
  journal = {RAIRO - Operations Research},
  volume = {44},
  number = {2},
  pages = {119--138},
  issn = {0399-0559, 1290-3868},
  doi = {10.1051/ro/2010010},
  abstract = {In the last decade, several robustness approaches have been developed to deal with uncertainty. In decision problems, and particularly in location problems, the most used robustness approach rely either on maximal cost or on maximal regret criteria. However, it is well known that these criteria are too conservative. In this paper, we present a new robustness approach, called lexicographic {$\alpha$}-robustness, which compensates for the drawbacks of criteria based on the worst case. We apply this approach to the 1-median location problem under uncertainty on node weights and we give a specific algorithm to determine robust solutions in the case of a tree. We also show that this algorithm can be extended to the case of a general network.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CV3RILW5\\KalaÃ¯ et al. - 2010 - Lexicographic iÎ±i -robustness an application.pdf}
}

@book{kalbfleisch_probability_1985,
  title = {Probability and {{Statistical Inference}}},
  author = {Kalbfleisch, J. G.},
  year = {1985},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-1096-2},
  isbn = {978-1-4612-7009-6 978-1-4612-1096-2},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PMS7TENJ\\Kalbfleisch - 1985 - Probability and Statistical Inference.pdf}
}

@article{kandel_efficient_2021,
  title = {Efficient Ptychographic Phase Retrieval via a Matrix-Free {{Levenberg-Marquardt}} Algorithm},
  author = {Kandel, Saugat and Maddali, S. and Nashed, Youssef S. G. and Hruszkewycz, Stephan O. and Jacobsen, Chris and Jacobsen, Chris and Jacobsen, Chris and Allain, Marc},
  year = {2021},
  month = jul,
  journal = {Optics Express},
  volume = {29},
  number = {15},
  pages = {23019--23055},
  publisher = {{Optica Publishing Group}},
  issn = {1094-4087},
  doi = {10.1364/OE.422768},
  abstract = {The phase retrieval problem, where one aims to recover a complex-valued image from far-field intensity measurements, is a classic problem encountered in a range of imaging applications. Modern phase retrieval approaches usually rely on gradient descent methods in a nonlinear minimization framework. Calculating closed-form gradients for use in these methods is tedious work, and formulating second order derivatives is even more laborious. Additionally, second order techniques often require the storage and inversion of large matrices of partial derivatives, with memory requirements that can be prohibitive for data-rich imaging modalities. We use a reverse-mode automatic differentiation (AD) framework to implement an efficient matrix-free version of the Levenberg-Marquardt (LM) algorithm, a longstanding method that finds popular use in nonlinear least-square minimization problems but which has seen little use in phase retrieval. Furthermore, we extend the basic LM algorithm so that it can be applied for more general constrained optimization problems (including phase retrieval problems) beyond just the least-square applications. Since we use AD, we only need to specify the physics-based forward model for a specific imaging application; the first and second-order derivative terms are calculated automatically through matrix-vector products, without explicitly forming the large Jacobian or Gauss-Newton matrices typically required for the LM method. We demonstrate that this algorithm can be used to solve both the unconstrained ptychographic object retrieval problem and the constrained ``blind'' ptychographic object and probe retrieval problems, under the popular Gaussian noise model as well as the Poisson noise model. We compare this algorithm to state-of-the-art first order ptychographic reconstruction methods to demonstrate empirically that this method outperforms best-in-class first-order methods: it provides excellent convergence guarantees with (in many cases) a superlinear rate of convergence, all with a computational cost comparable to, or lower than, the tested first-order algorithms.},
  copyright = {\&\#169; 2021 Optical Society of America},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RUU8PE76\\Kandel et al. - 2021 - Efficient ptychographic phase retrieval via a matr.pdf;C\:\\Users\\a846735\\Zotero\\storage\\77CLSNSN\\fulltext.html}
}

@techreport{kass_bayes_1993,
  title = {Bayes Factors and Model Uncertainty},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1993},
  institution = {{DEPARTMENT OF STATISTICS, UNIVERSITY OFWASHINGTON}},
  abstract = {In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications. The points we emphasize are:- from Jeffreys's Bayesian point of view, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory;- Bayes factors offer a way of evaluating evidence in favor ofa null hypothesis;- Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis;- Bayes factors are very general, and do not require alternative models to be nested;- several techniques are available for computing Bayes factors, including asymptotic approximations which are easy to compute using the output from standard packages that maximize likelihoods;- in "non-standard " statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SVAWKKFQ\\Kass et Raftery - 1993 - Bayes factors and model uncertainty.pdf;C\:\\Users\\a846735\\Zotero\\storage\\M5LI2ZNZ\\summary.html}
}

@article{kass_bayes_1995,
  title = {Bayes Factors},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1995},
  journal = {Journal of the american statistical association},
  volume = {90},
  number = {430},
  pages = {773--795},
  publisher = {{Taylor \& Francis}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QEAUAFJ7\\KassRaftery1995.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LFW2HV6E\\01621459.1995.html}
}

@article{kennedy_bayesian_2001,
  title = {Bayesian Calibration of Computer Models},
  author = {Kennedy, Marc C. and O'Hagan, Anthony},
  year = {2001},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {63},
  number = {3},
  pages = {425--464},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00294},
  abstract = {We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.},
  langid = {english},
  keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZYFTNCPQ\\Kennedy et O'Hagan - 2001 - Bayesian calibration of computer models.pdf;C\:\\Users\\a846735\\Zotero\\storage\\NXU9WKLY\\abstract.html}
}

@article{kent_robust_1982,
  title = {Robust Properties of Likelihood Ratio Tests},
  author = {Kent, John T.},
  year = {1982},
  journal = {Biometrika},
  volume = {69},
  number = {1},
  pages = {19--27},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/69.1.19},
  abstract = {The usual asymptotic chi-squared distribution for the likelihood ratio test statistic is based on the assumptions that the data come from the parametric model under consideration and that the parameter satisfies the null hypothesis. In this paper we examine the distribution of the likelihood ratio statistic when the data do not come from the parametric model, but when the 'nearest' member of the parametric family still satisfies the null hypothesis. In general, the likelihood ratio statistic no longer follows an asymptotic chi-squared distribution, and an alternative statistic based on the unionintersection approach is proposed.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y3RIYDF5\\Kent - 1982 - Robust properties of likelihood ratio tests.pdf}
}

@article{kerschen_method_2005,
  title = {The {{Method}} of {{Proper Orthogonal Decomposition}} for {{Dynamical Characterization}} and {{Order Reduction}} of {{Mechanical Systems}}: {{An Overview}}},
  shorttitle = {The {{Method}} of {{Proper Orthogonal Decomposition}} for {{Dynamical Characterization}} and {{Order Reduction}} of {{Mechanical Systems}}},
  author = {Kerschen, Gaetan and Golinval, Jean-claude and Vakakis, Alexander F. and Bergman, Lawrence A.},
  year = {2005},
  month = aug,
  journal = {Nonlinear Dynamics},
  volume = {41},
  number = {1-3},
  pages = {147--169},
  issn = {0924-090X, 1573-269X},
  doi = {10.1007/s11071-005-2803-2},
  abstract = {Modal analysis is used extensively for understanding the dynamic behavior of structures. However, a major concern for structural dynamicists is that its validity is limited to linear structures. New developments have been proposed in order to examine nonlinear systems, among which the theory based on nonlinear normal modes is indubitably the most appealing. In this paper, a different approach is adopted, and proper orthogonal decomposition is considered. The modes extracted from the decomposition may serve two purposes, namely order reduction by projecting high-dimensional data into a lower-dimensional space and feature extraction by revealing relevant but unexpected structure hidden in the data. The utility of the method for dynamic characterization and order reduction of linear and nonlinear mechanical systems is demonstrated in this study.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JY3MA6WK\\Kerschen et al. - 2005 - The Method of Proper Orthogonal Decomposition for .pdf}
}

@article{kieffer_guaranteed_2013,
  title = {Guaranteed Characterization of Exact Non-Asymptotic Confidence Regions in Nonlinear Parameter Estimation},
  author = {Kieffer, Michel and Walter, Eric},
  year = {2013},
  journal = {IFAC Proceedings Volumes},
  volume = {46},
  number = {23},
  pages = {56--61},
  issn = {14746670},
  doi = {10.3182/20130904-3-FR-2041.00019},
  abstract = {Recently, a new family of methods has been proposed for characterizing accuracy in nonlinear parameter estimation by Campi et al.. These methods make it possible to obtain exact, nonasymptotic confidence regions for the parameter estimates under relatively mild assumptions on the noise distribution, namely that the noise samples are independently and symmetrically distributed.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\M85H2643\\Kieffer et Walter - 2013 - Guaranteed characterization of exact non-asymptoti.pdf}
}

@incollection{kim_guide_2015,
  title = {A {{Guide}} to {{Sample Average Approximation}}},
  author = {Kim, Sujin and Pasupathy, Raghu and Henderson, Shane},
  year = {2015},
  month = jan,
  volume = {216},
  pages = {207--243},
  doi = {10.1007/978-1-4939-1384-8-8},
  abstract = {This chapter reviews the principles of sample average approximation (SAA) for solving simulation optimization problems. We provide an accessible overview of the area and survey interesting recent developments. We explain when one might want to use SAA and when one might expect it to provide good-quality solutions. We also review some of the key theoretical properties of the solutions obtained through SAA. We contrast SAA with stochastic approximation (SA) methods in terms of the computational effort required to obtain solutions of a given quality, explaining why SA ``wins'' asymptotically. However, an extension of SAA known as retrospective optimization can match the asymptotic convergence rate of SA, at least up to a multiplicative constant.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CIX5IVAF\\Kim et al. - 2015 - A Guide to Sample Average Approximation.pdf}
}

@article{kim_tractable_2015,
  title = {Tractable {{Fully Bayesian Inference}} via {{Convex Optimization}} and {{Optimal Transport Theory}}},
  author = {Kim, Sanggyun and Mesa, Diego and Ma, Rui and Coleman, Todd P.},
  year = {2015},
  month = sep,
  journal = {arXiv:1509.08582 [stat]},
  eprint = {1509.08582},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We consider the problem of transforming samples from one continuous source distribution into samples from another target distribution. We demonstrate with optimal transport theory that when the source distribution can be easily sampled from and the target distribution is log-concave, this can be tractably solved with convex optimization. We show that a special case of this, when the source is the prior and the target is the posterior, is Bayesian inference. Here, we can tractably calculate the normalization constant and draw posterior i.i.d. samples. Remarkably, our Bayesian tractability criterion is simply log concavity of the prior and likelihood: the same criterion for tractable calculation of the maximum a posteriori point estimate. With simulated data, we demonstrate how we can attain the Bayes risk in simulations. With physiologic data, we demonstrate improvements over point estimation in intensive care unit outcome prediction and electroencephalography-based sleep staging.},
  archiveprefix = {arXiv},
  keywords = {Bayesian inference,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UV4VLRVW\\Kim et al. - 2015 - Tractable Fully Bayesian Inference via Convex Opti.pdf;C\:\\Users\\a846735\\Zotero\\storage\\CWFHYF8C\\1509.html}
}

@article{kingma_adam:_2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3CTW2N4D\\Kingma et Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LBC2YKV7\\1412.html}
}

@article{kisiala_conditional_nodate,
  title = {Conditional {{Value-at-Risk}}: {{Theory}} and {{Applications}}},
  author = {Kisiala, Jakob},
  pages = {96},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\U6CSBR4I\\Kisiala - Conditional Value-at-Risk Theory and Applications.pdf}
}

@article{kitanidis_parameter_1986,
  title = {Parameter Uncertainty in Estimation of Spatial Functions: {{Bayesian}} Analysis},
  shorttitle = {Parameter Uncertainty in Estimation of Spatial Functions},
  author = {Kitanidis, Peter K.},
  year = {1986},
  journal = {Water resources research},
  volume = {22},
  number = {4},
  pages = {499--507},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5NLQMAEG\\Kitanidis - 1986 - Parameter uncertainty in estimation of spatial fun;C\:\\Users\\a846735\\Zotero\\storage\\U8Q6L32Z\\Kitanidis - 1986 - Parameter uncertainty in estimation of spatial fun.pdf}
}

@article{kiureghian_aleatory_2009,
  title = {Aleatory or Epistemic? {{Does}} It Matter?},
  shorttitle = {Aleatory or Epistemic?},
  author = {Kiureghian, Armen Der and Ditlevsen, Ove},
  year = {2009},
  month = mar,
  journal = {Structural Safety},
  volume = {31},
  number = {2},
  pages = {105--112},
  issn = {01674730},
  doi = {10.1016/j.strusafe.2008.06.020},
  abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\L53MFISU\\Kiureghian et Ditlevsen - 2009 - Aleatory or epistemic Does it matter.pdf}
}

@article{kobyzev_normalizing_2021,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  year = {2021},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {11},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  pages = {3964--3979},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Normalizin,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9NERKN3E\\Kobyzev et al. - 2021 - Normalizing Flows An Introduction and Review of C.pdf}
}

@misc{koenker_maximum_2012,
  title = {``{{Maximum Likelihood Asymptotics}} under {{Non-standard Conditions}}: {{A Heuristic Introduction}} to {{Sandwiches}}''},
  author = {Koenker, Roger},
  year = {2012},
  publisher = {{University of Illinois}},
  abstract = {Econ 574 Lecture notes},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KGIXYHHN\\L10.pdf}
}

@article{kothari_trumpets_2021,
  title = {Trumpets: {{Injective Flows}} for {{Inference}} and {{Inverse Problems}}},
  shorttitle = {Trumpets},
  author = {Kothari, Konik and Khorashadizadeh, AmirEhsan and {de Hoop}, Maarten and Dokmani{\'c}, Ivan},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.10461 [cs, eess]},
  eprint = {2102.10461},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We propose injective generative models called TRUMPETs that generalize invertible normalizing flows. The proposed generators progressively increase dimension from a low-dimensional latent space. We demonstrate that TRUMPETs can be trained orders of magnitudes faster than standard flows while yielding samples of comparable or better quality. They retain many of the advantages of the standard flows such as training based on maximum likelihood and a fast, exact inverse of the generator. Since TRUMPETs are injective and have fast inverses, they can be effectively used for downstream Bayesian inference. To wit, we use TRUMPET priors for maximum a posteriori estimation in the context of image reconstruction from compressive measurements, outperforming competitive baselines in terms of reconstruction quality and speed. We then propose an efficient method for posterior characterization and uncertainty quantification with TRUMPETs by taking advantage of the low-dimensional latent space.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RWFSIG3H\\Kothari et al. - 2021 - Trumpets Injective Flows for Inference and Invers.pdf}
}

@article{koutsourelakis_multi-resolution_2009,
  title = {A Multi-Resolution, Non-Parametric, {{Bayesian}} Framework for Identification of Spatially-Varying Model Parameters},
  author = {Koutsourelakis, P. S.},
  year = {2009},
  month = sep,
  journal = {Journal of Computational Physics},
  volume = {228},
  number = {17},
  eprint = {0810.0744},
  eprinttype = {arxiv},
  pages = {6184--6211},
  issn = {00219991},
  doi = {10.1016/j.jcp.2009.05.016},
  abstract = {This paper proposes a hierarchical, multi-resolution framework for the identification of model parameters and their spatially variability from noisy measurements of the response or output. Such parameters are frequently encountered in PDE-based models and correspond to quantities such as density or pressure fields, elasto-plastic moduli and internal variables in solid mechanics, conductivity fields in heat diffusion problems, permeability fields in fluid flow through porous media etc. The proposed model has all the advantages of traditional Bayesian formulations such as the ability to produce measures of confidence for the inferences made and providing not only predictive estimates but also quantitative measures of the predictive uncertainty. In contrast to existing approaches it utilizes a parsimonious, non-parametric formulation that favors sparse representations and whose complexity can be determined from the data. The proposed framework in non-intrusive and makes use of a sequence of forward solvers operating at various resolutions. As a result, inexpensive, coarse solvers are used to identify the most salient features of the unknown field(s) which are subsequently enriched by invoking solvers operating at finer resolutions. This leads to significant computational savings particularly in problems involving computationally demanding forward models but also improvements in accuracy. It is based on a novel, adaptive scheme based on Sequential Monte Carlo sampling which is embarrassingly parallelizable and circumvents issues with slow mixing encountered in Markov Chain Monte Carlo schemes.},
  archiveprefix = {arXiv},
  keywords = {stochastic inverse problem},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2S4Q82F6\\Koutsourelakis - 2009 - A multi-resolution, non-parametric, Bayesian frame.pdf}
}

@article{kouvelis_algorithms_1992,
  title = {Algorithms for Robust Single and Multiple Period Layout Planning for Manufacturing Systems},
  author = {Kouvelis, Panagiotis and Kurawarwala, Abbas A. and Guti{\'e}rrez, Genaro J.},
  year = {1992},
  month = dec,
  journal = {European Journal of Operational Research},
  volume = {63},
  number = {2},
  pages = {287--303},
  issn = {03772217},
  doi = {10.1016/0377-2217(92)90032-5},
  langid = {english}
}

@article{kouvelis_algorithms_1992-1,
  title = {Algorithms for Robust Single and Multiple Period Layout Planning for Manufacturing Systems},
  author = {Kouvelis, Panagiotis and Kurawarwala, Abbas A. and Guti{\'e}rrez, Genaro J.},
  year = {1992},
  month = dec,
  journal = {European Journal of Operational Research},
  series = {Strategic {{Planning}} of {{Facilities}}},
  volume = {63},
  number = {2},
  pages = {287--303},
  issn = {0377-2217},
  doi = {10.1016/0377-2217(92)90032-5},
  abstract = {In many layout design situations, the use of `optimality' with respect to a design objective, such as the minimization of the material handling cost, is insufficiently discriminating. Robustness of the layout, in cases of demand uncertainty, is more important for the manufacturing manager. A robust layout is one that is close to the optimal solution for a wide variety of demand scenarios even though it may not be optimal under any specific demand scenario. In this paper, we develop algorithms to generate robust layout designs for manufacturing systems. Our robustness approach to the layout decision making can be applied to single and multiple period problems in the presence of considerable uncertainty, both in terms of products to be produced as well as their production volumes. Our algorithms, executed in a heuristic fashion, can be effectively used for layout design of large size manufacturing systems.},
  langid = {english},
  keywords = {design,integer programming,Plant layout,production},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8AN67IDJ\\0377221792900325.html}
}

@article{kramer_feedback_2016,
  title = {Feedback {{Control}} for {{Systems}} with {{Uncertain Parameters Using Online-Adaptive Reduced Models}}},
  author = {Kramer, Boris and Peherstorfer, Benjamin and Willcox, Karen},
  year = {2016},
  keywords = {Optim Robuste},
  file = {C\:\\Users\\Victor\\Downloads\\adaptive-model-reduction-control-kramer-peherstorfer-willcox.pdf}
}

@article{krantz_primer_2016,
  title = {A {{Primer}} of {{Mathematical Writing}}, {{Second Edition}}},
  author = {Krantz, Steven G.},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.04888 [math]},
  eprint = {1612.04888},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {This is a tract on the art and practice of mathematical writing. Not only does the book cover basic principles of grammar, syntax, and usage, but it takes into account developments of the last twenty years that have been inspired by the Internet. There is considerable discussion of TeX and other modern writing environments. We also consider electronic journals, print-on-demand books, Open Access Journals, preprint servers, and many other aspects of modern publishing life.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - History and Overview},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RG9Z3FHI\\Krantz - 2016 - A Primer of Mathematical Writing, Second Edition.pdf}
}

@article{kreitmair_effect_2019,
  title = {The Effect of Uncertain Bottom Friction on Estimates of Tidal Current Power},
  author = {Kreitmair, M. J. and Draper, S. and Borthwick, A. G. L. and {van den Bremer}, T. S.},
  year = {2019},
  month = jan,
  journal = {Royal Society Open Science},
  volume = {6},
  number = {1},
  issn = {2054-5703},
  doi = {10.1098/rsos.180941},
  abstract = {Uncertainty affects estimates of the power potential of tidal currents, resulting in large ranges in values reported for a given site, such as the Pentland Firth, UK. We examine the role of bottom friction, one of the most important sources of uncertainty. We do so by using perturbation methods to find the leading-order effect of bottom friction uncertainty in theoretical models by Garrett \& Cummins (2005 Proc. R. Soc. A 461, 2563\textendash 2572. (doi:10.1098/rspa.2005.1494); 2013 J. Fluid Mech. 714, 634\textendash 643. (doi:10.1017/jfm.2012.515)) and Vennell (2010 J. Fluid Mech. 671, 587\textendash 604. (doi:10.1017/S0022112010006191)), which consider quasi-steady flow in a channel completely spanned by tidal turbines, a similar channel but retaining the inertial term, and a circular turbine farm in laterally unconfined flow. We find that bottom friction uncertainty acts to increase estimates of expected power in a fully spanned channel, but generally has the reverse effect in laterally unconfined farms. The optimal number of turbines, accounting for bottom friction uncertainty, is lower for a fully spanned channel and higher in laterally unconfined farms. We estimate the typical magnitude of bottom friction uncertainty, which suggests that the effect on estimates of expected power lies in the range -5 to +30\%, but is probably small for deep channels such as the Pentland Firth (5\textendash 10\%). In such a channel, the uncertainty in power estimates due to bottom friction uncertainty remains considerable, and we estimate a relative standard deviation of 30\%, increasing to 50\% for small channels.},
  pmcid = {PMC6366226},
  pmid = {30800352},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EHQQKSRK\\Kreitmair et al. - 2019 - The effect of uncertain bottom friction on estimat.pdf}
}

@article{kreitmair_effect_2020,
  title = {The Effect of Bed Roughness Uncertainty on Tidal Stream Power Estimates for the {{Pentland Firth}}},
  author = {Kreitmair, M. J. and Adcock, T. A. A. and Borthwick, A. G. L. and Draper, S. and {van den Bremer}, T. S.},
  year = {2020},
  month = jan,
  journal = {Royal Society Open Science},
  volume = {7},
  number = {1},
  issn = {2054-5703},
  doi = {10.1098/rsos.191127},
  abstract = {Uncertainty affects estimates of the power potential of tidal currents, resulting in large ranges in values reported for sites such as the Pentland Firth, UK. Kreitmair et al. (2019, R. Soc. open sci. 6, 180941. (doi:10.1098/rsos.191127)) have examined the effect of uncertainty in bottom friction on tidal power estimates by considering idealized theoretical models. The present paper considers the role of bottom friction uncertainty in a realistic numerical model of the Pentland Firth spanned by different fence configurations. We find that uncertainty in removable power estimates resulting from bed roughness uncertainty depends on the case considered, with relative uncertainty between 2\% (for a fully spanned channel with small values of mean roughness and input uncertainty) and 44\% (for an asymmetrically confined channel with high values of bed roughness and input uncertainty). Relative uncertainty in power estimates is generally smaller than (input) relative uncertainty in bottom friction by a factor of between 0.2 and 0.7, except for low turbine deployments and very high mean values of friction. This paper makes a start at quantifying uncertainty in tidal stream power estimates, and motivates further work for proper characterization of the resource, accounting for uncertainty inherent in resource modelling.},
  pmcid = {PMC7029936},
  pmid = {32218944},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RAY9NE9T\\Kreitmair et al. - 2020 - The effect of bed roughness uncertainty on tidal s.pdf}
}

@article{kreutz_likelihood_2012,
  title = {Likelihood Based Observability Analysis and Confidence Intervals for Predictions of Dynamic Models},
  author = {Kreutz, Clemens and Raue, Andreas and Timmer, Jens},
  year = {2012},
  month = sep,
  journal = {BMC Systems Biology},
  volume = {6},
  number = {1},
  pages = {120},
  issn = {1752-0509},
  doi = {10.1186/1752-0509-6-120},
  abstract = {Predicting a system's behavior based on a mathematical model is a primary task in Systems Biology. If the model parameters are estimated from experimental data, the parameter uncertainty has to be translated into confidence intervals for model predictions. For dynamic models of biochemical networks, the nonlinearity in combination with the large number of parameters hampers the calculation of prediction confidence intervals and renders classical approaches as hardly feasible.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Q23QL5VX\\Kreutz et al. - 2012 - Likelihood based observability analysis and confid.pdf;C\:\\Users\\a846735\\Zotero\\storage\\EWX9XXT3\\1752-0509-6-120.html}
}

@article{krige_statistical_1951,
  title = {A Statistical Approach to Some Basic Mine Valuation Problems on the {{Witwatersrand}}},
  author = {Krige, Daniel G.},
  year = {1951},
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  volume = {52},
  number = {6},
  pages = {119--139},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JZM426MV\\Krige - 1951 - A statistical approach to some basic mine valuatio.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ELTXPCEN\\AJA0038223X_4792.html}
}

@article{krokhmal_modeling_2011,
  title = {Modeling and Optimization of Risk},
  author = {Krokhmal, Pavlo and Zabarankin, Michael and Uryasev, Stan},
  year = {2011},
  month = jul,
  journal = {Surveys in Operations Research and Management Science},
  volume = {16},
  number = {2},
  pages = {49--66},
  issn = {1876-7354},
  doi = {10.1016/j.sorms.2010.08.001},
  abstract = {This paper surveys the most recent advances in the context of decision making under uncertainty, with an emphasis on the modeling of risk-averse preferences using the apparatus of axiomatically defined risk functionals, such as coherent measures of risk and deviation measures, and their connection to utility theory, stochastic dominance, and other more established methods.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\46S2PMER\\1-s2.0-S187673541000005X-main.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KLYHLRYI\\S187673541000005X.html}
}

@article{krzysztofowicz_bayesian_1999,
  title = {Bayesian Theory of Probabilistic Forecasting via Deterministic Hydrologic Model},
  author = {Krzysztofowicz, Roman},
  year = {1999},
  journal = {Water Resources Research},
  volume = {35},
  number = {9},
  pages = {2739--2750},
  issn = {1944-7973},
  doi = {10.1029/1999WR900099},
  abstract = {Rational decision making (for flood warning, navigation, or reservoir systems) requires that the total uncertainty about a hydrologic predictand (such as river stage, discharge, or runoff volume) be quantified in terms of a probability distribution, conditional on all available information and knowledge. Hydrologic knowledge is typically embodied in a deterministic catchment model. Fundamentals are presented of a Bayesian forecasting system (BFS) for producing a probabilistic forecast of a hydrologic predictand via any deterministic catchment model. The BFS decomposes the total uncertainty into input uncertainty and hydrologic uncertainty, which are quantified independently and then integrated into a predictive (Bayes) distribution. This distribution results from a revision of a prior (climatic) distribution, is well calibrated, and has a nonnegative ex ante economic value. The BFS is compared with Monte Carlo simulation and ``ensemble forecasting'' technique, none of which can alone produce a probabilistic forecast that meets requirements of rational decision making, but each can serve as a component of the BFS.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TNQG4DV4\\Krzysztofowicz - 1999 - Bayesian theory of probabilistic forecasting via d.pdf;C\:\\Users\\a846735\\Zotero\\storage\\6L4MQFIH\\1999WR900099.html}
}

@article{kuczera_there_2010,
  title = {There Are No Hydrological Monsters, Just Models and Observations with Large Uncertainties!},
  author = {Kuczera, George and Renard, Benjamin and Thyer, Mark and Kavetski, Dmitri},
  year = {2010},
  month = aug,
  journal = {Hydrological Sciences Journal},
  volume = {55},
  number = {6},
  pages = {980--991},
  issn = {0262-6667},
  doi = {10.1080/02626667.2010.504677},
  abstract = {Catchments that do not behave in the way the hydrologist expects, expose the frailties of hydrological science, particularly its unduly simplistic treatment of input and model uncertainty. A conceptual rainfall\textendash runoff model represents a highly simplified hypothesis of the transformation of rainfall into runoff. Sub-grid variability and mis-specification of processes introduce an irreducible model error, about which little is currently known. In addition, hydrological observation systems are far from perfect, with the principal catchment forcing (rainfall) often subject to large sampling errors. When ignored or treated simplistically, these errors develop into monsters that destroy our ability to model certain catchments. In this paper, these monsters are tackled using Bayesian Total Error Analysis, a framework that accounts for user-specified sources of error and yields quantitative insights into how prior knowledge of these uncertainties affects our ability to infer models and use them for predictive purposes. A case study involving a catchment with an apparent water balance anomaly (a hydrological monstrosity!) illustrates these concepts. It is found that, in the absence of additional information, the rainfall\textendash runoff record is insufficient to explain this anomaly \textendash{} it could be due to a large export of groundwater, systematic overestimation of catchment rainfall of the order of 40\%, or a conspiracy of these factors. There is ``no free lunch'' in hydrology. The rainfall\textendash runoff record on its own is insufficient to decompose the different sources of uncertainty affecting calibration, testing and prediction, and hydrological monstrosities will persist until additional independent knowledge of uncertainties is obtained. Citation Kuczera, G., Renard, B., Thyer, M. \& Kavetski, D. (2010) There are no hydrological monsters, just models and observations with large uncertainties! Hydrol. Sci. J. 55(6), 980\textendash 991.},
  keywords = {analyse Bayesienne de l'erreur totale,Bayesian total error analysis,data errors,erreur structurelle du modÃ¨le,erreurs d'observation,ill-posedness,infÃ©rence mal posÃ©e,model structural error,modÃ¨les pluieâdÃ©bit,rainfallârunoff models},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VDZCEVA2\\Kuczera et al. - 2010 - There are no hydrological monsters, just models an.pdf;C\:\\Users\\a846735\\Zotero\\storage\\82J26CGU\\02626667.2010.html}
}

@article{kullback_information_1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR39968},
  zmnumber = {0042.38403},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JPHCNDHI\\1177729694.html}
}

@article{kumar_adjoint_nodate,
  title = {Adjoint Based Multi-Objective Shape Optimization of a Transonic Airfoil under Uncertainties},
  author = {Kumar, D. and Miranda, J. and Raisee, M. and Lacor, C.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P6MU9IAM\\10873.pdf;C\:\\Users\\a846735\\Zotero\\storage\\Y84GAN9M\\310.pdf}
}

@article{kumar_adjoint_nodate-1,
  title = {Adjoint Based Multi-Objective Shape Optimization of a Transonic Airfoil under Uncertainties},
  author = {Kumar, D and Miranda, J and Raisee, M and Lacor, C},
  pages = {10},
  abstract = {In the present paper, the results of ongoing work within the FP7 project UMRIDA are presented and discussed. The main idea of this work is to combine the non-intrusive polynomial chaos based uncertainty quantification methods with the gradient based methods for stochastic optimization. When introducing uncertainties in a design process, the objective is no longer deterministic and can be characterized by its mean and its variance, i.e. in a robust design the optimization becomes multiobjective. Gradient based optimization of the mean objective and of the variance of the objective therefore requires the gradient of both quantities. The gradient of the mean objective is combined with the gradient of its variance using weights. By changing the weights, the Pareto front (if any, i.e. if the 2 objectives are conflicting) or at least part of it can be recovered. The proposed method is applied to the optimal shape design of the transonic RAE2822 airfoil under uncertainties. In the current work, the flight conditions (the Mach number and the angle of attack) are considered as uniformly distributed uncertain parameters. The objectives considered are the mean drag coefficient and its standard deviation. In this work, the adjoint solver and the CFD solver of SU2 (an open source CFD solver) are coupled with the polynomial chaos methods for the optimal shape design of a transonic airfoil. Hicks-Henne functions are employed to parameterize the airfoil and to represent a new geometry in the design process. The optimization procedure is performed using the sequential least square programming (SLSQP) algorithm. Two optimal designs are obtained by considering two different points on the Pareto front.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IJTQ9B7Q\\Kumar et al. - Adjoint based multi-objective shape optimization o.pdf}
}

@phdthesis{kumar_sequential_2008,
  title = {Sequential {{Calibration Of Computer Models}}},
  author = {Kumar, Arun},
  year = {2008},
  langid = {english},
  school = {The Ohio State University},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RLQ6FWEZ\\Kumar - 2008 - Sequential Calibration Of Computer Models.pdf;C\:\\Users\\a846735\\Zotero\\storage\\5YQ8FL22\\pg_10.html}
}

@article{kunstner_limitations_2020,
  title = {Limitations of the {{Empirical Fisher Approximation}} for {{Natural Gradient Descent}}},
  author = {Kunstner, Frederik and Balles, Lukas and Hennig, Philipp},
  year = {2020},
  month = jun,
  journal = {arXiv:1905.12558 [cs, stat]},
  eprint = {1905.12558},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\69S7EWW2\\Kunstner et al. - Limitations of the Empirical Fisher Approximation .pdf;C\:\\Users\\a846735\\Zotero\\storage\\VZ483H2I\\Kunstner et al. - 2020 - Limitations of the Empirical Fisher Approximation .pdf;C\:\\Users\\a846735\\Zotero\\storage\\8ZGFHKIK\\1905.html}
}

@inproceedings{lai_mean-variance-skewness-kurtosis-based_2006,
  title = {Mean-{{Variance-Skewness-Kurtosis-based Portfolio Optimization}}},
  booktitle = {First {{International Multi-Symposiums}} on {{Computer}} and {{Computational Sciences}} ({{IMSCCS}}'06)},
  author = {Lai, Kin Keung and Yu, Lean and Wang, Shouyang},
  year = {2006},
  month = jun,
  volume = {2},
  pages = {292--297},
  doi = {10.1109/IMSCCS.2006.239},
  abstract = {In the mean-variance-skewness-kurtosis framework, this study solve multiple conflicting and competing portfolio objectives such as maximizing expected return and skewness and minimizing risk and kurtosis simultaneously, by construction of a polynomial goal programming (PGP) model into which investor preferences over higher return moments are incorporated. To examine its practicality, the approach is tested on four major stock indices. Empirical results indicate that, for all examined investor preferences and stock indices, the PGP approach is significantly efficient way to solve multiple conflicting portfolio objectives in the mean-variance-skewness-kurtosis framework. In the meantime, we find that the different investors' preferences not only affect asset allocations of portfolio, but also affect the four moment statistics of return},
  keywords = {Asset management,econometrics,Educational institutions,investment,Mathematical model,mathematical programming,Mathematical programming,Mathematics,mean-variance-skewness-kurtosis-based portfolio optimization,polynomial goal programming model,Polynomials,portfolio asset allocation,Portfolios,Risk management,risk minimization,share prices,statistical analysis,Statistics,stock indices,stock markets,Testing},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NZ4YW5MX\\4673719.html}
}

@article{laloe_nonparametric_2013,
  title = {Nonparametric Estimation of Regression Level Sets Using Kernel Plug-in Estimator},
  author = {Lalo{\"e}, T. and Servien, R.},
  year = {2013},
  month = sep,
  journal = {Journal of the Korean Statistical Society},
  volume = {42},
  number = {3},
  pages = {301--311},
  issn = {12263192},
  doi = {10.1016/j.jkss.2012.10.001},
  abstract = {Let (X , Y ) be a random pair taking values in Rd \texttimes J, where J {$\subset$} R is supposed to be bounded. We propose a plug-in estimator of the level sets of the regression function r of Y on X , using a kernel estimator of r. We consider an error criterion defined by the volume of the symmetrical difference between the real and estimated level sets. We state the consistency of our estimator, and we get a rate of convergence equivalent to the one obtained by Cadre (2006) for the density function level sets.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IIEL8EJ3\\LaloÃ« et Servien - 2013 - Nonparametric estimation of regression level sets .pdf}
}

@article{laloy_efficient_2013,
  title = {Efficient Posterior Exploration of a High-Dimensional Groundwater Model from Two-Stage {{Markov}} Chain {{Monte Carlo}} Simulation and Polynomial Chaos Expansion},
  author = {Laloy, Eric and Rogiers, Bart and Vrugt, Jasper A. and Mallants, Dirk and Jacques, Diederik},
  year = {2013},
  month = may,
  journal = {Water Resources Research},
  volume = {49},
  number = {5},
  pages = {2664--2682},
  issn = {1944-7973},
  doi = {10.1002/wrcr.20226},
  abstract = {This study reports on two strategies for accelerating posterior inference of a highly parameterized and CPU-demanding groundwater flow model. Our method builds on previous stochastic collocation approaches, e.g., Marzouk and Xiu (2009) and Marzouk and Najm (2009), and uses generalized polynomial chaos (gPC) theory and dimensionality reduction to emulate the output of a large-scale groundwater flow model. The resulting surrogate model is CPU efficient and serves to explore the posterior distribution at a much lower computational cost using two-stage MCMC simulation. The case study reported in this paper demonstrates a two to five times speed-up in sampling efficiency.},
  langid = {english},
  keywords = {MCMC,PCE},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TNSMADNB\\Laloy et al. - 2013 - Efficient posterior exploration of a high-dimensio.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7282JYD5\\abstract.html}
}

@article{lam_lookahead_nodate,
  title = {Lookahead {{Bayesian Optimization}} with {{Inequality Constraints}}},
  author = {Lam, Remi and Willcox, Karen},
  pages = {15},
  abstract = {We consider the task of optimizing an objective function subject to inequality constraints when both the objective and the constraints are expensive to evaluate. Bayesian optimization (BO) is a popular way to tackle optimization problems with expensive objective function evaluations, but has mostly been applied to unconstrained problems. Several BO approaches have been proposed to address expensive constraints but are limited to greedy strategies maximizing immediate reward. To address this limitation, we propose a lookahead approach that selects the next evaluation in order to maximize the long-term feasible reduction of the objective function. We present numerical experiments demonstrating the performance improvements of such a lookahead approach compared to several greedy BO algorithms, including constrained expected improvement (EIC) and predictive entropy search with constraint (PESC).},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YZWECBRI\\Lam et Willcox - Lookahead Bayesian Optimization with Inequality Co.pdf}
}

@misc{lan_scaling_2022,
  title = {Scaling {{Up Bayesian Uncertainty Quantification}} for {{Inverse Problems}} Using {{Deep Neural Networks}}},
  author = {Lan, Shiwei and Li, Shuyi and Shahbaba, Babak},
  year = {2022},
  month = apr,
  number = {arXiv:2101.03906},
  eprint = {2101.03906},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  abstract = {Due to the importance of uncertainty quantification (UQ), Bayesian approach to inverse problems has recently gained popularity in applied mathematics, physics, and engineering. However, traditional Bayesian inference methods based on Markov Chain Monte Carlo (MCMC) tend to be computationally intensive and inefficient for such high dimensional problems. To address this issue, several methods based on surrogate models have been proposed to speed up the inference process. More specifically, the calibration-emulation-sampling (CES) scheme has been proven to be successful in large dimensional UQ problems. In this work, we propose a novel CES approach for Bayesian inference based on deep neural network models for the emulation phase. The resulting algorithm is computationally more efficient and more robust against variations in the training set. Further, by using an autoencoder (AE) for dimension reduction, we have been able to speed up our Bayesian inference method up to three orders of magnitude. Overall, our method, henceforth called Dimension-Reduced Emulative Autoencoder Monte Carlo (DREAMC) algorithm, is able to scale Bayesian UQ up to thousands of dimensions for inverse problems. Using two low-dimensional (linear and nonlinear) inverse problems we illustrate the validity of this approach. Next, we apply our method to two high-dimensional numerical examples (elliptic and advection-diffussion) to demonstrate its computational advantages over existing algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MWX75FW2\\Lan et al. - 2022 - Scaling Up Bayesian Uncertainty Quantification for.pdf}
}

@misc{lan_scaling_2022-1,
  title = {Scaling {{Up Bayesian Uncertainty Quantification}} for {{Inverse Problems}} Using {{Deep Neural Networks}}},
  author = {Lan, Shiwei and Li, Shuyi and Shahbaba, Babak},
  year = {2022},
  month = apr,
  number = {arXiv:2101.03906},
  eprint = {2101.03906},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  abstract = {Due to the importance of uncertainty quantification (UQ), Bayesian approach to inverse problems has recently gained popularity in applied mathematics, physics, and engineering. However, traditional Bayesian inference methods based on Markov Chain Monte Carlo (MCMC) tend to be computationally intensive and inefficient for such high dimensional problems. To address this issue, several methods based on surrogate models have been proposed to speed up the inference process. More specifically, the calibration-emulation-sampling (CES) scheme has been proven to be successful in large dimensional UQ problems. In this work, we propose a novel CES approach for Bayesian inference based on deep neural network models for the emulation phase. The resulting algorithm is computationally more efficient and more robust against variations in the training set. Further, by using an autoencoder (AE) for dimension reduction, we have been able to speed up our Bayesian inference method up to three orders of magnitude. Overall, our method, henceforth called Dimension-Reduced Emulative Autoencoder Monte Carlo (DREAMC) algorithm, is able to scale Bayesian UQ up to thousands of dimensions for inverse problems. Using two low-dimensional (linear and nonlinear) inverse problems we illustrate the validity of this approach. Next, we apply our method to two high-dimensional numerical examples (elliptic and advection-diffussion) to demonstrate its computational advantages over existing algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PXIDGRUR\\Lan et al. - 2022 - Scaling Up Bayesian Uncertainty Quantification for.pdf}
}

@article{lan_wormhole_2013,
  title = {Wormhole {{Hamiltonian Monte Carlo}}},
  author = {Lan, Shiwei and Streets, Jeffrey and Shahbaba, Babak},
  year = {2013},
  month = may,
  journal = {arXiv:1306.0063 [stat]},
  eprint = {1306.0063},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create \textbackslash emph\{wormholes\} connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a \textbackslash emph\{residual energy\} function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SQAV645H\\Lan et al. - 2013 - Wormhole Hamiltonian Monte Carlo.pdf;C\:\\Users\\a846735\\Zotero\\storage\\UCNIUAYF\\1306.html}
}

@inproceedings{landrieu_cut_2016,
  title = {Cut {{Pursuit}}: Fast Algorithms to Learn Piecewise Constant Functions},
  shorttitle = {Cut {{Pursuit}}},
  booktitle = {19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}} 2016)},
  author = {Landrieu, Loic and Obozinski, Guillaume},
  year = {2016},
  month = may,
  address = {{Cadix, Spain}},
  abstract = {We propose working-set/greedy algorithms to efficiently find the solutions to convex optimization problems penalized respectively by the total variation and the Mumford Shah boundary size. Our algorithms exploit the piecewise constant structure of the level-sets of the solutions by recursively splitting them using graph cuts. We obtain significant speed up on images that can be approximated with few level-sets compared to state-of-the-art algorithms .},
  keywords = {greedy algorithm,minimal partition,Mumford-Shah,optimization,working-set},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GPVV5DNS\\Landrieu et Obozinski - 2016 - Cut Pursuit fast algorithms to learn piecewise co.pdf}
}

@article{laurent_overview_2019,
  title = {An {{Overview}} of {{Gradient-Enhanced Metamodels}} with {{Applications}}},
  author = {Laurent, Luc and Le Riche, Rodolphe and Soulier, Bruno and Boucard, Pierre-Alain},
  year = {2019},
  month = jan,
  journal = {Archives of Computational Methods in Engineering},
  volume = {26},
  number = {1},
  pages = {61--106},
  issn = {1134-3060, 1886-1784},
  doi = {10.1007/s11831-017-9226-3},
  abstract = {Metamodeling, the science of modeling functions observed at a finite number of points, benefits from all auxiliary information it can account for. Function gradients are a common auxiliary information and are useful for predicting functions with locally changing behaviors. This article is a review of the main metamodels that use function gradients in addition to function values. The goal of the article is to give the reader both an overview of the principles involved in gradientenhanced metamodels while also providing insightful formulations. The following metamodels have gradient-enhanced versions in the literature and are reviewed here: classical, weighted and moving least squares, Shepard weighting functions, and the kernel-based methods that are radial basis functions, kriging and support vector machines. The methods are set in a common framework of linear combinations between a priori chosen functions and coefficients that depend on the observations. The characteristics common to all kernel-based approaches are underlined. A new {$\nu$}-GSVR metamodel which uses gradients is given. Numerical comparisons of the metamodels are carried out for approximating analytical test functions. The experiments are replicable, as they are performed with an opensource available toolbox. The results indicate that there is a trade-off between the better computing time of least squares methods and the larger versatility of kernelbased approaches.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4C8L9TWE\\Laurent et al. - 2019 - An Overview of Gradient-Enhanced Metamodels with A.pdf}
}

@article{lawless_investigation_2005,
  title = {An Investigation of Incremental {{4D-Var}} Using Non-Tangent Linear Models},
  author = {Lawless, A. S. and Gratton, S. and Nichols, N. K.},
  year = {2005},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {131},
  pages = {459--476},
  issn = {0035-9009},
  doi = {10.1256/qj.04.20},
  abstract = {We investigate the convergence of incremental four-dimensional variational data assimilation (4D-Var) when an approximation to the tangent linear model is used within the inner loop. Using a semi-implicit semi-Lagrangian model of the one-dimensional shallow water equations, we perform data assimilation experiments using an exact tangent linear model and using an inexact linear model (a perturbation forecast model). We find that the two assimilations converge at a similar rate and the analyses are also similar, with the difference between them dependent on the amount of noise in the observations. To understand the numerical results, we present the incremental 4D-Var algorithm as a Gauss-Newton iteration for solving a least-squares problem and consider its fixed points.},
  keywords = {GAUSS NEWTON,INCREMENTAL FORMULATION,TANGENT LINEAR,VARIATIONAL DATA ASSIMILATION},
  annotation = {ADS Bibcode: 2005QJRMS.131..459L},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\R7EFTY4K\\Lawless et al. - 2005 - An investigation of incremental 4D-Var using non-t.pdf}
}

@article{le_bars_amandes_2010,
  title = {The {{AMANDES}} Tidal Model for the {{Amazon}} Estuary and Shelf},
  author = {Le Bars, Yoann and Lyard, Florent and Jeandel, Catherine and Dardengo, Leonardo},
  year = {2010},
  month = jan,
  journal = {Ocean Modelling},
  volume = {31},
  number = {3},
  pages = {132--149},
  issn = {1463-5003},
  doi = {10.1016/j.ocemod.2009.11.001},
  abstract = {The AMANDES project aims to study transports from the Andean mountains to the Atlantic Ocean through the Amazon system. This requires realistic estuarine modelling in this area strongly forced by tides and river discharge. As none of the existing models for this region would fit the actual needs of the project, a specific new generation model has been implemented. The model is based on the hydrodynamic finite element model T-UGOm. In a first step, we limit our investigations to tidal dynamics. As the Amazon estuary is a very shallow macro-tidal area, it is necessary to improve the available bathymetries and to develop a precise bottom friction parametrisation. In this paper, we discuss the implementation of a high resolution regional model. This allows us to develop a precise and accurate tidal model: for instance, the overall root mean square error on complex differences is reduced from 54cm in a standard model to 27cm in our best model. Such precise and accurate tidal modelling is a prerequisite for modelling particle transport.},
  langid = {english},
  keywords = {Amazon estuary,Bathymetry,Bottom friction,T-UGOm,Tide,Unstructured grid},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NFD8Y3TE\\S1463500309002078.html}
}

@phdthesis{le_bars_modelisation_2010,
  title = {Mod\'elisation de La Dynamique Oc\'eanique Barotrope Dans l'estuaire et Le Plateau Amazoniens},
  author = {Le Bars, Yoann},
  year = {2010},
  month = mar,
  abstract = {A la fronti\`ere entre les continents et les oc\'eans, les marges continentales sont le si\`ege de la grande majorit\'e des apports de mati\`ere - dissoutes et solides, organiques et inorganiques - d'origine continentale vers l'oc\'ean. Elles sont aussi directement en contact avec les masses d'eau oc\'eaniques qui \'echangent de la mati\`ere et des \'el\'ements avec ces marges. L'\'erosion et le transport par les rivi\`eres \'etant une des sources essentielles des \'el\'ements chimiques \`a l'oc\'ean, la remise en suspension de mat\'eriel s\'ediment\'e et les forts m\'elanges d'eau favorisent les transferts de mati\`ere du continent vers l'oc\'ean. Le projet Amandes a pour objectif l'\'etude de ces \'echanges entre continents et oc\'eans, en \'etudiant le cas particulier du transport d'\'el\'ements en provenance des montagnes andines vers l'oc\'ean Atlantique par le syst\`eme amazonien. Trois disciplines compl\'ementaires \textendash{} g\'eochimie continentale et marine, oc\'eanographie physique et mod\'elisation hydrodynamique (incluant l'assimilation de donn\'ees) \textendash{} sont associ\'ees pour atteindre ces objectifs.  Parmi les mod\`eles de la zone existants, aucun ne r\'epond aux besoins du projet Amandes. Cette th\`ese a consist\'e \`a \'etablir un nouveau mod\`ele, qui permettra d'explorer en profondeur le probl\`eme du transport,c'est-\`a-dire le d\'eplacement de toute la colonne d'eau et ce qu'elle contient, par exemple des s\'ediments. Cependant, \'etablir un nouveau mod\`ele hydrodynamique dans le but d'\'etudier le transport de mat\'eriaux par le fleuve Amazone dans l'oc\'ean Atlantique \'etait un sujet trop ambitieux pour une seule th\`ese. En cons\'equence, nous nous sommes attach\'es \`a \'etablir les bases d'un mod\`ele qui sera \'etendu par la suite. Pour r\'epondre \`a ses besoins de mod\'elisation, le projet Amandes a opt\'e pour le mod\`ele \`a grilles non-structur\'ees T-UGOm. L'avantage des grilles non-structur\'ees est leur grande souplesse, qui leur permet de s'adapter finement aux sp\'ecificit\'es g\'eographiques de la zone mod\'elis\'ee. Dans le cas de l'estuaire de l'Amazone, o\`u la g\'eographie peut \^etre complexe, cette particularit\'e est un avantage critique. Pour appliquer T-UGOm au cas de l'Amazone, il a fallu pr\'eciser certains sch\'emas num\'eriques. \'Egalement, l'estuaire \'etant une zone de fortes mar\'ees, avec des fonds de faibles profondeurs sur de grandes \'etendues et soumise \`a de forts courants, le frottement de fond a une influence primordiale. Nous avons donc apport\'e un effort important pour affiner la prise en compte de ce frottement par le mod\`ele. D'autre part, tant la faiblesse des fonds marin que la complexit\'e de la g\'eographie induisent \'egalement une forte influence du trait de c\^otes et de la bathym\'etrie. En cons\'equence, une part importante des efforts a \'et\'e d\'edi\'ee \`a \'etablir un trait de c\^otes et une bathym\'etrie de la zone aussi pr\'ecis que possible. La mar\'ee \'etant le ph\'enom\`ene hydrodynamique de plus forte amplitude dans la zone, nous nous sommes focalis\'es sur sa mod\'elisation. Les d\'eveloppements \'evoqu\'es pr\'ec\'edemment ont permis, lorsque l'on compare notre meilleure solution de mar\'ee avec les donn\'ees in situ et satellites, d'am\'eliorer sensiblement la mod\'elisation de la mar\'ee. En comparant notre meilleure solution avec celles d\'ej\`a publi\'ees, celle-ci est plus proche des donn\'ees collect\'ees. Ceci ouvre la voie \`a une mod\'elisation hydrodynamique plus compl\`ete, en particulier du transport.},
  langid = {english},
  school = {Universit\'e de Toulouse, Universit\'e Toulouse III - Paul Sabatier},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\S7FQTW4R\\Le Bars - 2010 - ModÃ©lisation de la dynamique ocÃ©anique barotrope d.pdf;C\:\\Users\\a846735\\Zotero\\storage\\DSDAIFL3\\879.html;C\:\\Users\\a846735\\Zotero\\storage\\KCE2RPZV\\879.html}
}

@incollection{le_gratiet_metamodel-based_2016,
  title = {Metamodel-Based Sensitivity Analysis: Polynomial Chaos Expansions and {{Gaussian}} Processes},
  shorttitle = {Metamodel-Based Sensitivity Analysis},
  booktitle = {Handbook of {{Uncertainty Quantification}} - {{Part III}}: {{Sensitivity}} Analysis},
  author = {Le Gratiet, Loic and Marelli, Stefano and Sudret, Bruno},
  year = {2016},
  abstract = {Global sensitivity analysis is now established as a powerful approach for determining the key random input parameters that drive the uncertainty of model output predictions. Yet the classical computation of the so-called Sobol' indices is based on Monte Carlo simulation, which is not af- fordable when computationally expensive models are used, as it is the case in most applications in engineering and applied sciences. In this respect metamodels such as polynomial chaos expansions (PCE) and Gaussian processes (GP) have received tremendous attention in the last few years, as they allow one to replace the original, taxing model by a surrogate which is built from an experimental design of limited size. Then the surrogate can be used to compute the sensitivity indices in negligible time. In this chapter an introduction to each technique is given, with an emphasis on their strengths and limitations in the context of global sensitivity analysis. In particular, Sobol' (resp. total Sobol') indices can be computed analytically from the PCE coefficients. In contrast, confidence intervals on sensitivity indices can be derived straightforwardly from the properties of GPs. The performance of the two techniques is finally compared on three well-known analytical benchmarks (Ishigami, G-Sobol and Morris functions) as well as on a realistic engineering application (deflection of a truss structure).},
  keywords = {Error estimation,Gaussian Processes,Kriging,Polynomial Chaos Expansions,Sobol' indices},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BU5KFTVK\\Le Gratiet et al. - 2016 - Metamodel-based sensitivity analysis polynomial c.pdf}
}

@misc{le_riche_introduction_2014,
  type = {Doctoral},
  title = {Introduction to Kriging},
  author = {Le Riche, Rodolphe},
  year = {2014},
  month = sep,
  address = {{France}},
  abstract = {This is a two hours class on conditional Gaussian processes, i.e., kriging.  We attempt to strike a compromise between a good theoretical foundation on Gaussian processes and practical issues (e.g., how to sample a Gaussian process). Note also that the case of Gaussian Processes with trends is discussed. Finally, we try to link kriging to Bayesian regression and Support Vector Machines. Illustrations are based on the R package DiceKriging.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F2Z35U8J\\Le Riche - 2014 - Introduction to kriging.pdf}
}

@article{lecun_machine_nodate,
  title = {{{MACHINE LEARNING AND PATTERN RECOGNITION}}},
  author = {LeCun, Yann},
  pages = {24},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\93HIVCCJ\\LeCun - MACHINE LEARNING AND PATTERN RECOGNITION.pdf}
}

@article{lee_robust_2010,
  title = {A Robust Structural Design Method Using the {{Kriging}} Model to Define the Probability of Design Success},
  author = {Lee, K-H},
  year = {2010},
  month = feb,
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
  volume = {224},
  number = {2},
  pages = {379--388},
  issn = {0954-4062},
  doi = {10.1243/09544062JMES1736},
  abstract = {Abstract, In this study, a robust optimization method is proposed by introducing the Kriging approximation model and defining the probability of design-success. A key problem in robust optimization is that the mean and the variation of a response cannot be calculated easily. This research presents an implementation of the approximate statistical moment method based on the Kriging metamodel. Furthermore, the statistics using the second-order statistical approximation method are adopted to avoid the local robust optimum. Thus, the probability of design-success, which is defined as the probability of satisfying the imposed design requirements, is represented as a function of approximate mean and variance. The formulation for the robust optimization can be defined as the probability of design-success of each response. The mathematical problem and the design problems of a two-bar structure and microgyroscope are investigated for the validation of the proposed method.},
  langid = {english}
}

@article{lehman_designing_2004,
  title = {Designing Computer Experiments to Determine Robust Control Variables},
  author = {Lehman, Jeffrey S. and Santner, Thomas J. and Notz, William I.},
  year = {2004},
  journal = {Statistica Sinica},
  pages = {571--590},
  keywords = {Optim Robuste},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SC8QPVVA\\DESIGNING COMP_EXP ROBUST CONTROL VARIABLES.pdf;C\:\\Users\\Victor\\Documents\\A14n213.pdf}
}

@article{lehman_designing_nodate,
  title = {{{DESIGNING COMPUTER EXPERIMENTS TO DETERMINE ROBUST CONTROL VARIABLES}}},
  author = {Lehman, Jeffrey S and Santner, Thomas J and Notz, William I},
  pages = {21},
  abstract = {This paper is concerned with the design of computer experiments when there are two types of inputs: control variables and environmental variables. Control variables, also called manufacturing variables, are determined by a product designer while environmental variables, called noise variables in the quality control literature, are uncontrolled in the field but take values that are characterized by a probability distribution. Our goal is to find a set of control variables at which the response is insensitive to the value of the environmental variables, a ``robust'' choice of control variables. Such a choice ensures that the mean response is as insensitive as possible to perturbations of the nominal environmental variable distribution. We present a sequential strategy to select the inputs at which to observe the response so as to determine a robust setting of the control variables. Our solution is Bayesian; the prior takes the response as a draw from a stationary Gaussian stochastic process. Given the previous information, the sequential algorithm computes for each untested site the ``improvement'' over the current guess of the optimal robust setting. The design selects the next site to maximize the expected improvement criterion.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KHQII6VL\\Lehman et al. - DESIGNING COMPUTER EXPERIMENTS TO DETERMINE ROBUST.pdf}
}

@book{lehmann_theory_2006,
  title = {Theory of Point Estimation},
  author = {Lehmann, Erich L. and Casella, George},
  year = {2006},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PIU47W8I\\Lehmann E.L., Casella G. Theory of point estimation (2ed., Springer, 1998)(ISBN 0387985026)(O)(617s)_MVsa_.pdf;C\:\\Users\\a846735\\Zotero\\storage\\QTZDRT6E\\books.html}
}

@article{lelievre_consideration_2016,
  title = {On the Consideration of Uncertainty in Design: Optimization-Reliability-Robustness},
  shorttitle = {On the Consideration of Uncertainty in Design},
  author = {Leli{\`e}vre, Nicolas and Beaurepaire, Pierre and Mattrand, C{\'e}cile and Gayton, Nicolas and Otsmane, Abdelkader},
  year = {2016},
  journal = {Structural and Multidisciplinary Optimization},
  volume = {54},
  number = {6},
  pages = {1423--1437},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XYKS6BH8\\LeliÃ¨vre et al. - 2016 - On the consideration of uncertainty in design opt.pdf;C\:\\Users\\a846735\\Zotero\\storage\\BKWV7C8V\\s00158-016-1556-5.html}
}

@article{lempert_general_2006,
  title = {A {{General}}, {{Analytic Method}} for {{Generating Robust Strategies}} and {{Narrative Scenarios}}},
  author = {Lempert, Robert J. and Groves, David G. and Popper, Steven W. and Bankes, Steve C.},
  year = {2006},
  month = apr,
  journal = {Management Science},
  volume = {52},
  number = {4},
  pages = {514--528},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.1050.0472},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SAHRJMET\\Lempert et al. - 2006 - A General, Analytic Method for Generating Robust S.pdf}
}

@article{lewis_unified_2011,
  title = {A Unified Approach to Model Selection Using the Likelihood Ratio Test},
  author = {Lewis, Fraser and Butler, Adam and Gilbert, Lucy},
  year = {2011},
  journal = {Methods in Ecology and Evolution},
  volume = {2},
  number = {2},
  pages = {155--162},
  issn = {2041-210X},
  doi = {10.1111/j.2041-210X.2010.00063.x},
  abstract = {1. Ecological count data typically exhibit complexities such as overdispersion and zero-inflation, and are often weakly associated with a relatively large number of correlated covariates. The use of an appropriate statistical model for inference is therefore essential. A common selection criteria for choosing between nested models is the likelihood ratio test (LRT). Widely used alternatives to the LRT are based on information-theoretic metrics such as the Akaike Information Criterion. 2. It is widely believed that the LRT can only be used to compare the performance of nested models \textendash{} i.e. in situations where one model is a special case of another. There are many situations in which it is important to compare non-nested models, so, if true, this would be a substantial drawback of using LRTs for model comparison. In reality, however, it is actually possible to use the LRT for comparing both nested and non-nested models. This fact is well-established in the statistical literature, but not widely used in ecological studies. 3. The main obstacle to the use of the LRT with non-nested models has, until relatively recently, been the fact that it is difficult to explicitly write down a formula for the distribution of the LRT statistic under the null hypothesis that one of the models is true. With modern computing power it is possible to overcome this difficulty by using a simulation-based approach. 4. To demonstrate the practical application of the LRT to both nested and non-nested model comparisons, a case study involving data on questing tick (Ixodes ricinus) abundance is presented. These data contain complexities typical in ecological analyses, such as zero-inflation and overdispersion, for which comparison between models of differing structure \textendash{} e.g. non-nested models \textendash{} is of particular importance. 5. Choosing between competing statistical models is an essential part of any applied ecological analysis. The LRT is a standard statistical test for comparing nested models. By use of simulation the LRT can also be used in an analogous fashion to compare non-nested models, thereby providing a unified approach for model comparison within the null hypothesis testing paradigm. A simple practical guide is provided in how to apply this approach to the key models required in the analyses of count data.},
  langid = {english},
  keywords = {information theoretic metrics,likelihood ratio test,model selection,non-nested models},
  annotation = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2010.00063.x},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BXCT3U68\\Lewis et al. - 2011 - A unified approach to model selection using the li.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZPATD5T6\\j.2041-210X.2010.00063.html}
}

@article{li_evaluation_2010,
  title = {Evaluation of Failure Probability via Surrogate Models},
  author = {Li, Jing and Xiu, Dongbin},
  year = {2010},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {229},
  number = {23},
  pages = {8966--8980},
  issn = {00219991},
  doi = {10.1016/j.jcp.2010.08.022},
  abstract = {Evaluation of failure probability of a given system requires sampling of the system response and can be computationally expensive. Therefore it is desirable to construct an accurate surrogate model for the system response and subsequently to sample the surrogate model. In this paper we discuss the properties of this approach. We demonstrate that the straightforward sampling of a surrogate model can lead to erroneous results, no matter how accurate the surrogate model is. We then propose a hybrid approach by sampling both the surrogate model in a ``large'' portion of the probability space and the original system in a ``small'' portion. The resulting algorithm is significantly more efficient than the traditional sampling method, and is more accurate and robust than the straightforward surrogate model approach. Rigorous convergence proof is established for the hybrid approach, and practical implementation is discussed. Numerical examples are provided to verify the theoretical findings and demonstrate the efficiency gain of the approach.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SBA5UB8Z\\Li et Xiu - 2010 - Evaluation of failure probability via surrogate mo.pdf}
}

@book{li_markov_2009,
  title = {Markov Random Field Modeling in Image Analysis},
  author = {Li, Stan Z.},
  year = {2009},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\S35XCXY8\\Li - 2009 - Markov random field modeling in image analysis.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LHKNVA3F\\books.html}
}

@inproceedings{li_measuring_2018,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  year = {2018},
  month = feb,
  abstract = {We train in random subspaces of parameter space to measure how many dimensions are really needed to find a solution.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EXYLDPEL\\Li et al. - 2018 - Measuring the Intrinsic Dimension of Objective Lan.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LPJWSH84\\forum.html}
}

@article{li_mingliang_calibration_2012,
  title = {Calibration of a Distributed Flood Forecasting Model with Input Uncertainty Using a {{Bayesian}} Framework},
  author = {{Li Mingliang} and {Yang Dawen} and {Chen Jinsong} and {Hubbard Susan S.}},
  year = {2012},
  month = aug,
  journal = {Water Resources Research},
  volume = {48},
  number = {8},
  issn = {0043-1397},
  doi = {10.1029/2010WR010062},
  abstract = {In the process of calibrating distributed hydrological models, accounting for input uncertainty is important, yet challenging. In this study, we develop a Bayesian model to estimate parameters associated with a geomorphology?based hydrological model (GBHM). The GBHM model uses geomorphic characteristics to simplify model structure and physically based methods to represent hydrological processes. We divide the observed discharge into low? and high?flow data, and use the first?order autoregressive model to describe their temporal dependence. We consider relative errors in rainfall as spatially distributed variables and estimate them jointly with the GBHM parameters. The joint posterior probability distribution is explored using Markov chain Monte Carlo methods, which include Metropolis?Hastings, delay rejection adaptive Metropolis, and Gibbs sampling methods. We evaluate the Bayesian model using both synthetic and field data sets. The synthetic case study demonstrates that the developed method generally is effective in calibrating GBHM parameters and in estimating their associated uncertainty. The calibration ignoring input errors has lower accuracy and lower reliability compared to the calibration that includes estimation of the input errors, especially under model structure uncertainty. The field case study shows that calibration of GBHM parameters under complex field conditions remains a challenge. Although jointly estimating input errors and GBHM parameters improves the continuous ranked probability score and the consistency of the predictive distribution with the observed data, the improvement is incremental. To better calibrate parameters in a distributed model, such as GBHM here, we need to develop a more complex model and incorporate much more information.},
  keywords = {Bayesian,calibration,distributed hydrological model,flood forecasting,input uncertainty,MCMC},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6FLWZTMQ\\Li Mingliang et al. - 2012 - Calibration of a distributed flood forecasting mod.pdf;C\:\\Users\\a846735\\Zotero\\storage\\P4PLU5MD\\2010WR010062.html}
}

@article{liang_proper_2002,
  title = {{{PROPER ORTHOGONAL DECOMPOSITION AND ITS APPLICATIONS}}\textemdash{{PART I}}: {{THEORY}}},
  shorttitle = {{{PROPER ORTHOGONAL DECOMPOSITION AND ITS APPLICATIONS}}\textemdash{{PART I}}},
  author = {Liang, Y.C. and Lee, H.P. and Lim, S.P. and Lin, W.Z. and Lee, K.H. and Wu, C.G.},
  year = {2002},
  month = may,
  journal = {Journal of Sound and Vibration},
  volume = {252},
  number = {3},
  pages = {527--544},
  issn = {0022460X},
  doi = {10.1006/jsvi.2001.4041},
  langid = {english}
}

@inproceedings{liang_single-loop_2004,
  title = {A {{Single-Loop Method}} for {{Reliability-Based Design Optimization}}},
  author = {Liang, Jinghong and Mourelatos, Zissimos P. and Tu, Jian},
  year = {2004},
  volume = {2004},
  pages = {419--430},
  publisher = {{ASME}},
  doi = {10.1115/DETC2004-57255},
  abstract = {Reliability-Based Design Optimization (RBDO) can provide optimum designs in the presence of uncertainty. It can therefore, be a powerful tool for design under uncertainty. The traditional, double-loop RBDO algorithm requires nested optimization loops, where the design optimization (outer) loop, repeatedly calls a series of reliability (inner) loops. Due to the nested optimization loops, the computational effort can be prohibitive for practical problems. A single-loop RBDO algorithm is proposed in this paper for both normal and nonnormal random variables. Its accuracy is the same with the double-loop approach and its efficiency is almost equivalent to deterministic optimization. It collapses the nested optimization loops into an equivalent single-loop optimization process by imposing the Karush-Kuhn-Tucker optimality conditions of the reliability loops as equivalent deterministic equality constraints of the design optimization loop. It therefore, converts the probabilistic optimization problem into an equivalent deterministic optimization problem, eliminating the need for calculating the Most Probable Point (MPP) in repeated reliability assessments. Several numerical applications including an automotive vehicle side impact example, demonstrate the accuracy and superior efficiency of the proposed single-loop RBDO algorithm.},
  isbn = {978-0-7918-4694-0},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XNMVG9MM\\Liang et al. - 2004 - A Single-Loop Method for Reliability-Based Design .pdf}
}

@article{liaw_tune_2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.05118 [cs, stat]},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5E8Q5SK9\\Liaw et al. - 2018 - Tune A Research Platform for Distributed Model Se.pdf;C\:\\Users\\a846735\\Zotero\\storage\\3BIVTRK8\\1807.html}
}

@article{lieberman_parameter_2010,
  title = {Parameter and {{State Model Reduction}} for {{Large-Scale Statistical Inverse Problems}}},
  author = {Lieberman, Chad and Willcox, Karen and Ghattas, Omar},
  year = {2010},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {32},
  number = {5},
  pages = {2523--2542},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/090775622},
  langid = {english},
  keywords = {MCMC,MOR methods,reduction parameter space,stochastic inverse problem},
  file = {C\:\\Users\\Victor\\Downloads\\60569.pdf}
}

@article{lim_likelihood_2010,
  title = {Likelihood Ratio Tests of Correlated Multivariate Samples},
  author = {Lim, Johan and Li, Erning and Lee, Shin-Jae},
  year = {2010},
  month = mar,
  journal = {Journal of Multivariate Analysis},
  volume = {101},
  number = {3},
  pages = {541--554},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2009.10.011},
  abstract = {We develop methods to compare multiple multivariate normally distributed samples which may be correlated. The methods are new in the context that no assumption is made about the correlations among the samples. Three types of null hypotheses are considered: equality of mean vectors, homogeneity of covariance matrices, and equality of both mean vectors and covariance matrices. We demonstrate that the likelihood ratio test statistics have finite-sample distributions that are functions of two independent Wishart variables and dependent on the covariance matrix of the combined multiple populations. Asymptotic calculations show that the likelihood ratio test statistics converge in distribution to central Chi-squared distributions under the null hypotheses regardless of how the populations are correlated. Following these theoretical findings, we propose a resampling procedure for the implementation of the likelihood ratio tests in which no restrictive assumption is imposed on the structures of the covariance matrices. The empirical size and power of the test procedure are investigated for various sample sizes via simulations. Two examples are provided for illustration. The results show good performance of the methods in terms of test validity and power.},
  langid = {english},
  keywords = {Correlated samples,Empirical rejection probability,Equality of mean vectors,Homogeneity of covariance matrices,Multivariate analysis,Resampling},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XAK8H8VG\\Lim et al. - 2010 - Likelihood ratio tests of correlated multivariate .pdf;C\:\\Users\\a846735\\Zotero\\storage\\6A7JNLPI\\S0047259X09002024.html}
}

@inproceedings{lin_bayesian_2006,
  title = {Bayesian {{L1-Norm Sparse Learning}}},
  author = {Lin, Yuanqing and Lee, Daniel},
  year = {2006},
  month = jun,
  volume = {5},
  pages = {V-V},
  doi = {10.1109/ICASSP.2006.1661348},
  abstract = {We propose a Bayesian framework for learning the optimal regularization parameter in the L1-norm penalized least-mean-square (LMS) problem, also known as LASSO (R. Tibshirani, 1996) or basis pursuit (S.S. Chen et al., 1998). The setting of the regularization parameter is critical for deriving a correct solution. In most existing methods, the scalar regularization parameter is often determined in a heuristic manner; in contrast, our approach infers the optimal regularization setting under a Bayesian framework. Furthermore, Bayesian inference enables an independent regularization scheme where each coefficient (or weight) is associated with an independent regularization parameter. Simulations illustrate the improvement using our method in discovering sparse structure from noisy data},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UVXTNNG2\\Lin et Lee - 2006 - Bayesian L1-Norm Sparse Learning.pdf}
}

@article{lindgren_spde_2021,
  title = {The {{SPDE}} Approach for {{Gaussian}} and Non-{{Gaussian}} Fields: 10 Years and Still Running},
  shorttitle = {The {{SPDE}} Approach for {{Gaussian}} and Non-{{Gaussian}} Fields},
  author = {Lindgren, Finn and Bolin, David and Rue, H{\aa}vard},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.01084 [stat]},
  eprint = {2111.01084},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Gaussian processes and random fields have a long history, covering multiple approaches to representing spatial and spatio-temporal dependence structures, such as covariance functions, spectral representations, reproducing kernel Hilbert spaces, and graph based models. This article describes how the stochastic partial differential equation approach to generalising Mate\textasciiacute rn covariance models via Hilbert space projections connects with several of these approaches, with each connection being useful in different situations. In addition to an overview of the main ideas, some important extensions, theory, applications, and other recent developments are discussed. The methods include both Markovian and non-Markovian models, non-Gaussian random fields, non-stationary fields and space-time fields on arbitrary manifolds, and practical computational considerations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {60G60 (Primary); 60G60; 62M40; 62-08 (Secondary),Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9QLKC3X7\\Lindgren et al. - 2021 - The SPDE approach for Gaussian and non-Gaussian fi.pdf}
}

@article{liu_variational_2013,
  title = {Variational Algorithms for Marginal {{MAP}}},
  author = {Liu, Qiang and Ihler, Alexander},
  year = {2013},
  journal = {The Journal of Machine Learning Research},
  volume = {14},
  number = {1},
  pages = {3165--3200},
  keywords = {EM algorithm,MMAP},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\USN5RNR6\\liu13b.pdf}
}

@article{lorenz_designing_2005,
  title = {Designing {{Chaotic Models}}},
  author = {Lorenz, Edward N.},
  year = {2005},
  month = may,
  journal = {Journal of the Atmospheric Sciences},
  volume = {62},
  number = {5},
  pages = {1574--1587},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/JAS3430.1},
  abstract = {Abstract After enumerating the properties of a simple model that has been used to simulate the behavior of a scalar atmospheric quantity at one level and one latitude, this paper describes the process of designing one modification to produce smoother variations from one longitude to the next and another to produce small-scale activity superposed on smooth large-scale waves. Use of the new models is illustrated by applying them to the problem of the growth of errors in weather prediction and, not surprisingly, they indicate that only limited improvement in prediction can be attained by improving the analysis but not the operational model, or vice versa. Additional applications and modifications are suggested.},
  chapter = {Journal of the Atmospheric Sciences},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7M98FBIF\\Lorenz - 2005 - Designing Chaotic Models.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KHLSVLJY\\jas3430.1.html}
}

@article{lorenz_deterministic_1963,
  title = {Deterministic Nonperiodic Flow},
  author = {Lorenz, Edward N.},
  year = {1963},
  journal = {Journal of the atmospheric sciences},
  volume = {20},
  number = {2},
  pages = {130--141},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\E5EJ45IY\\Lorenz - 1963 - Deterministic nonperiodic flow.pdf;C\:\\Users\\a846735\\Zotero\\storage\\IWFXT9YF\\1520-0469(1963)0200130DNF2.0.html}
}

@article{lu_limitations_2015,
  title = {Limitations of Polynomial Chaos Expansions in the {{Bayesian}} Solution of Inverse Problems},
  author = {Lu, Fei and Morzfeld, Matthias and Tu, Xuemin and Chorin, Alexandre J.},
  year = {2015},
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {282},
  eprint = {1404.7188},
  eprinttype = {arxiv},
  pages = {138--147},
  issn = {00219991},
  doi = {10.1016/j.jcp.2014.11.010},
  abstract = {Polynomial chaos expansions are used to reduce the computational cost in the Bayesian solutions of inverse problems by creating a surrogate posterior that can be evaluated inexpensively. We show, by analysis and example, that when the data contain significant information beyond what is assumed in the prior, the surrogate posterior can be very different from the posterior, and the resulting estimates become inaccurate. One can improve the accuracy by adaptively increasing the order of the polynomial chaos, but the cost may increase too fast for this to be cost effective compared to Monte Carlo sampling without a surrogate posterior.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7UYTAW8U\\Lu et al. - 2015 - Limitations of polynomial chaos expansions in the .pdf;C\:\\Users\\a846735\\Zotero\\storage\\IX3UP4AL\\LMTC14.pdf;C\:\\Users\\a846735\\Zotero\\storage\\K987393G\\1404.html}
}

@article{luedtke_sample_2008,
  title = {A Sample Approximation Approach for Optimization with Probabilistic Constraints},
  author = {Luedtke, James and Ahmed, Shabbir},
  year = {2008},
  journal = {SIAM Journal on Optimization},
  volume = {19},
  number = {2},
  pages = {674--699},
  publisher = {{SIAM}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\E47PNBV3\\Luedtke et Ahmed - 2008 - A sample approximation approach for optimization w.pdf;C\:\\Users\\a846735\\Zotero\\storage\\GVPGI4AX\\070702928.html}
}

@article{lunz_learned_2020,
  title = {On {{Learned Operator Correction}} in {{Inverse Problems}}},
  author = {Lunz, Sebastian and Hauptmann, Andreas and Tarvainen, Tanja and Sch{\"o}nlieb, Carola-Bibiane and Arridge, Simon},
  year = {2020},
  month = oct,
  journal = {arXiv:2005.07069 [cs, eess, math]},
  eprint = {2005.07069},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, math},
  abstract = {We discuss the possibility to learn a data-driven explicit model correction for inverse problems and whether such a model correction can be used within a variational framework to obtain regularised reconstructions. This paper discusses the conceptual difficulty to learn such a forward model correction and proceeds to present a possible solution as forward-adjoint correction that explicitly corrects in both data and solution spaces. We then derive conditions under which solutions to the variational problem with a learned correction converge to solutions obtained with the correct operator. The proposed approach is evaluated on an application to limited view photoacoustic tomography and compared to the established framework of Bayesian approximation error method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WWXGIULV\\Lunz et al. - 2020 - On Learned Operator Correction in Inverse Problems.pdf}
}

@article{luong_variational_1998,
  title = {A Variational Method for the Resolution of a Data Assimilation Problem in Oceanography},
  author = {Luong, Bruno and Blum, Jacques and Verron, Jacques},
  year = {1998},
  month = aug,
  journal = {Inverse Problems},
  volume = {14},
  number = {4},
  pages = {979--997},
  publisher = {{IOP Publishing}},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/14/4/014},
  abstract = {We consider the assimilation of satellite altimetric data into a general circulation model of the ocean at basin scale. The satellite observes only the sea-surface height of the ocean. With the assimilation of these data, we aim at reconstructing the four-dimensional space-time circulation of the ocean including the vertical. This problem is solved using the variational technique and the adjoint method. In the present case, a strong constraint approach is assumed, i.e. the quasi-geostrophic ocean circulation model used is assumed to be exact. The control vector is chosen as being the initial state of the dynamical system and it should minimize the mean-square difference between the model solution and the observed data. The assimilation procedure has been implemented and has the ability to transfer the surface data information downward to the deep flows, and hence to reconstruct the oceanic circulation in the various layers used to describe the vertical stratification of the ocean. The paper points out more specifically the crucial influence of the choice of the norm in the vector control space on the convergence speed of the optimization algorithm. Furthermore, various temporal strategies to perform the assimilation are presented and discussed with regard to their ability to properly control the initial state (which is the actual control variable) and the final state.},
  langid = {english}
}

@article{mack_attention-based_2020,
  title = {Attention-Based {{Convolutional Autoencoders}} for {{3D-Variational Data Assimilation}}},
  author = {Mack, Julian and Arcucci, Rossella and {Molina-Solana}, Miguel and Guo, Yi-Ke},
  year = {2020},
  month = dec,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {372},
  eprint = {2101.02121},
  eprinttype = {arxiv},
  pages = {113291},
  issn = {00457825},
  doi = {10.1016/j.cma.2020.113291},
  abstract = {We propose a new 'Bi-Reduced Space' approach to solving 3D Variational Data Assimilation using Convolutional Autoencoders. We prove that our approach has the same solution as previous methods but has significantly lower computational complexity; in other words, we reduce the computational cost without affecting the data assimilation accuracy. We tested the new method with data from a real-world application: a pollution model of a site in Elephant and Castle, London and found that we could reduce the size of the background covariance matrix representation by O(10\^3) and, at the same time, increase our data assimilation accuracy with respect to existing reduced space methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KCKFCLQP\\Mack et al. - 2020 - Attention-based Convolutional Autoencoders for 3D-.pdf;C\:\\Users\\a846735\\Zotero\\storage\\IAHRAR7G\\2101.html}
}

@misc{mackay_bayesian_nodate,
  title = {Bayesian {{Interpolation}}},
  author = {MacKay, David J.C.},
  howpublished = {https://authors.library.caltech.edu/13792/1/MACnc92a.pdf},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8VR5W2PJ\\MACnc92a.pdf}
}

@book{mackay_information_2003,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David JC},
  year = {2003},
  publisher = {{Cambridge university press}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\APYP3J87\\MacKay - 2003 - Information theory, inference and learning algorit.pdf;C\:\\Users\\a846735\\Zotero\\storage\\93KA4AD4\\books.html}
}

@article{mackay_information_nodate,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  author = {MacKay, David J C},
  pages = {640},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9WVZ9ZQQ\\MacKay - Information Theory, Inference, and Learning Algori.pdf}
}

@article{mackay_information-based_1992,
  title = {Information-{{Based Objective Functions}} for {{Active Data Selection}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = jul,
  journal = {Neural Computation},
  volume = {4},
  number = {4},
  pages = {590--604},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1992.4.4.590},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XHG73IPQ\\MacKay - 1992 - Information-Based Objective Functions for Active D.pdf}
}

@inproceedings{macqueen_methods_1967,
  title = {Some Methods for Classification and Analysis of Multivariate Observations},
  booktitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}, {{Volume}} 1: {{Statistics}}},
  author = {MacQueen, J.},
  year = {1967},
  publisher = {{The Regents of the University of California}},
  issn = {0097-0433},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LQMVWVCV\\MacQueen - 1967 - Some methods for classification and analysis of mu.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LJTGXGLG\\1200512992.html}
}

@inproceedings{mahalanobis_generalized_1936,
  title = {On the Generalized Distance in Statistics},
  author = {Mahalanobis, Prasanta Chandra},
  year = {1936},
  publisher = {{National Institute of Science of India}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GBQM6A93\\Mahalanobis - 1936 - On the generalized distance in statistics.pdf}
}

@article{maier_uncertain_2016,
  title = {An Uncertain Future, Deep Uncertainty, Scenarios, Robustness and Adaptation: {{How}} Do They Fit Together?},
  shorttitle = {An Uncertain Future, Deep Uncertainty, Scenarios, Robustness and Adaptation},
  author = {Maier, H.R. and Guillaume, J.H.A. and {van Delden}, H. and Riddell, G.A. and Haasnoot, M. and Kwakkel, J.H.},
  year = {2016},
  month = jul,
  journal = {Environmental Modelling \& Software},
  volume = {81},
  pages = {154--164},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2016.03.014},
  abstract = {A highly uncertain future due to changes in climate, technology and socio-economics has led to the realisation that identification of ``best-guess'' future conditions might no longer be appropriate. Instead, multiple plausible futures need to be considered, which requires (i) uncertainties to be described with the aid of scenarios that represent coherent future pathways based on different sets of assumptions, (ii) system performance to be represented by metrics that measure insensitivity (i.e. robustness) to changes in future conditions, and (iii) adaptive strategies to be considered alongside their more commonly used static counterparts. However, while these factors have been considered in isolation previously, there has been a lack of discussion of the way they are connected. In order to address this shortcoming, this paper presents a multidisciplinary perspective on how the above factors fit together to facilitate the development of strategies that are best suited to dealing with a deeply uncertain future.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UPRN7L7S\\Maier et al. - 2016 - An uncertain future, deep uncertainty, scenarios, .pdf}
}

@article{malherbe_global_2017,
  title = {Global Optimization of {{Lipschitz}} Functions},
  author = {Malherbe, C{\'e}dric and Vayatis, Nicolas},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.02628 [stat]},
  eprint = {1703.02628},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H\textasciidieresis older like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\R7XXF9DC\\Malherbe et Vayatis - 2017 - Global optimization of Lipschitz functions.pdf}
}

@misc{malmberg_argmax_nodate,
  title = {Argmax over {{Continuous Indices}} of {{Random Variables}} : {{An Approach Us-}} Ing {{Random Fields}}},
  author = {Malmberg, Hannes},
  keywords = {argmax measure},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QD8FUL8W\\report.pdf}
}

@book{manski_structural_1981,
  title = {Structural Analysis of Discrete Data with Econometric Applications},
  author = {Manski, Charles F. and McFadden, Daniel},
  year = {1981},
  publisher = {{Mit Press Cambridge, MA}}
}

@article{marchesiello_open_2001,
  title = {Open Boundary Conditions for Long-Term Integration of Regional Oceanic Models},
  author = {Marchesiello, Patrick and McWilliams, James C. and Shchepetkin, Alexander},
  year = {2001},
  journal = {Ocean modelling},
  volume = {3},
  number = {1-2},
  pages = {1--20},
  publisher = {{Elsevier}},
  file = {/home/victor/TÃ©lÃ©chargements/marchesiello_omod2001.pdf;C\:\\Users\\a846735\\Zotero\\storage\\TVAL7K5I\\S1463500300000135.html}
}

@article{markowitz_portfolio_1952,
  title = {Portfolio Selection},
  author = {Markowitz, Harry},
  year = {1952},
  journal = {The journal of finance},
  volume = {7},
  number = {1},
  pages = {77--91},
  keywords = {Bias Variance tradeoff,Portfolio selection},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HWSVRRWD\\Markowitz - 1952 - Portfolio selection;C\:\\Users\\a846735\\Zotero\\storage\\T6TEETNQ\\Markowitz - 1952 - Portfolio selection.pdf}
}

@article{marler_weighted_2010,
  title = {The Weighted Sum Method for Multi-Objective Optimization: New Insights},
  shorttitle = {The Weighted Sum Method for Multi-Objective Optimization},
  author = {Marler, R. Timothy and Arora, Jasbir S.},
  year = {2010},
  month = jun,
  journal = {Structural and Multidisciplinary Optimization},
  volume = {41},
  number = {6},
  pages = {853--862},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-009-0460-7},
  abstract = {As a common concept in multi-objective optimization, minimizing a weighted sum constitutes an independent method as well as a component of other methods. Consequently, insight into characteristics of the weighted sum method has far reaching implications. However, despite the many published applications for this method and the literature addressing its pitfalls with respect to depicting the Pareto optimal set, there is little comprehensive discussion concerning the conceptual significance of the weights and techniques for maximizing the effectiveness of the method with respect to a priori articulation of preferences. Thus, in this paper, we investigate the fundamental significance of the weights in terms of preferences, the Pareto optimal set, and objective-function values. We determine the factors that dictate which solution point results from a particular set of weights. Fundamental deficiencies are identified in terms of a priori articulation of preferences, and guidelines are provided to help avoid blind use of the method.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EJS8CI6X\\Marler et Arora - 2010 - The weighted sum method for multi-objective optimi.pdf}
}

@unpublished{marrel_global_2010,
  title = {Global Sensitivity Analysis for Models with Spatially Dependent Outputs},
  author = {Marrel, Amandine and Iooss, Bertrand and Jullien, Michel and Laurent, Beatrice and Volkova, Elena},
  year = {2010},
  month = feb,
  abstract = {The global sensitivity analysis of a complex numerical model often requires the estimation of variance-based importance measures, called Sobol' indices. Metamodel-based techniques have been developed in order to replace the cpu time expensive computer code with an inexpensive mathematical function, predicting the computer code output. The common metamodel-based sensitivity analysis methods are appropriate with computer codes having scalar model output. However, in the environmental domain, as in many areas of application, numerical models often give as output a spatial map, which is sometimes a spatio-temporal evolution, of some interest variables. In this paper, we introduce a novel way to obtain a spatial map of Sobol' indices with a minimal number of numerical model computations. It is based on the functional decomposition of the spatial output onto a wavelet basis and the metamodeling of the wavelet coefficients by Gaussian process. An analytical example allows us to clarify the various steps of our methodology. This technique is then applied to a real case of hydrogeological modeling: for each model input variable, a spatial map of Sobol' indices is thus obtained.},
  keywords = {AS,Gaussian process,Kriging,Monte-Carlo estimation,Sobol',wavelet},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Q6NIH5TD\\Marrel et al. - 2010 - Global sensitivity analysis for models with spatia.pdf}
}

@article{martens_new_nodate,
  title = {New {{Insights}} and {{Perspectives}} on the {{Natural Gradient Method}}},
  author = {Martens, James},
  pages = {76},
  abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used ``empirical'' approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature matrices, but notably not the Hessian).},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JLSZB74N\\Martens - New Insights and Perspectives on the Natural Gradi.pdf}
}

@article{martins_multidisciplinary_2013,
  title = {Multidisciplinary {{Design Optimization}}: {{A Survey}} of {{Architectures}}},
  shorttitle = {Multidisciplinary {{Design Optimization}}},
  author = {Martins, Joaquim and Lambe, Andrew},
  year = {2013},
  month = sep,
  journal = {AIAA Journal},
  volume = {51},
  pages = {2049--2075},
  doi = {10.2514/1.J051895},
  abstract = {Multidisciplinary design optimization is a field of research that studies the application of numerical optimization techniques to the design of engineering systems involving multiple disciplines or components. Since the inception of multidisciplinary design optimization, various methods (architectures) have been developed and applied to solve multidisciplinary design-optimization problems. This paper provides a survey of all the architectures that have been presented in the literature so far. All architectures are explained in detail using a unified description that includes optimization problem statements, diagrams, and detailed algorithms. The diagrams show both data and process flow through the multidisciplinary system and computational elements, which facilitate the understanding of the various architectures, and how they relate to each other. A classification of the multidisciplinary design-optimization architectures based on their problem formulations and decomposition strategies is also provided, and the benefits and drawbacks of the architectures are discussed from both theoretical and experimental perspectives. For each architecture, several applications to the solution of engineering-design problems are cited. The result is a comprehensive but straightforward introduction to multidisciplinary design optimization for nonspecialists and a reference detailing all current multidisciplinary design-optimization architectures for specialists. Read More: http://arc.aiaa.org/doi/abs/10.2514/1.J051895},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZVYGF8MG\\Martins et Lambe - 2013 - Multidisciplinary Design Optimization A Survey of.pdf}
}

@article{martinsson_randomized_2011,
  title = {A Randomized Algorithm for the Decomposition of Matrices},
  author = {Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
  year = {2011},
  month = jan,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {30},
  number = {1},
  pages = {47--68},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2010.02.003},
  abstract = {Given an m\texttimes n matrix A and a positive integer k, we describe a randomized procedure for the approximation of A with a matrix Z of rank k. The procedure relies on applying AT to a collection of l random vectors, where l is an integer equal to or slightly greater than k; the scheme is efficient whenever A and AT can be applied rapidly to arbitrary vectors. The discrepancy between A and Z is of the same order as lm times the (k+1)st greatest singular value {$\sigma$}k+1 of A, with negligible probability of even moderately large deviations. The actual estimates derived in the paper are fairly complicated, but are simpler when l-k is a fixed small nonnegative integer. For example, according to one of our estimates for l-k=20, the probability that the spectral norm {$\Vert$}A-Z{$\Vert$} is greater than 10(k+20)m{$\sigma$}k+1 is less than 10-17. The paper contains a number of estimates for {$\Vert$}A-Z{$\Vert$}, including several that are stronger (but more detailed) than the preceding example; some of the estimates are effectively independent of m. Thus, given a matrix A of limited numerical rank, such that both A and AT can be applied rapidly to arbitrary vectors, the scheme provides a simple, efficient means for constructing an accurate approximation to a singular value decomposition of A. Furthermore, the algorithm presented here operates reliably independently of the structure of the matrix A. The results are illustrated via several numerical examples.},
  langid = {english},
  keywords = {Algorithm,Lanczos,Low rank,Matrix,Randomized,SVD},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UGRK9EIA\\Martinsson et al. - 2011 - A randomized algorithm for the decomposition of ma.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PVV2Q5PW\\S1063520310000242.html}
}

@article{marzat_worst-case_2013,
  title = {Worst-Case Global Optimization of Black-Box Functions through {{Kriging}} and Relaxation},
  author = {Marzat, Julien and Walter, Eric and {Piet-Lahanier}, H{\'e}l{\`e}ne},
  year = {2013},
  month = apr,
  journal = {Journal of Global Optimization},
  volume = {55},
  number = {4},
  pages = {707--727},
  issn = {0925-5001, 1573-2916},
  doi = {10.1007/s10898-012-9899-y},
  abstract = {A new algorithm is proposed to deal with the worst-case optimization of black-box functions evaluated through costly computer simulations. The input variables of these computer experiments are assumed to be of two types. Control variables must be tuned while environmental variables have an undesirable effect, to which the design of the control variables should be robust. The algorithm to be proposed searches for a minimax solution, i.e., values of the control variables that minimize the maximum of the objective function with respect to the environmental variables. The problem is particularly difficult when the control and environmental variables live in continuous spaces. Combining a relaxation procedure with Krigingbased optimization makes it possible to deal with the continuity of the variables and the fact that no analytical expression of the objective function is available in most real-case problems. Numerical experiments are conducted to assess the accuracy and efficiency of the algorithm, both on analytical test functions with known results and on an engineering application.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F6CV6LGA\\Marzat et al. - 2013 - Worst-case global optimization of black-box functi.pdf}
}

@article{marzouk_introduction_2016,
  title = {An Introduction to Sampling via Measure Transport},
  author = {Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
  year = {2016},
  journal = {arXiv:1602.05023 [math, stat]},
  eprint = {1602.05023},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  pages = {1--41},
  doi = {10.1007/978-3-319-11259-6_23-1},
  abstract = {We present the fundamentals of a measure transport approach to sampling. The idea is to construct a deterministic coupling\textemdash i.e., a transport map\textemdash between a complex ``target'' probability measure of interest and a simpler reference measure. Given a transport map, one can generate arbitrarily many independent and unweighted samples from the target simply by pushing forward reference samples through the map. If the map is endowed with a triangular structure, one can also easily generate samples from conditionals of the target measure. We consider two different and complementary scenarios: first, when only evaluations of the unnormalized target density are available, and second, when the target distribution is known only through a finite collection of samples. We show that in both settings the desired transports can be characterized as the solutions of variational problems. We then address practical issues associated with the optimization\textendash based construction of transports: choosing finite-dimensional parameterizations of the map, enforcing monotonicity, quantifying the error of approximate transports, and refining approximate transports by enriching the corresponding approximation spaces. Approximate transports can also be used to ``Gaussianize'' complex distributions and thus precondition conventional asymptotically exact sampling schemes. We place the measure transport approach in broader context, describing connections with other optimization\textendash based samplers, with inference and density estimation schemes using optimal transport, and with alternative transformation\textendash based approaches to simulation. We also sketch current work aimed at the construction of transport maps in high dimensions, exploiting essential features of the target distribution (e.g., conditional independence, low-rank structure). The approaches and algorithms presented here have direct applications to Bayesian computation and to broader problems of stochastic simulation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Probability,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LY7BKMI9\\Marzouk et al. - 2016 - An introduction to sampling via measure transport.pdf}
}

@book{massart_concentration_2007,
  title = {Concentration Inequalities and Model Selection},
  author = {Massart, Pascal},
  year = {2007},
  volume = {6},
  publisher = {{Springer}},
  keywords = {Model selection},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\D6LDMJUI\\flour.pdf}
}

@article{massart_merits_2010,
  title = {On the {{Merits}} of {{Using}} a {{3D-FGAT Assimilation Scheme}} with an {{Outer Loop}} for {{Atmospheric Situations Governed}} by {{Transport}}},
  author = {Massart, S{\'e}bastien and Pajot, Benjamin and Piacentini, Andrea and Pannekoucke, Olivier},
  year = {2010},
  month = dec,
  journal = {Monthly Weather Review},
  volume = {138},
  number = {12},
  pages = {4509--4522},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/2010MWR3237.1},
  abstract = {Abstract Three-dimensional variational data assimilation (3D-Var) with the first guess at appropriate time (FGAT) appears to be an attractive compromise between accuracy and overall computing time. It is computationally cheaper than four-dimensional (4D)-Var as the increment is not propagated back and forth in time by a model, yet the comparison between the model and the observations is still computed at the right observation time. An interesting feature of the 4D-Var is the iterative process known as the outer loop. This outer-loop approach can also be used in conjunction with 3D-FGAT. But it requires the application of the 3D-FGAT analysis increment at the beginning of the assimilation window. The pros and cons of using this unusual 3D-FGAT variant are illustrated in this paper on two applications focused on the transport, one of the main phenomena governing the atmospheric evolution. The first one is the one-dimensional advection of a passive tracer. By three representative situations, it shows the benefits of the outer loop, except for practical situations driven by very rapid dynamics such as a zonal wind of 50 m s-1 on the earth's great circle, when the assimilation window has a size of 3 h. The second application is the 3D-FGAT assimilation of true ozone measurements into a chemical\textendash transport model. It confirms the previous results, showing that the 3D-FGAT analysis with the outer loop produces an overestimation of the ozone increment in regions where the wind speed is high compared to the time length of the assimilation window.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\K5TXSGHB\\Massart et al. - 2010 - On the Merits of Using a 3D-FGAT Assimilation Sche.pdf;C\:\\Users\\a846735\\Zotero\\storage\\LEW8YRA5\\2010mwr3237.1.html}
}

@book{matheron_traite_1962,
  title = {Trait\'e de G\'eostatistique Appliqu\'ee. 1 (1962)},
  author = {Matheron, Georges},
  year = {1962},
  volume = {1},
  publisher = {{Editions Technip}}
}

@article{mayer_diversity_2016,
  title = {Diversity of Immune Strategies Explained by Adaptation to Pathogen Statistics},
  author = {Mayer, Andreas and Mora, Thierry and Rivoire, Olivier and Walczak, Aleksandra M.},
  year = {2016},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  pages = {201600663},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1600663113},
  abstract = {Biological organisms have evolved a wide range of immune mechanisms to defend themselves against pathogens. Beyond molecular details, these mechanisms differ in how protection is acquired, processed, and passed on to subsequent generations\textemdash differences that may be essential to long-term survival. Here, we introduce a mathematical framework to compare the long-term adaptation of populations as a function of the pathogen dynamics that they experience and of the immune strategy that they adopt. We find that the two key determinants of an optimal immune strategy are the frequency and the characteristic timescale of the pathogens. Depending on these two parameters, our framework identifies distinct modes of immunity, including adaptive, innate, bet-hedging, and CRISPR-like immunities, which recapitulate the diversity of natural immune systems.},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {27432970},
  keywords = {adaptive immunity,bet hedging,CRISPR immunity,evolution of immunity,immune systems},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\STEVNHSJ\\Mayer et al. - 2016 - Diversity of immune strategies explained by adapta.pdf;C\:\\Users\\a846735\\Zotero\\storage\\Q467KF27\\1600663113.html}
}

@incollection{mayerhofer_reduced_2017,
  title = {A {{Reduced Basis Method}} for {{Parameter Functions Using Wavelet Approximations}}},
  booktitle = {Model {{Reduction}} of {{Parametrized Systems}}},
  author = {Mayerhofer, Antonia and Urban, Karsten},
  year = {2017},
  pages = {77--90},
  publisher = {{Springer}},
  keywords = {reduced basis,wavelet},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BTAFLU6L\\MU2-final.pdf}
}

@article{mcinnes_umap_2018,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John},
  year = {2018},
  month = feb,
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP as described has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IPIJTIRU\\McInnes et Healy - 2018 - UMAP Uniform Manifold Approximation and Projectio.pdf}
}

@article{mckay_comparison_2000,
  title = {A {{Comparison}} of {{Three Methods}} for {{Selecting Values}} of {{Input Variables}} in the {{Analysis}} of {{Output}} from a {{Computer Code}}},
  author = {Mckay, M. D. and Beckman, R. J. and Conover, W. J.},
  year = {2000},
  month = feb,
  journal = {Technometrics},
  volume = {42},
  number = {1},
  pages = {55},
  issn = {00401706},
  doi = {10.2307/1271432},
  keywords = {Sampling},
  file = {C\:\\Users\\Victor\\Downloads\\McKayConoverBeckman.pdf}
}

@article{mclachlan_mahalanobis_1999,
  title = {Mahalanobis Distance},
  author = {McLachlan, Goeffrey J.},
  year = {1999},
  journal = {Resonance},
  volume = {4},
  number = {6},
  pages = {20--26},
  publisher = {{Springer}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VGIE6RZ4\\McLachlan - 1999 - Mahalanobis distance.pdf;C\:\\Users\\a846735\\Zotero\\storage\\INZKHX8J\\10.html}
}

@book{mclachlan_mixture_1988,
  title = {Mixture Models: {{Inference}} and Applications to Clustering},
  shorttitle = {Mixture Models},
  author = {McLachlan, Geoffrey J. and Basford, Kaye E.},
  year = {1988},
  volume = {84},
  publisher = {{Marcel Dekker}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\N6ARLX48\\UQ308790.html}
}

@article{mcphail_robustness_2018,
  title = {Robustness {{Metrics}}: {{How Are They Calculated}}, {{When Should They Be Used}} and {{Why Do They Give Different Results}}?},
  shorttitle = {Robustness {{Metrics}}},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  year = {2018},
  journal = {Earth's Future},
  volume = {6},
  number = {2},
  pages = {169--191},
  issn = {2328-4277},
  doi = {10.1002/2017EF000649},
  abstract = {Robustness is being used increasingly for decision analysis in relation to deep uncertainty and many metrics have been proposed for its quantification. Recent studies have shown that the application of different robustness metrics can result in different rankings of decision alternatives, but there has been little discussion of what potential causes for this might be. To shed some light on this issue, we present a unifying framework for the calculation of robustness metrics, which assists with understanding how robustness metrics work, when they should be used, and why they sometimes disagree. The framework categorizes the suitability of metrics to a decision-maker based on (1) the decision-context (i.e., the suitability of using absolute performance or regret), (2) the decision-maker's preferred level of risk aversion, and (3) the decision-maker's preference toward maximizing performance, minimizing variance, or some higher-order moment. This article also introduces a conceptual framework describing when relative robustness values of decision alternatives obtained using different metrics are likely to agree and disagree. This is used as a measure of how ``stable'' the ranking of decision alternatives is when determined using different robustness metrics. The framework is tested on three case studies, including water supply augmentation in Adelaide, Australia, the operation of a multipurpose regulated lake in Italy, and flood protection for a hypothetical river based on a reach of the river Rhine in the Netherlands. The proposed conceptual framework is confirmed by the case study results, providing insight into the reasons for disagreements between rankings obtained using different robustness metrics.},
  copyright = {\textcopyright{} 2018 The Authors.},
  langid = {english},
  keywords = {decision analysis,deep uncertainty,ranking stability,robustness metrics,water systems},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SUZY5W3T\\McPhail et al. - 2018 - Robustness Metrics How Are They Calculated, When .pdf;C\:\\Users\\a846735\\Zotero\\storage\\J8QMG2HC\\2017EF000649.html}
}

@article{mcwilliams_irreducible_2007,
  title = {Irreducible Imprecision in Atmospheric and Oceanic Simulations},
  author = {McWilliams, J. C.},
  year = {2007},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {21},
  pages = {8709--8713},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0702971104},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JXQ342GR\\McWilliams - 2007 - Irreducible imprecision in atmospheric and oceanic.pdf;C\:\\Users\\a846735\\Zotero\\storage\\SUTMTMET\\McWilliams - 2007 - Irreducible imprecision in atmospheric and oceanic.pdf}
}

@article{menetrier_oceanographie_2014,
  title = {{Oc\'eanographie Mar\'ees Support de cours}},
  author = {M{\'e}n{\'e}trier, Benjamin},
  year = {2014},
  pages = {38},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NXVGNP9L\\MÃ©nÃ©trier - OcÃ©anographie MarÃ©es Support de cours.pdf}
}

@article{meng_ode-ru_2022,
  title = {{{ODE-RU}}: A Dynamical System View on Recurrent Neural Networks},
  shorttitle = {{{ODE-RU}}},
  author = {Meng, Pinchao and Wang, Xinyu and Yin, Weishi and Meng, Pinchao and Wang, Xinyu and Yin, Weishi},
  year = {2022},
  journal = {Electronic Research Archive},
  volume = {30},
  number = {era-30-01-014},
  pages = {257--271},
  issn = {2688-1594},
  doi = {10.3934/era.2022014},
  abstract = {{$<$}abstract{$><$}p{$>$}The core of the demonstration of this paper is to interpret the forward propagation process of machine learning as a parameter estimation problem of nonlinear dynamical systems. This process is to establish a connection between the Recurrent Neural Network and the discrete differential equation, so as to construct a new network structure: ODE-RU. At the same time, under the inspiration of the theory of ordinary differential equations, we propose a new forward propagation mode. In a large number of simulations and experiments, the forward propagation not only shows the trainability of the new architecture, but also achieves a low training error on the basis of main-taining the stability of the network. For the problem requiring long-term memory, we specifically study the obstacle shape reconstruction problem using the backscattering far-field features data set, and demonstrate the effectiveness of the proposed architecture using the data set. The results show that the network can effectively reduce the sensitivity to small changes in the input feature. And the error generated by the ordinary differential equation cyclic unit network in inverting the shape and position of obstacles is less than \$ 10\^\{-2\} \$.{$<$}/p{$><$}/abstract{$>$}},
  copyright = {2022 The Author(s)},
  langid = {english},
  annotation = {Cc\_license\_type: cc\_by Primary\_atype: Electronic Research Archive Subject\_term: Research article Subject\_term\_id: Research article},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\X2RZY45L\\era.html}
}

@article{mennad_slepian-bangs-type_2018,
  title = {Slepian-{{Bangs-type}} Formulas and the Related {{Misspecified Cram\'er-Rao Bounds}} for {{Complex Elliptically Symmetric}} Distributions},
  author = {Mennad, Abdelmalek and Fortunati, Stefano and El Korso, Mohammed Nabil and Younsi, Arezki and Zoubir, Abdelhak M. and Renaux, Alexandre},
  year = {2018},
  month = jan,
  journal = {Signal Processing},
  volume = {142},
  pages = {320--329},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2017.07.029},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W8DF7GG9\\Mennad et al. - 2018 - Slepian-Bangs-type formulas and the related Misspe.pdf}
}

@article{meyer_machine_nodate,
  title = {Machine {{Learning Emulation}} of {{3D Cloud Radiative Effects}}},
  author = {Meyer, David and Hogan, Robin J and Dueben, Peter D and Mason, Shannon L},
  journal = {Journal of Advances in Modeling Earth Systems},
  pages = {13},
  abstract = {The treatment of cloud structure in numerical weather and climate models is often greatly simplified to make them computationally affordable. Here we propose to correct the European Centre for Medium-Range Weather Forecasts 1D radiation scheme ecRad for 3D cloud effects using computationally cheap neural networks. 3D cloud effects are learned as the difference between ecRad's fast 1D Tripleclouds solver that neglects them and its 3D SPARTACUS (SPeedy Algorithm for Radiative TrAnsfer through CloUd Sides) solver that includes them but is about five times more computationally expensive. With typical errors between 20\% and 30\% of the 3D signal, neural networks improve Tripleclouds' accuracy for about 1\% increase in runtime. Thus, rather than emulating the whole of SPARTACUS, we keep Tripleclouds unchanged for cloudfree parts of the atmosphere and 3D-correct it elsewhere. The focus on the comparably small 3D correction instead of the entire signal allows us to improve predictions significantly if we assume a similar signal-to-noise ratio for both.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B8XITN5X\\Meyer et al. - Machine Learning Emulation of 3D Cloud Radiative E.pdf}
}

@misc{michael_waskom_mwaskom/seaborn:_2017,
  title = {Mwaskom/Seaborn: V0.8.1 ({{September}} 2017)},
  shorttitle = {Mwaskom/Seaborn},
  author = {Michael Waskom and Olga Botvinnik and Drew O'Kane and Paul Hobson and Saulius Lukauskas and David C Gemperline and Tom Augspurger and Yaroslav Halchenko and John B. Cole and Jordi Warmenhoven and {Julian de Ruiter} and Cameron Pye and Stephan Hoyer and Jake Vanderplas and Santi Villalba and Gero Kunter and Eric Quintero and Pete Bachant and Marcel Martin and Kyle Meyer and Alistair Miles and Yoav Ram and Tal Yarkoni and Mike Lee Williams and Constantine Evans and Clark Fitzgerald and Brian and Chris Fonnesbeck and Antony Lee and Adel Qalieh},
  year = {2017},
  month = sep,
  doi = {10.5281/zenodo.883859},
  abstract = {v0.8.1 (September 2017) Added a warning in FacetGrid when passing a categorical plot function without specifying order (or hue\_order when hue is used), which is likely to produce a plot that is incorrect. Improved compatibility between FacetGrid or PairGrid and interactive matplotlib backends so that the legend no longer remains inside the figure when using legend\_out=True. Changed categorical plot functions with small plot elements to use dark\_palette instead of `light\_palette`` when generating a sequential palette from a specified color. Improved robustness of kdeplot and distplot to data with fewer than two observations. Fixed a bug in clustermap when using yticklabels=False. Fixed a bug in pointplot where colors were wrong if exactly three points were being drawn. Fixed a bug inpointplot where legend entries for missing data appeared with empty markers. Fixed a bug in clustermap where an error was raised when annotating the main heatmap and showing category colors. Fixed a bug in clustermap where row labels were not being properly rotated when they overlapped. Fixed a bug in kdeplot where the maximum limit on the density axes was not being updated when multiple densities were drawn. Improved compatibility with future versions of pandas.},
  howpublished = {Zenodo},
  keywords = {software},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\M3DU4FR3\\883859.html}
}

@techreport{miller_surprised_2016,
  type = {{{SSRN Scholarly Paper}}},
  title = {Surprised by the {{Gambler}}'s and {{Hot Hand Fallacies}}? {{A Truth}} in the {{Law}} of {{Small Numbers}}},
  shorttitle = {Surprised by the {{Gambler}}'s and {{Hot Hand Fallacies}}?},
  author = {Miller, Joshua B. and Sanjurjo, Adam},
  year = {2016},
  month = nov,
  number = {ID 2627354},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {We prove that a subtle but substantial bias exists in a standard measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data. The magnitude of this novel form of selection bias generally decreases as the sequence gets longer, but increases in streak length, and remains substantial for a range of sequence lengths often used in empirical work.  The bias has important implications for the literature that investigates incorrect beliefs in sequential decision making---most notably the Hot Hand Fallacy and the Gambler's Fallacy. Upon correcting for the bias, the conclusions of prominent studies in the hot hand fallacy literature are reversed.   The bias also provides a novel structural explanation for how belief in the law of small numbers can persist in the face of experience.},
  langid = {english},
  keywords = {Alternation Bias,Finite Sample Bias,Gambler's Fallacy,Hot Hand Effect,Hot Hand Fallacy,Law of Small Numbers,Negative Recency Bias,Selection Bias,Sequential Data,Sequential Decision Making,Small Sample Bias},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\A9S8ISDT\\papers.html}
}

@inproceedings{miranda_adjoint-based_2016,
  title = {Adjoint-Based {{Robust Optimization}} Using {{Polynomial Chaos Expansions}}},
  booktitle = {Proceedings of the {{VII European Congress}} on {{Computational Methods}} in {{Applied Sciences}} and {{Engineering}} ({{ECCOMAS Congress}} 2016)},
  author = {Miranda, Joao and Kumar, Dinesh and Lacor, Chris},
  year = {2016},
  pages = {8351--8364},
  publisher = {{Institute of Structural Analysis and Antiseismic Research School of Civil Engineering National Technical University of Athens (NTUA) Greece}},
  address = {{Crete Island, Greece}},
  doi = {10.7712/100016.2418.10873},
  abstract = {Adjoint methods are nowadays widely used to efficiently perform optimization for problems with a large number of design variables. However, in reality, the problem at hand might be subjected to uncertainties in the operational conditions or, in case of optimizing geometries, the design variables itself might be uncertain due to manufacturing tolerances. For such applications, the optimum obtained using deterministic methods might be very sensitive to small variations in the uncertainties, i.e. it lacks robustness. In a robust optimization, the uncertainties are taken directly into account during the optimization process by introducing, next to the mean objective, its variance as a second objective. This implies that, when using gradient based optimization methods, the gradients of both objectives (mean and variance) must be known. In this work the Polynomial Chaos Expansion (PCE) is used in combination with adjoint methods to efficiently obtain both gradients. A non intrusive, regression based PCE is used, requiring a new adjoint solution for each sampling point in order to build the PCE of the gradient. A PCE for the objective is also built (at no extra cost) in order to compute the gradient of the variance.},
  isbn = {978-618-82844-0-1},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7SRD672D\\Miranda et al. - 2016 - ADJOINT-BASED ROBUST OPTIMIZATION USING POLYNOMIAL.pdf}
}

@article{moazeni_regularized_2013,
  title = {Regularized Robust Optimization: The Optimal Portfolio Execution Case},
  shorttitle = {Regularized Robust Optimization},
  author = {Moazeni, Somayeh and Coleman, Thomas F. and Li, Yuying},
  year = {2013},
  month = jun,
  journal = {Computational Optimization and Applications},
  volume = {55},
  number = {2},
  pages = {341--377},
  issn = {0926-6003, 1573-2894},
  doi = {10.1007/s10589-012-9526-3},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MV56F6II\\Moazeni et al. - 2013 - Regularized robust optimization the optimal portf.pdf}
}

@incollection{mockus_bayesian_1974,
  title = {On Bayesian Methods for Seeking the Extremum},
  booktitle = {Optimization {{Techniques IFIP Technical Conference Novosibirsk}}, {{July}} 1\textendash 7, 1974},
  author = {Mo{\v c}kus, J.},
  year = {1974},
  month = jul,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {400--404},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-07165-2_55},
  isbn = {978-3-540-07165-5 978-3-540-37497-8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UG95UKH3\\MoÄkus - 1974 - On bayesian methods for seeking the extremum.pdf;C\:\\Users\\a846735\\Zotero\\storage\\MCDFG96M\\3-540-07165-2_55.html}
}

@article{molchanov_lectures_nodate,
  title = {Lectures on Random Sets and Their Applications in Economics and Finance},
  author = {Molchanov, Ilya},
  pages = {32},
  abstract = {This course introduces main concepts from the theory of random sets with emphasis on applications in economics and finance: most importantly inference for partially identified models and transaction costs modelling.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DNHLWW2L\\Molchanov - Lectures on random sets and their applications in .pdf}
}

@article{monnier_data_nodate,
  title = {Data {{Assimilation}}, {{Optimal Control}} \& {{Model Learning}}},
  author = {Monnier, Jerome},
  pages = {216},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y7Z2YBLD\\Monnier - Data Assimilation, Optimal Control & Model Learnin.pdf}
}

@article{monnier_variational_nodate,
  title = {Variational {{Data Assimilation}} and {{Model Learning}}},
  author = {Monnier, Jerome},
  pages = {217},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZV6H6AN3\\Monnier - Variational Data Assimilation and Model Learning.pdf}
}

@article{moradi_robust_2015,
  title = {Robust Control of the Variable Speed Wind Turbines in the Presence of Uncertainties: {{A}} Comparison between {{H}}{$\infty$} and {{PID}} Controllers},
  shorttitle = {Robust Control of the Variable Speed Wind Turbines in the Presence of Uncertainties},
  author = {Moradi, Hamed and Vossoughi, Gholamreza},
  year = {2015},
  month = oct,
  journal = {Energy},
  volume = {90},
  pages = {1508--1521},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2015.06.100},
  abstract = {To achieve a cost-effective and reliable use of wind power generation, advanced control techniques are required. In this paper, the application of two control strategies for the improvement of wind turbine power output is investigated in the presence of model/environmental uncertainties. Rotational speed of the wind turbine and consequently its power output are controlled via manipulation of blades pitch angle (at a constant generator torque). First, the classical PID controller is designed based on root locus analysis while in the second scheme, an H{$\infty$}-robust controller is designed via {$\mu$}-synthesis based on DK-iteration algorithm. Performance of the two controllers in tracking of the desired power outputs (including the step, sequence of steps, ramp and sinusoidal signals) is compared. Results are presented for various profiles of the wind speed. It is shown that H{$\infty$} controller guarantees the robust stability and performance of the uncertain systems. Moreover, when H{$\infty$} controller is implemented, less oscillatory behaviour is observed for both of the output power and pitch angles (which are desired for the electric grid and actuating systems, respectively).},
  langid = {english},
  keywords = {PID control,Power output,Robust control,Tracking,Uncertainty,Wind turbine},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\I25D2UAU\\S0360544215008567.html}
}

@article{morio_global_2011,
  title = {Global and Local Sensitivity Analysis Methods for a Physical System},
  author = {Morio, J{\'e}r{\^o}me},
  year = {2011},
  month = oct,
  journal = {European Journal of Physics},
  volume = {32},
  pages = {1577},
  doi = {10.1088/0143-0807/32/6/011},
  abstract = {Sensitivity analysis is the study of how the different input variations of a mathematical model influence the variability of its output. In this paper, we review the principle of global and local sensitivity analyses of a complex black-box system. A simulated case of application is given at the end of this paper to compare both approaches.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\I2PNH54J\\Morio - 2011 - Global and local sensitivity analysis methods for .pdf}
}

@article{moritz_gohler_robustness_2016,
  title = {Robustness {{Metrics}}: {{Consolidating}} the {{Multiple Approaches}} to {{Quantify Robustness}}},
  shorttitle = {Robustness {{Metrics}}},
  author = {Moritz G{\"o}hler, Simon and Eifler, Tobias and Howard, Thomas J.},
  year = {2016},
  month = sep,
  journal = {Journal of Mechanical Design},
  volume = {138},
  number = {11},
  pages = {111407},
  issn = {1050-0472},
  doi = {10.1115/1.4034112},
  abstract = {The robustness of a design has a major influence on how much the product's performance will vary and is of great concern to design, quality and production engineers. While variability is always central to the definition of robustness, the concept does contain ambiguity and although subtle, this ambiguity can have significant influence on the strategies used to combat variability, the way it is quantified and ultimately, the quality of the final design. In this contribution the literature for robustness metrics was systematically reviewed. From the 108 relevant publications found, 38 metrics were determined to be conceptually different from one another. The metrics were classified by their meaning and interpretation based on the types of information necessary to calculate the metrics. Four different classes were identified: 1) Sensitivity robustness metrics; 2) Size of feasible design space robustness metrics; 3) Functional expectancy and dispersion robustness metrics; and 4) Probability of compliance robustness metrics. The goal was to give a comprehensive overview of robustness metrics and guidance to scholars and practitioners to understand the different types of robustness metrics and to remove the ambiguities of the term robustness. By applying an exemplar metric from each class to a case study, the differences between the classes were further highlighted. These classes form the basis for the definition of four specific sub-definitions of robustness, namely the `robust concept', `robust design', `robust function' and `robust product'.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BFB4RZIJ\\Moritz GÃ¶hler et al. - 2016 - Robustness Metrics Consolidating the Multiple App.pdf}
}

@article{morzfeld_parameter_2015,
  title = {Parameter Estimation by Implicit Sampling},
  author = {Morzfeld, Matthias and Tu, Xuemin and Wilkening, Jon and Chorin, Alexandre},
  year = {2015},
  month = sep,
  journal = {Communications in Applied Mathematics and Computational Science},
  volume = {10},
  number = {2},
  pages = {205--225},
  issn = {2157-5452, 1559-3940},
  doi = {10.2140/camcos.2015.10.205},
  abstract = {Implicit sampling is a weighted sampling method that is used in data assimilation to sequentially update state estimates of a stochastic model based on noisy and incomplete data. Here we apply implicit sampling to sample the posterior probability density of parameter estimation problems. The posterior probability combines prior information about the parameter with information from a numerical model, e.g. a partial differential equation (PDE), and noisy data. The result of our computations are parameters that lead to simulations that are compatible with the data. We demonstrate the usefulness of our implicit sampling algorithm with an example from subsurface flow. For an efficient implementation we make use of multiple grids, BFGS optimization coupled to adjoint equations, and Karhunen-Lo`eve expansions for dimensional reduction. Moreover, several difficulties of Markov Chain Monte Carlo methods, e.g. estimation of burn-in times or correlations among the samples, are avoided because the implicit samples are independent.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\D9HZN5V6\\Morzfeld et al. - 2015 - Parameter estimation by implicit sampling.pdf}
}

@article{moustapha_quantile-based_2016,
  title = {Quantile-Based Optimization under Uncertainties Using Adaptive {{Kriging}} Surrogate Models},
  author = {Moustapha, Maliki and Sudret, Bruno and Bourinet, Jean-Marc and Guillaume, Beno{\^i}t},
  year = {2016},
  month = dec,
  journal = {Structural and Multidisciplinary Optimization},
  volume = {54},
  number = {6},
  pages = {1403--1421},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-016-1504-4},
  abstract = {Uncertainties are inherent to real-world systems. Taking them into account is crucial in industrial design problems and this might be achieved through reliability-based design optimization (RBDO) techniques. In this paper, we propose a quantile-based approach to solve RBDO problems. We first transform the safety constraints usually formulated as admissible probabilities of failure into constraints on quantiles of the performance criteria. In this formulation, the quantile level controls the degree of conservatism of the design. Starting with the premise that industrial applications often involve high-fidelity and time-consuming computational models, the proposed approach makes use of Kriging surrogate models (a.k.a. Gaussian process modeling). Thanks to the Kriging variance (a measure of the local accuracy of the surrogate), we derive a procedure with two stages of enrichment of the design of computer experiments (DoE) used to construct the surrogate model. The first stage globally reduces the Kriging epistemic uncertainty and adds points in the vicinity of the limit-state surfaces describing the system performance to be attained. The second stage locally checks, and if necessary, improves the accuracy of the quantiles estimated along the optimization iterations. Applications to three analytical examples and to the optimal design of a car body subsystem (minimal mass under mechanical safety constraints) show the accuracy and the remarkable efficiency brought by the proposed procedure.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\A5F7IMTC\\Moustapha et al. - 2016 - Quantile-based optimization under uncertainties us.pdf}
}

@incollection{muller_simulation_2005,
  title = {Simulation {{Based Optimal Design}}},
  booktitle = {Handbook of {{Statistics}}},
  author = {M{\"u}ller, Peter},
  year = {2005},
  volume = {25},
  pages = {509--518},
  publisher = {{Elsevier}},
  doi = {10.1016/S0169-7161(05)25017-4},
  abstract = {We review simulation based methods in optimal design. Expected utility maximization, i.e., optimal design, is concerned with maximizing an integral expression representing expected utility with respect to some design parameter. Except in special cases neither the maximization nor the integration can be solved analytically and approximations and/or simulation based methods are needed. On one hand the integration problem is easier to solve than the integration appearing in posterior inference problems. This is because the expectation is with respect to the joint distribution of parameters and data, which typically allows efficient random variate generation. On the other hand, the problem is difficult because the integration is embedded in the maximization and has to possibly be evaluated many times for different design parameters.},
  isbn = {978-0-444-51539-1},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5KPF7B7I\\MÃ¼ller - 2005 - Simulation Based Optimal Design.pdf}
}

@article{murphy_conjugate_2007,
  title = {Conjugate {{Bayesian}} Analysis of the {{Gaussian}} Distribution},
  author = {Murphy, Kevin P.},
  year = {2007},
  journal = {def},
  volume = {1},
  number = {2{$\sigma$}2},
  pages = {16},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JXBXPSL6\\bayesGauss.pdf}
}

@inproceedings{murphy_loopy_1999,
  title = {Loopy Belief Propagation for Approximate Inference: {{An}} Empirical Study},
  shorttitle = {Loopy Belief Propagation for Approximate Inference},
  booktitle = {Proceedings of the {{Fifteenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Murphy, Kevin P. and Weiss, Yair and Jordan, Michael I.},
  year = {1999},
  pages = {467--475},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4YGEXIA8\\qt92p8w3xb.pdf}
}

@article{nagel_bayesian_nodate,
  title = {Bayesian Techniques for Inverse Uncertainty Quantification},
  author = {Nagel, Joseph Benjamin},
  pages = {211},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\93SNZ7CK\\Nagel - Bayesian techniques for inverse uncertainty quanti.pdf}
}

@article{nagel_spectral_2016,
  title = {Spectral Likelihood Expansions for {{Bayesian}} Inference},
  author = {Nagel, Joseph B. and Sudret, Bruno},
  year = {2016},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {309},
  eprint = {1506.07564},
  eprinttype = {arxiv},
  pages = {267--294},
  issn = {00219991},
  doi = {10.1016/j.jcp.2015.12.047},
  abstract = {A spectral approach to Bayesian inference is presented. It pursues the emulation of the posterior probability density. The starting point is a series expansion of the likelihood function in terms of orthogonal polynomials. From this spectral likelihood expansion all statistical quantities of interest can be calculated semi-analytically. The posterior is formally represented as the product of a reference density and a linear combination of polynomial basis functions. Both the model evidence and the posterior moments are related to the expansion coefficients. This formulation avoids Markov chain Monte Carlo simulation and allows one to make use of linear least squares instead. The pros and cons of spectral Bayesian inference are discussed and demonstrated on the basis of simple applications from classical statistics and inverse modeling.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5NHUE98M\\Nagel et Sudret - 2016 - Spectral likelihood expansions for Bayesian infere.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7L887RQ8\\1506.html}
}

@article{nagler_evading_2016,
  title = {Evading the Curse of Dimensionality in Nonparametric Density Estimation with Simplified Vine Copulas},
  author = {Nagler, Thomas and Czado, Claudia},
  year = {2016},
  month = oct,
  journal = {Journal of Multivariate Analysis},
  volume = {151},
  eprint = {1503.03305},
  eprinttype = {arxiv},
  pages = {69--89},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2016.07.003},
  abstract = {Practical applications of nonparametric density estimators in more than three dimensions suffer a great deal from the well-known curse of dimensionality: convergence slows down as dimension increases. We show that one can evade the curse of dimensionality by assuming a simplified vine copula model for the dependence between variables. We formulate a general nonparametric estimator for such a model and show under high-level assumptions that the speed of convergence is independent of dimension. We further discuss a particular implementation for which we validate the high-level assumptions and establish its asymptotic normality. Simulation experiments illustrate a large gain in finite sample performance when the simplifying assumption is at least approximately true. But even when it is severely violated, the vine copula based approach proves advantageous as soon as more than a few variables are involved. Lastly, we give an application of the estimator to a classification problem from astrophysics.},
  archiveprefix = {arXiv},
  keywords = {Density estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\T63XS699\\Nagler et Czado - 2016 - Evading the curse of dimensionality in nonparametr.pdf;C\:\\Users\\a846735\\Zotero\\storage\\NMMKIWIW\\1503.html}
}

@article{natarajan_constructing_2009,
  title = {Constructing {{Risk Measures}} from {{Uncertainty Sets}}},
  author = {Natarajan, Karthik and Pachamanova, Dessislava and Sim, Melvyn},
  year = {2009},
  month = oct,
  journal = {Operations Research},
  volume = {57},
  number = {5},
  pages = {1129--1141},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.1080.0683},
  abstract = {We propose a unified theory that links uncertainty sets in robust optimization to risk measures in portfolio optimization. We illustrate the correspondence between uncertainty sets and some popular risk measures in finance, and show how robust optimization can be used to generalize the concepts of these measures. We also show that by using properly defined uncertainty sets in robust optimization models, one can in fact construct coherent risk measures. Our approach to creating coherent risk measures is easy to apply in practice, and computational experiments suggest that it may lead to superior portfolio performance. Our results have implications for efficient portfolio optimization under different measures of risk.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7ASYSSKY\\Natarajan et al. - 2009 - Constructing Risk Measures from Uncertainty Sets.pdf}
}

@phdthesis{ngodock_assimilation_1996,
  title = {{Assimilation de donn\'ees et analyse de sensibilit\'e. Une application \`a la circulation oc\'eanique}},
  author = {Ngodock, Hans Emmanuel},
  year = {1996},
  month = mar,
  abstract = {Le travail men\'e dans cette th\`ese porte sur l'\'etude "\`a posteriori" de l'assimilation variationnelle de donn\'ees. Il s'agit d'une d\'emarche de faisabilit\'e pour la mise au point des outils permettant de faire une analyse diagnostique (qualitative et quantitative) du processus d'assimilation variationnelle, notamment en ce qui concerne l'influence du bruit des observations sur le processus d'assimilation ainsi que sa propagation sur les champs reconstitu\'es (nous sommes alors amen\'es \`a faire une \'etude de sensibilit\'e), et l'influence de la configuration spatio-temporelle des observations sur le processus d'assimilation. L'application usuelle des \'equations adjointes pour l'analyse de sensibilit\'e est revis\'ee, car dans le contexte de l'assimilation variationnelle, nous avons montr\'e par un exemple simple qu'il faut s'y prendre diff\'eremment. Nous proposons alors une m\'ethode pour mener correctement cette analyse de sensibilit\'e. Cette m\'ethode est bas\'ee sur l'utilisation des \'equations adjointes au second ordre, obtenues en prenant l'adjoint du syst\`eme d'optimalit\'e. La sensibilit\'e en est d\'eduite par inversion du Hessien de la fonction co\^ut via la minimisation d'une fonctionnelle quadratique. L'application est faite sur un mod\`ele de circulation g\'en\'erale oc\'eanique de type quasi-g\'eostrophique, et nous faisons aussi l'\'etude de l'existence et l'unicit\'e de la solution de l'\'equation adjointe au second ordre du mod\`ele consid\'er\'e, pour justifier l'utilisation du Hessien et l'applicabilit\'e de notre m\'ethode. Nous \'etudions aussi l'influence de la configuration spatio-temporelle des observations sur le processus d'assimilation au travers du Hessien (\`a l'optimum) dont les \'el\'ements propres varient lorsqu'on fait varier la configuration. Enfin, nous \'etudions la pr\'edicibilit\'e du syst\`eme d'optimalit\'e.},
  langid = {french},
  school = {Universit\'e Joseph-Fourier - Grenoble I},
  keywords = {AS,Assimilation de donnÃ©es,ThÃ¨se},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JWJL94ZA\\Ngodock - 1996 - Assimilation de donnÃ©es et analyse de sensibilitÃ©..pdf;C\:\\Users\\a846735\\Zotero\\storage\\5YDEDP49\\tel-00005006.html}
}

@article{ngodock_weak_2017,
  title = {Weak and {{Strong Constraints Variational Data Assimilation}} with the {{NCOM-4DVAR}} in the {{Agulhas Region Using}} the {{Representer Method}}},
  author = {Ngodock, Hans and Carrier, Matthew and Smith, Scott and Souopgui, Innocent},
  year = {2017},
  month = may,
  journal = {Monthly Weather Review},
  volume = {145},
  number = {5},
  pages = {1755--1764},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-16-0264.1},
  abstract = {Abstract The difference between the strong and weak constraints four-dimensional variational (4DVAR) analyses is examined using the representer method formulation, which expresses the analysis as the sum of a first guess and a finite linear combination of representer functions. The latter are computed analytically for a single observation under both strong and weak constraints assumptions. Even though the strong constraints representer coefficients are different from their weak constraints counterparts, that difference is unable to help the strong constraints compensate for the loss of information that the weak constraints includes. Numerical experiments carried out in the Agulhas retroflection for single and multiobservation assimilations clearly show that the weak constraint 4DVAR produces analyses that fit the observations with significantly higher accuracy than the strong constraints.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SDKCMT9H\\Ngodock et al. - 2017 - Weak and Strong Constraints Variational Data Assim.pdf;C\:\\Users\\a846735\\Zotero\\storage\\SCAPE6LB\\mwr-d-16-0264.1.html}
}

@unpublished{nguyen_nonparametric_2018,
  title = {Nonparametric Method for Sparse Conditional Density Estimation in Moderately Large Dimensions},
  author = {Nguyen, Minh-Lien Jeanne},
  year = {2018},
  month = jan,
  abstract = {In this paper, we consider the problem of estimating a conditional density in moderately large dimensions. Much more informative than regression functions, conditional densities are of main interest in recent methods, particularly in the Bayesian framework (studying the posterior distribution, finding its modes...). Considering a recently studied family of kernel estimators, we select a pointwise multivariate bandwidth by revisiting the greedy algorithm Rodeo (Regularisation Of Derivative Expectation Operator). The method addresses several issues: being greedy and computationally efficient by an iterative procedure, avoiding the curse of high dimensionality under some suitably defined sparsity conditions by early variable selection during the procedure, converging at a quasi-optimal minimax rate.},
  keywords = {conditional density,Density estimation,greedy algorithm,kernel density estimators,nonparametric inference},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4RZ5J4QG\\Nguyen - 2018 - Nonparametric method for sparse conditional densit.pdf}
}

@article{nguyen_variational_nodate,
  title = {Variational {{Deep Learning}} for the {{Identification}} and {{Reconstruction}} of {{Chaotic}} and {{Stochastic Dynamical Systems}} from {{Noisy}} and {{Partial Observations}}},
  author = {Nguyen, Duong and Ouala, Said and Drumetz, Lucas and Fablet, Ronan},
  pages = {16},
  abstract = {The data-driven recovery of the unknown governing equations of dynamical systems has recently received an increasing interest. However, the identification of governing equations remains challenging when dealing with noisy and partial observations. Here, we address this challenge and investigate variational deep learning schemes. Within the proposed framework, we jointly learn an inference model to reconstruct the true states of the system and the governing laws of these states from series of noisy and partial data. In doing so, this framework bridges classical data assimilation and state-of-the-art machine learning techniques. We also demonstrate that it generalises stateof-the-art methods. Importantly, both the inference model and the governing model embed stochastic components to account for stochastic variabilities, model errors, and reconstruction uncertainties. Various experiments on chaotic and stochastic dynamical systems support the relevance of our scheme w.r.t. state-of-the-art approaches.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7DVDZ7T9\\Nguyen et al. - Variational Deep Learning for the Identification a.pdf}
}

@article{nguyen-thien_approximation_1999,
  title = {Approximation of Functions and Their Derivatives: {{A}} Neural Network Implementation with Applications},
  shorttitle = {Approximation of Functions and Their Derivatives},
  author = {{Nguyen-Thien}, T. and {Tran-Cong}, T.},
  year = {1999},
  month = sep,
  journal = {Applied Mathematical Modelling},
  volume = {23},
  number = {9},
  pages = {687--704},
  issn = {0307-904X},
  doi = {10.1016/S0307-904X(99)00006-2},
  abstract = {This paper reports a neural network (NN) implementation for the numerical approximation of functions of several variables and their first and second order partial derivatives. This approach can result in improved numerical methods for solving partial differential equations by eliminating the need to discretise the volume of the analysis domain. Instead only an unstructured distribution of collocation points throughout the volume is needed. An NN approximation of relevant variables for the whole domain based on these data points is then achieved. Excellent test results are obtained. It is shown how the method of approximation can then be used as part of a boundary element method (BEM) for the analysis of viscoelastic flows. Planar Couette and Poiseuille flows are used as illustrative examples.},
  langid = {english},
  keywords = {Boundary element method,Function approximation,Function derivatives approximation,Neural network}
}

@incollection{nicosia_fast_2013,
  title = {Fast {{Computation}} of the {{Multi-Points Expected Improvement}} with {{Applications}} in {{Batch Selection}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David},
  editor = {Nicosia, Giuseppe and Pardalos, Panos},
  year = {2013},
  volume = {7997},
  pages = {59--69},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-44973-4_7},
  abstract = {The Multi-points Expected Improvement criterion (or q-EI) has recently been studied in batch-sequential Bayesian Optimization. This paper deals with a new way of computing q-EI, without using Monte-Carlo simulations, through a closed-form formula. The latter allows a very fast computation of q-EI for reasonably low values of q (typically, less than 10). New parallel kriging-based optimization strategies, tested on different toy examples, show promising results.},
  isbn = {978-3-642-44972-7 978-3-642-44973-4},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WQFDMN8K\\Chevalier et Ginsbourger - 2013 - Fast Computation of the Multi-Points Expected Impr.pdf}
}

@incollection{nicosia_fast_2013-1,
  title = {Fast {{Computation}} of the {{Multi-Points Expected Improvement}} with {{Applications}} in {{Batch Selection}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David},
  editor = {Nicosia, Giuseppe and Pardalos, Panos},
  year = {2013},
  volume = {7997},
  pages = {59--69},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-44973-4_7},
  abstract = {The Multi-points Expected Improvement criterion (or q-EI) has recently been studied in batch-sequential Bayesian Optimization. This paper deals with a new way of computing q-EI, without using Monte-Carlo simulations, through a closed-form formula. The latter allows a very fast computation of q-EI for reasonably low values of q (typically, less than 10). New parallel kriging-based optimization strategies, tested on different toy examples, show promising results.},
  isbn = {978-3-642-44972-7 978-3-642-44973-4},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z5DB9E6X\\Chevalier et Ginsbourger - 2013 - Fast Computation of the Multi-Points Expected Impr.pdf}
}

@article{nielsen_cramer-rao_2013,
  title = {Cramer-{{Rao Lower Bound}} and {{Information Geometry}}},
  author = {Nielsen, Frank},
  year = {2013},
  month = jan,
  journal = {arXiv:1301.3578 [cs, math]},
  eprint = {1301.3578},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {This article focuses on an important piece of work of the world renowned Indian statistician, Calyampudi Radhakrishna Rao. In 1945, C. R. Rao (25 years old then) published a pathbreaking paper, which had a profound impact on subsequent statistical research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IBXL2CD4\\Nielsen - 2013 - Cramer-Rao Lower Bound and Information Geometry.pdf}
}

@incollection{nielsen_hierarchical_2016,
  title = {Hierarchical {{Clustering}}},
  author = {Nielsen, Frank},
  year = {2016},
  month = feb,
  pages = {195--211},
  doi = {10.1007/978-3-319-21903-5_8},
  abstract = {Agglomerative hierarchical clustering differs from partition-based clustering since it builds a binary merge tree starting from leaves that contain data elements to the root that contains the full data-set. The graphical representation of that tree that embeds the nodes on the plane is called a dendrogram. To implement a hierarchical clustering algorithm, one has to choose a linkage function (single linkage, average linkage, complete linkage, Ward linkage, etc.) that defines the distance between any two sub-sets (and rely on the base distance between elements). A hierarchical clustering is monotonous if and only if the similarity decreases along the path from any leaf to the root, otherwise there exists at least one inversion. The single, complete, and average linkage criteria guarantee the monotonic property, but not the often used Ward's criterion.},
  isbn = {978-3-319-21902-8},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\65CLNELH\\Nielsen - 2016 - Hierarchical Clustering.pdf}
}

@article{ning_optimization_2019,
  title = {Optimization under Uncertainty in the Era of Big Data and Deep Learning: {{When}} Machine Learning Meets Mathematical Programming},
  shorttitle = {Optimization under Uncertainty in the Era of Big Data and Deep Learning},
  author = {Ning, Chao and You, Fengqi},
  year = {2019},
  month = jun,
  journal = {Computers \& Chemical Engineering},
  volume = {125},
  pages = {434--448},
  issn = {0098-1354},
  doi = {10.1016/j.compchemeng.2019.03.034},
  abstract = {This paper reviews recent advances in the field of optimization under uncertainty via a modern data lens, highlights key research challenges and promise of data-driven optimization that organically integrates machine learning and mathematical programming for decision-making under uncertainty, and identifies potential research opportunities. A brief review of classical mathematical programming techniques for hedging against uncertainty is first presented, along with their wide spectrum of applications in Process Systems Engineering. A comprehensive review and classification of the relevant publications on data-driven distributionally robust optimization, data-driven chance constrained program, data-driven robust optimization, and data-driven scenario-based optimization is then presented. This paper also identifies fertile avenues for future research that focuses on a closed-loop data-driven optimization framework, which allows the feedback from mathematical programming to machine learning, as well as scenario-based optimization leveraging the power of deep learning techniques. Perspectives on online learning-based data-driven multistage optimization with a learning-while-optimizing scheme are presented.},
  langid = {english},
  keywords = {Big data,Data-driven optimization,Decision making under uncertainty,Deep learning,Machine learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2276QMQP\\Ning et You - 2019 - Optimization under uncertainty in the era of big d.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PIMDR9DT\\S0098135419300687.html}
}

@misc{noauthor_[1209.2736]_nodate,
  title = {[1209.2736] {{The Ensemble Kalman Filter}} for {{Inverse Problems}}},
  howpublished = {https://arxiv.org/abs/1209.2736},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YUSX97KI\\1209.html}
}

@misc{noauthor_[1703.04156]_nodate,
  title = {[1703.04156] {{A}} Trust-Region Method for Derivative-Free Nonlinear Constrained Stochastic Optimization},
  howpublished = {https://arxiv.org/abs/1703.04156},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2S3MXR35\\1703.html}
}

@misc{noauthor_airsea_nodate,
  title = {{{AIRSEA}} \textendash{} {{Mathematics}} and Computing Applied to Oceanic and Atmospheric Flows},
  langid = {american},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MC3GRPCQ\\en.html}
}

@misc{noauthor_alvaro_nodate,
  title = {{\'Alvaro Lozano-Robledo sur Twitter : "A thread about academic jobs in math based on the latest post on my blog: If you are going to take away one piece of advice from this thread, let it be this one: ** Start thinking about your resume as early as possible during your grad school years. ** 1/n" / Twitter}},
  shorttitle = {{\'Alvaro Lozano-Robledo sur Twitter}},
  journal = {Twitter},
  howpublished = {https://twitter.com/MathAndCobb/status/1288108324467044353},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z6P4M6JP\\1288108324467044353.html}
}

@misc{noauthor_association_nodate,
  title = {{Association Bernard Gregory}},
  abstract = {Evolution professionnelle et recrutement des docteurs (PhD) \textendash{} Recruitment and career development of PhD holders.},
  howpublished = {https://www.abg.asso.fr/fr/},
  langid = {fr, en},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FE54Z7YG\\fr.html}
}

@misc{noauthor_brief_2017,
  title = {A {{Brief Introduction}} to {{Neural Networks}}},
  year = {2017},
  month = sep,
  journal = {D. Kriesel},
  abstract = {A Brief Introduction to Neural Networks  Manuscript Download - Zeta2 Version  Filenames are subject to change. Thus, if you place links, please do so with this subpage as target.      Original version    eBookReader optimized   English   [PDF], 6.2MB, 244 pages},
  chapter = {2014-12-18T12:57:47+01:00},
  howpublished = {https://www.dkriesel.com/en/science/neural\_networks},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5YDWE2KN\\neural_networks.html}
}

@misc{noauthor_data_nodate,
  title = {{Data Assimilation with a Machine Learned Observation Operator and Application to the Assimilation of Satellite data for Sea Ice Models}},
  langid = {http://id.loc.gov/vocabulary/iso639-2/eng},
  annotation = {Context Object: url\_ver=Z39.88-2004\&ctx\_ver=Z39.88-2004\&rft\_val\_fmt=info\%3Aofi\%2Ffmt\%3Akev\%3Amtx\%3Adc\&rfr\_id=info\%3Asid\%2Fblacklight.rubyforge.org\%3Agenerator\&rft.title=Data+Assimilation+with+a+Machine+Learned+Observation+Operator+and+Application+to+the+Assimilation+of+Satellite+data+for+Sea+Ice+Models\&rft.format=Honors+Thesis\&rft.language=http\%3A\%2F\%2Fid.loc.gov\%2Fvocabulary\%2Fiso639-2\%2Feng\&rft.relation=Machine+Learning\&rft.relation=Melt+Pond\&rft.relation=Ensemble+Kalman+Filter\&rft.relation=Sea+Ice\&rft.relation=Neural+Network\&rft.relation=Data+Assimilation\&rft.relation=Dynamical+System},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\U2UZ7IGS\\rx913t935.html}
}

@misc{noauthor_density_nodate,
  title = {Density Power Divergence - {{Recherche Google}}},
  howpublished = {https://www.google.com/search?client=ubuntu\&channel=fs\&q=density+power+divergence\&ie=utf-8\&oe=utf-8},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P3UESICT\\search.html}
}

@article{noauthor_distance_2020,
  title = {{Distance de Mahalanobis}},
  year = {2020},
  month = jun,
  journal = {Wikip\'edia},
  abstract = {En statistique, la distance de Mahalanobis est une mesure de distance math\'ematique introduite par Prasanta Chandra Mahalanobis en 1936. Elle est bas\'ee sur la corr\'elation entre des variables par lesquelles diff\'erents mod\`eles peuvent \^etre identifi\'es et analys\'es. C'est une mani\`ere utile de d\'eterminer la similarit\'e entre une s\'erie de donn\'ees connues et inconnues. Elle diff\`ere de la distance euclidienne par le fait qu'elle prend en compte la variance et la corr\'elation de la s\'erie de donn\'ees. Ainsi, \`a la diff\'erence de la distance euclidienne o\`u toutes les composantes des vecteurs sont trait\'ees ind\'ependamment et de la m\^eme fa\c{c}on, la distance de Mahalanobis accorde un poids moins important aux composantes les plus dispers\'ees. Dans le cas de l'analyse des signaux, et en supposant que chaque composante soit une variable al\'eatoire de type gaussien, cela revient \`a minimiser l'influence des composantes les plus bruit\'ees (celles ayant la plus grande variance). La distance de Mahalanobis est souvent utilis\'ee pour la d\'etection de donn\'ees aberrantes dans un jeu de donn\'ees, ou bien pour d\'eterminer la coh\'erence de donn\'ees fournies par un capteur par exemple : cette distance est calcul\'ee entre les donn\'ees re\c{c}ues et celles pr\'edites par un mod\`ele. En pratique, la distance de Mahalanobis d'un vecteur \`a plusieurs variables                         x         =         (                    x                        1                             ,                    x                        2                             ,                    x                        3                             ,         \ldots{}         ,                    x                        p                                        )                        T                                     \{\textbackslash displaystyle x=(x\_\{1\},x\_\{2\},x\_\{3\},\textbackslash dots ,x\_\{p\})\^\{T\}\}    \`a un ensemble de vecteurs de valeurs moyennes                         {$\mu$}         =         (                    {$\mu$}                        1                             ,                    {$\mu$}                        2                             ,                    {$\mu$}                        3                             ,         \ldots{}         ,                    {$\mu$}                        p                                        )                        T                                     \{\textbackslash displaystyle \textbackslash mu =(\textbackslash mu \_\{1\},\textbackslash mu \_\{2\},\textbackslash mu \_\{3\},\textbackslash dots ,\textbackslash mu \_\{p\})\^\{T\}\}    et poss\'edant une matrice de covariance {$\Sigma$} est d\'efinie comme suit :                                   D                        M                             (         x         )         =                                 (             x             -             {$\mu$}                            )                                T                                                        {$\Sigma$}                                -                 1                                         (             x             -             {$\mu$}             )                             .                          \{\textbackslash displaystyle D\_\{M\}(x)=\{\textbackslash sqrt \{(x-\textbackslash mu )\^\{T\}\textbackslash Sigma \^\{-1\}(x-\textbackslash mu )\}\}.\textbackslash,\}   La distance de Mahalanobis peut aussi \^etre d\'efinie comme \'etant la mesure de dissimilarit\'e entre deux vecteurs al\'eatoires                                                                x               \textrightarrow{}                                                  \{\textbackslash displaystyle \{\textbackslash vec \{x\}\}\}    et                                                                y               \textrightarrow{}                                                  \{\textbackslash displaystyle \{\textbackslash vec \{y\}\}\}    de m\^eme distribution avec une matrice de covariance {$\Sigma$} :                        d         (                                                x               \textrightarrow{}                                          ,                                                y               \textrightarrow{}                                          )         =                                 (                                                                x                   \textrightarrow{}                                                          -                                                                y                   \textrightarrow{}                                                                         )                                T                                                        {$\Sigma$}                                -                 1                                         (                                                                x                   \textrightarrow{}                                                          -                                                                y                   \textrightarrow{}                                                          )                             .                          \{\textbackslash displaystyle d(\{\textbackslash vec \{x\}\},\{\textbackslash vec \{y\}\})=\{\textbackslash sqrt \{(\{\textbackslash vec \{x\}\}-\{\textbackslash vec \{y\}\})\^\{T\}\textbackslash Sigma \^\{-1\}(\{\textbackslash vec \{x\}\}-\{\textbackslash vec \{y\}\})\}\}.\textbackslash,\}   Si la matrice de covariance est la matrice identit\'e, cette distance est simplement la distance euclidienne. Si la matrice de covariance est diagonale, on obtient la distance euclidienne normalis\'ee :                        d         (                                                x               \textrightarrow{}                                          ,                                                y               \textrightarrow{}                                          )         =                                                {$\sum$}                                i                 =                 1                                               p                                                                                            (                                        x                                            i                                                           -                                        y                                            i                                                                                )                                            2                                                                                             {$\sigma$}                                        i                                                           2                                                                                                     \{\textbackslash displaystyle d(\{\textbackslash vec \{x\}\},\{\textbackslash vec \{y\}\})=\{\textbackslash sqrt \{\textbackslash sum \_\{i=1\}\^\{p\}\{(x\_\{i\}-y\_\{i\})\^\{2\} \textbackslash over \textbackslash sigma \_\{i\}\^\{2\}\}\}\}\}   o\`u {$\sigma$}i est l'\'ecart type de xi sur la s\'erie de donn\'ees. Cette distance est souvent utilis\'ee en analyse des donn\'ees. (ex: analyse discriminante)},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 172145118},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JKM6Z92H\\index.html}
}

@article{noauthor_efficient_nodate,
  title = {Efficient {{Global Optimization}} of {{Expensive Black-Box Functions}}},
  pages = {38},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JSEY6UYJ\\Efficient Global Optimization of Expensive Black-B.pdf}
}

@misc{noauthor_iterative_nodate,
  title = {An {{Iterative EnKF}} for {{Strongly Nonlinear Systems}} in: {{Monthly Weather Review Volume}} 140 {{Issue}} 6 (2012)},
  howpublished = {https://journals.ametsoc.org/view/journals/mwre/140/6/mwr-d-11-00176.1.xml},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HUZV77VV\\mwr-d-11-00176.1.html}
}

@misc{noauthor_learning_nodate,
  title = {Learning {{Poisson Binomial Distributions}}},
  howpublished = {http://www.cs.columbia.edu/\textasciitilde rocco/papers/stoc12pbd.html},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VDTTXQGL\\stoc12pbd.html}
}

@article{noauthor_letat_nodate,
  title = {{L'\'etat de l'emploi scientifique en France}},
  pages = {200},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W5975FAL\\LâÃ©tat de l'emploi scientifique en France.pdf}
}

@article{noauthor_loi_2018,
  title = {{Loi normale g\'en\'eralis\'ee}},
  year = {2018},
  month = jul,
  journal = {Wikip\'edia},
  abstract = {En th\'eorie des probabilit\'es et en statistique, la loi normale g\'en\'eralis\'ee ou loi gaussienne g\'en\'eralis\'ee d\'esigne deux familles de lois de probabilit\'e \`a densit\'e dont les supports sont l'ensemble des r\'eels. Cette loi rajoute un param\`etre de forme \`a la loi normale. Pour les diff\'erencier, les deux familles seront appel\'ees \guillemotleft{} version 1 \guillemotright{} et \guillemotleft{} version 2 \guillemotright, ce ne sont cependant pas des appellations standards.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 150178985},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Q44Z4MY6\\index.html}
}

@misc{noauthor_modeling_nodate,
  title = {Modeling - {{Why}} Should {{I}} Be {{Bayesian}} When My Model Is Wrong?},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/274815/why-should-i-be-bayesian-when-my-model-is-wrong},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZCPTGJZK\\why-should-i-be-bayesian-when-my-model-is-wrong.html}
}

@misc{noauthor_optimization_nodate,
  title = {Optimization under Uncertainty: State-of-the-Art and Opportunities - {{ScienceDirect}}},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0098135403002369},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RKSNRNZE\\S0098135403002369.html}
}

@article{noauthor_post-modern_2018,
  title = {Post-Modern Portfolio Theory},
  year = {2018},
  month = sep,
  journal = {Wikipedia},
  abstract = {Post-modern portfolio theory (or PMPT) is an extension of the traditional modern portfolio theory (MPT, which is an application of mean-variance analysis or MVA). Both theories propose how rational investors should use diversification to optimize their portfolios, and how a risky asset should be priced.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 860848261},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6SXFXTKC\\index.html}
}

@article{noauthor_prevision_2020,
  title = {{Pr\'evision d'ensembles}},
  year = {2020},
  month = apr,
  journal = {Wikip\'edia},
  abstract = {La pr\'evision d'ensembles est une m\'ethode de pr\'evision num\'erique du temps utilis\'e pour tenter de g\'en\'erer un \'echantillon repr\'esentatif des \'etats futurs possibles d'un syst\`eme dynamique. En effet, ni les observations, ni l'analyse, ni le mod\`ele de pr\'evision ne sont parfaits et la dynamique atmosph\'erique est tr\`es sensible, dans certaines conditions, \`a la moindre fluctuation.  C'est une forme de m\'ethode de Monte Carlo : plusieurs pr\'edictions num\'eriques sont r\'ealis\'ees \`a l'aide de conditions initiales l\'eg\`erement diff\'erentes mais plausibles \'etant donn\'e les limites de r\'esolution des observations et des \'equations. Dans chaque cas, l'analyse est d\'elib\'er\'ement rendue l\'eg\`erement diff\'erente des autres membres de l'ensemble, \`a l'int\'erieur des incertitudes intrins\`eques de mesure ou d'analyse. Les sc\'enarios plus ou moins divergents des pr\'evisions offertes par les membres de l'ensemble permettent de quantifier la pr\'evisibilit\'e de l'atmosph\`ere et d'offrir une marge d'erreur statistique sur la pr\'evision.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 169948197},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BQYYUF4U\\index.html}
}

@misc{noauthor_probability_nodate,
  title = {Probability - {{Sum}} of {{Bernoulli}} Random Variables with Different Success Probabilities},
  journal = {Mathematics Stack Exchange},
  howpublished = {https://math.stackexchange.com/questions/392860/sum-of-bernoulli-random-variables-with-different-success-probabilities},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LG4XNV2F\\sum-of-bernoulli-random-variables-with-different-success-probabilities.html}
}

@article{noauthor_profile_nodate,
  title = {On {{Profile Likelihood}}},
  pages = {18},
  langid = {english},
  keywords = {Profile lik},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LYQFCP87\\On Profile Likelihood.pdf}
}

@misc{noauthor_pylink88awesome-normalizing-flows_nodate,
  title = {{{PyLink88}}/Awesome-Normalizing-Flows: {{A}} List of Awesome Resources on Normalizing Flows.},
  howpublished = {https://github.com/PyLink88/awesome-normalizing-flows}
}

@misc{noauthor_savage:_nodate,
  title = {Savage: {{The}} Theory of Statistical Decision - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?hl=en\&volume=46\&publication\_year=1951\&pages=55-67\&journal=Journal+of+the+American+Statistical+Association\&issue=253\&author=L.+J.+Savage\&title=The+theory+of+statistical+decision},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\AIZINKIG\\scholar_lookup.html}
}

@misc{noauthor_sci-hub_nodate,
  title = {Sci-{{Hub}} | {{AK-MCS}}: {{An}} Active Learning Reliability Method Combining {{Kriging}} and {{Monte Carlo Simulation}}. {{Structural Safety}}, 33(2), 145\textendash 154 | 10.1016/j.Strusafe.2011.01.002},
  howpublished = {https://sci-hub.tw/10.1016/j.strusafe.2011.01.002},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2PMMALVS\\j.strusafe.2011.01.html}
}

@misc{noauthor_shogun-toolboxshogun_nodate,
  title = {Shogun-Toolbox/Shogun},
  journal = {GitHub},
  abstract = {Sh\=ogun. Contribute to shogun-toolbox/shogun development by creating an account on GitHub.},
  howpublished = {https://github.com/shogun-toolbox/shogun},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RGHC5AZS\\GSoC_2016_project_large_gps.html}
}

@misc{noauthor_structuring_nodate,
  title = {Structuring {{Your Project}} \textemdash{} {{The Hitchhiker}}'s {{Guide}} to {{Python}}},
  howpublished = {https://docs.python-guide.org/writing/structure},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QRS5EI7R\\structure.html}
}

@misc{noauthor_time_nodate,
  title = {Time Series - {{How}} to You Measure the Accuracy of a Model That Gives Quantile Forecasts or Distributions of Forecasts?},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/367306/how-to-you-measure-the-accuracy-of-a-model-that-gives-quantile-forecasts-or-dist?rq=1},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TS5CGX9R\\how-to-you-measure-the-accuracy-of-a-model-that-gives-quantile-forecasts-or-dist.html}
}

@misc{noauthor_virtual_nodate,
  title = {Virtual {{Library}} of {{Simulation Experiments}}: {{Test Functions}} and {{Datasets}}},
  howpublished = {https://www.sfu.ca/\textasciitilde ssurjano/index.html},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EN5EEF58\\index.html}
}

@misc{noauthor_whatsapp_nodate,
  title = {{WhatsApp Web}},
  abstract = {Quickly send and receive WhatsApp messages right from your computer.},
  howpublished = {https://web.whatsapp.com/},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\G7UI67RX\\web.whatsapp.com.html}
}

@misc{noauthor_wilks_nodate,
  title = {Wilks Estimate Quantile - {{Recherche Google}}},
  howpublished = {https://www.google.com/search?client=ubuntu\&hs=2sf\&channel=fs\&biw=1867\&bih=982\&ei=yF3hW-f3DszMgAaf5YqABA\&q=wilks+estimate+quantile\&oq=wilks+estimate+quantile\&gs\_l=psy-ab.3..33i21k1l2.193064.194150.0.194190.9.8.0.0.0.0.165.665.6j1.7.0....0...1c.1.64.psy-ab..2.7.664...33i160k1.0.x0NctscuA9I},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3JTKPD57\\search.html}
}

@phdthesis{oakley_bayesian_1999,
  title = {Bayesian Uncertainty Analysis for Complex Computer Codes},
  author = {Oakley, Jeremy},
  year = {1999},
  school = {University of Sheffield},
  keywords = {Bayesian inference},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GDRTYMEY\\jeothesis.pdf}
}

@article{oakley_estimating_2004,
  title = {Estimating Percentiles of Uncertain Computer Code Outputs},
  author = {Oakley, Jeremy},
  year = {2004},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {53},
  number = {1},
  pages = {83--93},
  issn = {0035-9254, 1467-9876},
  doi = {10.1046/j.0035-9254.2003.05044.x},
  abstract = {A deterministic computer model is to be used in a situation where there is uncertainty about the values of some or all of the input parameters. This uncertainty induces uncertainty in the output of the model. We consider the problem of estimating a specific percentile of the distribution of this uncertain output. We also suppose that the computer code is computationally expensive, so we can run the model only at a small number of distinct inputs. This means that we must consider our uncertainty about the computer code itself at all untested inputs. We model the output, as a function of its inputs, as a Gaussian process, and after a few initial runs of the code use a simulation approach to choose further suitable design points and to make inferences about the percentile of interest itself. An example is given involving a model that is used in sewer design.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WHCIQTWZ\\Oakley - 2004 - Estimating percentiles of uncertain computer code .pdf}
}

@article{oberkampf_measures_2006,
  title = {Measures of Agreement between Computation and Experiment: {{Validation}} Metrics},
  shorttitle = {Measures of Agreement between Computation and Experiment},
  author = {Oberkampf, William L. and Barone, Matthew F.},
  year = {2006},
  month = sep,
  journal = {Journal of Computational Physics},
  volume = {217},
  number = {1},
  pages = {5--36},
  issn = {00219991},
  doi = {10.1016/j.jcp.2006.03.037},
  abstract = {With the increasing role of computational modeling in engineering design, performance estimation, and safety assessment, improved methods are needed for comparing computational results and experimental measurements. Traditional methods of graphically comparing computational and experimental results, though valuable, are essentially qualitative. Computable measures are needed that can quantitatively compare computational and experimental results over a range of input, or control, variables to sharpen assessment of computational accuracy. This type of measure has been recently referred to as a validation metric. We discuss various features that we believe should be incorporated in a validation metric, as well as features that we believe should be excluded. We develop a new validation metric that is based on the statistical concept of confidence intervals. Using this fundamental concept, we construct two specific metrics: one that requires interpolation of experimental data and one that requires regression (curve fitting) of experimental data. We apply the metrics to three example problems: thermal decomposition of a polyurethane foam, a turbulent buoyant plume of helium, and compressibility effects on the growth rate of a turbulent free-shear layer. We discuss how the present metrics are easily interpretable for assessing computational model accuracy, as well as the impact of experimental measurement uncertainty on the accuracy assessment.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LM68SXBZ\\Oberkampf et Barone - 2006 - Measures of agreement between computation and expe.pdf}
}

@article{oczkowski_mechanisms_2005,
  title = {Mechanisms for the {{Development}} of {{Locally Low-Dimensional Atmospheric Dynamics}}},
  author = {Oczkowski, Michael and Szunyogh, Istvan and Patil, D. J.},
  year = {2005},
  month = apr,
  journal = {Journal of the Atmospheric Sciences},
  volume = {62},
  number = {4},
  pages = {1135--1156},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/JAS3403.1},
  abstract = {Abstract The complexity of atmospheric instabilities is investigated by a combination of numerical experiments and diagnostic tools that do not require the assumption of linear error dynamics. These tools include the well-established analysis of the local energetics of the atmospheric flow and the recently introduced ensemble dimension (E dimension). The E dimension is a local measure that varies in both space and time and quantifies the distribution of the variance between phase space directions for an ensemble of nonlinear model solutions over a geographically localized region. The E dimension is maximal, that is, equal to the number of ensemble members (k), when the variance is equally distributed between k phase space directions. The more unevenly distributed the variance, the lower the E dimension. Numerical experiments with the state-of-the-art operational Global Forecast System (GFS) of the National Centers for Environmental Prediction (NCEP) at a reduced resolution are carried out to investigate the spatiotemporal evolution of the E dimension. This evolution is characterized by an initial transient phase in which coherent regions of low dimensionality develop through a rapid local decay of the E dimension. The typical duration of the transient is between 12 and 48 h depending on the flow; after the initial transient, the E dimension gradually increases with time. The main goal of this study is to identify processes that contribute to transient local low-dimensional behavior. Case studies are presented to show that local baroclinic and barotropic instabilities, downstream development of upper-tropospheric wave packets, phase shifts of finite amplitude waves, anticyclonic wave breaking, and some combinations of these processes can all play crucial roles in lowering the E dimension. The practical implication of the results is that a wide range of synoptic-scale weather events may exist whose prediction can be significantly improved in the short and early medium range by enhancing the prediction of only a few local phase space directions. This potential is demonstrated by a reexamination of the targeted weather observations missions from the 2000 Winter Storm Reconnaissance (WSR00) program.},
  chapter = {Journal of the Atmospheric Sciences},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GZ9YPWQX\\Oczkowski et al. - 2005 - Mechanisms for the Development of Locally Low-Dime.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KUEX6BFI\\jas3403.1.html}
}

@article{oczkowski_mechanisms_2005-1,
  title = {Mechanisms for the {{Development}} of {{Locally Low-Dimensional Atmospheric Dynamics}}},
  author = {Oczkowski, Michael and Szunyogh, Istvan and Patil, D. J.},
  year = {2005},
  month = apr,
  journal = {Journal of the Atmospheric Sciences},
  volume = {62},
  number = {4},
  pages = {1135--1156},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/JAS3403.1},
  abstract = {Abstract The complexity of atmospheric instabilities is investigated by a combination of numerical experiments and diagnostic tools that do not require the assumption of linear error dynamics. These tools include the well-established analysis of the local energetics of the atmospheric flow and the recently introduced ensemble dimension (E dimension). The E dimension is a local measure that varies in both space and time and quantifies the distribution of the variance between phase space directions for an ensemble of nonlinear model solutions over a geographically localized region. The E dimension is maximal, that is, equal to the number of ensemble members (k), when the variance is equally distributed between k phase space directions. The more unevenly distributed the variance, the lower the E dimension. Numerical experiments with the state-of-the-art operational Global Forecast System (GFS) of the National Centers for Environmental Prediction (NCEP) at a reduced resolution are carried out to investigate the spatiotemporal evolution of the E dimension. This evolution is characterized by an initial transient phase in which coherent regions of low dimensionality develop through a rapid local decay of the E dimension. The typical duration of the transient is between 12 and 48 h depending on the flow; after the initial transient, the E dimension gradually increases with time. The main goal of this study is to identify processes that contribute to transient local low-dimensional behavior. Case studies are presented to show that local baroclinic and barotropic instabilities, downstream development of upper-tropospheric wave packets, phase shifts of finite amplitude waves, anticyclonic wave breaking, and some combinations of these processes can all play crucial roles in lowering the E dimension. The practical implication of the results is that a wide range of synoptic-scale weather events may exist whose prediction can be significantly improved in the short and early medium range by enhancing the prediction of only a few local phase space directions. This potential is demonstrated by a reexamination of the targeted weather observations missions from the 2000 Winter Storm Reconnaissance (WSR00) program.},
  chapter = {Journal of the Atmospheric Sciences},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\N4KPVPDE\\jas3403.1.html}
}

@article{ogryczak_stochastic_1997,
  title = {On Stochastic Dominance and Mean-Semideviation Models},
  author = {Ogryczak, W{\l}odzimierz and Ruszczynski, Andrzej},
  year = {1997},
  publisher = {{IR-97-043}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QYHURWIE\\Ogryczak et Ruszczynski - 1997 - On stochastic dominance and mean-semideviation mod.pdf}
}

@article{ogryczak_stochastic_nodate,
  title = {On {{Stochastic Dominance}} and {{Mean-Semideviation Models}}},
  author = {Ogryczak, W{\l}odzimierz and Ogryczak, Wlodzimierz},
  pages = {15},
  abstract = {We analyse relations between two methods frequently used for modeling the choice among uncertain outcomes: stochastic dominance and mean\{risk approaches. The concept of -consistency of these approaches is de ned as the consistency within a bounded range of mean\{risk trade-o s. We show that mean\{risk models using central semideviations as risk measures are -consistent with stochastic dominance relations of the corresponding degree if the trade-o coe cient for the semideviation is bounded by one.\vphantom{\}\}\}}},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P37ZLUIU\\Ogryczak et Ogryczak - On Stochastic Dominance and Mean-Semideviation Mod.pdf}
}

@article{oliveira_bayesian_2019,
  title = {Bayesian Optimisation under Uncertain Inputs},
  author = {Oliveira, Rafael and Ott, Lionel and Ramos, Fabio},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.07908 [cs, stat]},
  eprint = {1902.07908},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Bayesian optimisation (BO) has been a successful approach to optimise functions which are expensive to evaluate and whose observations are noisy. Classical BO algorithms, however, do not account for errors about the location where observations are taken, which is a common issue in problems with physical components. In these cases, the estimation of the actual query location is also subject to uncertainty. In this context, we propose an upper confidence bound (UCB) algorithm for BO problems where both the outcome of a query and the true query location are uncertain. The algorithm employs a Gaussian process model that takes probability distributions as inputs. Theoretical results are provided for both the proposed algorithm and a conventional UCB approach within the uncertain-inputs setting. Finally, we evaluate each method's performance experimentally, comparing them to other input noise aware BO approaches on simulated scenarios involving synthetic and real data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GMIN3HSI\\Oliveira et al. - 2019 - Bayesian optimisation under uncertain inputs.pdf}
}

@article{onken_ot-flow_2021,
  title = {{{OT-Flow}}: {{Fast}} and {{Accurate Continuous Normalizing Flows}} via {{Optimal Transport}}},
  shorttitle = {{{OT-Flow}}},
  author = {Onken, Derek and Fung, Samy Wu and Li, Xingjian and Ruthotto, Lars},
  year = {2021},
  month = mar,
  journal = {arXiv:2006.00104 [cs, stat]},
  eprint = {2006.00104},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE's dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow's Jacobian can be obtained by integrating the trace of the dynamics' Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to state-of-the-art CNFs while on average requiring one-fourth of the number of weights with an 8x speedup in training time and 24x speedup in inference.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\STPEV6MV\\2006.00104.pdf}
}

@article{orbanz_nonparametric_2008,
  title = {Nonparametric {{Bayesian Image Segmentation}}},
  author = {Orbanz, Peter and Buhmann, Joachim M.},
  year = {2008},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {77},
  number = {1-3},
  pages = {25--45},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-007-0061-0},
  abstract = {Image segmentation algorithms partition the set of pixels of an image into a specific number of different, spatially homogeneous groups. We propose a nonparametric Bayesian model for histogram clustering which automatically determines the number of segments when spatial smoothness constraints on the class assignments are enforced by a Markov Random Field. A Dirichlet process prior controls the level of resolution which corresponds to the number of clusters in data with a unique cluster structure. The resulting posterior is efficiently sampled by a variant of a conjugate-case sampling algorithm for Dirichlet process mixture models. Experimental results are provided for real-world gray value images, synthetic aperture radar images and magnetic resonance imaging data.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QAEDKYMY\\porbanz_OrbanzBuhmann_2008_1.pdf;C\:\\Users\\a846735\\Zotero\\storage\\D96J2TFF\\10.html}
}

@article{ortega_thermodynamics_2013,
  title = {Thermodynamics as a Theory of Decision-Making with Information Processing Costs},
  author = {Ortega, Pedro A. and Braun, Daniel A.},
  year = {2013},
  month = may,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {469},
  number = {2153},
  eprint = {1204.6481},
  eprinttype = {arxiv},
  pages = {20120683},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2012.0683},
  abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here we propose an information-theoretic formalization of bounded rational decision-making where decision-makers trade off expected utility and information processing costs. Such bounded rational decision-makers can be thought of as thermodynamic machines that undergo physical state changes when they compute. Their behavior is governed by a free energy functional that trades off changes in internal energy\textemdash as a proxy for utility\textemdash and entropic changes representing computational costs induced by changing states. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known concepts from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss the relation to satisficing decision-making procedures as well as links to existing theoretical frameworks and human decision-making experiments that describe deviations from expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to axiomatically derive and interpret the thermodynamic free energy as a model of bounded rational decision-making.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Optimization and Control,Mathematics - Statistics Theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BITHKJGS\\Ortega et Braun - 2013 - Thermodynamics as a theory of decision-making with.pdf}
}

@article{over_strategy_2013,
  title = {A Strategy for Improved Computational Efficiency of the Method of Anchored Distributions},
  author = {Over, Matthew William and Yang, Yarong and Chen, Xingyuan and Rubin, Yoram},
  year = {2013},
  month = jun,
  journal = {Water Resources Research},
  volume = {49},
  number = {6},
  pages = {3257--3275},
  issn = {1944-7973},
  doi = {10.1002/wrcr.20182},
  abstract = {This paper proposes a strategy for improving the computational efficiency of model inversion using the method of anchored distributions (MAD) by ``bundling'' similar model parametrizations in the likelihood function. Inferring the likelihood function typically requires a large number of forward model (FM) simulations for each possible model parametrization; as a result, the process is quite expensive. To ease this prohibitive cost, we present an approximation for the likelihood function called bundling that relaxes the requirement for high quantities of FM simulations. This approximation redefines the conditional statement of the likelihood function as the probability of a set of similar model parametrizations ``bundle'' replicating field measurements, which we show is neither a model reduction nor a sampling approach to improving the computational efficiency of model inversion. To evaluate the effectiveness of these modifications, we compare the quality of predictions and computational cost of bundling relative to a baseline MAD inversion of 3-D flow and transport model parameters. Additionally, to aid understanding of the implementation we provide a tutorial for bundling in the form of a sample data set and script for the R statistical computing language. For our synthetic experiment, bundling achieved a 35\% reduction in overall computational cost and had a limited negative impact on predicted probability distributions of the model parameters. Strategies for minimizing error in the bundling approximation, for enforcing similarity among the sets of model parametrizations, and for identifying convergence of the likelihood function are also presented.},
  langid = {english},
  keywords = {approximation,Bayesian,clustering,computational efficiency,inversion modeling},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\68NCTQID\\Over et al. - 2013 - A strategy for improved computational efficiency o.pdf;C\:\\Users\\a846735\\Zotero\\storage\\HXLC3EW5\\abstract.html}
}

@article{pagnoncelli_sample_2009,
  title = {Sample {{Average Approximation Method}} for {{Chance Constrained Programming}}: {{Theory}} and {{Applications}}},
  shorttitle = {Sample {{Average Approximation Method}} for {{Chance Constrained Programming}}},
  author = {Pagnoncelli, B. K. and Ahmed, S. and Shapiro, A.},
  year = {2009},
  month = aug,
  journal = {Journal of Optimization Theory and Applications},
  volume = {142},
  number = {2},
  pages = {399--416},
  issn = {0022-3239, 1573-2878},
  doi = {10.1007/s10957-009-9523-6},
  abstract = {We study sample approximations of chance constrained problems. In particular, we consider the sample average approximation (SAA) approach and discuss the convergence properties of the resulting problem. We discuss how one can use the SAA method to obtain good candidate solutions for chance constrained problems. Numerical experiments are performed to correctly tune the parameters involved in the SAA. In addition, we present a method for constructing statistical lower bounds for the optimal value of the considered problem and discuss how one should tune the underlying parameters. We apply the SAA to two chance constrained problems. The first is a linear portfolio selection problem with returns following a multivariate lognormal distribution. The second is a joint chance constrained version of a simple blending problem.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IZDYKJ38\\Pagnoncelli et al. - 2009 - Sample Average Approximation Method for Chance Con.pdf}
}

@article{palmer_representing_2005,
  title = {Representing {{Model Uncertainty}} in {{Weather}} and {{Climate Prediction}}},
  author = {Palmer, T.N. and Shutts, G.J. and Hagedorn, R. and {Doblas-Reyes}, F.J. and Jung, T. and Leutbecher, M.},
  year = {2005},
  month = may,
  journal = {Annual Review of Earth and Planetary Sciences},
  volume = {33},
  number = {1},
  pages = {163--193},
  issn = {0084-6597, 1545-4495},
  doi = {10.1146/annurev.earth.33.092203.122552},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H5C7STD2\\Palmer et al. - 2005 - REPRESENTING MODEL UNCERTAINTY IN WEATHER AND CLIM.pdf}
}

@article{pan_preconditioning_2015,
  title = {Preconditioning for the {{Hessian-free Gauss-Newton}} Full-Waveform Inversion},
  author = {Pan, Wenyong and Innanen, Kris and Liao, Wenyuan},
  year = {2015},
  volume = {27},
  pages = {26},
  abstract = {Full-waveform inversion (FWI) has emerged as a powerful strategy for estimating the subsurface model parameters by iteratively minimizing the difference between the synthetic data and observed data. The gradient-based methods promise to converge globally but suffer from slow convergence rate. The Newton-type methods provide a quadratic convergence, but the computation, storage and inversion of the Hessian are beyond the current computation ability for large-scale inverse problem. The Hessian-free (HF) optimization method represents an attractive alternative to these above-mentioned optimization methods. At each iteration, it obtains the search direction by approximately solving the Newton linear system using a conjugate-gradient (CG) algorithm with a matrix-free fashion. One problem of the HF optimization method is that the CG algorithm requires many iterations. The main goal of this paper is to accelerate the HF FWI by preconditioning the CG algorithm. In this research, different preconditioning schemes for the HF Gauss-Newton optimization method are developed. The preconditioners are designed as Hessian approximations (e.g., diagonal pseudo-Hessian and diagonal Gauss-Newton Hessian) or its inverse approximations. We also developed a new pseudo diagonal Gauss-Newton Hessian approximation for preconditioning based on the reciprocal property of the Green's function. Furthermore, a quasi-Newton l-BFGS inverse Hessian approximation preconditioner with the diagonal Hessian approximation as initial guess is proposed and developed. Several numerical examples are solved to demonstrate the effectiveness of the preconditioning schemes. It is concluded that the quasi-Newton l-BFGS preconditioning scheme with the pseudo diagonal Gauss-Newton Hessian as initial guess shows the best performances in speeding up the HF Gauss-Newton FWI, improving the convergence rate and reducing the computation burden.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\78GF4NX2\\Pan et al. - 2015 - Preconditioning for the Hessian-free Gauss-Newton .pdf}
}

@techreport{paquet_bayesian_2008,
  title = {Bayesian Inference for Latent Variable Models},
  author = {Paquet, Ulrich},
  year = {2008},
  institution = {{University of Cambridge, Computer Laboratory}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9S33KVNC\\UCAM-CL-TR-724.pdf}
}

@incollection{pardalos_differentiating_2015,
  title = {Differentiating the {{Multipoint Expected Improvement}} for {{Optimal Batch Design}}},
  booktitle = {Machine {{Learning}}, {{Optimization}}, and {{Big Data}}},
  author = {Marmin, S{\'e}bastien and Chevalier, Cl{\'e}ment and Ginsbourger, David},
  editor = {Pardalos, Panos and Pavone, Mario and Farinella, Giovanni Maria and Cutello, Vincenzo},
  year = {2015},
  volume = {9432},
  pages = {37--48},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-27926-8_4},
  abstract = {This work deals with parallel optimization of expensive objective functions which are modelled as sample realizations of Gaussian processes. The study is formalized as a Bayesian optimization problem, or continuous multi-armed bandit problem, where a batch of q {$>$} 0 arms is pulled in parallel at each iteration. Several algorithms have been developed for choosing batches by trading off exploitation and exploration. As of today, the maximum Expected Improvement (EI) and Upper Confidence Bound (UCB) selection rules appear as the most prominent approaches for batch selection. Here, we build upon recent work on the multipoint Expected Improvement criterion, for which an analytic expansion relying on Tallis' formula was recently established. The computational burden of this selection rule being still an issue in application, we derive a closed-form expression for the gradient of the multipoint Expected Improvement, which aims at facilitating its maximization using gradient-based ascent algorithms. Substantial computational savings are shown in application. In addition, our algorithms are tested numerically and compared to state-of-the-art UCB-based batch-sequential algorithms. Combining starting designs relying on UCB with gradient-based EI local optimization finally appears as a sound option for batch design in distributed Gaussian Process optimization.},
  isbn = {978-3-319-27925-1 978-3-319-27926-8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UBYNM2DS\\Marmin et al. - 2015 - Differentiating the Multipoint Expected Improvemen.pdf}
}

@inbook{pardalos_survey_1999,
  title = {A {{Survey}} of {{Some Nonsmooth Equations}} and {{Smoothing Newton Methods}}},
  booktitle = {Progress in {{Optimization}}},
  author = {Qi, L. and Sun, D.},
  year = {1999},
  volume = {30},
  pages = {121--146},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4613-3285-5_7},
  abstract = {In this article we review and summarize recent developments on nonsmooth equations and smoothing Newton methods. Several new suggestions are presented.},
  collaborator = {Eberhard, Andrew and Hill, Robin and Ralph, Daniel and Glover, Barney M.},
  isbn = {978-1-4613-3287-9 978-1-4613-3285-5},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MK5NPSU7\\Qi et Sun - 1999 - A Survey of Some Nonsmooth Equations and Smoothing.pdf}
}

@incollection{pardo-iguzquiza_corrected_2014,
  title = {Corrected {{Kriging Update Formulae}} for {{Batch-Sequential Data Assimilation}}},
  booktitle = {Mathematics of {{Planet Earth}}},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David and Emery, Xavier},
  editor = {{Pardo-Ig{\'u}zquiza}, Eulogio and {Guardiola-Albert}, Carolina and Heredia, Javier and {Moreno-Merino}, Luis and Dur{\'a}n, Juan Jos{\'e} and {Vargas-Guzm{\'a}n}, Jose Antonio},
  year = {2014},
  pages = {119--122},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32408-6_29},
  isbn = {978-3-642-32407-9 978-3-642-32408-6},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LYXH8TW3\\Chevalier et al. - 2014 - Corrected Kriging Update Formulae for Batch-Sequen.pdf}
}

@incollection{park_linearized_2013,
  title = {Linearized {{Physics}} for {{Data Assimilation}} at {{ECMWF}}},
  booktitle = {Data {{Assimilation}} for {{Atmospheric}}, {{Oceanic}} and {{Hydrologic Applications}} ({{Vol}}. {{II}})},
  author = {Janiskov{\'a}, Marta and Lopez, Philippe},
  editor = {Park, Seon Ki and Xu, Liang},
  year = {2013},
  pages = {251--286},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35088-7_11},
  abstract = {A comprehensive set of linearized physical parametrizations has been developed for the global ECMWF Integrated Forecasting System. Implications of the linearity constraint for any parametrization scheme, such as the need for simplification and regularization, are discussed. The description of the methodology to develop linearized parametrizations highlights the complexity of obtaining a physics package that can be efficiently used in practical applications. The impact of the different physical processes on the tangent-linear approximation and adjoint sensitivities, as well as their performance in data assimilation are demonstrated.},
  isbn = {978-3-642-35087-0 978-3-642-35088-7},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7Z5IB8KW\\10163-linearized-physics-data-assimilation-ecmwf.pdf}
}

@book{park_robust_2006,
  title = {Robust {{Design}}: {{An Overview}}},
  shorttitle = {Robust {{Design}}},
  author = {Park, Gyung-Jin and Hwang, Kwang-Hyeon and Lee, Tae and Lee, Kwon-Hee},
  year = {2006},
  month = jan,
  volume = {44},
  doi = {10.2514/1.13639},
  abstract = {Robust design has been developed with the expectation that an insensitive design can be obtained. That is, a product designed by robust design should be insensitive to external noises or tolerances. An insensitive design has more probability to obtain a target value, although there are uncertain noises. Theories of robust design have been developed by adopting the theories of other fields. Based on the theories, robust design can be classified into three methods: 1) the Taguchi method, 2) robust optimization, and 3) robust design with the axiomatic approach. Each method is reviewed and investigated. The methods are examined from a theoretical viewpoint and are discussed from an application viewpoint. The advantages and drawbacks of each method are discussed, and future directions for development are proposed. Copyright \textcopyright{} 2005 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.},
  keywords = {overview,Robust optimization}
}

@article{parzen_estimation_1962,
  title = {On Estimation of a Probability Density Function and Mode},
  author = {Parzen, Emanuel},
  year = {1962},
  journal = {The annals of mathematical statistics},
  volume = {33},
  number = {3},
  pages = {1065--1076},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LPM2WCGR\\Parzen - 1962 - On estimation of a probability density function an.pdf;C\:\\Users\\a846735\\Zotero\\storage\\JAE2F2MP\\2237880.html}
}

@phdthesis{pastel_estimation_2012,
  type = {Thesis},
  title = {Estimation de Probabilit\'es d'\'ev\`enements Rares et de Quantiles Extr\^emes : Applications Dans Le Domaine A\'erospatial},
  shorttitle = {Estimation de Probabilit\'es d'\'ev\`enements Rares et de Quantiles Extr\^emes},
  author = {Pastel, Rudy},
  year = {2012},
  month = jan,
  journal = {http://www.theses.fr},
  abstract = {Rare event dedicated techniques are of great interest for the aerospace industry because of the large amount money that can be lost because of risks associated with minute probabilities. This thesis is focused on the search of probability techniques able to estimate rare event probabilities and extreme quantiles associated with a black box system with static random inputs through two case studies from the industry. The first one is the estimation of the probability of collision between satellites Iridium and Cosmos. The Cross-Entropy (CE), the Non-parametric Adaptive Importance Sampling (NAIS) and an Adaptive Splitting Technique (AST) are compared. Through the comparison, an improved version of NAIS is designed. Whereas NAIS needs to be initiated with a auxiliary random variable which straight away generates rare events, the Adaptive NAIS (ANAIS) allows one to use the original input random as initial auxiliary density and therefore does not require a priori knowledge. The second case is the estimation of the safety zone with respect to the fall of a spacecraft booster. Though they can be estimated via ANAIS or AST, extreme quantiles are shown to be not adapted to spatial distribution. For that purpose, the Minimum Volume Set (MVS) is chosen from the literature. The Crude Monte Carlo (CMC) plug-in MVS estimator being not adapted to extreme level MVS estimation, both ANAIS and AST are adapted into plug-in extreme MVS estimators. These two algorithms outperform the CMC plug-in MVS estimator.},
  school = {Rennes 1},
  keywords = {DÃ©bris spatiaux,Ãchantillonnage adaptatif (statistique),Ãvaluation du risque,MathÃ©matiques et applications,Monte-Carlo; MÃ©thode de},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BYSZJNMY\\2012REN1S024.html}
}

@article{paszke_pytorch_2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.01703 [cs, stat]},
  eprint = {1912.01703},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IDRYF3LB\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf;C\:\\Users\\a846735\\Zotero\\storage\\DZIUY9AG\\1912.html}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FH5S8YDG\\Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@article{pedregosa_scikit-learn:_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  month = oct,
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825-2830},
  issn = {1533-7928},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XZ34DBFU\\Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@phdthesis{pellerej_etude_2018,
  title = {{Etude et d\'eveloppement d'algorithmes d'assimilation de donn\'ees variationnelle adapt\'es aux mod\`eles coupl\'es oc\'ean-atmosph\`ere}},
  author = {Pellerej, R{\'e}mi},
  year = {2018},
  month = mar,
  abstract = {La qualit\'e des pr\'evisions m\'et\'eorologiques repose principalement sur la qualit\'e du mod\`ele utilis\'e et de son \'etat initial. Cet \'etat initial est reconstitu\'e en combinant les informations provenant du mod\`ele et des observations disponibles en utilisant des techniques d'assimilation de donn\'ees. Historiquement, les pr\'evisions et l'assimilation sont r\'ealis\'ees dans l'atmosph\`ere et l'oc\'ean de mani\`ere d\'ecoupl\'ee. Cependant, les centres op\'erationnels d\'eveloppent et utilisent de plus en plus des mod\`eles coupl\'es oc\'ean-atmosph\`ere. Or, assimiler des donn\'ees de mani\`ere d\'ecoupl\'ee n'est pas satisfaisant pour des syst\`emes coupl\'es. En effet, l'\'etat initial ainsi obtenu pr\'esente des inconsistances de flux \`a l'interface entre les milieux, engendrant des erreurs de pr\'evision. Il y a donc besoin d'adapter les m\'ethodes d'assimilation aux syst\`emes coupl\'es. Ces travaux de th\`ese s'inscrivent dans ce contexte et ont \'et\'e effectu\'es dans le cadre du projet FP7 ERA-Clim2, visant \`a produire une r\'eanalyse globale du syst\`eme terrestre.Dans une premi\`ere partie, nous introduisons les notions d'assimilation de donn\'ees, de couplage et les diff\'erentes m\'ethodologies existantes appliqu\'ees au probl\`eme de l'assimilation coupl\'ee. Ces m\'ethodologies n'\'etant pas satisfaisantes en terme de qualit\'e de couplage ou de co\^ut de calcul, nous proposons, dans une seconde partie, des m\'ethodes alternatives. Nous faisons le choix de m\'ethodes d'assimilation bas\'ees sur la th\'eorie du contr\^ole optimal. Ces alternatives se distinguent alors par le choix de la fonction co\^ut \`a minimiser, des variables contr\^ol\'ees et de l'algorithme de couplage utilis\'e. Une \'etude th\'eorique de ces algorithmes a permis de d\'eterminer un crit\`ere n\'ecessaire et suffisant de convergence dans un cadre lin\'eaire. Pour conclure cette seconde partie, les performances des diff\'erentes m\'ethodes introduites sont \'evalu\'ees en terme de qualit\'e de l'analyse produite et de co\^ut de calcul \`a l'aide d'un mod\`ele coupl\'e lin\'eaire 1D. Dans une troisi\`eme et derni\`ere partie, un mod\`ele coupl\'e non-lin\'eaire 1D incluant des param\'etrisations physique a \'et\'e d\'evelopp\'e et impl\'ement\'e dans OOPS (textit\{Object-Oriented Prediction System\}) qui est une surcouche logicielle permettant la mise en \oe uvre d'un ensemble d'algorithmes d'assimilation de donn\'ees. Nous avons alors pu \'evaluer la robustesse de nos algorithmes dans un cadre plus r\'ealiste, et conclure sur leurs performances vis \`a vis de m\'ethodes existantes. Le fait d'avoir d\'evelopp\'e nos m\'ethodes dans le cadre de OOPS devrait permettre \`a l'avenir de les appliquer ais\'ement \`a des mod\`eles r\'ealistes de pr\'evision. Nous exposons enfin quelques perspectives d'am\'elioration de ces algorithmes.},
  langid = {french},
  school = {Universit\'e Grenoble Alpes},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LE54ZN4X\\Pellerej - 2018 - Etude et dÃ©veloppement d'algorithmes d'assimilatio.pdf;C\:\\Users\\a846735\\Zotero\\storage\\9YS7EXAN\\tel-01897347.html}
}

@article{penny_bayesian_2014,
  title = {Bayesian {{Inference}} for the {{Multivariate Normal}}},
  author = {Penny, Will},
  year = {2014},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F5IYKDPX\\bmn.pdf}
}

@article{penny_integrating_2022,
  title = {Integrating {{Recurrent Neural Networks With Data Assimilation}} for {{Scalable Data-Driven State Estimation}}},
  author = {Penny, S. G. and Smith, T. A. and Chen, T.-C. and Platt, J. A. and Lin, H.-Y. and Goodliff, M. and Abarbanel, H. D. I.},
  year = {2022},
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {14},
  number = {3},
  pages = {e2021MS002843},
  issn = {1942-2466},
  doi = {10.1029/2021MS002843},
  abstract = {Data assimilation (DA) is integrated with machine learning in order to perform entirely data-driven online state estimation. To achieve this, recurrent neural networks (RNNs) are implemented as pretrained surrogate models to replace key components of the DA cycle in numerical weather prediction (NWP), including the conventional numerical forecast model, the forecast error covariance matrix, and the tangent linear and adjoint models. It is shown how these RNNs can be initialized using DA methods to directly update the hidden/reservoir state with observations of the target system. The results indicate that these techniques can be applied to estimate the state of a system for the repeated initialization of short-term forecasts, even in the absence of a traditional numerical forecast model. Further, it is demonstrated how these integrated RNN-DA methods can scale to higher dimensions by applying domain localization and parallelization, providing a path for practical applications in NWP.},
  langid = {english},
  keywords = {4D-var,artificial intelligence,data assimilation,ensemble kalman filter,machine learning,recurrent neural networks},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002843},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PYMCQLRZ\\Penny et al. - 2022 - Integrating Recurrent Neural Networks With Data As.pdf;C\:\\Users\\a846735\\Zotero\\storage\\UFWUAY8C\\2021MS002843.html}
}

@misc{perren_gabriel-ppythonmcmc_2020,
  title = {Gabriel-p/{{pythonMCMC}}},
  author = {Perren, Gabriel},
  year = {2020},
  month = may,
  abstract = {A list of Python-based MCMC packages. Contribute to Gabriel-p/pythonMCMC development by creating an account on GitHub.},
  copyright = {GPL-3.0}
}

@misc{perrier_robust_nodate,
  title = {Robust {{Bayesian}} Filter with Student-t Likelihood Noise},
  author = {Perrier, Regis},
  file = {/home/victor/Documents/robustBF.pdf}
}

@article{petra_computational_2013,
  title = {A Computational Framework for Infinite-Dimensional {{Bayesian}} Inverse Problems: {{Part II}}. {{Stochastic Newton MCMC}} with Application to Ice Sheet Flow Inverse Problems},
  shorttitle = {A Computational Framework for Infinite-Dimensional {{Bayesian}} Inverse Problems},
  author = {Petra, Noemi and Martin, James and Stadler, Georg and Ghattas, Omar},
  year = {2013},
  month = aug,
  journal = {arXiv:1308.6221 [math, stat]},
  eprint = {1308.6221},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We address the numerical solution of infinite-dimensional inverse problems in the framework of Bayesian inference. In the Part I companion to this paper (arXiv.org:1308.1313), we considered the linearized infinite-dimensional inverse problem. Here in Part II, we relax the linearization assumption and consider the fully nonlinear infinite-dimensional inverse problem using a Markov chain Monte Carlo (MCMC) sampling method. To address the challenges of sampling high-dimensional pdfs arising from Bayesian inverse problems governed by PDEs, we build on the stochastic Newton MCMC method. This method exploits problem structure by taking as a proposal density a local Gaussian approximation of the posterior pdf, whose construction is made tractable by invoking a low-rank approximation of its data misfit component of the Hessian. Here we introduce an approximation of the stochastic Newton proposal in which we compute the low-rank-based Hessian at just the MAP point, and then reuse this Hessian at each MCMC step. We compare the performance of the proposed method to the original stochastic Newton MCMC method and to an independence sampler. The comparison of the three methods is conducted on a synthetic ice sheet inverse problem. For this problem, the stochastic Newton MCMC method with a MAP-based Hessian converges at least as rapidly as the original stochastic Newton MCMC method, but is far cheaper since it avoids recomputing the Hessian at each step. On the other hand, it is more expensive per sample than the independence sampler; however, its convergence is significantly more rapid, and thus overall it is much cheaper. Finally, we present extensive analysis and interpretation of the posterior distribution, and classify directions in parameter space based on the extent to which they are informed by the prior or the observations.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z22783B7\\Petra et al. - 2013 - A computational framework for infinite-dimensional.pdf;C\:\\Users\\a846735\\Zotero\\storage\\C555J7FY\\1308.html}
}

@article{petrone_robustness_2011,
  title = {Robustness Criteria in Optimization under Uncertainty},
  author = {Petrone, Giovanni and Iaccarino, Gianluca and Quagliarella, D.},
  year = {2011},
  journal = {Evolutionary and deterministic methods for design, optimization and control (EUROGEN 2011). CIRA, Capua},
  pages = {244--252},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BBSY2QRE\\Petrone et al. - 2011 - Robustness criteria in optimization under uncertai.pdf}
}

@article{peyre_computational_2019,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2019},
  month = apr,
  journal = {arXiv:1803.00567 [stat]},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\326ZB4MZ\\PeyrÃ© et Cuturi - 2019 - Computational Optimal Transport.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RXCU72DH\\1803.html}
}

@article{peyre_computational_2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2020},
  month = mar,
  journal = {arXiv:1803.00567 [stat]},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DHFNKVGH\\PeyrÃ© et Cuturi - 2020 - Computational Optimal Transport.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XXCUZ4P9\\1803.html}
}

@article{peyron_latent_2021,
  title = {Latent {{Space Data Assimilation}} by Using {{Deep Learning}}},
  author = {Peyron, Mathis and Fillion, Anthony and G{\"u}rol, Selime and Marchais, Victor and Gratton, Serge and Boudier, Pierre and Goret, Gael},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.00430 [cs, math]},
  eprint = {2104.00430},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Performing Data Assimilation (DA) at a low cost is of prime concern in Earth system modeling, particularly at the time of big data where huge quantities of observations are available. Capitalizing on the ability of Neural Networks techniques for approximating the solution of PDE's, we incorporate Deep Learning (DL) methods into a DA framework. More precisely, we exploit the latent structure provided by autoencoders (AEs) to design an Ensemble Transform Kalman Filter with model error (ETKF-Q) in the latent space. Model dynamics are also propagated within the latent space via a surrogate neural network. This novel ETKF-Q-Latent (thereafter referred to as ETKF-Q-L) algorithm is tested on a tailored instructional version of Lorenz 96 equations, named the augmented Lorenz 96 system: it possesses a latent structure that accurately represents the observed dynamics. Numerical experiments based on this particular system evidence that the ETKF-Q-L approach both reduces the computational cost and provides better accuracy than state of the art algorithms, such as the ETKF-Q.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6B38QS6U\\Peyron et al. - 2021 - Latent Space Data Assimilation by using Deep Learn.pdf}
}

@incollection{pflug_remarks_2000,
  title = {Some Remarks on the Value-at-Risk and the Conditional Value-at-Risk},
  booktitle = {Probabilistic Constrained Optimization},
  author = {Pflug, Georg Ch},
  year = {2000},
  pages = {272--281},
  publisher = {{Springer}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7K85S8BF\\Pflug - 2000 - Some remarks on the value-at-risk and the conditio.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PWB3L5GT\\978-1-4757-3150-7_15.html}
}

@article{piano_tidal_2017,
  title = {Tidal Stream Resource Assessment Uncertainty Due to Flow Asymmetry and Turbine Yaw Misalignment},
  author = {Piano, M. and Neill, S. P. and Lewis, M. J. and Robins, P. E. and Hashemi, M. R. and Davies, A. G. and Ward, S. L. and Roberts, M. J.},
  year = {2017},
  month = dec,
  journal = {Renewable Energy},
  volume = {114},
  pages = {1363--1375},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2017.05.023},
  abstract = {The majority of tidal energy convertors (TECs) currently under development are of a non-yawing horizontal axis design. However, most energetic regions that have been identified as candidate sites for installation of TEC arrays exhibit some degree of directional and magnitude asymmetry between incident flood and ebb flow angles and velocities, particularly in nearshore environments where topographic, bathymetric and seabed frictional effects and interactions are significant. Understanding the contribution of directional and magnitude asymmetry to resource power density along with off axis rotor alignment to flow could influence site selection and help elucidate optimal turbine orientation. Here, 2D oceanographic model simulations and field data were analysed to investigate these effects at potential deployment locations in the Irish Sea; an energetic semi-enclosed shelf sea region. We find that observed sites exhibiting a high degree of asymmetry may be associated with a reduction of over 2\% in annual energy yield when deployment design optimisation is ignored. However, at the majority of sites, even in the presence of significant asymmetry, the difference is {$<$}0.3\%. Although the effects are shown to have less significance than other uncertainties in resource assessment, these impacts could be further investigated and quantified using CFD and 3D modelling.},
  langid = {english},
  keywords = {Marine renewable energy,Telemac 2D Irish Sea hydrodynamic modelling and ADCP observations,Tidal flow asymmetry,Tidal resource assessment and optimisation,Tidal stream characterisation,Turbine yaw misalignment},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B5I6CFNP\\Piano et al. - 2017 - Tidal stream resource assessment uncertainty due t.pdf;C\:\\Users\\a846735\\Zotero\\storage\\KJQD76MP\\S0960148117304081.html}
}

@article{picheny_adaptive_2010,
  title = {Adaptive {{Designs}} of {{Experiments}} for {{Accurate Approximation}} of a {{Target Region}}},
  author = {Picheny, Victor and Ginsbourger, David and Roustant, Olivier and Haftka, Raphael T. and Kim, Nam-Ho},
  year = {2010},
  journal = {Journal of Mechanical Design},
  volume = {132},
  number = {7},
  pages = {071008},
  issn = {10500472},
  doi = {10.1115/1.4001873},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YLSGBQYE\\Picheny et al. - 2010 - Adaptive Designs of Experiments for Accurate Appro.pdf}
}

@article{picheny_benchmark_2013,
  title = {A Benchmark of Kriging-Based Infill Criteria for Noisy Optimization},
  author = {Picheny, Victor and Wagner, Tobias and Ginsbourger, David},
  year = {2013},
  month = sep,
  journal = {Structural and Multidisciplinary Optimization},
  volume = {48},
  number = {3},
  pages = {607--626},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-013-0919-4},
  abstract = {Responses of many real-world problems can only be evaluated perturbed by noise. In order to make an efficient optimization of these problems possible, intelligent optimization strategies successfully coping with noisy evaluations are required. In this article, a comprehensive comparison of existing kriging-based methods for the optimization of noisy functions is provided. Ten methods are described using a unified formalism, and compared on analytical benchmark problems with different configurations (noise level, maximum number of observations, initial number of observations). It is found that the optimal method depends on the optimization problem, even though some criteria are consistently more efficient than others.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B65I8QBM\\Picheny et al. - 2013 - A benchmark of kriging-based infill criteria for n.pdf}
}

@article{picheny_noisy_2014,
  title = {Noisy Kriging-Based Optimization Methods: {{A}} Unified Implementation within the {{DiceOptim}} Package},
  shorttitle = {Noisy Kriging-Based Optimization Methods},
  author = {Picheny, Victor and Ginsbourger, David},
  year = {2014},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {71},
  pages = {1035--1053},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2013.03.018},
  abstract = {Kriging-based optimization relying on noisy evaluations of complex systems has recently motivated contributions from various research communities. Five strategies have been implemented in the DiceOptim package. The corresponding functions constitute a user-friendly tool for solving expensive noisy optimization problems in a sequential framework, while offering some flexibility for advanced users. Besides, the implementation is done in a unified environment, making this package a useful device for studying the relative performances of existing approaches depending on the experimental setup. An overview of the package structure and interface is provided, as well as a description of the strategies and some insight about the implementation challenges and the proposed solutions. The strategies are compared to some existing optimization packages on analytical test functions and show promising performances.},
  langid = {english},
  keywords = {Active learning,Computer experiments,Gaussian processes},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GC4LURL4\\Picheny et Ginsbourger - 2014 - Noisy kriging-based optimization methods A unifie.pdf;C\:\\Users\\a846735\\Zotero\\storage\\AU2HT6XM\\S0167947313001205.html}
}

@misc{pillaud-vivien_learning_2020,
  title = {Learning with {{Reproducing Kernel Hilbert Spaces}}: {{Stochastic Gradient Descent}} and {{Laplacian Estimation}}},
  shorttitle = {Learning with {{Reproducing Kernel Hilbert Spaces}}},
  author = {{Pillaud-vivien}, Loucas},
  year = {2020},
  month = oct,
  journal = {http://www.theses.fr},
  abstract = {L'apprentissage automatique a re\c{c}u beaucoup d'attention au cours des deux derni\`eres d\'ecennies, \`a la fois de la part de l'industrie pour des probl\`emes de d\'ecision bas\'es sur des donn\'ees et de la communaut\'e scientifique en g\'en\'eral. Cette attention r\'ecente est certainement due \`a sa capacit\'e \`a r\'esoudre efficacement une large classe de probl\`emes en grande dimension gr\^ace \`a des algorithmes rapides et faciles \`a mettre en oeuvre. Quel est le type de probl\`emes abord\'es par l'apprentissage automatique ? D'une mani\`ere g\'en\'erale, r\'epondre \`a cette question n\'ecessite de le diviser en deux th\`emes distincts: l'apprentissage supervis\'e et l'apprentissage non supervis\'e. Ces deux axes principaux trouvent un \'echo dans cette th\`ese. Dans un premier temps, la partie concernant l'apprentissage supervis\'e \'etudie th\'eoriquement la pierre angulaire de toutes les techniques d'optimisation li\'ees \`a ces probl\`emes: les m\'ethodes de gradient stochastique. Gr\^ace \`a leur polyvalence, elles participent largement au r\'ecent succ\`es de l'apprentissage. Cependant, malgr\'e leur simplicit\'e, leur efficacit\'e n'est pas encore pleinement comprise. L'\'etude de certaines propri\'et\'es de cet algorithme est l'une des deux questions importantes de cette th\`ese. Dans un second temps, la partie consacr\'ee \`a l'apprentissage non supervis\'e est li\'ee \`a un probl\`eme plus sp\'ecifique : nous concevons dans cette \'etude un algorithme pour trouver des mod\`eles r\'eduits pour des dynamiques emprunt\'ees \`a la physique. Cette partie aborde une question cruciale en physique statistique computationnelle (\'egalement appel\'ee dynamique mol\'eculaire). M\^eme si les deux probl\`emes sont de nature diff\'erente, ces deux directions partagent une caract\'eristique commune : elles tirent parti de l'utilisation d'espaces \`a noyau reproduisant, qui poss\`edent deux propri\'et\'es essentielles : (i) ils s'adaptent naturellement au cadre stochastique tout en pr\'eservant une certaine efficacit\'e num\'erique, (ii) ils montrent une grande expressivit\'e en tant que classe de fonctions de test. La premi\`ere contribution de cette th\`ese est de montrer la convergence exponentielle de la descente de gradient stochastique pour la perte binaire dans le cas o\`u la t\^ache de classification est ÂfacileÂ. Ce travail \'etablit \'egalement des bornes th\'eoriques fines sur la descente de gradient stochastique dans les espaces \`a noyau reproduisant, ce qui peut \^etre consid\'er\'e comme un r\'esultat en lui-m\^eme. La deuxi\`eme contribution se concentre sur l'optimalit\'e de la descente de gradient stochastique dans le cadre non param\'etrique pour des probl\`emes de r\'egression. Plus pr\'ecis\'ement, ce travail est le premier \`a montrer que de multiples passages sur les donn\'ees permettent d'atteindre l'optimalit\'e dans certains cas o\`u l'optimum de Bayes est difficile \`a approcher. Ce travail tente de r\'econcilier la th\'eorie et la pratique car les travaux actuels sur la descente de gradient stochastique ont toujours montr\'e qu'il suffisait d'un passage sur les donn\'ees. En physique statistique computationnelle comme en apprentissage automatique, la question de trouver des repr\'esentations de faible dimension (principaux degr\'es de libert\'e) est cruciale. Telle est la question abord\'ee par la troisi\`eme contribution de cette th\`ese. Nous montrons plus pr\'ecis\'ement comment il est possible d'estimer la constante de Poincar\'e d'une distribution \`a travers des \'echantillons de celle-ci. Ensuite, nous exploitons cette estimation pour concevoir un algorithme \`a la recherche de coordonn\'ees de r\'eaction qui sont les pierres angulaires des techniques d'acc\'el\'eration dans le contexte de la dynamique mol\'eculaire. D\'etailler, affiner et \'etendre ce r\'esultat est la quatri\`eme contribution de ce manuscrit.},
  howpublished = {http://www.theses.fr/s191786},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y5DH4MWU\\These.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RKXJXBZ4\\s191786.html}
}

@article{pinar_mean_2014,
  title = {Mean Semi-Deviation from a Target and Robust Portfolio Choice under Distribution and Mean Return Ambiguity},
  author = {P{\i}nar, Mustafa {\c C}. and Burak Pa{\c c}, A.},
  year = {2014},
  month = mar,
  journal = {Journal of Computational and Applied Mathematics},
  series = {Recent {{Advances}} in {{Applied}} and {{Computational Mathematics}}: {{ICACM-IAM-METU}}},
  volume = {259},
  pages = {394--405},
  issn = {0377-0427},
  doi = {10.1016/j.cam.2013.06.028},
  abstract = {We consider the problem of optimal portfolio choice using the lower partial moments risk measure for a market consisting of n risky assets and a riskless asset. For when the mean return vector and variance/covariance matrix of the risky assets are specified without specifying a return distribution, we derive distributionally robust portfolio rules. We then address potential uncertainty (ambiguity) in the mean return vector as well, in addition to distribution ambiguity, and derive a closed-form portfolio rule for when the uncertainty in the return vector is modelled via an ellipsoidal uncertainty set. Our result also indicates a choice criterion for the radius of ambiguity of the ellipsoid. Using the adjustable robustness paradigm we extend the single-period results to multiple periods, and derive closed-form dynamic portfolio policies which mimic closely the single-period policy.},
  keywords = {Adjustable robustness,Distributional robustness,Dynamic portfolio rules,Ellipsoidal uncertainty,Lower partial moments,Portfolio choice},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3ELPEWAP\\PÄ±nar et Burak PaÃ§ - 2014 - Mean semi-deviation from a target and robust portf.pdf;C\:\\Users\\a846735\\Zotero\\storage\\9JNPBK3H\\S0377042713003269.html}
}

@article{pires_extending_1996-1,
  title = {On Extending the Limits of Variational Assimilation in Nonlinear Chaotic Systems},
  author = {Pires, Carlos and Vautard, Robert and Talagrand, Olivier},
  year = {1996},
  journal = {Tellus A},
  volume = {48},
  number = {1},
  pages = {96--121},
  issn = {1600-0870},
  doi = {10.1034/j.1600-0870.1996.00006.x},
  abstract = {A study is made of the limits imposed on variational assimilation of observations by the chaotic character of the atmospheric flow. The primary goal of the study is to determine to which degree, and how, the knowledge of past noisy observations can improve the knowledge of the present state of a chaotic system. The study is made under the hypothesis of a perfect model. Theoretical results are illustrated by numerical experiments performed with the classical three-variable system introduced by Lorenz. Both theoretical and numerical results show that, even in the chaotic regime, appropriate use of past observations improves the accuracy on the estimate of the present state of the flow. However, the resulting estimation error mostly projects onto the unstable modes of the system, and the corresponding gain in predictability is limited. Theoretical considerations provide explicit estimates of the statistics of the assimilation error. The error depends on the state of the flow over the assimilation period. It is largest when there has been a period of strong instability in the very recent past. In the limit of infinitely long assimilation periods, the behaviour of the cost-function of variational assimilation is singular: it tends to fold into deep narrow ``valleys'' parallel to the sheets of the unstable manifold of the system. An unbounded number of secondary minima appear, where solutions of minimization algorithms can be trapped. The absolute minimum of the cost-function always lies on the sheet of the unstable manifold containing the exact state of the flow. But the error along the unstable manifold saturates to a finite value, and the absolute minimum of the cost function does not, in general, converge to the exact state of the flow. Even so, the absolute minimum of the cost function is the best estimate that can be obtained of the state of the flow. An algorithm is proposed, the quasi-static variational assimilation, for determining the absolute minimum, based on successive small increments of the assimilation period and quasi-static adjustments of the minimizing solution. Finally, the impact of assimilation on predictability is assessed by forecast experiments with that system. The ability of the present paper lies mainly in the qualitative results it presents. Qualitative estimates relevant for the atmosphere call for further studies.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1034/j.1600-0870.1996.00006.x},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2TAPR2MP\\Pires et al. - 1996 - On extending the limits of variational assimilatio.pdf;C\:\\Users\\a846735\\Zotero\\storage\\MW6DVB3D\\j.1600-0870.1996.00006.html}
}

@article{plessix_review_2006,
  title = {A Review of the Adjoint-state Method for Computing the Gradient of a Functional with Geophysical Applications},
  author = {Plessix, Rene-Edouard},
  year = {2006},
  month = nov,
  journal = {Geophysical Journal International},
  volume = {167},
  pages = {495--503},
  doi = {10.1111/j.1365-246X.2006.02978.x},
  abstract = {Estimating the model parameters from measured data generally consists of minimizing an error functional. A classic technique to solve a minimization problem is to successively determine the minimum of a series of linearized problems. This formulation requires the Fr\'echet derivatives (the Jacobian matrix), which can be expensive to compute. If the minimization is viewed as a non-linear optimization problem, only the gradient of the error functional is needed. This gradient can be computed without the Fr\'echet derivatives. In the 1970s, the adjoint-state method was developed to efficiently compute the gradient. It is now a well-known method in the numerical community for computing the gradient of a functional with respect to the model parameters when this functional depends on those model parameters through state variables, which are solutions of the forward problem. However, this method is less well understood in the geophysical community. The goal of this paper is to review the adjoint-state method. The idea is to define some adjoint-state variables that are solutions of a linear system. The adjoint-state variables are independent of the model parameter perturbations and in a way gather the perturbations with respect to the state variables. The adjoint-state method is efficient because only one extra linear system needs to be solved. Several applications are presented. When applied to the computation of the derivatives of the ray trajectories, the link with the propagator of the perturbed ray equation is established.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EMJGCC7C\\Plessix - 2006 - A review of the adjointâstate method for computing.pdf}
}

@article{pogany_characteristic_2010,
  title = {On the Characteristic Function of the Generalized Normal Distribution},
  author = {Pog{\'a}ny, Tibor K. and Nadarajah, Saralees},
  year = {2010},
  month = feb,
  journal = {Comptes Rendus Mathematique},
  volume = {348},
  number = {3-4},
  pages = {203--206},
  issn = {1631073X},
  doi = {10.1016/j.crma.2009.12.010},
  abstract = {For the first time, an explicit closed form expression is derived for the characteristic function of the generalized normal distribution (GND). Also derived is an expression for the correlation coefficient between variate-values and their ranks in samples from the GND. The expression for the former involves the Fox-Wright generalized confluent hypergeometric 1{$\Psi$}0-function, while the latter is expressed via the Gaussian hypergeometric 2F1.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\D93CMEFF\\PogÃ¡ny et Nadarajah - 2010 - On the characteristic function of the generalized .pdf}
}

@article{polavarapu_data_2005,
  title = {Data Assimilation with the {{Canadian Middle Atmosphere Model}}},
  author = {Polavarapu, Saroja and Ren, Shuzhan and Rochon, Y and Sankey, David and Ek, Nils and Koshyk, John and Tarasick, David},
  year = {2005},
  month = mar,
  journal = {Atmosphere-ocean - ATMOS OCEAN},
  volume = {43},
  pages = {77--100},
  doi = {10.3137/ao.430105},
  abstract = {A data assimilation scheme has been coupled to the Canadian Middle Atmosphere Model, providing, for the first time, the capability of assimilating data from the ground to the top of the mesosphere (about 95 km). This model is a full general circulation model with on-line fully interactive chemistry involving 127 gas-phase and heterogeneous reactions. Thus, feedback between dynamics, chemistry and radiation occurs in every model time step. In this work, validation of the system for tropospheric and lower stratospheric analyses is undertaken with the standard observation set used in operational weather forecasting. Results are found to agree reasonably well with radiosonde observations and with Met Office (UK) analyses. Although ozone is not assimilated, ozone fields match total column observations well in terms of synoptic patterns. However, due to model biases, total column values are too large at mid-latitudes and too small in the tropics. Since the assimilation scheme was designed for tropospheric weather prediction, its application to a middle atmosphere model can help to identify the challenges of assimilating data from this region of the atmosphere.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2XL957FF\\Polavarapu et al. - 2005 - Data assimilation with the Canadian Middle Atmosph.pdf}
}

@article{poursoltani_adjustable_nodate,
  title = {Adjustable {{Robust Optimization Reformulations}} of {{Two-Stage Worst-case Regret Minimization Problems}}},
  author = {Poursoltani, Mehran and Delage, Erick},
  pages = {43},
  abstract = {This paper explores the idea that two-stage worst-case regret minimization problems with either objective or right-hand side uncertainty can be reformulated as two-stage robust optimization problems and can therefore benefit from the solution schemes and theoretical knowledge that have been developed in the last decade for this class of problems. In particular, we identify conditions under which a first-stage decision can be obtained either exactly using popular adjustable robust optimization decomposition schemes, or approximately by conservatively employing affine decision rules. Furthermore, we provide both numerical and theoretical evidence that in practice the first-stage decision obtained using affine decision rules is of high quality. Initially, this is done by establishing mild conditions under which these decisions can be proven exact, which effectively extends the space of regret minimization problems known to be solvable in polynomial time. We further evaluate both the sub-optimality and computational efficiency of this tractable approximation scheme in a multi-item newsvendor problem and a production transportation problem.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XRJKI96Q\\Poursoltani et Delage - Adjustable Robust Optimization Reformulations of T.pdf}
}

@article{pronzato_minimax_2017,
  title = {Minimax and Maximin Space-Filling Designs: Some Properties and Methods for Construction},
  author = {Pronzato, Luc},
  year = {2017},
  pages = {31},
  abstract = {A few properties of minimax and maximin optimal designs in a compact subset of Rd are presented, and connections with other space-filling constructions are indicated. Several methods are given for the evaluation of the minimax-distance (or dispersion) criterion for a given n-point design. Various optimisation methods are proposed and their limitations, in particular in terms of dimension d, are indicated. A large majority of the results presented are not new, but their collection in a single document containing a respectable bibliography will hopefully be useful to the reader.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7APNG8N5\\Pronzato - 2017 - Minimax and maximin space-filling designs some pr.pdf}
}

@article{pujol_minimisation_nodate,
  title = {{Minimisation de quantiles \textendash{} application en m\'ecanique}},
  author = {Pujol, Gilles and Riche, Rodolphe Le and Bay, Xavier and Roustant, Olivier},
  pages = {7},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\GSYTV4DK\\Pujol et al. - Minimisation de quantiles â application en mÃ©caniq.pdf}
}

@article{pujol_minimisation_nodate-1,
  title = {{Minimisation de quantiles \textendash{} application en m\'ecanique}},
  author = {Pujol, Gilles and Riche, Rodolphe Le and Bay, Xavier and Roustant, Olivier},
  pages = {7},
  langid = {french},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FBJFENED\\Pujol et al. - Minimisation de quantiles â application en mÃ©caniq.pdf}
}

@article{puthawala_universal_2022,
  title = {Universal {{Joint Approximation}} of {{Manifolds}} and {{Densities}} by {{Simple Injective Flows}}},
  author = {Puthawala, Michael and Lassas, Matti and Dokmani{\'c}, Ivan and {de Hoop}, Maarten},
  year = {2022},
  month = jan,
  journal = {arXiv:2110.04227 [cs]},
  eprint = {2110.04227},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study approximation of probability measures supported on \$n\$-dimensional manifolds embedded in R\^m by injective flows -- neural networks composed of invertible flows and injective layers. We show that in general, injective flows between R\^n and R\^m universally approximate measures supported on images of extendable embeddings, which are a subset of standard embeddings: when the embedding dimension m is small, topological obstructions may preclude certain manifolds as admissible targets. When the embedding dimension is sufficiently large, m \textbackslash geq 3n+1, we use an argument from algebraic topology known as the clean trick to prove that the topological obstructions vanish and injective flows universally approximate any differentiable embedding. Along the way we show that the studied injective flows admit efficient projections on the range, and that their optimality can be established "in reverse," resolving a conjecture made in Brehmer and Cranmer 2020},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3BKLNYF4\\Puthawala et al. - 2022 - Universal Joint Approximation of Manifolds and Den.pdf}
}

@article{qin_improving_2017,
  title = {Improving the {{Expected Improvement Algorithm}}},
  author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
  year = {2017},
  month = may,
  journal = {arXiv:1705.10033 [cs, stat]},
  eprint = {1705.10033},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\V2QHCQYZ\\Qin et al. - 2017 - Improving the Expected Improvement Algorithm.pdf;C\:\\Users\\a846735\\Zotero\\storage\\NBKTIDCQ\\1705.html}
}

@article{quagliarella_optimization_2014,
  title = {Optimization {{Under Uncertainty Using}} the {{Generalized Inverse Distribution Function}}},
  author = {Quagliarella, Domenico and Petrone, Giovanni and Iaccarino, Gianluca},
  year = {2014},
  month = jul,
  journal = {arXiv:1407.4636 [cs, math]},
  eprint = {1407.4636},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {A framework for robust optimization under uncertainty based on the use of the generalized inverse distribution function (GIDF), also called quantile function, is here proposed. Compared to more classical approaches that rely on the usage of statistical moments as deterministic attributes that define the objectives of the optimization process, the inverse cumulative distribution function allows for the use of all the possible information available in the probabilistic domain. Furthermore, the use of a quantile based approach leads naturally to a multi-objective methodology which allows an a-posteriori selection of the candidate design based on risk/opportunity criteria defined by the designer. Finally, the error on the estimation of the objectives due to the resolution of the GIDF will be proven to be quantifiable},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IK2F8YTC\\Quagliarella et al. - 2014 - Optimization Under Uncertainty Using the Generaliz.pdf}
}

@book{rachdi_apprentissage_2011,
  title = {Apprentissage Statistique et Computer Experiments : Approche Quantitative Du Risque et Des Incertitudes En Mod\'elisation},
  shorttitle = {Apprentissage Statistique et Computer Experiments},
  author = {Rachdi, Nabil},
  year = {2011},
  month = jan,
  publisher = {{Toulouse 3}},
  abstract = {Cette th\`ese s'inscrit dans le domaine de l'apprentissage statistique et dans celui des exp\'eriences simul\'ees (computer experiments). Son objet est de proposer un cadre g\'en\'eral permettant d'estimer les param\`etres d'un code de simulation num\'erique de fa\c{c}on \`a reproduire au mieux certaines caract\'eristiques d'int\'er\^et extraites de donn\'ees observ\'ees. Ce travail recouvre le cadre classique de l'estimation param\'etrique dans un mod\`ele de r\'egression et \'egalement la calibration de la densit\'e de probabilit\'e des variables d'entr\'ee d'un code num\'erique afin de reproduire une loi de probabilit\'e donn\'ee en sortie. Une partie importante de ce travail consiste dans l'estimation param\'etrique d'un code num\'erique \`a partir d'observations. Nous proposons une classe de m\'ethode originale n\'ecessitant une simulation intensive du code num\'erique, que l'on remplacera par un m\'eta-mod\`ele s'il est trop co\^uteux. Nous validons th\'eoriquement les algorithmes propos\'es du point de vue non-asymptotique, en prouvant des bornes sur l'exc\`es de risque. Ces r\'esultats reposent entres autres sur des in\'egalit\'es de concentration. Un second probl\`eme que nous abordons est celui de l'\'etude d'une dualit\'e entre proc\'edure d'estimation et nature de la pr\'ediction recherch\'ee. Il s'agit ici de mieux comprendre l'effet d'une proc\'edure d'estimation des param\`etres d'un code num\'erique sur une caract\'eristique d'int\'er\^et donn\'ee. Enfin, en pratique la d\'etermination des param\`etres optimaux au sens du crit\`ere donn\'e par le risque empirique n\'ecessite la recherche du minimum d'une fonction g\'en\'eralement non convexe et poss\'edant plusieurs minima locaux. Nous proposons un algorithme stochastique consistant \`a combiner une r\'egularisation du crit\`ere par convolution avec un noyau gaussien, de variance d\'ecroissante au fil des it\'erations, avec une m\'ethode d'approximation stochastique du type Kiefer-Wolfowitz.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2FJ9UWPY\\2011TOU30283.pdf;C\:\\Users\\a846735\\Zotero\\storage\\H6T3HW7X\\2011TOU30283.html}
}

@article{rahimian_distributionally_2019,
  title = {Distributionally {{Robust Optimization}}: {{A Review}}},
  shorttitle = {Distributionally {{Robust Optimization}}},
  author = {Rahimian, Hamed and Mehrotra, Sanjay},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.05659 [cs, math, stat]},
  eprint = {1908.05659},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {The concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. Statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. A modeling framework, called distributionally robust optimization (DRO), has recently received significant attention in both the operations research and statistical learning communities. This paper surveys main concepts and contributions to DRO, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DA6HC82X\\Rahimian et Mehrotra - 2019 - Distributionally Robust Optimization A Review.pdf}
}

@article{rakes_selecting_1985,
  title = {Selecting Tolerances in Chance-Constrained Programming: {{A}} Multiple Objective Linear Programming Approach},
  shorttitle = {Selecting Tolerances in Chance-Constrained Programming},
  author = {Rakes, Terry R and Reeves, Gary R},
  year = {1985},
  month = jun,
  journal = {Operations Research Letters},
  volume = {4},
  number = {2},
  pages = {65--69},
  issn = {01676377},
  doi = {10.1016/0167-6377(85)90034-3},
  langid = {english}
}

@article{ranjan_sequential_2008,
  title = {Sequential {{Experiment Design}} for {{Contour Estimation From Complex Computer Codes}}},
  author = {Ranjan, Pritam and Bingham, Derek and Michailidis, George},
  year = {2008},
  month = nov,
  journal = {Technometrics},
  volume = {50},
  number = {4},
  pages = {527--541},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/004017008000000541},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y7A8CR4C\\Ranjan et al. - 2008 - Sequential Experiment Design for Contour Estimatio.pdf}
}

@article{rao_robust_2015,
  title = {Robust Data Assimilation Using \${{L}}\_1\$ and {{Huber}} Norms},
  author = {Rao, Vishwas and Sandu, Adrian and Ng, Michael and {Nino-Ruiz}, Elias},
  year = {2015},
  month = nov,
  journal = {SciRate},
  abstract = {Data assimilation is the process to fuse information from priors, observations of nature, and numerical models, in order to obtain best estimates of the parameters or state of a physical system of interest. Presence of large errors in some observational data, e.g., data collected from a faulty instrument, negatively affect the quality of the overall assimilation results. This work develops a systematic framework for robust data assimilation. The new algorithms continue to produce good analyses in the presence of observation outliers. The approach is based on replacing the traditional \$\textbackslash L\_2\$ norm formulation of data assimilation problems with formulations based on \$\textbackslash L\_1\$ and Huber norms. Numerical experiments using the Lorenz-96 and the shallow water on the sphere models illustrate how the new algorithms outperform traditional data assimilation approaches in the presence of data outliers.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\A7T67T3Z\\Rao et al. - 2015 - Robust data assimilation using $L_1$ and Huber nor.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ELRSCLTJ\\1511.html}
}

@incollection{rasmussen_gaussian_2004,
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2 - 14, 2003, {{T\"ubingen}}, {{Germany}}, {{August}} 4 - 16, 2003, {{Revised Lectures}}},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and {von Luxburg}, Ulrike and R{\"a}tsch, Gunnar},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {63--71},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28650-9_4},
  abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
  isbn = {978-3-540-28650-9},
  langid = {english},
  keywords = {Covariance Function,Gaussian Process,Joint Gaussian Distribution,Marginal Likelihood,Posterior Variance}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NP79YRUY\\Rasmussen et Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@article{raue_joining_2013,
  title = {Joining Forces of {{Bayesian}} and Frequentist Methodology: A Study for Inference in the Presence of Non-Identifiability},
  shorttitle = {Joining Forces of {{Bayesian}} and Frequentist Methodology},
  author = {Raue, Andreas and Kreutz, Clemens and Theis, Fabian Joachim and Timmer, Jens},
  year = {2013},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {371},
  number = {1984},
  pages = {20110544},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2011.0544},
  langid = {english},
  keywords = {profile lik},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YNPGQZKX\\Raue et al. - 2013 - Joining forces of Bayesian and frequentist methodo.pdf}
}

@article{raynaud_vacumm_2014,
  title = {{{VACUMM}} - {{A Python}} Library for Ocean Science},
  author = {Raynaud, Stephane and Charria, Guillaume and Wilkins, Jonathan and Garnier, Val{\'e}rie and Garreau, P and Theetten, S{\'e}bastien},
  year = {2014},
  month = apr,
  journal = {Mercator-Ocean Newsletter},
  pages = {99},
  abstract = {VACUMM is an open-source Python library for processing data from observations and numerical models. The library is now used for several years in research and operational contexts, for instance for producing figures and reports, validating models, converting data, or making simple or advanced diagnostics. In this paper, we introduce how the library is built, and we present two applications of its use: one in an perational context and one in a research context.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CNG8K37B\\Raynaud et al. - 2014 - VACUMM - A Python library for ocean science.pdf}
}

@article{razaaly_extension_2020,
  title = {Extension of {{AK-MCS}} for the Efficient Computation of Very Small Failure Probabilities},
  author = {Razaaly, Nassim and Congedo, Pietro Marco},
  year = {2020},
  month = nov,
  journal = {Reliability Engineering \& System Safety},
  volume = {203},
  pages = {107084},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2020.107084},
  abstract = {We consider the problem of estimating a probability of failure pf, defined as the volume of the excursion set of a complex (e.g. output of an expensive-to-run finite element model) scalar performance function J below a given threshold, under a probability measure that can be recast as a multivariate standard gaussian law using an isoprobabilistic transformation. We propose a method able to deal with cases characterized by multiple failure regions, possibly very small failure probability pf (say {$\sim$}10-6-10-9), and when the number of evaluations of J is limited. The present work is an extension of the popular Kriging-based active learning algorithm known as AK-MCS, as presented in [1], permitting to deal with very low failure probabilities. The key idea merely consists in replacing the Monte-Carlo sampling, used in the original formulation to propose candidates and evaluate the failure probability, by a centered isotropic Gaussian sampling in the standard space, whose standard deviation is iteratively tuned. This extreme AK-MCS (eAK-MCS) inherits its former multi-point enrichment algorithm allowing to add several points at each iteration step, and provide an estimated failure probability based on the Gaussian nature of the Kriging surrogate. Both the efficiency and the accuracy of the proposed method are showcased through its application to two to eight dimensional analytic examples, characterized by very low failure probabilities: pf{$\sim$}10-6-10-9. Numerical experiments conducted with unfavorable initial Design of Experiment suggests the ability of the proposed method to detect failure domains.},
  langid = {english},
  keywords = {AK-MCS,Importance sampling,Low failure probability,Multiple failure regions,Rare event,Risk analysis,Tail probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3EZQE5R8\\Razaaly et Congedo - 2020 - Extension of AK-MCS for the efficient computation .pdf;C\:\\Users\\a846735\\Zotero\\storage\\IUR6JJ3J\\S0951832020305858.html}
}

@article{razaaly_quantile-based_2020,
  title = {Quantile-Based Robust Optimization of a Supersonic Nozzle for Organic Rankine Cycle Turbines},
  author = {Razaaly, Nassim and Persico, Giacomo and Gori, Giulio and Congedo, Pietro Marco},
  year = {2020},
  month = jun,
  journal = {Applied Mathematical Modelling},
  volume = {82},
  pages = {802--824},
  issn = {0307904X},
  doi = {10.1016/j.apm.2020.01.048},
  abstract = {Organic Rankine Cycle (ORC) turbines usually operate in thermodynamic regions characterized by high-pressure ratios and strong non-ideal gas effects, complicating the aerodynamic design significantly. Systematic optimization methods accounting for multiple uncertainties due to variable operating conditions, referred to as Robust Optimization may benefit to ORC turbines aerodynamic design. This study presents an original and fast robust shape optimization approach to overcome the limitation of a deterministic optimization that neglects operating conditions variability, applied to a well-known supersonic turbine nozzle for ORC applications. The flow around the blade is assumed inviscid and adiabatic and it is reconstructed using the opensource SU2 code. The non-ideal gasdynamics is modeled through the Peng-Robinson-Stryjek-Vera equation of state. We propose here a mono-objective formulation which consists in minimizing the {$\alpha$}-quantile of the targeted Quantity of Interest (QoI) under a probabilistic constraint, at a low computational cost. This problem is solved by using an efficient robust optimization approach, coupling a state-of-the-art quantile estimation and a classical Bayesian optimization method. First, the advantages of a quantile-based formulation are illustrated with respect to a conventional mean-based robust optimization. Secondly, we demonstrate the effectiveness of applying this robust optimization framework with a low-fidelity inviscid solver by comparing the resulting optimal design with the ones obtained with a deterministic optimization using a fully turbulent solver.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7ZEFUYJ9\\Razaaly et al. - 2020 - Quantile-based robust optimization of a supersonic.pdf}
}

@phdthesis{razaaly_rare_2019,
  type = {These de Doctorat},
  title = {Rare {{Event Estimation}} and {{Robust Optimization Methods}} with {{Application}} to {{ORC Turbine Cascade}}},
  author = {Razaaly, Nassim},
  year = {2019},
  month = jul,
  abstract = {Cette th\`ese vise \`a formuler des m\'ethodes innovantes de quantification d'incertitude (UQ) \`a la fois pour l'optimisation robuste (RO) et l'optimisation robuste et fiable (RBDO). L'application vis\'ee est l'optimisation des turbines supersoniques pour les Cycles Organiques de Rankine (ORC).Les sources d'\'energie typiques des syst\`emes d'alimentation ORC sont caract\'eris\'ees par une  source de chaleur et des conditions thermodynamiques entr\'ee/sortie de turbine variables. L'utilisation de compos\'es organiques, g\'en\'eralement  de masse mol\'eculaire \'elev\'ee, conduit \`a des configurations de turbines sujettes \`a des \'ecoulements supersoniques et des chocs, dont l'intensit\'e augmente dans les conditions off-design; ces caract\'eristiques d\'ependent \'egalement de la forme locale de la p\^ale, qui peut \^etre influenc\'ee par la variabilit\'e g\'eom\'etrique induite par les proc\'edures de fabrication. Il existe un consensus sur la n\'ecessit\'e d'inclure ces incertitudes dans la conception, n\'ecessitant ainsi des m\'ethodes UQ et un outil permettant l'optimisation de form adapt\'e.Ce travail est d\'ecompos\'e en deux parties principales. La premi\`ere partie aborde le probl\`eme de l'estimation des \'ev\'enements rares en proposant deux m\'ethodes originales pour l'estimation de probabilit\'e de d\'efaillance (metaAL-OIS et eAK-MCS) et un pour le calcul quantile (QeAK-MCS). Les trois m\'ethodes reposent sur des strat\'egies d'adaptation bas\'ees sur des m\'etamod\`eles (Kriging), visant \`a affiner directement la r\'egion dite Limit-State-Surface (LSS), contrairement aux methodes de type Subset Simulation (SS). En effet, ces derni\`eres consid\`erent  diff\'erents seuils interm\'ediaires associ\'es \`a des  LSSs devant \^etre raffin\'es. Cette propri\'et\'e de raffinement direct est cruciale, car elle permet la compatibilit\'e de couplage \`a des m\'ethodes RBDO existantes.En particulier, les algorithmes propos\'es ne sont pas soumis \`a des hypoth\`eses restrictives sur le LSS (contrairement aux m\'ethodes de type FORM/SORM), tel que le nombre de modes de d\'efaillance, cependant doivent \^etre formul\'es dans l'espace standard. Les m\'ethodes eAK-MCS et QeAK-MCS sont d\'eriv\'ees de la m\'ethode AK-MCS, et d'un \'echantillonnage adaptatif et parall\`ele bas\'e sur des algorithmes de type K-Means pond\'er\'e. MetaAL-OIS pr\'esente une strat\'egie de raffinement s\'equentiel plus \'elabor\'ee bas\'ee sur des \'echantillons MCMC tir\'es \`a partir d'une densit\'e d'\'echantillonage d'importance (ISD) quasi optimale. Par ailleurs, il propose la construction d'une ISD de type m\'elange de gaussiennes, permettant l'estimation pr\'ecise de petites probabilit\'es de d\'efaillance lorsqu'un grand nombre d'\'echantillons (plusieurs millions) est disponible, comme alternative au SS. Les trois m\'ethodes sont tr\`es performantes pour des exemples analytiques 2D \`a 8D  classiques, tir\'es de la litt\'erature sur la fiabilit\'e des  structures, certaines pr\'esentant plusieurs modes de d\'efaillance, et tous caract\'eris\'es par une tr\`es faible probabilit\'e de d\'efaillance/niveau de quantile. Des estimations pr\'ecises sont obtenues pour  les cas consid\'er\'es en un nombre raisonnable  d'appels \`a la fonction de performance.},
  collaborator = {Congedo, Pietro Marco},
  copyright = {Licence Etalab},
  school = {Universit\'e Paris-Saclay (ComUE)},
  keywords = {519,EvÃ©nements rares,Extreme Quantile,Failure Probability,Fluides; MÃ©canique des,Gaussian Processes,Geometric manufacturing Variability,MathÃ©matiques,ModÃ¨les mathÃ©matiques,Optimisation mathÃ©matique,Optimisation Robuste,ORC Turbine,ProbabilitÃ© de dÃ©faillance,Processus gaussiens,Quantile extrÃªme,Robust Optimization,Simulation par ordinateur,Turbine ORC,Turbines Ã  vapeur,VariabilitÃ© gÃ©omÃ©trique}
}

@article{razaaly_rare_nodate,
  title = {Rare {{Event Estimation}} and {{Robust Optimization Methods}} with {{Application}} to {{ORC Turbine Cascade}}},
  author = {Razaaly, Nassim},
  pages = {241},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\S4C357SS\\Razaaly - Rare Event Estimation and Robust Optimization Meth.pdf}
}

@article{reid_aspects_2013,
  title = {Aspects of Likelihood Inference},
  author = {Reid, Nancy},
  year = {2013},
  month = sep,
  journal = {Bernoulli},
  volume = {19},
  number = {4},
  eprint = {1309.7816},
  eprinttype = {arxiv},
  pages = {1404--1418},
  issn = {1350-7265},
  doi = {10.3150/12-BEJSP03},
  abstract = {I review the classical theory of likelihood based inference and consider how it is being extended and developed for use in complex models and sampling schemes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\V9SSH2CL\\Reid - 2013 - Aspects of likelihood inference.pdf}
}

@article{reid_likelihood_2003,
  title = {Likelihood Inference in the Presence of Nuisance Parameters},
  author = {Reid, N. and Fraser, D. A. S.},
  year = {2003},
  month = dec,
  journal = {arXiv:physics/0312079},
  eprint = {physics/0312079},
  eprinttype = {arxiv},
  abstract = {We describe some recent approaches to likelihood based inference in the presence of nuisance parameters. Our approach is based on plotting the likelihood function and the \$p\$-value function, using recently developed third order approximations. Orthogonal parameters and adjustments to profile likelihood are also discussed. Connections to classical approaches of conditional and marginal inference are outlined.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Physics - Data Analysis; Statistics and Probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\85NDWW8U\\Reid et Fraser - 2003 - Likelihood inference in the presence of nuisance p.pdf}
}

@article{rezende_variational_2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2016},
  month = jun,
  journal = {arXiv:1505.05770 [cs, stat]},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Normalizing flows,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\T8CLTHAZ\\1505.05770.pdf}
}

@phdthesis{ribaud_krigeage_2018,
  type = {Thesis},
  title = {Krigeage Pour La Conception de Turbomachines : Grande Dimension et Optimisation Multi-Objectif Robuste},
  shorttitle = {Krigeage Pour La Conception de Turbomachines},
  author = {Ribaud, M{\'e}lina},
  year = {2018},
  month = oct,
  abstract = {Dans le secteur de l'automobile, les turbomachines sont des machines tournantes participant au refroidissement des moteurs des voitures. Leur performance d\'epend de multiples param\`etres g\'eom\'etriques qui d\'eterminent leur forme. Cette th\`ese s'inscrit dans le projet ANR PEPITO r\'eunissant industriels et acad\'emiques autour de l'optimisation de ces turbomachines. L'objectif du projet est de trouver la forme du ventilateur maximisant le rendement en certains points de fonctionnement. Dans ce but, les industriels ont d\'evelopp\'e des codes CFD (computational fluid dynamics) simulant le fonctionnement de la machine. Ces codes sont tr\`es co\^uteux en temps de calcul. Il est donc impossible d'utiliser directement le r\'esultat de ces simulations pour conduire une optimisation.Par ailleurs, lors de la construction des turbomachines, on observe des perturbations sur les param\`etres d'entr\'ee. Elles sont le reflet de fluctuations des machines de production. Les \'ecarts observ\'es sur la forme g\'eom\'etrique finale de la turbomachine peuvent provoquer une perte de performance cons\'equente. Il est donc n\'ecessaire de prendre en compte ces perturbations et de proc\'eder \`a une optimisation robuste \`a ces fluctuations. Dans ce travail de th\`ese, nous proposons des m\'ethodes bas\'ees sur du krigeage r\'epondant aux deux principales probl\'ematiques li\'ees \`a ce contexte de simulations co\^uteuses :\textbullet{}	Comment construire une bonne surface de r\'eponse pour le rendement lorsqu'il y a beaucoup de param\`etres g\'eom\'etriques ?\textbullet{}	Comment proc\'eder \`a une optimisation du rendement efficace tout en prenant en compte les perturbations des entr\'ees ?Nous r\'epondons \`a la premi\`ere probl\'ematique en proposant plusieurs algorithmes permettant de construire un noyau de covariance pour le krigeage adapt\'e \`a la grande dimension. Ce noyau est un produit tensoriel de noyaux isotropes o\`u chacun de ces noyaux est li\'e \`a un sous groupe de variables d'entr\'ee. Ces algorithmes sont test\'es sur des cas simul\'es et sur une fonction r\'eelle. Les r\'esultats montrent que l'utilisation de ce noyau permet d'am\'eliorer la qualit\'e de pr\'ediction en grande dimension. Concernant la seconde probl\'ematique, nous proposons plusieurs strat\'egies it\'eratives bas\'ees sur un co-krigeage avec d\'eriv\'ees pour conduire l'optimisation robuste. A chaque it\'eration, un front de Pareto est obtenu par la minimisation de deux objectifs calcul\'es \`a partir des pr\'edictions de la fonction co\^uteuse. Le premier objectif repr\'esente la fonction elle-m\^eme et le second la robustesse. Cette robustesse est quantifi\'ee par un crit\`ere estimant une variance locale et bas\'ee sur le d\'eveloppement de Taylor. Ces strat\'egies sont compar\'ees sur deux cas tests en petite et plus grande dimension. Les r\'esultats montrent que les meilleures strat\'egies permettent bien de trouver l'ensemble des solutions robustes. Enfin, les m\'ethodes propos\'ees sont appliqu\'ees sur les cas industriels propres au projet PEPITO.},
  school = {Lyon},
  keywords = {Algorithm,Algorithme,Algorithmes,Covariance kernel,Grande dimension,High dimension,Krigeage,Kriging,MathÃ©matiques,Noyau de covariance,Optimisation robuste,Robust optimization,Turbomachines},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CKTSANXI\\Ribaud - 2018 - Krigeage pour la conception de turbomachines  gra.pdf;C\:\\Users\\a846735\\Zotero\\storage\\R65EGT37\\Ribaud - 2018 - Krigeage pour la conception de turbomachines  gra.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7E6NAWNZ\\2018LYSEC026.html;C\:\\Users\\a846735\\Zotero\\storage\\JRFU2DP6\\2018LYSEC026.html}
}

@unpublished{ribaud_robustness_2019,
  title = {Robustness Kriging-Based Optimization},
  author = {Ribaud, M{\'e}lina and {Blanchet-Scalliet}, Christophette and Gillot, Frederic and Helbert, C{\'e}line},
  year = {2019},
  month = feb,
  abstract = {In the context of robust shape optimization, the estimation cost of some physical models is reduced 10 by the use of a response surface. The multi objective methodology for robust optimization that requires the partitioning of the Pareto front (minimization of the function and the robustness criterion) has already been developed. However, the efficient estimation of the robustness criterion in the context of time-consuming simulation has not been much explored. We propose a robust optimization procedure based on the prediction of the function and its derivatives by a kriging. The 15 usual moment 2 is replaced by an approximated version using Taylor theorem. A Pareto front of the robust solutions is generated by a genetic algorithm named NSGA-II. This algorithm gives a Pareto front in an reasonable time of calculation. We detail seven relevant strategies and compare them for the same budget in two test functions (2D and 6D). In each case, we compare the results when the derivatives are observed and not.},
  keywords = {Expected Improvement 25,Gaussian process modelling,Gaussian process regression,Multi-objective optimization,Robust Optimization,robustness criterion,Tay-lor expansion},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\22TIVRVK\\Ribaud et al. - 2019 - Robustness kriging-based optimization.pdf}
}

@article{riddle_uncertainties_1996,
  title = {Uncertainties in Modelling of Tidal Flows off {{Singapore Island}}},
  author = {Riddle, A. M.},
  year = {1996},
  month = sep,
  journal = {Journal of Marine Systems},
  series = {Joint {{Numerical Sea Modelling}}},
  volume = {8},
  number = {3},
  pages = {133--145},
  issn = {0924-7963},
  doi = {10.1016/0924-7963(96)00003-6},
  abstract = {A hydrodynamic model has been constructed for predicting the tidal currents off the south west coast of Singapore Island and has been verified against survey data from the area. A study examining the sensitivity of the model to changes in the input parameters has been undertaken and a statistical experimental design technique has been employed to determine the minimum number of computer runs required to quantify the sensitivity of the model. The uncertainty in predicting the time of the turn of tide is estimated from the statistical results as is the uncertainty in the predicted movement of drogues in both simple and complex tidal flows. The method is extended to estimate the uncertainties in the spread and concentration of a dye patch using a particle tracking random walk model and allowing for the uncertainties in the wind and in horizontal and vertical mixing rates as determined from a series of experimental studies of dye patch spread.},
  langid = {english},
  keywords = {Hydrodynamic,Mathematical model,Parameter variation,Random walk,Singapore,Tidal flow,Uncertainty},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5CGR98WT\\0924796396000036.html}
}

@book{rios_insua_robust_2000,
  title = {Robust {{Bayesian}} Analysis},
  editor = {R{\'i}os Insua, David and Ruggeri, Fabrizio},
  year = {2000},
  series = {Lecture Notes in Statistics},
  number = {152},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-98866-5},
  langid = {english},
  lccn = {QA279.5 .R64 2000},
  keywords = {Bayesian statistical decision theory},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UVCAEIW7\\RÃ­os Insua et Ruggeri - 2000 - Robust Bayesian analysis.pdf}
}

@article{robbins_measure_1944,
  title = {On the {{Measure}} of a {{Random Set}}},
  author = {Robbins, H. E.},
  year = {1944},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {15},
  number = {1},
  pages = {70--74},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177731315},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR10347},
  zmnumber = {0060.29406},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JMFXEH7K\\Robbins - 1944 - On the Measure of a Random Set.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XU8WEAC9\\1177731315.html}
}

@article{robbins_stochastic_1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  abstract = {Let M(x)M(x)M(x) denote the expected value at level xxx of the response to a certain experiment. M(x)M(x)M(x) is assumed to be a monotone function of xxx but is unknown to the experimenter, and it is desired to find the solution x=\texttheta x=\texttheta x = \textbackslash theta of the equation M(x)={$\alpha$}M(x)={$\alpha$}M(x) = \textbackslash alpha, where {$\alpha\alpha\backslash$}alpha is a given constant. We give a method for making successive experiments at levels x1,x2,{$\cdots$}x1,x2,{$\cdots$}x\_1,x\_2,\textbackslash cdots in such a way that xnxnx\_n will tend to \texttheta\texttheta\textbackslash theta in probability.},
  langid = {english},
  mrnumber = {MR42668},
  zmnumber = {0054.05901},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B6H3EKLA\\Robbins et Monro - 1951 - A Stochastic Approximation Method.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XLRD9927\\1177729586.html}
}

@inproceedings{robert_marginal_1999,
  title = {Marginal {{MAP}} Estimation Using {{Markov}} Chain {{Monte Carlo}}},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, 1999. {{Proceedings}}., 1999 {{IEEE International Conference}} On},
  author = {Robert, Christian P. and Doucet, Arnaud and Godsill, Simon J.},
  year = {1999},
  volume = {3},
  pages = {1753--1756},
  publisher = {{IEEE}},
  keywords = {MCMC,MMAP},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H5FADK4V\\Robert et al. - 1999 - Marginal MAP estimation using Markov chain Monte C.pdf;C\:\\Users\\a846735\\Zotero\\storage\\AII4L2FS\\756334.html}
}

@article{robert_monte_nodate,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P and Casella, George},
  pages = {85},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TQ3MFXTJ\\Robert et Casella - Monte Carlo Statistical Methods.pdf}
}

@article{robert_reduced-order_2006,
  title = {Reduced-Order {{4D-Var}}: {{A}} Preconditioner for the {{Incremental 4D-Var}} Data Assimilation Method: {{A PRECONDITIONER FOR THE 4D-VAR METHOD}}},
  shorttitle = {Reduced-Order {{4D-Var}}},
  author = {Robert, C. and Blayo, E. and Verron, J.},
  year = {2006},
  month = sep,
  journal = {Geophysical Research Letters},
  volume = {33},
  number = {18},
  pages = {n/a-n/a},
  issn = {00948276},
  doi = {10.1029/2006GL026555},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\96LQBUBT\\Robert et al. - 2006 - Reduced-order 4D-Var A preconditioner for the Inc.pdf;C\:\\Users\\a846735\\Zotero\\storage\\C2TCWX2C\\hal-00172926.html}
}

@article{rockafellar_conditional_2002,
  title = {Conditional Value-at-Risk for General Loss Distributions},
  author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
  year = {2002},
  journal = {Journal of banking \& finance},
  volume = {26},
  number = {7},
  pages = {1443--1471},
  keywords = {VaR},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RPR7LHY8\\790b149edfb586db318363e28182a6fedc80.pdf}
}

@techreport{rockafellar_deviation_2002,
  type = {{{SSRN Scholarly Paper}}},
  title = {Deviation {{Measures}} in {{Risk Analysis}} and {{Optimization}}},
  author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav P. and Zabarankin, Michael},
  year = {2002},
  month = dec,
  number = {ID 365640},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {General deviation measures, which include standard deviation as a special case but need not be symmetric with respect to ups and downs, are defined and shown to correspond to risk measures in the sense of Artzner, Delbaen, Eber and Heath when those are applied to the difference between a random variable and its expectation, instead of to the random variable itself.   A property called expectation-boundedness of the risk measure is uncovered as essential for this correspondence.  It is shown to be satisfied by conditional value-at-risk and by worst-case risk, as well as various mixtures, although not by ordinary value-at-risk.},
  langid = {english},
  keywords = {coherent risk,deviation measures,risk management},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3RZUKHRL\\Rockafellar et al. - 2002 - Deviation Measures in Risk Analysis and Optimizati.pdf;C\:\\Users\\a846735\\Zotero\\storage\\AACZGBUN\\papers.html}
}

@book{rogers_backlund_2002,
  title = {B\"acklund and {{Darboux}} Transformations: Geometry and Modern Applications in Soliton Theory},
  shorttitle = {B\"acklund and {{Darboux}} Transformations},
  author = {Rogers, Colin and Schief, Wolfgang Karl},
  year = {2002},
  volume = {30},
  publisher = {{Cambridge University Press}},
  keywords = {Finite Volume},
  file = {C\:\\Users\\Victor\\Documents\\Randall_J._LeVeque_Finite_Volume_Methods_for_Hyperbolic_Problems.pdf}
}

@article{rolland_characterization_nodate,
  title = {Characterization of {{Space}} and {{Time-Dependence}} of 3-{{Point Shots}} in {{Basketball}}},
  author = {Rolland, Gabin and Vuillemot, Romain and Bos, Wouter and Rivi{\`e}re, Nathan},
  pages = {16},
  abstract = {Understanding characteristics of 3-point shots is paramount for modern basketball success, as in recent decades, 3-point shots have become more prevalent in the NBA. They accounted for 33,6\% of the number of total shots in 2017-2018, compared to only 3\% in 1979-1980 [1]. In this paper, we aim at better understanding the connections between the type of 3-point shooting (catch-and-shoots and pull-ups) and the timing for shooting, using two distinct space-time models of player motion. Those models allow us to identify individual behavior as a function of specific defensive settings, e.g. shot-behavior when a player is guarded closely. We assess our models using SportVU data for specific NBA players and our code is open-source to enable more players and teams explorations, as well as to support further research and application of those models, beyond basketball and sport.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\U6RWYATJ\\Rolland et al. - Characterization of Space and Time-Dependence of 3.pdf}
}

@article{rontsis_distributionally_2017,
  title = {Distributionally {{Ambiguous Optimization Techniques}} for {{Batch Bayesian Optimization}}},
  author = {Rontsis, Nikitas and Osborne, Michael A. and Goulart, Paul J.},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.04191 [stat]},
  eprint = {1707.04191},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian Expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly - even on large batch sizes - as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VASPF8V9\\Rontsis et al. - 2017 - Distributionally Ambiguous Optimization Techniques.pdf;C\:\\Users\\a846735\\Zotero\\storage\\RKIUBHSG\\1707.html}
}

@article{rosasco_reproducing_nodate,
  title = {Reproducing {{Kernel Hilbert Spaces}}},
  author = {Rosasco, Lorenzo and Durrett, Greg},
  pages = {8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y2LFAF22\\Rosasco et Durrett - Reproducing Kernel Hilbert Spaces.pdf}
}

@book{rossant_learning_2013,
  title = {Learning {{IPython}} for Interactive Computing and Data Visualization: Learn {{IPython}} for Interactive {{Python}} Programming, High-Performance Numerical Computing, and Data Visualization},
  shorttitle = {Learning {{IPython}} for Interactive Computing and Data Visualization},
  author = {Rossant, Cyrille},
  year = {2013},
  series = {Community Experience Distilled},
  publisher = {{Packt Publ}},
  address = {{Birmingham}},
  isbn = {978-1-78216-993-2},
  langid = {english},
  annotation = {OCLC: 854711306},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IAMR59A7\\Learning IPython for Interactive Computing and Data Visualization [Rossant 2013-04-25].pdf}
}

@article{rougier_scientific_nodate,
  title = {Scientific {{Visualization}}: {{Python}} + {{Matplotlib}}},
  author = {Rougier, Nicolas},
  pages = {249},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4BB9TNY5\\Rougier - Scientific Visualization Python + Matplotlib.pdf}
}

@article{rougier_ten_2014,
  title = {Ten {{Simple Rules}} for {{Better Figures}}},
  author = {Rougier, Nicolas P. and Droettboom, Michael and Bourne, Philip E.},
  year = {2014},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {10},
  number = {9},
  pages = {e1003833},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003833},
  langid = {english},
  keywords = {Data visualization,Eye movements,Radii,Research design,Seismic signal processing,Software design,Software tools,Vision},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ILBKZ6QT\\Rougier et al. - 2014 - Ten Simple Rules for Better Figures.pdf;C\:\\Users\\a846735\\Zotero\\storage\\YYLAVTWY\\article.html}
}

@article{rubin_bayesian_2010,
  title = {A {{Bayesian}} Approach for Inverse Modeling, Data Assimilation, and Conditional Simulation of Spatial Random Fields: {{METHOD OF ANCHORED DISTRIBUTIONS}}},
  shorttitle = {A {{Bayesian}} Approach for Inverse Modeling, Data Assimilation, and Conditional Simulation of Spatial Random Fields},
  author = {Rubin, Yoram and Chen, Xingyuan and Murakami, Haruko and Hahn, Melanie},
  year = {2010},
  month = oct,
  journal = {Water Resources Research},
  volume = {46},
  number = {10},
  issn = {00431397},
  doi = {10.1029/2009WR008799},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CNUY359C\\wrcr12530.pdf}
}

@article{rue_approximate_2009,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  author = {avard Rue, H{\textbackslash}a and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  journal = {Journal of the royal statistical society: Series b (statistical methodology)},
  volume = {71},
  number = {2},
  pages = {319--392},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\AEMPKWJW\\Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XZ5P7BEX\\Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian}
}

@article{russo_tutorial_2017,
  title = {A {{Tutorial}} on {{Thompson Sampling}}},
  author = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.02038 [cs]},
  eprint = {1707.02038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5YEIEJDI\\Russo et al. - 2017 - A Tutorial on Thompson Sampling.pdf;C\:\\Users\\a846735\\Zotero\\storage\\FNS6338N\\1707.html}
}

@book{ruud_introduction_2000,
  title = {An {{Introduction}} to {{Classical Econometric Theory}}},
  author = {Ruud, Paul Arthur and Ruud, Professor of Economics Paul A.},
  year = {2000},
  publisher = {{Oxford University Press}},
  abstract = {In An Introduction to Classical Econometric Theory Paul A. Ruud shows the practical value of an intuitive approach to econometrics. Students learn not only why but how things work. Through geometry, seemingly distinct ideas are presented as the result of one common principle, making econometrics more than mere recipes or special tricks. In doing this, the author relies on such concepts as the linear vector space, orthogonality, and distance. Parts I and II introduce the ordinary least squares fitting method and the classical linear regression model, separately rather than simultaneously as in other texts. Part III contains generalizations of the classical linear regression model and Part IV develops the latent variable models that distinguish econometrics from statistics. To motivate formal results in a chapter, the author begins with substantive empirical examples. Main results are followed by illustrative special cases; technical proofs appear toward the end of each chapter. Intended for a graduate audience, An Introduction to Classical Econometric Theory fills the gap between introductory and more advanced texts. It is the most conceptually complete text for graduate econometrics courses and will play a vital role in graduate instruction.},
  googlebooks = {PnVCEZOOFr0C},
  isbn = {978-0-19-511164-4},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Economics / General},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2GFKQMAL\\Ruud.pdf}
}

@article{rychlik_reliability_nodate,
  title = {On {{Some Reliability Applications}} of {{Rice}}'s {{Formula}} for the {{Intensity}} of {{Level Crossings}}},
  author = {RYCHLIK, IGOR},
  pages = {19},
  abstract = {Let X be a stationary process with absolutely continuous sample paths. If EÂjX\_Â0ÂjÂ is \textregistered nite and if the distribution of XÂ0Â is absolutely continuous, then, for almost all u, the crossing intensity mÂuÂ of the level u by XÂtÂ is given by the generalized Rice's formula mÂuÂRÂ EÂjX\_Â0ÂjjXÂ0Â Â uÂ fXÂ0ÂÂuÂ. The classical Rice's formula for mÂuÂ, which is valid for a \textregistered xed level u, mÂuÂ Â jzj fX\_Â0Â;XÂ0ÂÂz; uÂdz, holds under more restrictive technical conditions that can be dif\textregistered cult to check in applications. In this paper it is shown that often in practice the weaker form of Rice's formula (valid for almost all u) is suf\textregistered cient. Three engineering problems are discussed; prediction of fatigue life time; computing the average stress at slams and analysis of crest height of sea waves.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YEIM26HS\\RYCHLIK - On Some Reliability Applications of Rice's Formula.pdf}
}

@book{saad_iterative_2003,
  title = {Iterative Methods for Sparse Linear Systems},
  author = {Saad, Yousef},
  year = {2003},
  publisher = {{SIAM}},
  keywords = {Iterative methods,Krylov},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4V35HM3M\\IterMethBook_2ndEd.pdf}
}

@article{sacks_designs_1989,
  title = {Designs for {{Computer Experiments}}},
  author = {Sacks, Jerome and Schiller, Susannah B. and Welch, William J.},
  year = {1989},
  month = feb,
  journal = {Technometrics},
  volume = {31},
  number = {1},
  pages = {41--47},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1989.10488474},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\J8CW6PRK\\Sacks et al. - 1989 - Designs for Computer Experiments.pdf}
}

@article{sakov_iterative_2012,
  title = {An {{Iterative EnKF}} for {{Strongly Nonlinear Systems}}},
  author = {Sakov, Pavel and Oliver, Dean S. and Bertino, Laurent},
  year = {2012},
  month = jun,
  journal = {Monthly Weather Review},
  volume = {140},
  number = {6},
  pages = {1988--2004},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-11-00176.1},
  abstract = {Abstract The study considers an iterative formulation of the ensemble Kalman filter (EnKF) for strongly nonlinear systems in the perfect-model framework. In the first part, a scheme is introduced that is similar to the ensemble randomized maximal likelihood (EnRML) filter by Gu and Oliver. The two new elements in the scheme are the use of the ensemble square root filter instead of the traditional (perturbed observations) EnKF and rescaling of the ensemble anomalies with the ensemble transform matrix from the previous iteration instead of estimating sensitivities between the ensemble observations and ensemble anomalies at the start of the assimilation cycle by linear regression. A simple modification turns the scheme into an ensemble formulation of the iterative extended Kalman filter. The two versions of the algorithm are referred to as the iterative EnKF (IEnKF) and the iterative extended Kalman filter (IEKF). In the second part, the performance of the IEnKF and IEKF is tested in five numerical experiments: two with the 3-element Lorenz model and three with the 40-element Lorenz model. Both the IEnKF and IEKF show a considerable advantage over the EnKF in strongly nonlinear systems when the quality or density of observations are sufficient to constrain the model to the regime of mainly linear propagation of the ensemble anomalies as well as constraining the fast-growing modes, with a much smaller advantage otherwise. The IEnKF and IEKF can potentially be used with large-scale models, and can represent a robust and scalable alternative to particle filter (PF) and hybrid PF\textendash EnKF schemes in strongly nonlinear systems.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VQESTUZM\\Sakov et al. - 2012 - An Iterative EnKF for Strongly Nonlinear Systems.pdf}
}

@article{sakov_iterative_2018,
  title = {An Iterative Ensemble {{Kalman}} Filter in Presence of Additive Model Error},
  author = {Sakov, Pavel and Haussaire, Jean-Matthieu and Bocquet, Marc},
  year = {2018},
  month = apr,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {144},
  number = {713},
  eprint = {1711.06110},
  eprinttype = {arxiv},
  pages = {1297--1309},
  issn = {0035-9009, 1477-870X},
  doi = {10.1002/qj.3213},
  abstract = {The iterative ensemble Kalman filter (IEnKF) in a deterministic framework was introduced in Sakov et al. (2012) to extend the ensemble Kalman filter (EnKF) and improve its performance in mildly up to strongly nonlinear cases. However, the IEnKF assumes that the model is perfect. This assumption simplified the update of the system at a time different from the observation time, which made it natural to apply the IEnKF for smoothing. In this study, we generalise the IEnKF to the case of imperfect model with additive model error. The new method called IEnKF-Q conducts a Gauss-Newton minimisation in ensemble space. It combines the propagated analysed ensemble anomalies from the previous cycle and model noise ensemble anomalies into a single ensemble of anomalies, and by doing so takes an algebraic form similar to that of the IEnKF. The performance of the IEnKF-Q is tested in a number of experiments with the Lorenz-96 model, which show that the method consistently outperforms both the EnKF and the IEnKF naively modified to accommodate additive model noise.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Physics - Atmospheric and Oceanic Physics,Statistics - Applications},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6GFR2EH6\\Sakov et al. - 2018 - An iterative ensemble Kalman filter in presence of.pdf}
}

@article{santarelli_framework_2010,
  title = {A Framework for Reduced Order Modeling with Mixed Moment Matching and Peak Error Objectives},
  author = {Santarelli, Keith R.},
  year = {2010},
  journal = {SIAM Journal on Scientific Computing},
  volume = {32},
  number = {2},
  pages = {745--773},
  keywords = {Moment matching,MOR methods},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PXPVHUJV\\l1_mod_reduction.pdf}
}

@article{santner_design_nodate,
  title = {The {{Design}} and {{Analysis}} of {{Computer Experiments}}},
  author = {Santner, Thomas J and Williams, Brian J and Notz, William I},
  pages = {236},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FM6QXR9F\\Santner et al. - The Design and Analysis of Computer Experiments.pdf}
}

@book{sappl_deep_2019,
  title = {Deep {{Learning}} of {{Preconditioners}} for {{Conjugate Gradient Solvers}} in {{Urban Water Related Problems}}},
  author = {Sappl, Johannes and Seiler, Laurent and Harders, Matthias and Rauch, Wolfgang},
  year = {2019},
  month = jun,
  abstract = {Solving systems of linear equations is a problem occuring frequently in water engineering applications. Usually the size of the problem is too large to be solved via direct factorization. One can resort to iterative approaches, in particular the conjugate gradients method if the matrix is symmetric positive definite. Preconditioners further enhance the rate of convergence but hitherto only handcrafted ones requiring expert knowledge have been used. We propose an innovative approach employing Machine Learning, in particular a Convolutional Neural Network, to unassistedly design preconditioning matrices specifically for the problem at hand. Based on an in-depth case study in fluid simulation we are able to show that our learned preconditioner is able to improve the convergence rate even beyond well established methods like incomplete Cholesky factorization or Algebraic MultiGrid.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IPJBU2YV\\Sappl et al. - 2019 - Deep Learning of Preconditioners for Conjugate Gra.pdf}
}

@article{sasaki_clustering_nodate,
  title = {Clustering via {{Mode Seeking}} by {{Direct Estimation}} of the {{Gradient}} of a {{Log-Density}}},
  author = {Sasaki, Hiroaki and Hyvarinen, Aapo and Sugiyama, Masashi},
  pages = {16},
  abstract = {Mean shift clustering finds the modes of the data probability density by identifying the zero points of the density gradient. Since it does not require to fix the number of clusters in advance, the mean shift has been a popular clustering algorithm in various application fields. A typical implementation of the mean shift is to first estimate the density by kernel density estimation and then compute its gradient. However, since a good density estimation does not necessarily imply an accurate estimation of the density gradient, such an indirect two-step approach is not reliable. In this paper, we propose a method to directly estimate the gradient of the log-density without going through density estimation. The proposed method gives the global solution analytically and thus is computationally efficient. We then develop a mean-shift-like fixed-point algorithm to find the modes of the density for clustering. As in the mean shift, one does not need to set the number of clusters in advance. We experimentally show that the proposed clustering method significantly outperforms the mean shift especially for high-dimensional data.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TWV2B3NR\\Sasaki et al. - Clustering via Mode Seeking by Direct Estimation o.pdf}
}

@article{savage_theory_1951,
  title = {The {{Theory}} of {{Statistical Decision}}},
  author = {Savage, L. J.},
  year = {1951},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {46},
  number = {253},
  pages = {55--67},
  issn = {0162-1459},
  doi = {10.1080/01621459.1951.10500768},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NFKIH2DJ\\Savage - 1951 - The theory of statistical decision.ps;C\:\\Users\\a846735\\Zotero\\storage\\FXV6BBDL\\01621459.1951.html;C\:\\Users\\a846735\\Zotero\\storage\\J6RUE4WP\\01621459.1951.html}
}

@article{saynisch_estimating_2018,
  title = {Estimating Ocean Tide Model Uncertainties for Electromagnetic Inversion Studies},
  author = {Saynisch, Jan and Irrgang, Christopher and Thomas, Maik},
  year = {2018},
  month = apr,
  journal = {Annales Geophysicae Discussions},
  pages = {1--9},
  doi = {10.5194/angeo-2018-27},
  abstract = {Over a decade ago the semidiurnal lunar M2 ocean tide was identified in CHAMP satellite magnetometer data. Since then and especially since the launch of the satellite magnetometer mission Swarm, electromagnetic tidal observations from satellites are used increasingly to infer electric properties of the upper lithosphere. In most of these inversions, numerical ocean tidal models are used to generate oceanic tidal electromagnetic signals via electromagnetic induction. The modelled signals are subsequently compared to the satellite observations. During the inversion, since the tidal models are considered error free, discrepancies between forward models and observations are projected only onto the induction part of the modelling, e.g., Earth's resistivity distribution. Our study analyses uncertainties in oceanic tidal models from an electromagnetic point of view. Velocities from hydrodynamic and assimilative tidal models are converted into tidal electromagnetic signals and compared. Respective uncertainties are estimated. The studies main goal is to provide errors for electromagnetic inversion studies. At satellite height, the differences between the hydrodynamic tidal models are found to reach up to 2 nT, i.e., over 100 \% of the M2 signal. Assimilative tidal models show smaller differences of up to 0.1 nT, i.e., over 30 \% of the M2 signal.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\694QVRSL\\Saynisch et al. - 2018 - Estimating ocean tide model uncertainties for elec.pdf}
}

@article{schobi_combining_nodate,
  title = {Combining Polynomial Chaos Expansions and {{Kriging}} for Solving Structural Reliability Problems},
  author = {Schobi, Roland and Sudret, Bruno},
  pages = {11},
  abstract = {Nowadays advanced simulation models such as finite element models are used in every domain of science and engineering in order to predict the behaviour of systems and, in case of engineering applications, to optimize them and assess their performance. In parallel, engineers are all the more concerned with structural reliability and robust design, meaning that the quantification of uncertainties has become a key challenge.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UWZH64MW\\Schobi et Sudret - Combining polynomial chaos expansions and Kriging .pdf}
}

@article{schobi_rare_2017,
  title = {Rare {{Event Estimation Using Polynomial-Chaos Kriging}}},
  author = {Sch{\"o}bi, R. and Sudret, B. and Marelli, S.},
  year = {2017},
  month = jun,
  journal = {ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering},
  volume = {3},
  number = {2},
  issn = {2376-7642, 2376-7642},
  doi = {10.1061/AJRUA6.0000870},
  abstract = {Structural reliability analysis aims at computing the probability of failure of systems whose performance may be assessed by using complex computational models (e.g. expensive-to-run finite element models). A direct use of Monte Carlo simulation is not feasible in practice, unless a surrogate model (such as Kriging, a.k.a Gaussian process modeling) is used. Such meta-models are often used in conjunction with adaptive experimental designs (i.e. design enrichment strategies), which allows one to iteratively increase the accuracy of the surrogate for the estimation of the failure probability while keeping low the overall number of runs of the costly original model.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KTQEW6DJ\\SchÃ¶bi et al. - 2017 - Rare Event Estimation Using Polynomial-Chaos Krigi.pdf}
}

@article{schraudolph_fast_2002,
  title = {Fast {{Curvature Matrix-Vector Products}} for {{Second-Order Gradient Descent}}},
  author = {Schraudolph, Nicol N.},
  year = {2002},
  month = jul,
  journal = {Neural Computation},
  volume = {14},
  number = {7},
  pages = {1723--1738},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/08997660260028683},
  abstract = {We propose a generic method for iteratively approximating various second-order gradient steps\textemdash -Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient\textemdash -in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XYFCIYLR\\Schraudolph - 2002 - Fast Curvature Matrix-Vector Products for Second-O.pdf}
}

@article{schwarz_estimating_1978,
  title = {Estimating the {{Dimension}} of a {{Model}}},
  author = {Schwarz, Gideon},
  year = {1978},
  month = mar,
  journal = {Annals of Statistics},
  volume = {6},
  number = {2},
  pages = {461--464},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176344136},
  abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
  langid = {english},
  mrnumber = {MR468014},
  zmnumber = {0379.62005},
  keywords = {Akaike information criterion,asymptotics,Dimension},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\N2G3GXWE\\Schwarz - 1978 - Estimating the Dimension of a Model.pdf;C\:\\Users\\a846735\\Zotero\\storage\\488UBES6\\1176344136.html}
}

@book{scott_multivariate_2015,
  title = {Multivariate Density Estimation: Theory, Practice, and Visualization},
  shorttitle = {Multivariate Density Estimation},
  author = {Scott, David W.},
  year = {2015},
  publisher = {{John Wiley \& Sons}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MZV5PRKU\\multivariate-density-estimation-theory-practice-and-visualization.html;C\:\\Users\\a846735\\Zotero\\storage\\UWH7X4SX\\books.html}
}

@article{scott_optimal_1979,
  title = {On {{Optimal}} and {{Data-Based Histograms}}},
  author = {Scott, David W.},
  year = {1979},
  month = dec,
  journal = {Biometrika},
  volume = {66},
  number = {3},
  pages = {605},
  issn = {00063444},
  doi = {10.2307/2335182},
  abstract = {In thispaper the formulaforthe optimalhistogrambin widthis derivedwhichasymptotically minimizesthe integratedmean squared error.Monte Carlo methodsare used to verify the usefulnessofthisformulaforsmall samples. A data-based procedureforchoosingthe bin widthparameteris proposed,whichassumes a Gaussian referencestandard and requiresonly the sample size and an estimateofthe standard deviation. The sensitivityofthe procedureis investigatedusingseveral probabilitymodelswhichviolate the Gaussian assumption.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7EJWJC8N\\Scott - 1979 - On Optimal and Data-Based Histograms.pdf}
}

@article{seijo_continuous_2011,
  title = {A Continuous Mapping Theorem for the Smallest Argmax Functional},
  author = {Seijo, Emilio and Sen, Bodhisattva},
  year = {2011},
  journal = {Electronic Journal of Statistics},
  volume = {5},
  pages = {421--439},
  issn = {1935-7524},
  doi = {10.1214/11-EJS613},
  abstract = {This paper introduces a version of the argmax continuous mapping theorem that applies to M-estimation problems in which the objective functions converge to a limiting process with multiple maximizers. The concept of the smallest maximizer of a function in the d-dimensional Skorohod space is introduced and its main properties are studied. The resulting continuous mapping theorem is applied to three problems arising in change-point regression analysis. Some of the results proved in connection to the d-dimensional Skorohod space are also of independent interest.},
  langid = {english},
  mrnumber = {MR2802050},
  zmnumber = {1329.60090},
  keywords = {Change-point,compound Poisson process,Cox proportional hazards model,multiple maximizers,Skorohod spaces with multidimensional parameter},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WZANPFUU\\Seijo et Sen - 2011 - A continuous mapping theorem for the smallest argm.pdf;C\:\\Users\\a846735\\Zotero\\storage\\AHU5JSRH\\1305034909.html}
}

@phdthesis{sengers_schemas_2019,
  type = {Thesis},
  title = {Sch\'emas Semi-Implicites et de Diffusion-Redistanciation Pour La Dynamique Des Globules Rouges},
  author = {Sengers, Arnaud},
  year = {2019},
  month = jul,
  journal = {http://www.theses.fr},
  abstract = {Dans ce travail, nous nous sommes int\'eress\'es \`a la mise en place de sch\'emas semi-implicites pour l'am\'elioration des simulations num\'eriques du d\'eplacement d'un globule rouge dans le sang. Nous consid\'erons la m\'ethode levelset, o\`u l'interface fluide-structure est repr\'esent\'ee comme la ligne de niveau 0 d'une fonction auxiliaire et le couplage est effectu\'e en ajoutant un terme source dans l'\'equation fluide.Le principe de ces sch\'emas semi-implicites est de pr\'edire la position et la forme de la structure par une \'equation de la chaleur et d'utiliser cette pr\'ediction pour obtenir un terme de force dans l'\'equation fluide plus pr\'ecis. Ce type de sch\'emas semi-implicites a d'abord \'et\'e mis en place dans le cadre d'un syst\`eme diphasique ou d'une membrane \'elastique immerg\'ee afin d'utiliser un plus grand pas de temps que pour un couplage explicite. Cela a permis d'am\'eliorer les conditions sur le pas de temps et ainsi augmenter l'efficacit\'e globale de l'algorithme complet par rapport \`a un sch\'ema explicite classique.Pour \'etendre ce raisonnement au cas d'un globule rouge, nous proposons un algorithme pour simuler le flot de Willmore en dimension 2 et 3. Notre m\'ethode s'inspire des m\'ethodes de mouvements d'interface g\'en\'er\'es par diffusion et nous arrivons \`a obtenir un flot non lin\'eaire d'ordre 4 uniquement avec des r\'esolutions d'\'equations de la chaleur. Pour assurer la conservation du volume et de l'aire d'un globule rouge, nous proposons ensuite une m\'ethode de correction qui d\'eplace l\'eg\`erement l'interface afin de recoller aux contraintes.La combinaison des deux \'etapes pr\'ec\'edentes d\'ecrit le comportement d'un globule rouge laiss\'e au repos. Nous validons cette m\'ethode en obtenant une formed'\'equilibre d'un globule rouge. Nous proposons enfin un sch\'ema semi-implicite dans le cas d'un globule rouge qui ouvre la voie vers l'utilisation de cette m\'ethode comme pr\'edicteur de l'algorithme de couplage complet.},
  school = {Grenoble Alpes},
  keywords = {Analyse fonctionnelle,Analyse numÃ©rique,Convolution-Redistanciation Method,Ãquation de la chaleur,Finite element method,Fluides; MÃ©canique des,Levelset methods,Mathematical Analysis,MathÃ©matiques appliquÃ©es,MÃ©thode Convolution-Redistanciation,MÃ©thode ElÃ©ments finis,MÃ©thodes Levelset,SchÃ©mas semi-Implicites,Semi-Implicit scheme},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F9F4WN25\\Sengers - 2019 - SchÃ©mas semi-implicites et de diffusion-redistanci.pdf;C\:\\Users\\a846735\\Zotero\\storage\\6LWDF8A3\\2019GREAM032.html}
}

@article{seshadri_density-matching_2014,
  title = {A Density-Matching Approach for Optimization under Uncertainty},
  author = {Seshadri, Pranay and Constantine, Paul and Iaccarino, Gianluca and Parks, Geoffrey},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.7089 [math, stat]},
  eprint = {1409.7089},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Modern computers enable methods for design optimization that account for uncertainty in the system---so-called optimization under uncertainty. We propose a metric for OUU that measures the distance between a designer-specified probability density function of the system response the target and system response's density function at a given design. We study an OUU formulation that minimizes this distance metric over all designs. We discretize the objective function with numerical quadrature and approximate the response density function with a Gaussian kernel density estimate. We offer heuristics for addressing issues that arise in this formulation, and we apply the approach to a CFD-based airfoil shape optimization problem. We qualitatively compare the density-matching approach to a multi-objective robust design optimization to gain insight into the method.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Computation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KK6NXSQM\\Seshadri et al. - 2014 - A density-matching approach for optimization under.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XLQ37PBN\\1409.html}
}

@article{shah_student-t_2014,
  title = {Student-t {{Processes}} as {{Alternatives}} to {{Gaussian Processes}}},
  author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2014},
  month = feb,
  journal = {arXiv:1402.4306 [cs, stat]},
  eprint = {1402.4306},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VPPMUUCH\\Shah et al. - 2014 - Student-t Processes as Alternatives to Gaussian Pr.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZIFUTJNY\\1402.html}
}

@inproceedings{shankaran_robust_2011,
  title = {Robust Optimal Control Using Polynomial Chaos and Adjoints for Systems with Uncertain Inputs},
  booktitle = {20th {{AIAA Computational Fluid Dynamics Conference}}},
  author = {Shankaran, Sriram and Jameson, Antony},
  year = {2011},
  pages = {3069},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\P4J2SVCR\\Shankaran et Jameson - 2011 - Robust optimal control using polynomial chaos and .pdf;C\:\\Users\\a846735\\Zotero\\storage\\NLRPI8UX\\6.html}
}

@article{shapiro_asymptotic_1991,
  title = {Asymptotic Analysis of Stochastic Programs},
  author = {Shapiro, Alexander},
  year = {1991},
  month = dec,
  journal = {Annals of Operations Research},
  volume = {30},
  number = {1},
  pages = {169--186},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/BF02204815},
  abstract = {In this paper we discuss a general approach to studying asymptotic properties of statistical estimators in stochastic programming. The approach is based on an extended delta method and appears to be particularly suitable for deriving asymptotics of the optimal value of stochastic programs. Asymptotic analysis of the optimal value will be presented in detail. Asymptotic properties of the corresponding optimal solutions are briefly discussed.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\797X5VRH\\Shapiro - 1991 - Asymptotic analysis of stochastic programs.pdf}
}

@book{shapiro_lectures_2009,
  title = {Lectures on {{Stochastic Programming}}: {{Modeling}} and {{Theory}}},
  shorttitle = {Lectures on {{Stochastic Programming}}},
  author = {Shapiro, Alexander and Dentcheva, Darinka and Ruszczy{\'n}ski, Andrzej},
  year = {2009},
  month = jan,
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898718751},
  isbn = {978-0-89871-687-0 978-0-89871-875-1},
  langid = {english},
  keywords = {Stochastic programming},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QD5HH7UZ\\Lectures-on-stochastic-programming-Modeling-and-theory.pdf}
}

@article{shapiro_tutorial_nodate,
  title = {A {{Tutorial}} on {{Stochastic Programming}}},
  author = {Shapiro, Alexander and Philpott, Andy},
  pages = {35},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RNY8UMLT\\Shapiro et Philpott - A Tutorial on Stochastic Programming.pdf}
}

@article{shi_nondegenerate_2015,
  title = {A Nondegenerate {{Vuong}} Test: {{A}} Nondegenerate {{Vuong}} Test},
  shorttitle = {A Nondegenerate {{Vuong}} Test},
  author = {Shi, Xiaoxia},
  year = {2015},
  month = mar,
  journal = {Quantitative Economics},
  volume = {6},
  number = {1},
  pages = {85--121},
  issn = {17597323},
  doi = {10.3982/QE382},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\6U8WR7J5\\Shi - 2015 - A nondegenerate Vuong test A nondegenerate Vuong .pdf}
}

@book{silverman_density_2018,
  title = {Density Estimation: {{For}} Statistics and Data Analysis},
  shorttitle = {Density Estimation},
  author = {Silverman, B.W.},
  year = {2018},
  month = jan,
  doi = {10.1201/9781315140919},
  abstract = {Although there has been a surge of interest in density estimation in recent years, much of the published research has been concerned with purely technical matters with insufficient emphasis given to the technique's practical value. Furthermore, the subject has been rather inaccessible to the general statistician. The account presented in this book places emphasis on topics of methodological importance, in the hope that this will facilitate broader practical application of density estimation and also encourage research into relevant theoretical work. The book also provides an introduction to the subject for those with general interests in statistics. The important role of density estimation as a graphical technique is reflected by the inclusion of more than 50 graphs and figures throughout the text. Several contexts in which density estimation can be used are discussed, including the exploration and presentation of data, nonparametric discriminant analysis, cluster analysis, simulation and the bootstrap, bump hunting, projection pursuit, and the estimation of hazard rates and other quantities that depend on the density. This book includes general survey of methods available for density estimation. The Kernel method, both for univariate and multivariate data, is discussed in detail, with particular emphasis on ways of deciding how much to smooth and on computation aspects. Attention is also given to adaptive methods, which smooth to a greater degree in the tails of the distribution, and to methods based on the idea of penalized likelihood.}
}

@article{sinha_principal_1997,
  title = {The Principal Lunar Semidiurnal Tide and Its Harmonics: Baseline Solutions for {{M2}} and {{M4}} Constituents on the {{North-West European Continental Shelf}}},
  shorttitle = {The Principal Lunar Semidiurnal Tide and Its Harmonics},
  author = {Sinha, B. and Pingree, R. D.},
  year = {1997},
  month = sep,
  journal = {Continental Shelf Research},
  volume = {17},
  number = {11},
  pages = {1321--1365},
  issn = {0278-4343},
  doi = {10.1016/S0278-4343(97)00007-1},
  abstract = {A 2D numerical model of the shelf seas around the U.K. is used to derive the M2 tide and the M4 and M6 tidal harmonics. The accuracy of the tidal harmonics is shown to be related to the form of the dissipation. In particular a quadratic form of the bottom friction results in unrealistically high amplitudes for the M6 tide. It is shown that this effect can be reduced by use of a combination of a linearised friction tensor and quadratic friction without significantly affecting the M2 constituent. The role of diffusion in controlling the amplitude of the M4 tide is demonstrated. The 2D results provide a baseline or reference solution for further development using 3D and/or multiconstituent models. Amplitude and phase diagrams derived from the model show a progression with frequency in the North Sea and the English Channel suggestive of a transition from Kelvin wave to Poincar\'e wave propagation. It is shown that both the North Sea and the English Channel can be considered from the point of view of waves propagating in a closed channel in the presence of friction. For the inviscid case, the critical frequency above which Poincar\'e waves are able to propagate is calculated to be between the M2 and M4 frequencies for the North Sea and between the M6 and M8 frequencies for the English Channel. A dispersion relation is derived for Poincar\'e and Kelvin waves in the presence of friction. Both types of wave possess decaying and propagating characteristics when friction is present with wavelength and decay scale dependent on frequency and the coefficient of friction. There is no longer a sharp critical frequency, but instead a transition region where wavelength and decay scales vary relatively rapidly with frequency. The response of a semi-infinite rectangular channel to Kelvin and Poincar\'e wave propagation is calculated providing a partial explanation of the numerical model results. \textcopyright{} 1997 Elsevier Science Ltd},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5AIPIYRK\\Sinha et Pingree - 1997 - The principal lunar semidiurnal tide and its harmo.pdf;C\:\\Users\\a846735\\Zotero\\storage\\J2WHXNT7\\S0278434397000071.html}
}

@article{smith_optimizers_2006,
  title = {The {{Optimizer}}'s {{Curse}}: {{Skepticism}} and {{Postdecision Surprise}} in {{Decision Analysis}}},
  shorttitle = {The {{Optimizer}}'s {{Curse}}},
  author = {Smith, James E. and Winkler, Robert L.},
  year = {2006},
  month = mar,
  journal = {Management Science},
  volume = {52},
  number = {3},
  pages = {311--322},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.1050.0451},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VAK8B93Y\\Smith et Winkler - 2006 - The Optimizerâs Curse Skepticism and Postdecision.pdf}
}

@article{smyl_efficient_2021,
  title = {An Efficient {{Quasi-Newton}} Method for Nonlinear Inverse Problems via Learned Singular Values},
  author = {Smyl, Danny and Tallman, Tyler N. and Liu, Dong and Hauptmann, Andreas},
  year = {2021},
  journal = {IEEE Signal Processing Letters},
  volume = {28},
  eprint = {2012.07676},
  eprinttype = {arxiv},
  pages = {748--752},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2021.3063622},
  abstract = {Solving complex optimization problems in engineering and the physical sciences requires repetitive computation of multi-dimensional function derivatives, which commonly require computationally-demanding numerical differentiation such as perturbation techniques. In particular, Gauss-Newton methods are used for nonlinear inverse problems that require iterative updates to be computed from the Jacobian and allow for flexible incorporation of prior knowledge. Computationally more efficient alternatives are Quasi-Newton methods, where the repeated computation of the Jacobian is replaced by an approximate update, but unfortunately are often too restrictive for highly ill-posed problems. To overcome this limitation, we present a highly efficient data-driven Quasi-Newton method applicable to nonlinear inverse problems, by using the singular value decomposition and learning a mapping from model outputs to the singular values to compute the updated Jacobian. Enabling time-critical applications and allowing for flexible incorporation of prior knowledge necessary to solve ill-posed problems. We present results for the highly non-linear inverse problem of electrical impedance tomography with experimental data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\49FJSL39\\Smyl et al. - 2021 - An efficient Quasi-Newton method for nonlinear inv.pdf}
}

@article{smyl_learning_2021,
  title = {Learning and Correcting Non-{{Gaussian}} Model Errors},
  author = {Smyl, Danny and Tallman, Tyler N. and Black, Jonathan A. and Hauptmann, Andreas and Liu, Dong},
  year = {2021},
  month = may,
  journal = {Journal of Computational Physics},
  volume = {432},
  eprint = {2005.14592},
  eprinttype = {arxiv},
  pages = {110152},
  issn = {00219991},
  doi = {10.1016/j.jcp.2021.110152},
  abstract = {All discretized numerical models contain modelling errors \textendash{} this reality is amplified when reduced-order models are used. The ability to accurately approximate modelling errors informs statistics on model confidence and improves quantitative results from frameworks using numerical models in prediction, tomography, and signal processing. Further to this, the compensation of highly nonlinear and non-Gaussian modelling errors, arising in many ill-conditioned systems aiming to capture complex physics, is a historically difficult task. In this work, we address this challenge by proposing a neural network approach capable of accurately approximating and compensating for such modelling errors in augmented direct and inverse problems. The viability of the approach is demonstrated using simulated and experimental data arising from differing physical direct and inverse problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis,Physics - Computational Physics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TMPTVGKI\\Smyl et al. - 2021 - Learning and correcting non-Gaussian model errors.pdf}
}

@article{sniedovich_solution_1991,
  title = {Solution Strategies for Variance Minimization Problems},
  author = {Sniedovich, Moshe},
  year = {1991},
  month = jan,
  journal = {Computers \& Mathematics with Applications},
  volume = {21},
  number = {2},
  pages = {49--56},
  issn = {0898-1221},
  doi = {10.1016/0898-1221(91)90080-N},
  abstract = {We outline two solutions strategies for optimization problems requiring the minimization of an objective function with a variance or variance-like term.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Y9PIPQAH\\Sniedovich - 1991 - Solution strategies for variance minimization prob.pdf;C\:\\Users\\a846735\\Zotero\\storage\\2WMLP4CY\\089812219190080N.html}
}

@article{snyder_stochastic_2004,
  title = {Stochastic P-Robust Location Problems},
  author = {Snyder, Lawrence and Daskin, Mark},
  year = {2004},
  month = aug,
  journal = {Iie Transactions},
  volume = {38},
  doi = {10.1080/07408170500469113},
  abstract = {Many objectives have been proposed for optimization under uncertainty. The typical stochastic programming ob-jective of minimizing expected cost may yield solutions that are inexpensive in the long run but perform poorly under certain realizations of the random data. On the other hand, the typical robust optimization objective of minimizing maximum cost or regret tends to be overly conservative, planning against a disastrous but unlikely scenario. In this paper, we present facility location models that combine the two objectives by minimizing the expected cost while bounding the relative regret in each scenario. In particular, the models seek the minimum-expected-cost solution that is p-robust; i.e., whose relative regret is no more than 100p\% in each scenario. We present p-robust models based on two classical facility location problems, the P -median problem and the uncapacitated fixed-charge location problem. We solve both problems using variable splitting (Lagrangian decomposition), in which the subproblem reduces to the multiple-choice knapsack problem. Feasible solutions are found using an upper-bounding heuristic. For many instances of the problems, finding a feasible solution, and even determining whether the instance is feasible, is difficult; we discuss a mechanism for determining infeasibility. We also show how the algorithm can be used as a heuristic to solve minimax-regret versions of the location problems.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BU2CJ9WS\\Snyder et Daskin - 2004 - Stochastic p-robust location problems.pdf}
}

@article{sobol_global_2001,
  title = {Global Sensitivity Indices for Nonlinear Mathematical Models and Their {{Monte Carlo}} Estimates},
  author = {Sobol, Ilya M.},
  year = {2001},
  journal = {Mathematics and computers in simulation},
  volume = {55},
  number = {1-3},
  pages = {271--280},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5JECGDSG\\Sobol - 2001 - Global sensitivity indices for nonlinear mathemati.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ENBQ3KDK\\S0378475400002706.html}
}

@article{sobol_sensitivity_1993,
  title = {Sensitivity Analysis for Non-Linear Mathematical Models},
  author = {Sobol, Ilya M.},
  year = {1993},
  journal = {Mathematical modelling and computational experiment},
  volume = {1},
  pages = {407--414},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HTVU7J36\\10027137978.html}
}

@inproceedings{sohns_efficient_2006,
  title = {Efficient Parameterization of Large-Scale Dynamic Models through the Use of Activity Analysis},
  booktitle = {Proceedings of the {{ASME IMECE}}},
  author = {Sohns, B. and Allison, James and Fathy, Hosam K. and Stein, Jeffrey L.},
  year = {2006},
  pages = {5--10},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MX4BU3SH\\Sohns et al. - 2006 - Efficient parameterization of large-scale dynamic .pdf;C\:\\Users\\a846735\\Zotero\\storage\\GYA5TDAS\\proceeding.html}
}

@article{sonnewald_bridging_2021,
  title = {Bridging Observation, Theory and Numerical Simulation of the Ocean Using {{Machine Learning}}},
  author = {Sonnewald, Maike and Lguensat, Redouane and Jones, Daniel C. and Dueben, Peter D. and Brajard, Julien and Balaji, Venkatramani},
  year = {2021},
  month = jul,
  journal = {Environmental Research Letters},
  volume = {16},
  number = {7},
  eprint = {2104.12506},
  eprinttype = {arxiv},
  pages = {073008},
  issn = {1748-9326},
  doi = {10.1088/1748-9326/ac0eb0},
  abstract = {Progress within physical oceanography has been concurrent with the increasing sophistication of tools available for its study. The incorporation of machine learning (ML) techniques offers exciting possibilities for advancing the capacity and speed of established methods and also for making substantial and serendipitous discoveries. Beyond vast amounts of complex data ubiquitous in many modern scientific fields, the study of the ocean poses a combination of unique challenges that ML can help address. The observational data available is largely spatially sparse, limited to the surface, and with few time series spanning more than a handful of decades. Important timescales span seconds to millennia, with strong scale interactions and numerical modelling efforts complicated by details such as coastlines. This review covers the current scientific insight offered by applying ML and points to where there is imminent potential. We cover the main three branches of the field: observations, theory, and numerical modelling. Highlighting both challenges and opportunities, we discuss both the historical context and salient ML tools. We focus on the use of ML in situ sampling and satellite observations, and the extent to which ML applications can advance theoretical oceanographic exploration, as well as aid numerical simulations. Applications that are also covered include model error and bias correction and current and potential use within data assimilation. While not without risk, there is great interest in the potential benefits of oceanographic ML applications; this review caters to this interest within the research community.},
  archiveprefix = {arXiv},
  keywords = {Physics - Atmospheric and Oceanic Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\AESQEA35\\Sonnewald et al. - 2021 - Bridging observation, theory and numerical simulat.pdf;C\:\\Users\\a846735\\Zotero\\storage\\MHSBPB2U\\Sonnewald et al. - 2021 - Bridging observation, theory and numerical simulat.pdf;C\:\\Users\\a846735\\Zotero\\storage\\WTWEJ4M4\\2104.html}
}

@article{spantini_optimal_2015,
  title = {Optimal Low-Rank Approximations of {{Bayesian}} Linear Inverse Problems},
  author = {Spantini, Alessio and Solonen, Antti and Cui, Tiangang and Martin, James and Tenorio, Luis and Marzouk, Youssef},
  year = {2015},
  month = jul,
  journal = {arXiv:1407.3463 [math, stat]},
  eprint = {1407.3463},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {In the Bayesian approach to inverse problems, data are often informative, relative to the prior, only on a low-dimensional subspace of the parameter space. Significant computational savings can be achieved by using this subspace to characterize and approximate the posterior distribution of the parameters. We first investigate approximation of the posterior covariance matrix as a low-rank update of the prior covariance matrix. We prove optimality of a particular update, based on the leading eigendirections of the matrix pencil defined by the Hessian of the negative log-likelihood and the prior precision, for a broad class of loss functions. This class includes the F\textbackslash "\{o\}rstner metric for symmetric positive definite matrices, as well as the Kullback-Leibler divergence and the Hellinger distance between the associated distributions. We also propose two fast approximations of the posterior mean and prove their optimality with respect to a weighted Bayes risk under squared-error loss. These approximations are deployed in an offline-online manner, where a more costly but data-independent offline calculation is followed by fast online evaluations. As a result, these approximations are particularly useful when repeated posterior mean evaluations are required for multiple data sets. We demonstrate our theoretical results with several numerical examples, including high-dimensional X-ray tomography and an inverse heat conduction problem. In both of these examples, the intrinsic low-dimensional structure of the inference problem can be exploited while producing results that are essentially indistinguishable from solutions computed in the full space.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\N69QUXCT\\Spantini et al. - 2015 - Optimal low-rank approximations of Bayesian linear.pdf;C\:\\Users\\a846735\\Zotero\\storage\\8HCL9ZAC\\1407.html}
}

@article{srinivas_gaussian_2012,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}: {{No Regret}} and {{Experimental Design}}},
  shorttitle = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
  year = {2012},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {58},
  number = {5},
  eprint = {0912.3995},
  eprinttype = {arxiv},
  pages = {3250--3265},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2011.2182033},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5S6ZESFE\\Srinivas et al. - 2012 - Gaussian Process Optimization in the Bandit Settin.pdf}
}

@article{stanimirovic_simulation_2021,
  title = {Simulation of {{Varying Parameter Recurrent Neural Network}} with Application to Matrix Inversion},
  author = {Stanimirovi{\'c}, Predrag and Gerontitis, Dimitris and Tzekis, Panagiotis and Behera, Ratikanta and Sahoo, Jajati Keshari},
  year = {2021},
  journal = {Mathematics and Computers in Simulation (MATCOM)},
  volume = {185},
  number = {C},
  pages = {614--628},
  publisher = {{Elsevier}},
  abstract = {A class of adaptive recurrent neural networks (RNN) for computing the inverse of a time-varying matrix with accelerated convergence time is defined and considered. The proposed neural dynamic model involves an exponential gain time-varying term in the nonlinear activation of the finite-time Zhang neural network (FTZNN) dynamical equation. Individual models belonging to the proposed class are defined by means of corresponding error functions. It is shown theoretically and experimentally that usage of the exponential nonlinear activation accelerates the convergence rate of the error function compared to previous dynamical systems for solving the time-varying (TV) and time-invariant (TI) matrix inversion.},
  langid = {english},
  keywords = {Finite-time convergence,Recurrent neural network,Time-varying matrix inversion,Time-varying systems},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\E4EFST3A\\v185y2021icp614-628.html}
}

@article{steen_gaussian_1969,
  title = {Gaussian Quadratures for the Integrals {$_{0}\sphat\lbrace\infty\rbrace\mathsl{e}\mathsl{x}\mathsl{p}$}(-{$\mathsl{x}^2$}){$\mathsl{f}$}({$\mathsl{x}$}){$\mathsl{d}\mathsl{x}$} and {$_{0}\sphat\lbrace\mathsl{b}\rbrace\mathsl{e}\mathsl{x}\mathsl{p}$}(-{$\mathsl{x}^2$}){$\mathsl{f}$}({$\mathsl{x}$}){$\mathsl{d}\mathsl{x}$}},
  author = {Steen, N. M. and Byrne, G. D. and Gelbard, E. M.},
  year = {1969},
  journal = {Mathematics of Computation},
  volume = {23},
  number = {107},
  pages = {661--671},
  issn = {0025-5718, 1088-6842},
  doi = {10.1090/S0025-5718-1969-0247744-3},
  abstract = {Gaussian quadratures are developed for the evaluation of the integrals given in the title. The weights and abscissae for the semi-infinite integral are given for two through fifteen points with fifteen places. For , the weights and abscissae are given for two through ten points with fifteen places.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8SA9JFKV\\Steen et al. - 1969 - Gaussian quadratures for the integrals â^ â .pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZYBLTSPL\\home.html}
}

@article{stefanski_calculus_2002,
  title = {The Calculus of {{M-estimation}}},
  author = {Stefanski, Leonard A. and Boos, Dennis D.},
  year = {2002},
  journal = {The American Statistician},
  volume = {56},
  number = {1},
  pages = {29--38},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z5S4DBRT\\Stefanski et Boos - 2002 - The calculus of M-estimation.pdf;C\:\\Users\\a846735\\Zotero\\storage\\V9N9YLHS\\000313002753631330.html}
}

@article{steward_impact_2012,
  title = {Impact of Non-Smooth Observation Operators on Variational and Sequential Data Assimilation for a Limited-Area Shallow-Water Equation Model},
  author = {Steward, J. L. and Navon, I. M. and Zupanski, M. and Karmitsa, N.},
  year = {2012},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {138},
  number = {663},
  pages = {323--339},
  issn = {00359009},
  doi = {10.1002/qj.935},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\885MZBFG\\Steward et al. - 2012 - Impact of non-smooth observation operators on vari.pdf}
}

@phdthesis{steward_practical_2012,
  title = {Practical Optimization Algorithms in the Data Assimilation of Large-Scale Systems with Non-Linear and Non-Smooth Observation Operators},
  author = {Steward, Jeffrey L.},
  year = {2012},
  address = {{United States -- Florida}},
  abstract = {This dissertation compares and contrasts large-scale optimization algorithms in the use of variational and sequential data assimilation on two novel problems chosen to highlight the challenges in non-linear and non-smooth data assimilation. The first problem explores the impact of a highly non-linear observation operator and highlights the importance of background information on the data assimilation problem. The second problem tackles large-scale data assimilation with a non-smooth observation operator. Together, these two cases show both the importance of choosing an appropriate data assimilation method and, when a variational or variationally-inspired method is chosen, the importance of choosing the right optimization algorithm for the problem at hand.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781267522580},
  langid = {english},
  school = {The Florida State University},
  keywords = {Applied sciences,Clouds,Data assimilation,Earth sciences,Infrared satellites,Inverse problem,Limited memory bundle method,Non-differentiable},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\8Z42Z625\\Steward - 2012 - Practical optimization algorithms in the data assi.pdf}
}

@article{steyerberg_assessing_2010,
  title = {Assessing the Performance of Prediction Models: A Framework for Some Traditional and Novel Measures},
  shorttitle = {Assessing the Performance of Prediction Models},
  author = {Steyerberg, Ewout W. and Vickers, Andrew J. and Cook, Nancy R. and Gerds, Thomas and Gonen, Mithat and Obuchowski, Nancy and Pencina, Michael J. and Kattan, Michael W.},
  year = {2010},
  month = jan,
  journal = {Epidemiology (Cambridge, Mass.)},
  volume = {21},
  number = {1},
  pages = {128--138},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e3181c30fb2},
  abstract = {The performance of prediction models can be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or area under the receiver operating characteristic (ROC) curve), and goodness-of-fit statistics for calibration., Several new measures have recently been proposed that can be seen as refinements of discrimination measures, including variants of the c statistic for survival, reclassification tables, net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Moreover, decision\textendash analytic measures have been proposed, including decision curves to plot the net benefit achieved by making decisions based on model predictions., We aimed to define the role of these relatively novel approaches in the evaluation of the performance of prediction models. For illustration we present a case study of predicting the presence of residual tumor versus benign tissue in patients with testicular cancer (n=544 for model development, n=273 for external validation)., We suggest that reporting discrimination and calibration will always be important for a prediction model. Decision-analytic measures should be reported if the predictive model is to be used for making clinical decisions. Other measures of performance may be warranted in specific applications, such as reclassification metrics to gain insight into the value of adding a novel predictor to an established model.},
  pmcid = {PMC3575184},
  pmid = {20010215},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XXH8BP7U\\Steyerberg et al. - 2010 - Assessing the performance of prediction models a .pdf}
}

@misc{stock_notes_nodate,
  title = {Notes on Optimal Transport},
  author = {Stock, Michiel},
  abstract = {This summer, I stumbled upon the optimal transportation problem, an optimization paradigm where the goal is to transform one probability distribution into another with a minimal cost. It is so simple to understand, yet it has a mind-boggling number of applications in probability, computer vision, machine learning, computational fluid dynamics, and computational biology. I recently gave a seminar on this topic, and this post is an overview of the topic. Slides can be found on my SlideShare and some implementations can are shown in a Jupyter notebook in my Github repo. Enjoy!},
  howpublished = {https://michielstock.github.io/OptimalTransport/},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PA4SC2UE\\OptimalTransport.html}
}

@article{stoye_statistical_2011,
  title = {Statistical Decisions under Ambiguity},
  author = {Stoye, J{\"o}rg},
  year = {2011},
  month = feb,
  journal = {Theory and Decision},
  volume = {70},
  number = {2},
  pages = {129--148},
  issn = {0040-5833, 1573-7187},
  doi = {10.1007/s11238-010-9227-2},
  abstract = {Consider a decision maker who faces a number of possible models of the world. Every model generates objective probabilities, but no probabilities of models are given. This is the classic setting of statistical decision theory; recent and less standard applications include decision making with model uncertainty, e.g. due to concerns for misspecification, treatment choice with partial identification, and robust Bayesian analysis.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JCW4V3PE\\Stoye - 2011 - Statistical decisions under ambiguity.pdf}
}

@article{stuart_inverse_2010,
  title = {Inverse Problems: {{A Bayesian}} Perspective},
  shorttitle = {Inverse Problems},
  author = {Stuart, A. M.},
  year = {2010},
  month = may,
  journal = {Acta Numerica},
  volume = {19},
  pages = {451--559},
  issn = {0962-4929, 1474-0508},
  doi = {10.1017/S0962492910000061},
  langid = {english},
  keywords = {Bayesian inference,Inverse problems},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EAVUSLYB\\stuart15c.pdf}
}

@article{sudret_global_2008,
  title = {Global Sensitivity Analysis Using Polynomial Chaos Expansion},
  author = {Sudret, Bruno},
  year = {2008},
  month = jul,
  journal = {Reliability Engineering \& System Safety},
  volume = {93},
  pages = {964--979},
  doi = {10.1016/j.ress.2007.04.002},
  abstract = {Global sensitivity analysis (SA) aims at quantifying the respective effects of input random variables (or combinations thereof) onto the variance of the response of a physical or mathematical model. Among the abundant literature on sensitivity measures, the Sobol' indices have received much attention since they provide accurate information for most models. The paper introduces generalized polynomial chaos expansions (PCE) to build surrogate models that allow one to compute the Sobol' indices analytically as a post-processing of the PCE coefficients. Thus the computational cost of the sensitivity indices practically reduces to that of estimating the PCE coefficients. An original non intrusive regression-based approach is proposed, together with an experimental design of minimal size. Various application examples illustrate the approach, both from the field of global SA (i.e. well-known benchmark problems) and from the field of stochastic mechanics. The proposed method gives accurate results for various examples that involve up to eight input random variables, at a computational cost which is 2\textendash 3 orders of magnitude smaller than the traditional Monte Carlo-based evaluation of the Sobol' indices.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YJFEYP88\\Sudret - 2008 - Global sensitivity analysis using polynomial chaos.pdf}
}

@incollection{sudret_polynomial_2015,
  title = {Polynomial Chaos Expansions and Stochastic Finite Element Methods},
  booktitle = {Risk and {{Reliability}} in {{Geotechnical Engineering}}},
  author = {Sudret, Bruno},
  editor = {{Kok-Kwang Phoon}, Jianye Ching},
  year = {2015},
  pages = {265--300},
  publisher = {{CRC Press}},
  abstract = {This paper is a state-of-the art review on sparse polynomial chaos expansions (PCE) for engineering applications. It contains a step-by-step presentation of PCEs and their use in moment-, sensitivity- and reliability analysis.  Error estimators and sparse expansions for addressing high-dimensional problems are discussed. Applications in geotechnical engineering showcase the efficiency of sparse PCEs.},
  keywords = {geotechnics,global sensitivity analysis,polynomial chaos expansions,risk analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VMUHQL7E\\Sudret - 2015 - Polynomial chaos expansions and stochastic finite .pdf}
}

@article{sudret_polynomial_nodate,
  title = {Polynomial Chaos Expansions and Stochastic Finite Element Methods},
  author = {Sudret, Bruno},
  pages = {43},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WLS85HD6\\Sudret - Polynomial chaos expansions and stochastic finite .pdf}
}

@article{sullivan_introduction_nodate,
  title = {Introduction to {{Uncertainty Quantification}}},
  author = {Sullivan, T J},
  pages = {74},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DAT6V64I\\Sullivan - Introduction to Uncertainty Quantiï¬cation.pdf}
}

@article{sunar_comparative_nodate,
  title = {A {{Comparative Study}} of {{Multiobjective Optimization Methods}} in {{Structural Design}}},
  author = {Sunar, Mehmet and Kahraman, Ramazan},
  pages = {10},
  abstract = {The computational algorithms of different multiobjective optimization techniques and their applications to structural systems are presented. The weighting, -constraint, goal programming and modified game theory methods are described along with a comparative study of the results. The conflicting nature of the objective functions is studied through two multiobjective optimization problems. Specifically, the design of a 25-bar space truss and that of a satellite with flexible appendages are considered in numerical studies. The results from the multiobjective optimization methods are evaluated in terms of a supercriterion. It is concluded that the results obtained using the goal programming and modified game theory/goal programming approaches are properly balanced yielding the best compromise in the presence of conflicting objectives.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\CBH38ZKP\\Sunar et Kahraman - A Comparative Study of Multiobjective Optimization.pdf}
}

@article{sunnaker_approximate_2013,
  title = {Approximate {{Bayesian Computation}}},
  author = {Sunn{\aa}ker, Mikael and Busetto, Alberto Giovanni and Numminen, Elina and Corander, Jukka and Foll, Matthieu and Dessimoz, Christophe},
  year = {2013},
  month = jan,
  journal = {PLoS Computational Biology},
  volume = {9},
  number = {1},
  pages = {e1002803},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1002803},
  abstract = {Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences (e.g., in population genetics, ecology, epidemiology, and systems biology).},
  pmcid = {PMC3547661},
  pmid = {23341757},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Q6Y5FHLD\\SunnÃ¥ker et al. - 2013 - Approximate Bayesian Computation.pdf}
}

@article{syring_gibbs_nodate,
  title = {Gibbs Posterior Inference on Value-at-Risk},
  author = {Syring, Nicholas and Hong, Liang and Martin, Ryan},
  pages = {13},
  abstract = {Accurate estimation of value-at-risk (VaR) and assessment of associated uncertainty is crucial for both insurers and regulators, particularly in Europe. Existing approaches link data and VaR indirectly by first linking data to the parameter of a probability model, and then expressing VaR as a function of that parameter. This indirect approach exposes the insurer to model misspecification bias or estimation inefficiency, depending on whether the parameter is finite- or infinite-dimensional. In this paper, we link data and VaR directly via what we call a discrepancy function, and this leads naturally to a Gibbs posterior distribution for VaR that does not suffer from the aforementioned biases and inefficiencies. Asymptotic consistency and root-n concentration rate of the Gibbs posterior are established, and simulations highlight its superior finite-sample performance compared to other approaches.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZUWS6KBX\\Syring et al. - Gibbs posterior inference on value-at-risk.pdf}
}

@article{tabeart_conditioning_2018,
  title = {The Conditioning of Least-Squares Problems in Variational Data Assimilation},
  author = {Tabeart, Jemima M. and Dance, Sarah L. and Haben, Stephen A. and Lawless, Amos S. and Nichols, Nancy K. and Waller, Joanne A.},
  year = {2018},
  journal = {Numerical Linear Algebra with Applications},
  volume = {25},
  number = {5},
  pages = {e2165},
  issn = {1099-1506},
  doi = {10.1002/nla.2165},
  abstract = {In variational data assimilation a least-squares objective function is minimised to obtain the most likely state of a dynamical system. This objective function combines observation and prior (or background) data weighted by their respective error statistics. In numerical weather prediction, data assimilation is used to estimate the current atmospheric state, which then serves as an initial condition for a forecast. New developments in the treatment of observation uncertainties have recently been shown to cause convergence problems for this least-squares minimisation. This is important for operational numerical weather prediction centres due to the time constraints of producing regular forecasts. The condition number of the Hessian of the objective function can be used as a proxy to investigate the speed of convergence of the least-squares minimisation. In this paper we develop novel theoretical bounds on the condition number of the Hessian. These new bounds depend on the minimum eigenvalue of the observation error covariance matrix and the ratio of background error variance to observation error variance. Numerical tests in a linear setting show that the location of observation measurements has an important effect on the condition number of the Hessian. We identify that the conditioning of the problem is related to the complex interactions between observation error covariance and background error covariance matrices. Increased understanding of the role of each constituent matrix in the conditioning of the Hessian will prove useful for informing the choice of correlated observation error covariance matrix and observation location, particularly for practical applications.},
  langid = {english},
  keywords = {condition number,correlated observation errors,data assimilation,Hessian,least squares},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2165},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\USERBFQD\\Tabeart et al. - 2018 - The conditioning of least-squares problems in vari.pdf;C\:\\Users\\a846735\\Zotero\\storage\\8JC2X437\\nla.html}
}

@misc{tabeart_new_2021,
  title = {New Bounds on the Condition Number of the {{Hessian}} of the Preconditioned Variational Data Assimilation Problem},
  author = {Tabeart, Jemima M. and Dance, Sarah L. and Lawless, Amos S. and Nichols, Nancy K. and Waller, Joanne A.},
  year = {2021},
  month = may,
  number = {arXiv:2010.08416},
  eprint = {2010.08416},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  abstract = {Data assimilation algorithms combine prior and observational information, weighted by their respective uncertainties, to obtain the most likely posterior of a dynamical system. In variational data assimilation the posterior is computed by solving a nonlinear least squares problem. Many numerical weather prediction (NWP) centres use full observation error covariance (OEC) weighting matrices, which can slow convergence of the data assimilation procedure. Previous work revealed the importance of the minimum eigenvalue of the OEC matrix for conditioning and convergence of the unpreconditioned data assimilation problem. In this paper we examine the use of correlated OEC matrices in the preconditioned data assimilation problem for the first time. We consider the case where there are more state variables than observations, which is typical for applications with sparse measurements e.g. NWP and remote sensing. We find that similarly to the unpreconditioned problem, the minimum eigenvalue of the OEC matrix appears in new bounds on the condition number of the Hessian of the preconditioned objective function. Numerical experiments reveal that the condition number of the Hessian is minimised when the background and observation lengthscales are equal. This contrasts with the unpreconditioned case, where decreasing the observation error lengthscale always improves conditioning. Conjugate gradient experiments show that in this framework the condition number of the Hessian is a good proxy for convergence. Eigenvalue clustering explains cases where convergence is faster than expected.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZTUPJYGP\\Tabeart et al. - 2021 - New bounds on the condition number of the Hessian .pdf}
}

@article{taddy_fast_2008,
  title = {Fast {{Bayesian Inference}} for {{Computer Simulation Inverse Problems}}},
  author = {Taddy, Matthew and Lee, Herbert KH and Sans{\'o}, Bruno},
  year = {2008},
  journal = {submitted to Inverse Problems},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\BUF9L83Q\\Taddy et al. - 2008 - Fast Bayesian Inference for Computer Simulation In.pdf;C\:\\Users\\a846735\\Zotero\\storage\\EM6S6634\\Taddy et al. - 2008 - Fast Bayesian Inference for Computer Simulation In.pdf;C\:\\Users\\a846735\\Zotero\\storage\\UUJJQSNS\\Fast Bayesian Inference for Computer Simulation Inverse Problems.pdf}
}

@article{taddy_fast_2009,
  title = {Fast Inference for Statistical Inverse Problems},
  author = {Taddy, Matthew A and Lee, Herbert K H and Sans{\'o}, Bruno},
  year = {2009},
  month = aug,
  journal = {Inverse Problems},
  volume = {25},
  number = {8},
  pages = {085001},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/25/8/085001},
  keywords = {stochastic inverse problem},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\C7AZEST2\\invprob09.pdf}
}

@book{tarantola_inverse_2005,
  title = {Inverse Problem Theory and Methods for Model Parameter Estimation},
  author = {Tarantola, Albert},
  year = {2005},
  publisher = {{SIAM, Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, Pa}},
  isbn = {978-0-89871-572-9},
  langid = {english},
  keywords = {stochastic inverse problem},
  annotation = {OCLC: 265659758},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XZX7CY5V\\InverseProblemTheory.pdf}
}

@article{tavakkoli_novel_2019,
  title = {A {{Novel Recurrent Neural Network-Based Ultra-Fast}}, {{Robust}}, and {{Scalable Solver}} for {{Inverting}} a ``{{Time-Varying Matrix}}''},
  author = {Tavakkoli, Vahid and Chedjou, Jean Chamberlain and Kyamakya, Kyandoghere},
  year = {2019},
  month = sep,
  journal = {Sensors (Basel, Switzerland)},
  volume = {19},
  number = {18},
  pages = {4002},
  issn = {1424-8220},
  doi = {10.3390/s19184002},
  abstract = {The concept presented in this paper is based on previous dynamical methods to realize a time-varying matrix inversion. It is essentially a set of coupled ordinary differential equations (ODEs) which does indeed constitute a recurrent neural network (RNN) model. The coupled ODEs constitute a universal modeling framework for realizing a matrix inversion provided the matrix is invertible. The proposed model does converge to the inverted matrix if the matrix is invertible, otherwise it converges to an approximated inverse. Although various methods exist to solve a matrix inversion in various areas of science and engineering, most of them do assume that either the time-varying matrix inversion is free of noise or they involve a denoising module before starting the matrix inversion computation. However, in the practice, the noise presence issue is a very serious problem. Also, the denoising process is computationally expensive and can lead to a violation of the real-time property of the system. Hence, the search for a new `matrix inversion' solving method inherently integrating noise-cancelling is highly demanded. In this paper, a new combined/extended method for time-varying matrix inversion is proposed and investigated. The proposed method is extending both the gradient neural network (GNN) and the Zhang neural network (ZNN) concepts. Our new model has proven that it has exponential stability according to Lyapunov theory. Furthermore, when compared to the other previous related methods (namely GNN, ZNN, Chen neural network, and integration-enhanced Zhang neural network or IEZNN) it has a much better theoretical convergence speed. To finish, all named models (the new one versus the old ones) are compared through practical examples and both their respective convergence and error rates are measured. It is shown/observed that the novel/proposed method has a better practical convergence rate when compared to the other models. Regarding the amount of noise, it is proven that there is a very good approximation of the matrix inverse even in the presence of noise.},
  pmcid = {PMC6767331},
  pmid = {31527511},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\F3NZTY3Q\\Tavakkoli et al. - 2019 - A Novel Recurrent Neural Network-Based Ultra-Fast,.pdf}
}

@article{ten_brummelhuis_parameter_1990,
  title = {Parameter Identification in Tidal Models with Uncertain Boundary Conditions},
  author = {{ten Brummelhuis}, P. G. J. and Heemink, A. W.},
  year = {1990},
  month = sep,
  journal = {Stochastic Hydrology and Hydraulics},
  volume = {4},
  number = {3},
  pages = {193--208},
  issn = {1435-151X},
  doi = {10.1007/BF01543083},
  abstract = {In this paper a parameter estimation algorithm is developed to estimate uncertain parameters in two dimensional shallow water flow models. Since in practice the open boundary conditions of these models are usually not known accurately, the uncertainty of these boundary conditions has to be taken into account to prevent that boundary errors are interpreted by the estimation procedure as parameter fluctuations. Therefore the open boundary conditions are embedded into a stochastic environment and a constant gain extended Kalman filter is employed to identify the state of the system. Defining a error functional that measures the differences between the filtered state of the system and the measurements, a quasi Newton method is employed to determine the minimum of this functional. To reduce the computational burden, the gradient of the criterium that is required using the quasi Newton method is determined by solving the adjoint system.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UCC7PVFY\\ten Brummelhuis et Heemink - 1990 - Parameter identification in tidal models with unce.pdf}
}

@misc{teng_level_2021,
  title = {Level Set Learning with Pseudo-Reversible Neural Networks for Nonlinear Dimension Reduction in Function Approximation},
  author = {Teng, Yuankai and Wang, Zhu and Ju, Lili and Gruber, Anthony and Zhang, Guannan},
  year = {2021},
  month = dec,
  number = {arXiv:2112.01438},
  eprint = {2112.01438},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  abstract = {Due to the curse of dimensionality and the limitation on training data, approximating high-dimensional functions is a very challenging task even for powerful deep neural networks. Inspired by the Nonlinear Level set Learning (NLL) method that uses the reversible residual network (RevNet), in this paper we propose a new method of Dimension Reduction via Learning Level Sets (DRiLLS) for function approximation. Our method contains two major components: one is the pseudo-reversible neural network (PRNN) module that effectively transforms high-dimensional input variables to low-dimensional active variables, and the other is the synthesized regression module for approximating function values based on the transformed data in the low-dimensional space. The PRNN not only relaxes the invertibility constraint of the nonlinear transformation present in the NLL method due to the use of RevNet, but also adaptively weights the influence of each sample and controls the sensitivity of the function to the learned active variables. The synthesized regression uses Euclidean distance in the input space to select neighboring samples, whose projections on the space of active variables are used to perform local least-squares polynomial fitting. This helps to resolve numerical oscillation issues present in traditional local and global regressions. Extensive experimental results demonstrate that our DRiLLS method outperforms both the NLL and Active Subspace methods, especially when the target function possesses critical points in the interior of its input domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Functional Analysis,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NQDLL8TS\\Teng et al. - 2021 - Level set learning with pseudo-reversible neural n.pdf}
}

@misc{teng_level_2021-1,
  title = {Level Set Learning with Pseudo-Reversible Neural Networks for Nonlinear Dimension Reduction in Function Approximation},
  author = {Teng, Yuankai and Wang, Zhu and Ju, Lili and Gruber, Anthony and Zhang, Guannan},
  year = {2021},
  month = dec,
  number = {arXiv:2112.01438},
  eprint = {2112.01438},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.01438},
  abstract = {Due to the curse of dimensionality and the limitation on training data, approximating high-dimensional functions is a very challenging task even for powerful deep neural networks. Inspired by the Nonlinear Level set Learning (NLL) method that uses the reversible residual network (RevNet), in this paper we propose a new method of Dimension Reduction via Learning Level Sets (DRiLLS) for function approximation. Our method contains two major components: one is the pseudo-reversible neural network (PRNN) module that effectively transforms high-dimensional input variables to low-dimensional active variables, and the other is the synthesized regression module for approximating function values based on the transformed data in the low-dimensional space. The PRNN not only relaxes the invertibility constraint of the nonlinear transformation present in the NLL method due to the use of RevNet, but also adaptively weights the influence of each sample and controls the sensitivity of the function to the learned active variables. The synthesized regression uses Euclidean distance in the input space to select neighboring samples, whose projections on the space of active variables are used to perform local least-squares polynomial fitting. This helps to resolve numerical oscillation issues present in traditional local and global regressions. Extensive experimental results demonstrate that our DRiLLS method outperforms both the NLL and Active Subspace methods, especially when the target function possesses critical points in the interior of its input domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Functional Analysis,Mathematics - Numerical Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IV3MZDXF\\2112.html}
}

@techreport{terlizzese_relative_2008,
  title = {Relative {{Minimax}}},
  author = {Terlizzese, Daniele},
  year = {2008},
  month = may,
  number = {0804},
  institution = {{Einaudi Institute for Economics and Finance (EIEF)}},
  abstract = {To achieve robustness, a decision criterion that recently has been widely adopted is Wald's minimax, after Gilboa and Schmeidler (1989) showed that (one generalisation of) it can be given an axiomatic foundation with a behavioural interpretation. Yet minimax has known drawbacks. A better alternative is Savage's minimax regret, recently axiomatized by Stoye (2006). A related alternative is relative minimax, known as competitive ratio in the computer science literature, which is appealingly unit free. This paper provides an axiomatisation with behavioural content for relative minimax.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PUJCMRGI\\Terlizzese - 2008 - Relative Minimax.pdf;C\:\\Users\\a846735\\Zotero\\storage\\92U36CJ4\\0804.html}
}

@article{thommes_lattice_2007,
  title = {Lattice {{Boltzmann}} Methods for Shallow Water Flow Applications},
  author = {Th{\"o}mmes, G. and Seaid, Mohammed and Banda, Mapundi},
  year = {2007},
  month = nov,
  journal = {International Journal for Numerical Methods in Fluids},
  volume = {55},
  pages = {673--692},
  doi = {10.1002/fld.1489},
  abstract = {We apply the lattice Boltzmann (LB) method for solving the shallow water equations with source terms such as the bed slope and bed friction. Our aim is to use a simple and accurate representation of the source terms in order to simulate practical shallow water flows without relying on upwind discretization or Riemann problem solvers. We validate the algorithm on problems where analytical solutions are available. The numerical results are in good agreement with analytical solutions. Furthermore, we test the method on a practical problem by simulating mean flow in the Strait of Gibraltar. The main focus is to examine the performance of the LB method for complex geometries with irregular bathymetry. The results demonstrate its ability to capture the main flow features. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\K4NBD35P\\ThÃ¶mmes et al. - 2007 - Lattice Boltzmann methods for shallow water flow a.pdf}
}

@article{thulin_cost_2014,
  title = {The Cost of Using Exact Confidence Intervals for a Binomial Proportion},
  author = {Thulin, M{\textbackslash}aans},
  year = {2014},
  journal = {Electronic Journal of Statistics},
  volume = {8},
  number = {1},
  pages = {817--840},
  publisher = {{The Institute of Mathematical Statistics and the Bernoulli Society}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XX63WFAC\\Thulin - 2014 - The cost of using exact confidence intervals for a.pdf;C\:\\Users\\a846735\\Zotero\\storage\\H845LAJJ\\1402927499.html}
}

@article{thyer_critical_2009,
  title = {Critical Evaluation of Parameter Consistency and Predictive Uncertainty in Hydrological Modeling: {{A}} Case Study Using {{Bayesian}} Total Error Analysis: {{PARAMETER CONSISTENCY AND PREDICTIVE UNCERTAINTY}}},
  shorttitle = {Critical Evaluation of Parameter Consistency and Predictive Uncertainty in Hydrological Modeling},
  author = {Thyer, Mark and Renard, Benjamin and Kavetski, Dmitri and Kuczera, George and Franks, Stewart William and Srikanthan, Sri},
  year = {2009},
  month = dec,
  journal = {Water Resources Research},
  volume = {45},
  number = {12},
  issn = {00431397},
  doi = {10.1029/2008WR006825},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Q9Q3C5TA\\Thyer et al. - 2009 - Critical evaluation of parameter consistency and p.pdf}
}

@article{tibshirani_regression_2011,
  title = {Regression Shrinkage and Selection via the Lasso: A Retrospective},
  shorttitle = {Regression Shrinkage and Selection via the Lasso},
  author = {Tibshirani, Robert},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {3},
  pages = {273--282},
  publisher = {{Wiley Online Library}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KVZZZSDF\\Tibshirani - 2011 - Regression shrinkage and selection via the lasso .pdf;C\:\\Users\\a846735\\Zotero\\storage\\T5TJGWPU\\(ISSN)1467-9868.html}
}

@article{tieng_efficient_2021-1,
  title = {Efficient Solution of Linear Inverse Problems Using an Iterative Linear Neural Network with a Generalization Training Approach},
  author = {Tieng, Quang M. and Su, Jiasheng and Vegh, Viktor and Reutens, David C.},
  year = {2021},
  month = mar,
  journal = {Journal of Physics Communications},
  volume = {5},
  number = {3},
  pages = {035008},
  publisher = {{IOP Publishing}},
  issn = {2399-6528},
  doi = {10.1088/2399-6528/abebcf},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QDJ4PKAC\\Tieng et al. - 2021 - Efficient solution of linear inverse problems usin.pdf}
}

@book{tikhonov_solutions_1977,
  title = {Solutions of Ill-Posed Problems},
  author = {Tikhonov, Andrei and Arsenin, Vasily},
  year = {1977},
  volume = {14},
  keywords = {Regularization,Tikhonov}
}

@misc{tipping_bayesian_nodate,
  title = {Bayesian {{Inference}}: {{An Introduction}} to {{Principles}} and {{Practice}} in {{Machine Learning}}},
  author = {Tipping, Michael E.},
  howpublished = {http://www.miketipping.com/papers/met-mlbayes.pdf},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FEGY8BPK\\met-mlbayes.pdf}
}

@article{tipping_probabilistic_1999,
  title = {Probabilistic Principal Component Analysis},
  author = {Tipping, Michael E. and Bishop, Christopher M.},
  year = {1999},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {61},
  number = {3},
  pages = {611--622},
  publisher = {{Wiley Online Library}}
}

@article{tipping_probabilistic_nodate,
  title = {Probabilistic {{Principal Component Analysis}}},
  author = {Tipping, Michael E and Bishop, Christopher M},
  pages = {12},
  abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZG7KZHCL\\Tipping et Bishop - Probabilistic Principal Component Analysis.pdf}
}

@book{toro_riemann_2009,
  title = {Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction},
  shorttitle = {Riemann Solvers and Numerical Methods for Fluid Dynamics},
  author = {Toro, Eleuterio F.},
  year = {2009},
  edition = {3. ed},
  publisher = {{Springer}},
  address = {{Berlin}},
  abstract = {High resolution upwind and centered methods are today a mature generation of computational techniques applicable to a wide range of engineering and scientific disciplines, Computational Fluid Dynamics (CFD) being the most prominent up to now. This textbook gives a comprehensive, coherent and practical presentation of this class of techniques. The book is designed to provide readers with an understanding of the basic concepts, some of the underlying theory, the ability to critically use the current research papers on the subject, and, above all, with the required information for the practical implementation of the methods. Applications include: compressible, steady, unsteady, reactive, viscous, non-viscous and free surface flows. For this third edition, the book was thoroughly revised. It contains substantial more and new material both in its fundamental as well as in its applied part},
  isbn = {978-3-540-25202-3 978-3-540-49834-6},
  langid = {english},
  keywords = {Finite Volume},
  annotation = {OCLC: 391057413},
  file = {C\:\\Users\\Victor\\Documents\\Eleuterio_F._Toro_Riemann_Solvers_and_Numerical_Methods_for_Fluid_Dynamics.pdf}
}

@article{tran_spectral_2016,
  title = {Spectral {{M-estimation}} with {{Applications}} to {{Hidden Markov Models}}},
  author = {Tran, Dustin and Kim, Minjae and {Doshi-Velez}, Finale},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.08815 [cs, stat]},
  eprint = {1603.08815},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Method of moment estimators exhibit appealing statistical properties, such as asymptotic unbiasedness, for nonconvex problems. However, they typically require a large number of samples and are extremely sensitive to model misspecification. In this paper, we apply the framework of M-estimation to develop both a generalized method of moments procedure and a principled method for regularization. Our proposed M-estimator obtains optimal sample efficiency rates (in the class of moment-based estimators) and the same well-known rates on prediction accuracy as other spectral estimators. It also makes it straightforward to incorporate regularization into the sample moment conditions. We demonstrate empirically the gains in sample efficiency from our approach on hidden Markov models.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NJYEM92J\\Tran et al. - 2016 - Spectral M-estimation with Applications to Hidden .pdf;C\:\\Users\\a846735\\Zotero\\storage\\6MMGQQSJ\\1603.html}
}

@phdthesis{trappler_controparametre_2021,
  type = {These de Doctorat},
  title = {Contr\^ole de Param\`etre En Pr\'esence d'incertitudes},
  author = {Trappler, Victor},
  year = {2021},
  month = jun,
  abstract = {De nombreux ph\'enom\`enes physiques sont mod\'elis\'es afin d'en mieux conna\^itre les comportements ou de pouvoir les pr\'evoir. Cependant pour repr\'esenter la r\'ealit\'e, de nombreux processus doivent \^etre simplifi\'es, car ils sont souvent trop complexes, ou apparaissent \`a une \'echelle bien inf\'erieure \`a celle de l'\'etude du ph\'enom\`ene. Au lieu de compl\'etement les omettre, les effets de ces processus sont souventretranscrits dans les mod\`eles \`a l'aide de param\'etrisations, c'est-\`a-dire en introduisant des termes les quantifiant, et qui doivent \^etre ensuite estim\'ees. Les m\'ethodes classiques d'estimation se basent sur la d\'efinition d'une fonction objectif qui mesure l'\'ecart entre le mod\`ele num\'erique et la r\'ealit\'e, qui est ensuite optimis\'ee.Cependant, au del\`a de l'incertitude sur la valeur du param\`etre \`a estimer, un autre type d'incertitude peut aussi \^etre pr\'esent. Cela permet de repr\'esenter la variabilit\'e intrins\`eque de certains processusexternes, qui vont avoir un effet sur la mod\'elisation. Ces variables vont \^etre qualifi\'ees d'environnementales. En les les mod\'elisant \`a l'aide d'une variable al\'eatoire, la fonction objectif devient \`a son tour une variable al\'eatoire, que l'on va chercher \`a minimiser dans un certain sens. Si on omet ce caract\`ere al\'eatoire, on peut se retrouver avec un param\`etre optimal uniquement pour la valeur nominale du param\`etre environnemental, et le mod\`ele peut s'\'eloigner de la r\'ealit\'e pour d'autres r\'ealisations. Ce probl\`eme d'optimisation sous incertitudes est souvent abord\'e en optimisant les premiers moments de la variable al\'eatoire, l'esp\'erance en particulier.Dans cette th\`ese, nous nous int\'eressons plut\^ot \`a la notion de regret, qui mesure l'\'ecart entre la fonction objectif et la valeur optimale qu'elle peut atteindre, pour la r\'ealisation de la variable environnementale donn\'ee. Cette id\'ee de regret (additif ou bien relatif) nous permet de proposer une notion de robustesse \`a travers l'\'etude de sa probabilit\'e de d\'epasser un certain seuil, ou inversement \`a travers le calcul de ses quantiles. \`A l'aide de ce seuil, ou de l'ordre du quantile choisi, on peut donc d\'efinir une familled'estimateurs bas\'es sur le regret.N\'eanmoins, le calcul du regret, et donc des quantit\'es d\'eriv\'ees peut vite devenir tr\`es co\^uteux, car il n\'ecessite une optimisation par rapport au param\`etre de contr\^ole. Nous proposons donc d'utiliser desprocessus Gaussiens (GP) afin de construire un mod\`ele de substitution, et donc de r\'eduire cette contrainte en pratique. Nous proposons aussi des m\'ethodes it\'eratives bas\'ees notamment sur la strat\'egie SUR (Stepwise Uncertainty Reduction, R\'eduction d'incertitudes s\'equentielle): le point \`a \'evaluer ensuite est choisi selon un crit\`ere permettant d'am\'eliorer au mieux des quantit\'es associ\'ees auregret-relatif.Enfin, nous appliquons les outils pr\'esent\'es dans cette th\`ese \`a un probl\`eme acad\'emique d'estimation de param\`etre. Nous \'etudions ainsi la calibration sous incertitudes du param\`etre de friction de fond d'unmod\`ele oc\'eanique, repr\'esentant la fa\c{c}ade atlantique des c\^otes fran\c{c}aises, ainsi que la Manche dans un cadre d'exp\'eriences jumelles.},
  collaborator = {Vidard, Arthur and Debreu, Laurent and Arnaud, Elise},
  copyright = {Licence Etalab},
  school = {Universit\'e Grenoble Alpes},
  keywords = {510,Calibration robuste,Circulation ocÃ©anique,Gaussian Processes,Incertitude de mesure,ModÃ©lisation de l'ocÃ©an,ModÃ©lisation des donnÃ©es (informatique),Ocean modelling,Optimisation sous incertitudes,Optimisation under Uncertainties,Processus gaussiens,Processus Gaussiens,Regret,Robust Calibration}
}

@misc{trappler_regret-based_2022,
  title = {Regret-Based Calibration Using {{GPs}}},
  author = {Trappler, Victor and Arnaud, {\'E}lise and Debreu, Laurent and Vidard, Arthur},
  year = {2022},
  month = may,
  abstract = {How to calibrate a numerical model so that it performs reasonably well for different random operating conditions ? Objectives:  {$\RHD$} Define the notion of regret in a calibration context   {$\RHD$} Develop efficient methods and algorithms in order to estimate those parameters},
  annotation = {Published: Journ\'ees CIROQUO 2022}
}

@article{trappler_robust_2020,
  title = {Robust Calibration of Numerical Models Based on Relative Regret},
  author = {Trappler, Victor and Arnaud, {\'E}lise and Vidard, Arthur and Debreu, Laurent},
  year = {2020},
  month = nov,
  journal = {Journal of Computational Physics},
  pages = {109952},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2020.109952},
  abstract = {Classical methods of calibration usually imply the minimisation of an objective function with respect to some control parameters. This function measures the error between some observations and the results obtained by a numerical model. In the presence of uncontrollable additional parameters that we model as random inputs, the objective function becomes a random variable, and notions of robustness have to be introduced for such an optimisation problem. In this paper, we are going to present how to take into account those uncertainties by defining the relative-regret. This quantity allow us to compare the value of the objective function to its best performance achievable given a realisation of the random additional parameters. By controlling this relative-regret using a probabilistic constraint, we can then define a new family of estimators, whose robustness with respect to the random inputs can be adjusted.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Calibration,Relative-regret,Robust optimisation,Shallow-water equations},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\36939ZG3\\S0021999120307269.html}
}

@article{trevisan_four-dimensional_2009,
  title = {Four-Dimensional Variational Assimilation in the Unstable Subspace ({{4DVar-AUS}}) and the Optimal Subspace Dimension},
  author = {Trevisan, Anna and D'Isidoro, Massimo and Talagrand, Olivier},
  year = {2009},
  month = nov,
  journal = {arXiv:0902.2714 [nlin]},
  eprint = {0902.2714},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  abstract = {A key a priori information used in 4DVar is the knowledge of the system's evolution equations. In this paper we propose a method for taking full advantage of the knowledge of the system's dynamical instabilities in order to improve the quality of the analysis. We present an algorithm, four-dimensional variational assimilation in the unstable subspace (4DVar-AUS), that consists in confining in this subspace the increment of the control variable. The existence of an optimal subspace dimension for this confinement is hypothesized. Theoretical arguments in favor of the present approach are supported by numerical experiments in a simple perfect non-linear model scenario. It is found that the RMS analysis error is a function of the dimension N of the subspace where the analysis is confined and is minimum for N approximately equal to the dimension of the unstable and neutral manifold. For all assimilation windows, from 1 to 5 days, 4DVar-AUS performs better than standard 4DVar. In the presence of observational noise, the 4DVar solution, while being closer to the observations, if farther away from the truth. The implementation of 4DVar-AUS does not require the adjoint integration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NC2GDKB2\\Trevisan et al. - 2009 - Four-dimensional variational assimilation in the u.pdf}
}

@article{trossman_impact_2013,
  title = {Impact of Parameterized Lee Wave Drag on the Energy Budget of an Eddying Global Ocean Model},
  author = {Trossman, David S. and Arbic, Brian K. and Garner, Stephen T. and Goff, John A. and Jayne, Steven R. and Metzger, E. Joseph and Wallcraft, Alan J.},
  year = {2013},
  journal = {Ocean Modelling},
  volume = {72},
  pages = {119--142},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5DLGJ7RB\\Trossman et al. - 2013 - Impact of parameterized lee wave drag on the energ.pdf;C\:\\Users\\a846735\\Zotero\\storage\\Q2GEN43H\\S1463500313001595.html}
}

@article{tshimanga_class_2007,
  title = {On a Class of Limited Memory Preconditioners for Large-Scale Nonlinear Least-Squares Problems (with Application to Variational Ocean Data Assimilation)},
  author = {Tshimanga, Jean},
  year = {2007},
  publisher = {{Unpublished}},
  doi = {10.13140/RG.2.1.2140.9768},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZDEPZWJT\\TSHIMANGA - 2007 - On a class of limited memory preconditioners for l.pdf}
}

@article{tshimanga_limited-memory_2008,
  title = {Limited-Memory Preconditioners, with Application to Incremental Four-Dimensional Variational Data Assimilation},
  author = {Tshimanga, J. and Gratton, S. and Weaver, A. T. and Sartenaer, A.},
  year = {2008},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {134},
  number = {632},
  pages = {751--769},
  issn = {1477-870X},
  doi = {10.1002/qj.228},
  abstract = {Incremental four-dimensional variational assimilation (4D-Var) is an algorithm that approximately solves a nonlinear minimization problem by solving a sequence of linearized (quadratic) minimization problems of the form \$\$\textbackslash min\_\textbackslash bf x F[\textbackslash bf x] = \^1\o ver2\textbackslash bf x\^\textbackslash rm T \textbackslash bf A \textbackslash bf x - \textbackslash bf b\^\textbackslash rm T \textbackslash bf x + c,\$\$ where x is the control vector, A is a symmetric positive-definite matrix, b is a vector containing data and prior information, and c is a constant. This paper proposes a family of limited-memory preconditioners (LMPs) for accelerating the convergence of conjugate-gradient (CG) methods used to solve quadratic minimization problems such as those encountered in incremental 4D-Var. The family is constructed from a set of vectors si : i = 1, \ldots, l, where each si is assumed to be conjugate with respect to the (Hessian) matrix A. In incremental 4D-Var, approximate LMPs from this family can be built using conjugate vectors generated during the CG minimization on previous outer iterations. The spectral and quasi-Newton LMPs employed in many operational 4D-Var systems are shown to be special cases of the family of LMPs proposed here. In addition, a new LMP based on Ritz vectors (approximate eigenvectors) is derived. The Ritz LMP can be interpreted as a stabilized version of the spectral LMP. Numerical experiments performed with a realistic global ocean 4D-Var system are presented, to test the impact of the three different preconditioners. The Ritz LMP is shown to be more effective than, or at least as effective as, the spectral and quasi-Newton LMPs in our 4D-Var experiments. Our experiments also demonstrate the importance of limiting the number of CG (inner) iterations on certain outer iterations to avoid possible divergence of the cost function on the outer loop. The optimal number of CG iterations will depend on the specific preconditioner used, and can be computed a priori, albeit at the expense of several evaluations of the cost function on the outer loop. In a cycled 4D-Var system, it may be necessary to perform this computation periodically to account for changes in the Hessian matrix arising from changes in the observing system and background-flow field. Copyright \textcopyright{} 2008 Royal Meteorological Society},
  langid = {english},
  keywords = {4D-Var,conjugate gradient,minimization,preconditioning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.228},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\W7XQNSWE\\qj.html}
}

@article{ulaganathan_high_2016,
  title = {High Dimensional {{Kriging}} Metamodelling Utilising Gradient Information},
  author = {Ulaganathan, Selvakumar and Couckuyt, Ivo and Dhaene, Tom and Degroote, Joris and Laermans, Eric},
  year = {2016},
  journal = {Applied Mathematical Modelling},
  volume = {40},
  number = {9},
  pages = {5256--5270},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z94MSB8G\\8159395.pdf}
}

@article{uryasev_var_nodate,
  title = {{{VaR}} vs {{CVaR}} in {{Risk Management}} and {{Optimization}}},
  author = {Uryasev, Stan},
  journal = {Risk Management},
  pages = {75},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3VFL725Y\\Uryasev - VaR vs CVaR in Risk Management and Optimization.pdf}
}

@article{valpine_monte_2004,
  title = {Monte {{Carlo State-Space Likelihoods}} by {{Weighted Posterior Lernel Density Estimation}}},
  author = {Valpine, Perry De},
  year = {2004},
  journal = {Journal of the American Statistical Association},
  pages = {523--536},
  abstract = {Maximum likelihood estimation and likelihood ratio tests for nonlinear, non-Gaussian state-space models require numerical integration for likelihood calculations. Several methods, including Monte Carlo (MC) expectation maximization, MC likelihood ratios, direct MC integra-tion, and particle  lter likelihoods, are inef  cient for the motivating problem of stage-structured population dynamics models in experimen-tal settings. An MC kernel likelihood (MCKL) method is presented that estimates classical likelihoods up to a constant by weighted kernel density estimates of Bayesian posteriors. MCKL is derived by using Bayesian posteriors as importance sampling densities for unnormalized kernel smoothing integrals. MC error and mode bias due to kernel smoothing are discussed and two methods for reducing mode bias are proposed: ``zooming in '' on the maximum likelihood parameters using a focused prior based on an initial estimate and using a posterior cumulant-based approximation of mode bias. A simulated example shows that MCKL can be much more ef  cient than previous approaches for the population dynamics problem. The zooming-in and cumulant-based corrections are illustrated with a multivariate variance estimation problem for which accurate results are obtained even in 20 parameter dimensions.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\258GQKR7\\Valpine - 2004 - Monte Carlo State-Space Likelihoods by Weighted Po.pdf;C\:\\Users\\a846735\\Zotero\\storage\\8UFFC2QL\\summary.html}
}

@article{van_barel_robust_2017,
  title = {Robust {{Optimization}} of {{PDEs}} with {{Random Coefficients Using}} a {{Multilevel Monte Carlo Method}}},
  author = {Van Barel, Andreas and Vandewalle, Stefan},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.02574 [cs, math]},
  eprint = {1711.02574},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {This paper addresses optimization problems constrained by partial differential equations with uncertain coefficients. In particular, the robust control problem and the average control problem are considered for a tracking type cost functional with an additional penalty on the variance of the state. The expressions for the gradient and Hessian corresponding to either problem contain expected value operators. Due to the large number of uncertainties considered in our model, we suggest to evaluate these expectations using a multilevel Monte Carlo (MLMC) method. Under mild assumptions, it is shown that this results in the gradient and Hessian corresponding to the MLMC estimator of the original cost functional. Furthermore, we show that the use of certain correlated samples yields a reduction in the total number of samples required. Two optimization methods are investigated: the nonlinear conjugate gradient method and the Newton method. For both, a specific algorithm is provided that dynamically decides which and how many samples should be taken in each iteration. The cost of the optimization up to some specified tolerance \$\textbackslash tau\$ is shown to be proportional to the cost of a gradient evaluation with requested root mean square error \$\textbackslash tau\$. The algorithms are tested on a model elliptic diffusion problem with lognormal diffusion coefficient. An additional nonlinear term is also considered.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Optimization and Control,Mathematics - Probability},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\ZHZ5Z22V\\Van Barel et Vandewalle - 2017 - Robust Optimization of PDEs with Random Coefficien.pdf;C\:\\Users\\a846735\\Zotero\\storage\\BDBZFZNI\\1711.html}
}

@inproceedings{van_der_merwe_square-root_2001,
  title = {The Square-Root Unscented {{Kalman}} Filter for State and Parameter-Estimation},
  booktitle = {2001 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37221}})},
  author = {{Van der Merwe}, R. and Wan, E.A.},
  year = {2001},
  month = may,
  volume = {6},
  pages = {3461-3464 vol.6},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2001.940586},
  abstract = {Over the last 20-30 years, the extended Kalman filter (EKF) has become the algorithm of choice in numerous nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system as well estimating parameters for nonlinear system identification (eg, learning the weights of a neural network). The EKF applies the standard linear Kalman filter methodology to a linearization of the true nonlinear system. This approach is sub-optimal, and can easily lead to divergence. Julier et al. (1997), proposed the unscented Kalman filter (UKF) as a derivative-free alternative to the extended Kalman filter in the framework of state estimation. This was extended to parameter estimation by Wan and Van der Merwe et al., (2000). The UKF consistently outperforms the EKF in terms of prediction and estimation error, at an equal computational complexity of (OL/sup 3/)/sup l/ for general state-space problems. When the EKF is applied to parameter estimation, the special form of the state-space equations allows for an O(L/sup 2/) implementation. This paper introduces the square-root unscented Kalman filter (SR-UKF) which is also O(L/sup 3/) for general state estimation and O(L/sup 2/) for parameter estimation (note the original formulation of the UKF for parameter-estimation was O(L/sup 3/)). In addition, the square-root forms have the added benefit of numerical stability and guaranteed positive semi-definiteness of the state covariances.},
  keywords = {Computational complexity,Equations,Estimation error,Machine learning,Machine learning algorithms,Neural networks,Nonlinear dynamical systems,Nonlinear systems,Parameter estimation,State estimation},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EG7EAUT9\\940586.html}
}

@article{van_parys_data_2017,
  title = {From {{Data}} to {{Decisions}}: {{Distributionally Robust Optimization}} Is {{Optimal}}},
  shorttitle = {From {{Data}} to {{Decisions}}},
  author = {Van Parys, Bart PG and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
  year = {2017},
  journal = {arXiv preprint arXiv:1704.04118},
  eprint = {1704.04118},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\KMNWRXK6\\5961.pdf}
}

@article{vapnik_overview_1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, Vladimir N.},
  year = {1999},
  journal = {IEEE transactions on neural networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\G66K28RE\\slt.pdf}
}

@inproceedings{vapnik_principles_1992,
  title = {Principles of Risk Minimization for Learning Theory},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vapnik, Vladimir},
  year = {1992},
  pages = {831--838},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9QIH4GBC\\Vapnik - 1992 - Principles of risk minimization for learning theor.pdf}
}

@article{vazquez_3._nodate,
  title = {3. {{Some}} Tools for the Analysis of Sequential Strategies Based on a {{Gaussian}} Process Prior},
  author = {Vazquez, E},
  pages = {14},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FSRSMT7Y\\Vazquez - 3. Some tools for the analysis of sequential strat.pdf}
}

@article{vazquez_convergence_2010,
  title = {Convergence Properties of the Expected Improvement Algorithm with Fixed Mean and Covariance Functions},
  author = {Vazquez, Emmanuel and Bect, Julien},
  year = {2010},
  month = nov,
  journal = {Journal of Statistical Planning and Inference},
  volume = {140},
  number = {11},
  pages = {3088--3095},
  issn = {03783758},
  doi = {10.1016/j.jspi.2010.04.018},
  abstract = {This paper deals with the convergence of the expected improvement algorithm, a popular global optimization algorithm based on a Gaussian process model of the function to be optimized. The first result is that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k. The second result states that the density property also holds for P-almost all continuous functions, where P is the (prior) probability distribution induced by the Gaussian process.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HYPX6PGX\\Vazquez et Bect - 2010 - Convergence properties of the expected improvement.pdf}
}

@inproceedings{vazquez_estimating_2005,
  title = {Estimating Derivatives and Integrals with {{Kriging}}},
  author = {Vazquez, E. and Walter, E.},
  year = {2005},
  pages = {8156--8161},
  publisher = {{IEEE}},
  doi = {10.1109/CDC.2005.1583482},
  abstract = {This paper formalizes a methodology based on Kriging, a technique developped by geostatisticians, for estimating derivatives and integrals of signals that are only known via possibly irregularly spaced and noisy observations. This finds direct applications, e.g., in system identification when differential algebra is used to express parameters as nonlinear functions of the inputs and outputs and their derivatives. The procedure is quite simple to implement, and allows confidence intervals on the predicted values to be derived.},
  isbn = {978-0-7803-9567-1},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\4BSCESRW\\Vazquez et Walter - 2005 - Estimating derivatives and integrals with Kriging.pdf}
}

@article{vazquez_new_2014,
  title = {A New Integral Loss Function for {{Bayesian}} Optimization},
  author = {Vazquez, Emmanuel and Bect, Julien},
  year = {2014},
  month = aug,
  journal = {arXiv:1408.4622 [cs, math, stat]},
  eprint = {1408.4622},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We consider the problem of maximizing a real-valued continuous function f using a Bayesian approach. Since the early work of Jonas Mockus and Antanas Z\textasciicaron{} ilinskas in the 70's, the problem of optimization is usually formulated by considering the loss function max f - Mn (where Mn denotes the best function value observed after n evaluations of f ). This loss function puts emphasis on the value of the maximum, at the expense of the location of the maximizer. In the special case of a one-step Bayes-optimal strategy, it leads to the classical Expected Improvement (EI) sampling criterion. This is a special case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk associated to a certain uncertainty measure (here, the expected loss) on the quantity of interest is minimized at each step of the algorithm. In this article, assuming that f is defined over a measure space (X, {$\lambda$}), we propose to consider instead the integral loss function X( f - Mn)+ d{$\lambda$}, and we show that this leads, in the case of a Gaussian process prior, to a new numerically tractable sampling criterion that we call EI2 (for Expected Integrated Expected Improvement). A numerical experiment illustrates that a SUR strategy based on this new sampling criterion reduces the error on both the value and the location of the maximizer faster than the EI-based strategy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TJXDPGCX\\Vazquez et Bect - 2014 - A new integral loss function for Bayesian optimiza.pdf}
}

@article{verdu_minimax_1984,
  title = {On Minimax Robustness: {{A}} General Approach and Applications},
  shorttitle = {On Minimax Robustness},
  author = {Verdu, Sergio and Poor, H.},
  year = {1984},
  journal = {IEEE Transactions on Information Theory},
  volume = {30},
  number = {2},
  pages = {328--340},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\X55UAWLR\\Verdu et Poor - 1984 - On minimax robustness A general approach and appl.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XCNS6EWW\\1056876.html}
}

@article{vernon_bayesian_2017,
  title = {A {{Bayesian}} Computer Model Analysis of {{Robust Bayesian}} Analyses},
  author = {Vernon, Ian and Gosling, John Paul},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.01234 [stat]},
  eprint = {1703.01234},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\MUPGW78H\\Vernon et Gosling - 2017 - A Bayesian computer model analysis of Robust Bayes.pdf}
}

@article{vidard_variational_2004,
  title = {Variational Data Analysis with Control of the Forecast Bias},
  author = {Vidard, P.A. and Piacentini, A. and Le Dimet, F. -X.},
  year = {2004},
  month = jan,
  journal = {Tellus A: Dynamic Meteorology and Oceanography},
  volume = {56},
  number = {3},
  pages = {177--188},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.3402/tellusa.v56i3.14414},
  abstract = {We propose a methodology for the treatment of the systematic model error in variational data assimilation. The principle of the method is to add a systematic error correction term in the model equations and to include it in the variational assimilation control vector.This method is applied to a simplified ocean circulation model in an identical twin experiment framework. It shows a noticeable improvement compared to the result of a classical variational assimilation scheme in which the systematic error is not corrected. The estimated systematic error correction term is sufficiently consistent with that needed by the model that it allows improvements not just to the analysis, but also during the forecast phase.},
  annotation = {\_eprint: https://doi.org/10.3402/tellusa.v56i3.14414},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\58MU2TF5\\Vidard et al. - 2004 - Variational data analysis with control of the fore.pdf;C\:\\Users\\a846735\\Zotero\\storage\\XSMHILMU\\tellusa.v56i3.html}
}

@article{villemonteix_informational_2006,
  title = {An Informational Approach to the Global Optimization of Expensive-to-Evaluate Functions},
  author = {Villemonteix, Julien and Vazquez, Emmanuel and Walter, Eric},
  year = {2006},
  month = nov,
  journal = {arXiv:cs/0611143},
  eprint = {cs/0611143},
  eprinttype = {arxiv},
  abstract = {In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizer entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on stepwise uncertainty reduction, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the Efficient Global Optimization (EGO) algorithm. An empirical comparison is carried out between our criterion and expected improvement, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization) is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Numerical Analysis,G.1.1,G.1.6},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\A85EICRS\\Villemonteix et al. - 2006 - An informational approach to the global optimizati.pdf}
}

@phdthesis{villemonteix_optimisation_2008,
  title = {{Optimisation de fonctions co\^uteuses{$<$}br /{$>$}Mod\`eles gaussiens pour une utilisation efficace du budget d'\'evaluations : th\'eorie et pratique industrielle}},
  shorttitle = {{Optimisation de fonctions co\^uteuses{$<$}br /{$>$}Mod\`eles gaussiens pour une utilisation efficace du budget d'\'evaluations}},
  author = {Villemonteix, Julien},
  year = {2008},
  month = dec,
  abstract = {Cette th\`ese traite d'une question centrale dans de nombreux probl\`emes d'optimisation, en particulier{$<$}br /{$>$}en ing\'enierie. Comment optimiser une fonction lorsque le nombre d'\'evaluations autoris\'e est tr\`es limit\'e au regard de la dimension et de la complexit\'e du probl\`eme ? Par exemple, lorsque le budget d'\'evaluations est limit\'e par la dur\'ee des simulations num\'eriques du syst\`eme \`a optimiser, il n'est pas rare de devoir optimiser trente param\`etres avec moins{$<$}br /{$>$}de cent \'evaluations. Ce travail traite d'algorithmes d'optimisation sp\'ecifiques \`a ce contexte pour lequel la plupart des m\'ethodes classiques sont inadapt\'ees.{$<$}br /{$>$}Le principe commun aux m\'ethodes propos\'ees est d'exploiter les propri\'et\'es des processus gaussiens et du krigeage pour construire une approximation peu co\^uteuse de la fonction \`a optimiser. Cette approximation est ensuite utilis\'ee pour choisir it\'erativement les \'evaluations \`a r\'ealiser. Ce choix est dict\'e par un crit\`ere d'\'echantillonnage qui combine recherche locale, \`a proximit\'e des r\'esultats prometteurs, et recherche globale, dans les zones non explor\'ees. La plupart des crit\`eres propos\'es dans la litt\'erature, tel celui de l'algorithme EGO (pour Efficient Global Optimization), cherchent \`a \'echantillonner la fonction l\`a o\`u l'apparition d'un optimum est jug\'ee la plus probable. En comparaison, l'algorithme IAGO (pour Informational Approach to Global Optimization), principale contribution de nos travaux, cherche \`a maximiser la quantit\'e d'information apport\'ee, sur la position de l'optimum, par l'\'evaluation r\'ealis\'ee. Des probl\'ematiques industrielles ont guid\'e l'organisation de ce m\'emoire, qui se destine \`a la communaut\'e de l'optimisation{$<$}br /{$>$}tout comme aux praticiens confront\'es \`a des fonctions \`a l'\'evaluation co\^uteuse. Aussi les applications industrielles y tiennent-elles une place importante tout comme la mise en place de l'algorithme IAGO. Nous d\'etaillons non seulement le cas standard de l'optimisation d'une fonction r\'eelle, mais aussi la prise en compte de contraintes, de{$<$}br /{$>$}bruit sur les r\'esultats des \'evaluations, de r\'esultats d'\'evaluation du gradient, de probl\`emes multi-objectifs, ou encore d'incertitudes de fabrication significatives.},
  langid = {french},
  school = {Universit\'e Paris Sud - Paris XI},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TLCKEE4N\\Villemonteix - 2008 - Optimisation de fonctions coÃ»teusesbr ModÃ¨les g.pdf;C\:\\Users\\a846735\\Zotero\\storage\\9LIXMHQU\\tel-00351406.html}
}

@article{volkwein_model_2011,
  title = {Model Reduction Using Proper Orthogonal Decomposition},
  author = {Volkwein, Stefan},
  year = {2011},
  journal = {Lecture Notes, Institute of Mathematics and Scientific Computing, University of Graz. see http://www. uni-graz. at/imawww/volkwein/POD. pdf},
  keywords = {MOR methods,POD},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PXXNM5ZI\\POD-Vorlesung.pdf}
}

@article{volkwein_proper_2005,
  title = {Proper Orthogonal Decomposition ({{POD}}) for Nonlinear Dynamical Systems},
  author = {Volkwein, Stefan},
  year = {2005},
  journal = {Dutch Institute of Systems and Control Summerschool},
  keywords = {MOR methods,POD},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\WA26LTST\\Volkwein-1.pdf}
}

@article{vorobyev_new_2003,
  title = {On the {{New Notion}} of the {{Set-Expectation}} for a {{Random Set}} of {{Events}}},
  author = {Vorobyev, Oleg and Vorobyev, Alexey},
  year = {2003},
  month = jan,
  journal = {University Library of Munich, Germany, MPRA Paper},
  abstract = {The paper introduces new notion for the set-valued mean set of a random set. The means are defined as families of sets that minimize mean distances to the random set. The distances are determined by metrics in spaces of sets or by suitable generalizations. Some examples illustrate the use of the new definitions.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\L6KWNU27\\Vorobyev et Vorobyev - 2003 - On the New Notion of the Set-Expectation for a Ran.pdf}
}

@article{vuong_likelihood_1989,
  title = {Likelihood {{Ratio Tests}} for {{Model Selection}} and {{Non-Nested Hypotheses}}},
  author = {Vuong, Quang H.},
  year = {1989},
  month = mar,
  journal = {Econometrica},
  volume = {57},
  number = {2},
  pages = {307},
  issn = {00129682},
  doi = {10.2307/1912557},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TX83J45K\\Vuong - 1989 - Likelihood Ratio Tests for Model Selection and Non.pdf}
}

@article{wald_statistical_1945,
  title = {Statistical {{Decision Functions Which Minimize}} the {{Maximum Risk}}},
  author = {Wald, Abraham},
  year = {1945},
  journal = {Annals of Mathematics},
  volume = {46},
  number = {2},
  pages = {265--280},
  issn = {0003-486X},
  doi = {10.2307/1969022}
}

@article{wald_statistical_1945-1,
  title = {Statistical {{Decision Functions Which Minimize}} the {{Maximum Risk}}},
  author = {Wald, Abraham},
  year = {1945},
  month = apr,
  journal = {The Annals of Mathematics},
  volume = {46},
  number = {2},
  pages = {265},
  issn = {0003486X},
  doi = {10.2307/1969022},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FSYG3D76\\Wald - 1945 - Statistical Decision Functions Which Minimize the .pdf}
}

@article{walker_defining_2003,
  title = {Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support},
  shorttitle = {Defining Uncertainty},
  author = {Walker, Warren E. and Harremo{\"e}s, Poul and Rotmans, Jan and {van der Sluijs}, Jeroen P. and {van Asselt}, Marjolein BA and Janssen, Peter and {Krayer von Krauss}, Martin P.},
  year = {2003},
  journal = {Integrated assessment},
  volume = {4},
  number = {1},
  pages = {5--17},
  keywords = {Uncertainty analysis,UQ},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\E6Q2GRNH\\79.pdf}
}

@article{wang_asymptotic_1999,
  title = {Asymptotic {{Properties}} of {{M-estimators Based}} on {{Estimating Equations}} and {{Censored Data}}},
  author = {Wang, Jane-Ling},
  year = {1999},
  journal = {Scandinavian journal of statistics},
  volume = {26},
  number = {2},
  pages = {297--318},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7KCAXALE\\Wang - 1999 - Asymptotic Properties of M-estimators Based on Est.pdf;C\:\\Users\\a846735\\Zotero\\storage\\HT2CNHF4\\Wang - 1999 - Asymptotic Properties of M-estimators Based on Est}
}

@article{wang_evaluation_2014,
  title = {An Evaluation of Adaptive Surrogate Modeling Based Optimization with Two Benchmark Problems},
  author = {Wang, Chen and Duan, Qingyun and Gong, Wei and Ye, Aizhong and Di, Zhenhua and Miao, Chiyuan},
  year = {2014},
  month = oct,
  journal = {Environmental Modelling \& Software},
  volume = {60},
  pages = {167--179},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2014.05.026},
  abstract = {Surrogate modeling uses cheap ``surrogates'' to represent the response surface of simulation models. It involves several steps, including initial sampling, regression and adaptive sampling. This study evaluates an adaptive surrogate modeling based optimization (ASMO) method on two benchmark problems: the Hartman function and calibration of the SAC-SMA hydrologic model. Our results show that: 1) Gaussian Processes are the best surrogate model construction method. A minimum Interpolation Surface method is the best adaptive sampling method. Low discrepancy Quasi Monte Carlo methods are the most suitable initial sampling designs. Some 15\textendash 20 times the dimension of the problem may be the proper initial sample size; 2) The ASMO method is much more efficient than the widely used Shuffled Complex Evolution global optimization method. However, ASMO can provide only approximate optimal solutions, whose precision is limited by surrogate modeling methods and problem-specific features; and 3) The identifiability of model parameters is correlated with parameter sensitivity.},
  keywords = {Adaptive sampling,Adaptive surrogate modeling based optimization,Computationally intensive computer models,Design of experiment,Global sensitivity analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\I4NUQW6Q\\Wang et al. - 2014 - An evaluation of adaptive surrogate modeling based.pdf;C\:\\Users\\a846735\\Zotero\\storage\\DB6ATLC3\\S1364815214001698.html}
}

@article{wang_max-value_2017,
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.01968 [stat]},
  eprint = {1703.01968},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the \$\textbackslash arg\textbackslash max\$ of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\NMAKJPDF\\Wang et Jegelka - 2017 - Max-value Entropy Search for Efficient Bayesian Op.pdf;C\:\\Users\\a846735\\Zotero\\storage\\TU5YA9D2\\1703.html}
}

@inproceedings{wang_new_2017,
  title = {A New Acquisition Function for {{Bayesian}} Optimization Based on the Moment-Generating Function},
  author = {Wang, Hao and {van Stein}, Bas and Emmerich, Michael and Back, Thomas},
  year = {2017},
  month = oct,
  pages = {507--512},
  publisher = {{IEEE}},
  doi = {10.1109/SMC.2017.8122656},
  abstract = {Bayesian Optimization or Efficient Global Optimization (EGO) is a global search strategy that is designed for expensive black-box functions. In this algorithm, a statistical model (usually the Gaussian process model) is constructed on some initial data samples. The global optimum is approached by iteratively maximizing a so-called acquisition function, that balances the exploration and exploitation effect of the search. The performance of such an algorithm is largely affected by the choice of the acquisition function. Inspired by the usage of higher moments from the Gaussian process model, it is proposed to construct a novel acquisition function based on the moment-generating function (MGF) of the improvement, which is the stochastic gain over the current best fitness value by sampling at an unknown point. This MGF-based acquisition function takes all the higher moments into account and introduces an additional real-valued parameter to control the trade-off between exploration and exploitation. The motivation, rationale and closed-form expression of the proposed function are discussed in detail. In addition, we also illustrate its advantage over other acquisition functions, especially the so-called generalized expected improvement.},
  isbn = {978-1-5386-1645-1},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H8NR48J2\\Wang et al. - 2017 - A new acquisition function for Bayesian optimizati.pdf}
}

@article{wang_optimization_2018,
  title = {Optimization of {{Smooth Functions}} with {{Noisy Observations}}: {{Local Minimax Rates}}},
  shorttitle = {Optimization of {{Smooth Functions}} with {{Noisy Observations}}},
  author = {Wang, Yining and Balakrishnan, Sivaraman and Singh, Aarti},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.08586 [cs, math, stat]},
  eprint = {1803.08586},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We consider the problem of global optimization of an unknown non-convex smooth function with zeroth-order feedback. In this setup, an algorithm is allowed to adaptively query the underlying function at different locations and receives noisy evaluations of function values at the queried points (i.e. the algorithm has access to zeroth-order information). Optimization performance is evaluated by the expected difference of function values at the estimated optimum and the true optimum. In contrast to the classical optimization setup, first-order information like gradients are not directly accessible to the optimization algorithm. We show that the classical minimax framework of analysis, which roughly characterizes the worst-case query complexity of an optimization algorithm in this setting, leads to excessively pessimistic results. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations, which provides a refined picture of the intrinsic difficulty of zeroth-order optimization. We show that for functions with fast level set growth around the global minimum, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems [1, 22]. At the other end of the spectrum, for worst-case smooth functions no algorithm can converge faster than the minimax rate of estimating the entire unknown function in the {$\mathscr{l}$}8-norm. We provide an intuitive and efficient algorithm that attains the derived upper error bounds. Finally, using the local minimax framework we are able to clearly dichotomize adaptive and non-adaptive algorithms by showing that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EBLYPEB9\\Wang et al. - 2018 - Optimization of Smooth Functions with Noisy Observ.pdf}
}

@article{wang_recurrent_nodate,
  title = {Recurrent {{Neural Networks}} for {{Solving Linear Matrix Equations}}},
  author = {Wang, J},
  pages = {12},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9ETL6MYE\\Wang - Recurrent Neural Networks for Solving Linear Matri.pdf}
}

@book{wasserman_all_2006,
  title = {All of Nonparametric Statistics},
  author = {Wasserman, Larry},
  year = {2006},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-25145-5},
  langid = {english},
  lccn = {QA278.8 .W37 2006},
  keywords = {Nonparametric statistics},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HHDBZCPS\\Wasserman - 2006 - All of nonparametric statistics.pdf}
}

@article{weaver_multivariate_2005,
  title = {A Multivariate Balance Operator for Variational Ocean Data Assimilation},
  author = {Weaver, A. T. and Deltel, C. and Machu, E. and Ricci, S. and Daget, N.},
  year = {2005},
  month = oct,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {131},
  number = {613},
  pages = {3605--3625},
  issn = {00359009, 1477870X},
  doi = {10.1256/qj.05.119},
  abstract = {It is common in meteorological applications of variational assimilation to specify the error covariances of the model background state implicitly via a transformation from model space where variables are highly correlated to a control space where variables can be considered to be approximately uncorrelated. An important part of this transformation is a balance operator which effectively establishes the multivariate component of the error covariances. The use of this technique in ocean data assimilation is less common. This paper describes a balance operator that can be used in a variable transformation for oceanographic applications of three- and four-dimensional variational assimilation. The proposed balance operator has been implemented in an incremental variational data assimilation system for a global ocean general circulation model. Evidence that the balance operator can explain a significant percentage of background-error variance is presented. The multivariate analysis structures implied by the balance operator are illustrated using single observation experiments.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\YMINJGC6\\Weaver et al. - 2005 - A multivariate balance operator for variational oc.pdf}
}

@article{wei_inner-loop_2017,
  title = {An Inner-Loop Free Solution to Inverse Problems Using Deep Neural Networks},
  author = {Wei, Qi and Fan, Kai and Carin, Lawrence and Heller, Katherine A.},
  year = {2017},
  month = nov,
  journal = {arXiv:1709.01841 [cs]},
  eprint = {1709.01841},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., it is data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z7SJ2EGN\\Wei et al. - 2017 - An inner-loop free solution to inverse problems us.pdf;C\:\\Users\\a846735\\Zotero\\storage\\JQAR4JHZ\\1709.html}
}

@article{weijs_kullbackleibler_2010,
  title = {Kullback\textendash{{Leibler Divergence}} as a {{Forecast Skill Score}} with {{Classic Reliability}}\textendash{{Resolution}}\textendash{{Uncertainty Decomposition}}},
  author = {Weijs, Steven V. and {van Nooijen}, Ronald and {van de Giesen}, Nick},
  year = {2010},
  month = sep,
  journal = {Monthly Weather Review},
  volume = {138},
  number = {9},
  pages = {3387--3399},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/2010MWR3229.1},
  abstract = {This paper presents a score that can be used for evaluating probabilistic forecasts of multicategory events. The score is a reinterpretation of the logarithmic score or ignorance score, now formulated as the relative entropy or Kullback\textendash Leibler divergence of the forecast distribution from the observation distribution. Using the information\textendash theoretical concepts of entropy and relative entropy, a decomposition into three components is presented, analogous to the classic decomposition of the Brier score. The information\textendash theoretical twins of the components uncertainty, resolution, and reliability provide diagnostic information about the quality of forecasts. The overall score measures the information conveyed by the forecast. As was shown recently, information theory provides a sound framework for forecast verification. The new decomposition, which has proven to be very useful for the Brier score and is widely used, can help acceptance of the logarithmic score in meteorology.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\9YB972RM\\Weijs et al. - 2010 - KullbackâLeibler Divergence as a Forecast Skill Sc.pdf}
}

@article{white_maximum_1982,
  title = {Maximum {{Likelihood Estimation}} of {{Misspecified Models}}},
  author = {White, Halbert},
  year = {1982},
  journal = {Econometrica},
  volume = {50},
  number = {1},
  pages = {1--25},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1912526},
  abstract = {This paper examines the consequences and detection of model misspecification when using maximum likelihood techniques for estimation and inference. The quasi-maximum likelihood estimator (OMLE) converges to a well defined limit, and may or may not be consistent for particular parameters of interest. Standard tests (Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in the presence of misspecification, but more general statistics are given which allow inferences to be drawn robustly. The properties of the QMLE and the information matrix are exploited to yield several useful tests for model misspecification.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\2LP5W55I\\white_maximum_likekihood_estimation.pdf}
}

@article{wiener_homogeneous_1938,
  title = {The {{Homogeneous Chaos}}},
  author = {Wiener, Norbert},
  year = {1938},
  journal = {American Journal of Mathematics},
  volume = {60},
  number = {4},
  pages = {897--936},
  issn = {0002-9327},
  doi = {10.2307/2371268}
}

@book{wilke_fundamentals_nodate,
  title = {Fundamentals of {{Data Visualization}}},
  author = {Wilke, Claus O.},
  abstract = {A guide to making visualizations that accurately reflect the data, tell a story, and look professional.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DK4WPX56\\dataviz.html}
}

@article{wilks_large-sample_1938,
  title = {The {{Large-Sample Distribution}} of the {{Likelihood Ratio}} for {{Testing Composite Hypotheses}}},
  author = {Wilks, S. S.},
  year = {1938},
  month = mar,
  journal = {Annals of Mathematical Statistics},
  volume = {9},
  number = {1},
  pages = {60--62},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732360},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  zmnumber = {0018.32003},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\X9BFZ7VP\\Wilks - 1938 - The Large-Sample Distribution of the Likelihood Ra.pdf;C\:\\Users\\a846735\\Zotero\\storage\\IASAJ8YW\\1177732360.html}
}

@article{williams_sequential_2000,
  title = {Sequential Design of Computer Experiments to Minimize Integrated Response Functions},
  author = {Williams, Brian J. and Santner, Thomas J. and Notz, William I.},
  year = {2000},
  journal = {Statistica Sinica},
  pages = {1133--1152},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\RM4Z5QR3\\Williams et al. - 2000 - Sequential design of computer experiments to minim.pdf;C\:\\Users\\a846735\\Zotero\\storage\\YGB5AB9Z\\24306770.html}
}

@article{wilson_maximizing_2018,
  title = {Maximizing Acquisition Functions for {{Bayesian}} Optimization},
  author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
  year = {2018},
  month = may,
  journal = {arXiv:1805.10196 [cs, stat]},
  eprint = {1805.10196},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide the search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, highdimensional, and intractable. We present two modern approaches for maximizing acquisition functions that exploit key properties thereof, namely the differentiability of Monte Carlo integration and the submodularity of parallel querying.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\Z5DW775A\\Wilson et al. - 2018 - Maximizing acquisition functions for Bayesian opti.pdf}
}

@article{wilson_probable_1927,
  title = {Probable Inference, the Law of Succession, and Statistical Inference},
  author = {Wilson, Edwin B.},
  year = {1927},
  journal = {Journal of the American Statistical Association},
  volume = {22},
  number = {158},
  pages = {209--212},
  publisher = {{Taylor \& Francis Group}},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IPZ2L6MF\\Wilson - 1927 - Probable inference, the law of succession, and sta.pdf;C\:\\Users\\a846735\\Zotero\\storage\\I3N2CEQI\\01621459.1927.html}
}

@article{wit_all_2012,
  title = {`{{All}} Models Are Wrong...': An Introduction to Model Uncertainty: {{{\emph{Introduction}}}}{\emph{ to Model Uncertainty}}},
  shorttitle = {`{{All}} Models Are Wrong...'},
  author = {Wit, Ernst and van den Heuvel, Edwin and Romeijn, Jan-Willem},
  year = {2012},
  month = aug,
  journal = {Statistica Neerlandica},
  volume = {66},
  number = {3},
  pages = {217--236},
  issn = {00390402},
  doi = {10.1111/j.1467-9574.2012.00530.x},
  langid = {english},
  keywords = {Model inadequacy,Model selection},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\E2B7ZAJV\\2012_wit_et_al_-_all_models_are_wrong.pdf}
}

@article{wolpert_no_1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  issn = {1089778X},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between e ective optimization algorithms and the problems they are solving. A number of \textbackslash no free lunch" (NFL) theorems are presented that establish that for any algorithm, any elevated performance over one class of problems is exactly paid for in performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed are time-varying optimization problems and a priori \textbackslash head-to-head" minimax distinctions between optimization algorithms, distinctions that can obtain despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3WVWVEGC\\Wolpert et Macready - 1997 - No free lunch theorems for optimization.pdf}
}

@article{wong_frequentist_2014,
  title = {A {{Frequentist Approach}} to {{Computer Model Calibration}}},
  author = {Wong, Raymond K. W. and Storlie, Curtis B. and Lee, Thomas C. M.},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.4723 [stat]},
  eprint = {1411.4723},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {This paper considers the computer model calibration problem and provides a general frequentist solution. Under the proposed framework, the data model is semi-parametric with a nonparametric discrepancy function which accounts for any discrepancy between the physical reality and the computer model. In an attempt to solve a fundamentally important (but often ignored) identifiability issue between the computer model parameters and the discrepancy function, this paper proposes a new and identifiable parametrization of the calibration problem. It also develops a two-step procedure for estimating all the relevant quantities under the new parameterization. This estimation procedure is shown to enjoy excellent rates of convergence and can be straightforwardly implemented with existing software. For uncertainty quantification, bootstrapping is adopted to construct confidence regions for the quantities of interest. The practical performance of the proposed methodology is illustrated through simulation examples and an application to a computational fluid dynamics model.},
  archiveprefix = {arXiv},
  keywords = {Calibration,Frequentist},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H2NR9V7X\\Wong et al. - 2014 - A Frequentist Approach to Computer Model Calibrati.pdf;C\:\\Users\\a846735\\Zotero\\storage\\J3MIEC85\\1411.html}
}

@article{wong_neglecting_2018,
  title = {Neglecting Model Structural Uncertainty Underestimates Upper Tails of Flood Hazard},
  author = {Wong, Tony E. and Klufas, Alexandra and Srikrishnan, Vivek and Keller, Klaus},
  year = {2018},
  month = jul,
  journal = {Environmental Research Letters},
  volume = {13},
  number = {7},
  pages = {074019},
  publisher = {{IOP Publishing}},
  issn = {1748-9326},
  doi = {10.1088/1748-9326/aacb3d},
  abstract = {Coastal flooding drives considerable risks to many communities, but projections of future flood risks are deeply uncertain. The paucity of observations of extreme events often motivates the use of statistical approaches to model the distribution of extreme storm surge events. One key deep uncertainty that is often overlooked is model structural uncertainty. There is currently no strong consensus among experts regarding which class of statistical model to use as a `best practice'. Robust management of coastal flooding risks requires coastal managers to consider the distinct possibility of non-stationarity in storm surges. This increases the complexity of the potential models to use, which tends to increase the data required to constrain the model. Here, we use a Bayesian model averaging approach to analyze the balance between (i) model complexity sufficient to capture decision-relevant risks and (ii) data availability to constrain complex model structures. We characterize deep model structural uncertainty through a set of calibration experiments. Specifically, we calibrate a set of models ranging in complexity using long-term tide gauge observations from the Netherlands and the United States. We find that in both considered cases, roughly half of the model weight is associated with the non-stationary models. Our approach provides a formal framework to integrate information across model structures, in light of the potentially sizable modeling uncertainties. By combining information from multiple models, our inference sharpens for the projected storm surge 100 year return levels, and estimated return levels increase by several centimeters. We assess the impacts of data availability through a set of experiments with temporal subsets and model comparison metrics. Our analysis suggests that about 70 years of data are required to stabilize estimates of the 100 year return level, for the locations and methods considered here.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\UMZS493E\\Wong et al. - 2018 - Neglecting model structural uncertainty underestim.pdf}
}

@article{wu_bayesian_nodate,
  title = {Bayesian {{Optimization}} with {{Gradients}}},
  author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter},
  pages = {12},
  abstract = {Bayesian optimization has been successful at global optimization of expensiveto-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledgegradient (d-KG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. d-KG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IW7GFGC8\\Wu et al. - Bayesian Optimization with Gradients.pdf}
}

@article{wu_estimating_2005,
  title = {Estimating the {{Uncertainty}} in a {{Regional Climate Model Related}} to {{Initial}} and {{Lateral Boundary Conditions}}},
  author = {Wu, Wanli and Lynch, Amanda H. and Rivers, Aaron},
  year = {2005},
  month = apr,
  journal = {Journal of Climate},
  volume = {18},
  number = {7},
  pages = {917--933},
  publisher = {{American Meteorological Society}},
  issn = {0894-8755},
  doi = {10.1175/JCLI-3293.1},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H49MW7ND\\Wu et al. - 2005 - Estimating the Uncertainty in a Regional Climate M.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7V5Z8SAD\\Estimating-the-Uncertainty-in-a-Regional-Climate.html}
}

@article{wu_inverse_2018,
  title = {Inverse {{Uncertainty Quantification}} Using the {{Modular Bayesian Approach}} Based on {{Gaussian Process}}, {{Part}} 1: {{Theory}}},
  shorttitle = {Inverse {{Uncertainty Quantification}} Using the {{Modular Bayesian Approach}} Based on {{Gaussian Process}}, {{Part}} 1},
  author = {Wu, Xu and Kozlowski, Tomasz and Meidani, Hadi and Shirvan, Koroush},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.01782 [stat]},
  eprint = {1801.01782},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In nuclear reactor system design and safety analysis, the Best Estimate plus Uncertainty (BEPU) methodology requires that computer model output uncertainties must be quantified in order to prove that the investigated design stays within acceptance criteria. "Expert opinion" and "user self-evaluation" have been widely used to specify computer model input uncertainties in previous uncertainty, sensitivity and validation studies. Inverse Uncertainty Quantification (UQ) is the process to inversely quantify input uncertainties based on experimental data in order to more precisely quantify such ad-hoc specifications of the input uncertainty information. In this paper, we used Bayesian analysis to establish the inverse UQ formulation, with systematic and rigorously derived metamodels constructed by Gaussian Process (GP). Due to incomplete or inaccurate underlying physics, as well as numerical approximation errors, computer models always have discrepancy/bias in representing the realities, which can cause over-fitting if neglected in the inverse UQ process. The model discrepancy term is accounted for in our formulation through the "model updating equation". We provided a detailed introduction and comparison of the full and modular Bayesian approaches for inverse UQ, as well as pointed out their limitations when extrapolated to the validation/prediction domain. Finally, we proposed an improved modular Bayesian approach that can avoid extrapolating the model discrepancy that is learnt from the inverse UQ domain to the validation/prediction domain.},
  archiveprefix = {arXiv},
  keywords = {Inverse UQ,inversion modeling,Statistics - Computation,UQ},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\SSSI56TQ\\Wu et al. - 2018 - Inverse Uncertainty Quantification using the Modul.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZKX8968Q\\1801.html}
}

@article{wu_parallel_2016,
  title = {The {{Parallel Knowledge Gradient Method}} for {{Batch Bayesian Optimization}}},
  author = {Wu, Jian and Frazier, Peter I.},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.04414 [cs, stat]},
  eprint = {1606.04414},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7MDQJAV2\\Wu et Frazier - 2016 - The Parallel Knowledge Gradient Method for Batch B.pdf;C\:\\Users\\a846735\\Zotero\\storage\\4C3GXJWK\\1606.html}
}

@article{wu_practical_nodate,
  title = {Practical {{Two-Step Lookahead Bayesian Optimization}}},
  author = {Wu, Jian and Frazier, Peter},
  pages = {11},
  abstract = {Expected improvement and other acquisition functions widely used in Bayesian optimization use a ``one-step'' assumption: they value objective function evaluations assuming no future evaluations will be performed. Because we usually evaluate over multiple steps, this assumption may leave substantial room for improvement. Existing theory gives acquisition functions looking multiple steps in the future but calculating them requires solving a high-dimensional continuous-state continuousaction Markov decision process (MDP). Fast exact solutions of this MDP remain out of reach of today's methods. As a result, previous two- and multi-step lookahead Bayesian optimization algorithms are either too expensive to implement in most practical settings or resort to heuristics that may fail to fully realize the promise of two-step lookahead. This paper proposes a computationally efficient algorithm that provides an accurate solution to the two-step lookahead Bayesian optimization problem in seconds to at most several minutes of computation per batch of evaluations. The resulting acquisition function provides increased query efficiency and robustness compared with previous two- and multi-step lookahead methods in both single-threaded and batch experiments. This unlocks the value of two-step lookahead in practice. We demonstrate the value of our algorithm with extensive experiments on synthetic test functions and real-world problems.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\H9PLN5WC\\Wu et Frazier - Practical Two-Step Lookahead Bayesian Optimization.pdf}
}

@article{xiu_wiener--askey_2002,
  title = {The {{Wiener--Askey Polynomial Chaos}} for {{Stochastic Differential Equations}}},
  author = {Xiu, D. and Karniadakis, G.},
  year = {2002},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {24},
  number = {2},
  pages = {619--644},
  issn = {1064-8275},
  doi = {10.1137/S1064827501387826},
  abstract = {We present a new method for solving stochastic differential equations based on Galerkin projections and extensions of Wiener's polynomial chaos. Specifically, we represent the stochastic processes with an optimum trial basis from the Askey family of orthogonal polynomials that reduces the dimensionality of the system and leads to exponential convergence of the error. Several continuous and discrete processes are treated, and numerical examples show substantial speed-up compared to Monte Carlo simulations for low dimensional stochastic inputs.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HGFIPU5K\\S1064827501387826.html}
}

@article{xiu_wiener-askey_nodate,
  title = {{{THE WIENER-ASKEY POLYNOMIAL CHAOS FOR STOCHASTIC DIFFERENTIAL EQUATIONS}}},
  author = {Xiu, Dongbin and Karniadakis, George Em},
  pages = {26},
  abstract = {We present a new method for solving stochastic differential equations based on Galerkin projections and extensions of Wiener's polynomial chaos. Specifically, we represent the stochastic processes with an optimum trial basis from the Askey family of orthogonal polynomials that reduces the dimensionality of the system and leads to exponential convergence of the error. Several continuous and discrete processes are treated, and numerical examples show substantial speed-up compared to Monte-Carlo simulations for low dimensional stochastic inputs.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\B2ENDBXJ\\Xiu et Karniadakis - THE WIENER-ASKEY POLYNOMIAL CHAOS FOR STOCHASTIC D.pdf}
}

@article{yang_b-pinns_2021,
  title = {B-{{PINNs}}: {{Bayesian}} Physics-Informed Neural Networks for Forward and Inverse {{PDE}} Problems with Noisy Data},
  shorttitle = {B-{{PINNs}}},
  author = {Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
  year = {2021},
  month = jan,
  journal = {Journal of Computational Physics},
  volume = {425},
  pages = {109913},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2020.109913},
  abstract = {We propose a Bayesian physics-informed neural network (B-PINN) to solve both forward and inverse nonlinear problems described by partial differential equations (PDEs) and noisy data. In this Bayesian framework, the Bayesian neural network (BNN) combined with a PINN for PDEs serves as the prior while the Hamiltonian Monte Carlo (HMC) or the variational inference (VI) could serve as an estimator of the posterior. B-PINNs make use of both physical laws and scattered noisy measurements to provide predictions and quantify the aleatoric uncertainty arising from the noisy data in the Bayesian framework. Compared with PINNs, in addition to uncertainty quantification, B-PINNs obtain more accurate predictions in scenarios with large noise due to their capability of avoiding overfitting. We conduct a systematic comparison between the two different approaches for the B-PINNs posterior estimation (i.e., HMC or VI), along with dropout used for quantifying uncertainty in deep neural networks. Our experiments show that HMC is more suitable than VI with mean field Gaussian approximation for the B-PINNs posterior estimation, while dropout employed in PINNs can hardly provide accurate predictions with reasonable uncertainty. Finally, we replace the BNN in the prior with a truncated Karhunen-Lo\`eve (KL) expansion combined with HMC or a deep normalizing flow (DNF) model as posterior estimators. The KL is as accurate as BNN and much faster but this framework cannot be easily extended to high-dimensional problems unlike the BNN based framework.},
  langid = {english},
  keywords = {Bayesian physics-informed neural networks,Hamiltonian Monte Carlo,Noisy data,Nonlinear PDEs,Variational inference},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\5U6FA9R7\\Yang et al. - 2021 - B-PINNs Bayesian physics-informed neural networks.pdf;C\:\\Users\\a846735\\Zotero\\storage\\FD4QUDZE\\S0021999120306872.html}
}

@article{yaremchuk_method_2009,
  title = {A {{Method}} of {{Successive Corrections}} of the {{Control Subspace}} in the {{Reduced-Order Variational Data Assimilation}}},
  author = {Yaremchuk, Max and Nechaev, Dmitri and Panteleev, Gleb},
  year = {2009},
  month = sep,
  journal = {Monthly Weather Review},
  volume = {137},
  number = {9},
  pages = {2966--2978},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/2009MWR2592.1},
  abstract = {Abstract A version of the reduced control space four-dimensional variational method (R4DVAR) of data assimilation into numerical models is proposed. In contrast to the conventional 4DVAR schemes, the method does not require development of the tangent linear and adjoint codes for implementation. The proposed R4DVAR technique is based on minimization of the cost function in a sequence of low-dimensional subspaces of the control space. Performance of the method is demonstrated in a series of twin-data assimilation experiments into a nonlinear quasigeostrophic model utilized as a strong constraint. When the adjoint code is stable, R4DVAR's convergence rate is comparable to that of the standard 4DVAR algorithm. In the presence of strong instabilities in the direct model, R4DVAR works better than 4DVAR whose performance is deteriorated because of the breakdown of the tangent linear approximation. Comparison of the 4DVAR and R4DVAR also shows that R4DVAR becomes advantageous when observations are sparse and noisy.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\XS7BH9DQ\\Yaremchuk et al. - 2009 - A Method of Successive Corrections of the Control .pdf;C\:\\Users\\a846735\\Zotero\\storage\\BN63N78Y\\2009mwr2592.1.html}
}

@article{yizong_cheng_mean_1995,
  title = {Mean Shift, Mode Seeking, and Clustering},
  author = {{Yizong Cheng}},
  year = {Aug./1995},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {17},
  number = {8},
  pages = {790--799},
  issn = {01628828},
  doi = {10.1109/34.400568},
  abstract = {Mean shift, a simple iterative procedure that shifts each data point to the average of data points in its neighborhood, is generalized and analyzed in this paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on a surface constructed with a ``shadow'' kernel. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis is treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\A2Z9TT4S\\Yizong Cheng - 1995 - Mean shift, mode seeking, and clustering.pdf}
}

@article{yizong_cheng_mean_1995-1,
  title = {Mean Shift, Mode Seeking, and Clustering},
  author = {{Yizong Cheng}},
  year = {Aug./1995},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {17},
  number = {8},
  pages = {790--799},
  issn = {01628828},
  doi = {10.1109/34.400568},
  abstract = {Mean shift, a simple iterative procedure that shifts each data point to the average of data points in its neighborhood, is generalized and analyzed in this paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on a surface constructed with a ``shadow'' kernel. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis is treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\7GDAAQ7W\\Yizong Cheng - 1995 - Mean shift, mode seeking, and clustering.pdf}
}

@article{yue_why_nodate,
  title = {Why {{Non-myopic Bayesian Optimization}} Is {{Promising}} and {{How Far Should We Look-ahead}}? {{A Study}} via {{Rollout}}},
  author = {Yue, Xubo and Kontar, Raed Al},
  pages = {10},
  abstract = {Lookahead, also known as non-myopic, Bayesian optimization (BO) aims to find optimal sampling policies through solving a dynamic programming (DP) formulation that maximizes a long-term reward over a rolling horizon. Though promising, lookahead BO faces the risk of error propagation through its increased dependence on a possibly misspecified model. In this work we focus on the rollout approximation for solving the intractable DP. We first prove the improving nature of rollout in tackling lookahead BO and provide a sufficient condition for the used heuristic to be rollout improving. We then provide both a theoretical and practical guideline to decide on the rolling horizon stagewise. This guideline is built on quantifying the negative effect of a mis-specified model. To illustrate our idea, we provide case studies on both single and multi-information source BO. Empirical results show the advantageous properties of our method over several myopic and non-myopic BO algorithms.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\IDZ5DIK7\\Yue et Kontar - Why Non-myopic Bayesian Optimization is Promising .pdf}
}

@unpublished{zahm_certified_2018,
  title = {Certified Dimension Reduction in Nonlinear {{Bayesian}} Inverse Problems},
  author = {Zahm, Olivier and Cui, Tiangang and Law, Kody and Spantini, Alessio and Marzouk, Youssef},
  year = {2018},
  month = jul,
  abstract = {We propose a dimension reduction technique for Bayesian inverse problems with nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation noise. The likelihood function is approximated by a ridge function, i.e., a map which depends non-trivially only on a few linear combinations of the parameters. We build this ridge approximation by minimizing an upper bound on the Kullback-Leibler divergence between the posterior distribution and its approximation. This bound, obtained via logarithmic Sobolev inequalities, allows one to certify the error of the posterior approximation. Computing the bound requires computing the second moment matrix of the gradient of the log-likelihood function. In practice, a sample-based approximation of the upper bound is then required. We provide an analysis that enables control of the posterior approximation error due to this sampling. Numerical and theoretical comparisons with existing methods illustrate the benefits of the proposed methodology.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\PLPEVGG7\\Zahm et al. - 2018 - Certified dimension reduction in nonlinear Bayesia.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PR4XUS8Z\\Zahm et al. - 2021 - Certified dimension reduction in nonlinear Bayesia.pdf}
}

@unpublished{zahm_certified_2021,
  title = {Certified Dimension Reduction in Nonlinear {{Bayesian}} Inverse Problems},
  author = {Zahm, Olivier and Cui, Tiangang and Law, Kody and Spantini, Alessio and Marzouk, Youssef},
  year = {2021},
  month = mar,
  abstract = {We propose a dimension reduction technique for Bayesian inverse problems with nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation noise. The likelihood function is approximated by a ridge function, i.e., a map which depends non-trivially only on a few linear combinations of the parameters. We build this ridge approximation by minimizing an upper bound on the Kullback-Leibler divergence between the posterior distribution and its approximation. This bound, obtained via logarithmic Sobolev inequalities, allows one to certify the error of the posterior approximation. Computing the bound requires computing the second moment matrix of the gradient of the log-likelihood function. In practice, a sample-based approximation of the upper bound is then required. We provide an analysis that enables control of the posterior approximation error due to this sampling. Numerical and theoretical comparisons with existing methods illustrate the benefits of the proposed methodology.},
  keywords = {Certified error bound,Dimension reduction,Logarithmic Sobolev inequality,Non-asymptotic analysis,Nonlinear Bayesian inverse problem}
}

@article{zahm_gradient-based_2019,
  title = {Gradient-Based Dimension Reduction of Multivariate Vector-Valued Functions},
  author = {Zahm, Olivier and Constantine, Paul and Prieur, Cl{\'e}mentine and Marzouk, Youssef},
  year = {2019},
  month = nov,
  journal = {arXiv:1801.07922 [math]},
  eprint = {1801.07922},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Multivariate functions encountered in high-dimensional uncertainty quantification problems often vary along a few dominant directions in the input parameter space. We propose a gradient-based method for detecting these directions and using them to construct ridge approximations of such functions, in a setting where the functions are vector-valued (e.g., taking values in Rn). The methodology consists of minimizing an upper bound on the approximation error, obtained by subspace Poincar\textasciiacute e inequalities. We provide a thorough mathematical analysis in the case where the parameter space is equipped with a Gaussian probability measure. The resulting method generalizes the notion of active subspaces associated with scalar-valued functions. A numerical illustration shows that using gradients of the function yields effective dimension reduction. We also show how the choice of norm on the codomain of the function has an impact on the function's low-dimensional approximation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {41A30; 41A63; 65D15,Mathematics - Analysis of PDEs},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\K8A4Y9Z7\\Zahm et al. - 2019 - Gradient-based dimension reduction of multivariate.pdf;C\:\\Users\\a846735\\Zotero\\storage\\PJBG2QTS\\Zahm et al. - 2019 - Gradient-based dimension reduction of multivariate.pdf}
}

@phdthesis{zahm_model_2015,
  title = {Model Order Reduction Methods for Parameter-Dependent Equations\textendash{{Applications}} in {{Uncertainty Quantification}}.},
  author = {Zahm, Olivier},
  year = {2015},
  school = {\'Ecole Centrale Nantes},
  keywords = {MOR methods,ThÃ¨se}
}

@article{zaman_robustness-based_2011,
  title = {Robustness-Based Design Optimization under Data Uncertainty},
  author = {Zaman, Kais and McDonald, Mark and Mahadevan, Sankaran and Green, Lawrence},
  year = {2011},
  month = aug,
  journal = {Structural and Multidisciplinary Optimization},
  volume = {44},
  number = {2},
  pages = {183--197},
  issn = {1615-147X, 1615-1488},
  doi = {10.1007/s00158-011-0622-2},
  abstract = {This paper proposes formulations and algorithms for design optimization under both aleatory (i.e., natural or physical variability) and epistemic uncertainty (i.e., imprecise probabilistic information), from the perspective of system robustness. The proposed formulations deal with epistemic uncertainty arising from both sparse and interval data without any assumption about the probability distributions of the random variables. A decoupled approach is proposed in this paper to un-nest the robustness-based design from the analysis of non-design epistemic variables to achieve computational efficiency. The proposed methods are illustrated for the upper stage design problem of a two-stage-toorbit (TSTO) vehicle, where the information on the random design inputs are only available as sparse point and/or interval data. As collecting more data reduces uncertainty but increases cost, the effect of sample size on the optimality and robustness of the solution is also studied. A method is developed to determine the optimal sample size for sparse point data that leads to the solutions of the design problem that are least sensitive to variations in the input random variables.},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TDFMEENU\\Zaman et al. - 2011 - Robustness-based design optimization under data un.pdf}
}

@article{zanette_robust_2018,
  title = {Robust {{Super-Level Set Estimation}} Using {{Gaussian Processes}}},
  author = {Zanette, Andrea and Zhang, Junzi and Kochenderfer, Mykel J.},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.09977 [cs, stat]},
  eprint = {1811.09977},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper focuses on the problem of determining as large a region as possible where a function exceeds a given threshold with high probability. We assume that we only have access to a noise-corrupted version of the function and that function evaluations are costly. To select the next query point, we propose maximizing the expected volume of the domain identified as above the threshold as predicted by a Gaussian process, robustified by a variance term. We also give asymptotic guarantees on the exploration effect of the algorithm, regardless of the prior misspecification. We show by various numerical examples that our approach also outperforms existing techniques in the literature in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\TJ484U2M\\Zanette et al. - 2018 - Robust Super-Level Set Estimation using Gaussian P.pdf}
}

@misc{zang_needs_2002,
  title = {Needs and {{Opportunities}} for {{Uncertainty- Based Multidisciplinary Design Methods}} for {{Aerospace Vehicles}}},
  author = {Zang, T. and Hemsch, M. and Hilburger, M. W. and Kenny, S. and Luckring, J. and Maghami, P. G. and Padula, S. and Stroud, W.},
  year = {2002},
  journal = {undefined},
  abstract = {This report consists of a survey of the state of the art in uncertainty-based design together with recommendations for a Base research activity in this area for the NASA Langley Research Center. This report identifies the needs and opportunities for computational and experimental methods that provide accurate, efficient solutions to nondeterministic multidisciplinary aerospace vehicle design problems. Barriers to the adoption of uncertainty-based design methods are identified. and the benefits of the use of such methods are explained. Particular research needs are listed.},
  howpublished = {/paper/Needs-and-Opportunities-for-Uncertainty-Based-for-Zang-Hemsch/f1750f42207adf383536b5d4bb38af003cb0c6fb},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\JI84A8HR\\f1750f42207adf383536b5d4bb38af003cb0c6fb.html}
}

@article{zanna_ocean_2011,
  title = {Ocean {{Model Uncertainty}} in {{Climate Prediction}}},
  author = {Zanna, Laure},
  year = {2011},
  pages = {8},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\C3BJB6HH\\Zanna - 2011 - Ocean Model Uncertainty in Climate Prediction.pdf}
}

@article{zauner_nudging-based_2022,
  title = {Nudging-Based Data Assimilation of the Turbulent Flow around a Square Cylinder},
  author = {Zauner, M. and Mons, V. and Marquet, O. and Leclaire, B.},
  year = {2022},
  month = apr,
  journal = {Journal of Fluid Mechanics},
  volume = {937},
  publisher = {{Cambridge University Press}},
  issn = {0022-1120, 1469-7645},
  doi = {10.1017/jfm.2022.133},
  abstract = {,  We investigate the estimation of the turbulent flow around a canonical square cylinder at {$\mathsl{R}\mathsl{e}$}=22000Re=22000Re=22\textbackslash,000 based on temporally resolved but spatially sparse velocity data and solving the unsteady Reynolds-averaged Navier\textendash Stokes (URANS) equations. Flow reconstruction from sparse data is achieved through the application of a nudging data assimilation technique. It involves the introduction of a feedback control term in the momentum equations which allows us to drive URANS predictions towards reference data, which are here extracted from a direct numerical simulation. Such a data assimilation approach induces negligible supplementary computational cost compared with that of a standard URANS simulation. The influence of the spatial resolution of the reference data on the reconstruction performances is systematically investigated. Using a spacing of the order of one cylinder length between data points, we already observe synchronisation of the low-frequency vortex shedding between the full reference flow and the one that is estimated by URANS. The present data assimilation procedure allows us to compensate for deficiencies in standard URANS calculations and leads to a significant decrease in temporal and spectral errors as computed by spectral proper orthogonal decomposition. Furthermore, high accuracy in terms of mean-flow prediction by URANS is achieved. When considering spacings between measurements that are of the order of the wavelength of the Kelvin\textendash Helmholtz vortices, such phenomena in the shear layers at the top and bottom of the cylinder are correctly estimated, while they are not self-sustained in standard URANS. The influence of the structure of the feedback control term in the data assimilation procedure is also investigated.},
  langid = {english},
  keywords = {turbulence modelling,turbulence simulation,vortex streets},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\QX8MDHPF\\Zauner et al. - 2022 - Nudging-based data assimilation of the turbulent f.pdf;C\:\\Users\\a846735\\Zotero\\storage\\3Z2E82WF\\15245437544203C73E5AE426DD921FDA.html}
}

@article{zhang_bayesian_2021,
  title = {Bayesian {{Geophysical Inversion Using Invertible Neural Networks}}},
  author = {Zhang, Xin and Curtis, Andrew},
  year = {2021},
  month = jul,
  journal = {Journal of Geophysical Research: Solid Earth},
  volume = {126},
  doi = {10.1029/2021JB022320},
  abstract = {Constraining geophysical models with observed data usually involves solving nonlinear and nonunique inverse problems. Neural mixture density networks (MDNs) provide an efficient way to estimate Bayesian posterior marginal probability density functions (pdf's) that represent the nonunique solution. However, it is difficult to infer correlations between parameters using MDNs, and in turn to draw samples from the posterior pdf. We introduce an alternative to resolve these issues: invertible neural networks (INNs). These are simultaneously trained to represent uncertain forward functions and to solve Bayesian inverse problems. In its usual form, the method does not account for uncertainty caused by data noise and becomes less effective in high dimensionality. To overcome these issues, in this study, we include data uncertainties as additional model parameters, and train the network by maximizing the likelihood of the data used for training. We apply the method to two types of imaging problems: One-dimensional surface wave dispersion inversion and two-dimensional travel time tomography, and we compare the results to those obtained using Monte Carlo and MDNs. Results show that INNs provide comparable posterior pdfs to those obtained using Monte Carlo, including correlations between parameters, and provide more accurate marginal distributions than MDNs. After training, INNs estimate posterior pdfs in seconds on a typical desktop computer. Hence they can be used to provide efficient solutions for repeated inverse problems using different data sets. Also even accounting for training time, our results show that INNs can be more efficient than Monte Carlo methods for solving inverse problems only once.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\DVF5CQSA\\Zhang et Curtis - 2021 - Bayesian Geophysical Inversion Using Invertible Ne.pdf}
}

@article{zhang_examination_2001,
  title = {Examination of {{Numerical Results}} from {{Tangent Linear}} and {{Adjoint}} of {{Discontinuous Nonlinear Models}}},
  author = {Zhang, S. and Zou, X. and Ahlquist, Jon},
  year = {2001},
  month = nov,
  journal = {Monthly Weather Review},
  volume = {129},
  pages = {2791},
  doi = {10.1175/1520-0493(2001)129<2791:EONRFT>2.0.CO;2},
  abstract = {The forward model solution and its functional (e.g., the cost function in 4DVAR) are discontinuous with respect to the model's control variables if the model contains discontinuous physical processes that occur during the assimilation window. In such a case, the tangent linear model (the first-order approximation of a finite perturbation) is unable to represent the sharp jumps of the nonlinear model solution. Also, the first order approximation provided by the adjoint model is unable to represent a finite perturbation of the cost function when the introduced perturbation in the control variables crosses discontinuous points. Using an idealized simple model and the Arakawa-Schubert cumulus parameterization scheme, the authors examined the behavior of a cost function and its gradient obtained by the adjoint model with discontinuous model physics. Numerical results show that a cost function involving discontinuous physical processes is zerothorder discontinuous, but piecewise differentiable. The maximum possible number of involved discontinuity points of a cost function increases exponentially as 2kn, where k is the total number of thresholds associated with on-off switches, and n is the total number of time steps in the assimilation window. A backward adjoint model integration with the proper forcings added at various time steps, similar to the backward adjoint model integration that provides the gradient of the cost function at a continuous point, produces a one-sided gradient (called a subgradient and denoted as {$\nabla$}sJ) at a discontinuous point. An accuracy check of the gradient shows that the adjoint-calculated gradient is computed exactly on either side of a discontinuous surface. While a cost function evaluated using a small interval in the control variable space oscillates, the distribution of the gradient calculated at the same resolution not only shows a rather smooth variation, but also is consistent with the general convexity of the original cost function. The gradients of discontinuous cost functions are observed roughly smooth since the adjoint integration correctly computes the one-sided gradient at either side of discontinuous surface. This implies that, although ({$\nabla$}sJ)T{$\delta$}x may not approximate {$\delta$}J = J(x + {$\delta$}x) - J(x) well near the discontinuous surface, the subgradient calculated by the adjoint of discontinuous physics may still provide useful information for finding the search directions in a minimization procedure. While not eliminating the possible need for the use of a nondifferentiable optimization algorithm for 4DVAR with discontinuous physics, consistency between the computed gradient by adjoints and the convexity of the cost function may explain why a differentiable limited-memory quasi-Newton algorithm still worked well in many 4DVAR experiments that use a diabatic assimilation model with discontinuous physics.},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\HLEDYAIE\\Zhang et al. - 2001 - Examination of Numerical Results from Tangent Line.pdf}
}

@article{zhang_global_2009,
  title = {Global Exponential Convergence and Stability of Gradient-Based Neural Network for Online Matrix Inversion},
  author = {Zhang, Yunong and Shi, Yanyan and Chen, Ke and Wang, Chaoli},
  year = {2009},
  month = oct,
  journal = {Applied Mathematics and Computation},
  volume = {215},
  number = {3},
  pages = {1301--1306},
  issn = {0096-3003},
  doi = {10.1016/j.amc.2009.06.048},
  abstract = {Wang proposed a gradient-based neural network (GNN) to solve online matrix-inverses. Global asymptotical convergence was shown for such a neural network when applied to inverting nonsingular matrices. As compared to the previously-presented asymptotical convergence, this paper investigates more desirable properties of the gradient-based neural network; e.g., global exponential convergence for nonsingular matrix inversion, and global stability even for the singular-matrix case. Illustrative simulation results further demonstrate the theoretical analysis of gradient-based neural network for online matrix inversion.},
  keywords = {Asymptotical convergence,Global exponential convergence,Gradient-based neural network,Lyapunov stability theory,Online matrix inversion}
}

@misc{zhang_learning_2019,
  title = {Learning Nonlinear Level Sets for Dimensionality Reduction in Function Approximation},
  author = {Zhang, Guannan and Zhang, Jiaxin and Hinkle, Jacob},
  year = {2019},
  month = jun,
  number = {arXiv:1902.10652},
  eprint = {1902.10652},
  eprinttype = {arxiv},
  primaryclass = {math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.10652},
  abstract = {We developed a Nonlinear Level-set Learning (NLL) method for dimensionality reduction in high-dimensional function approximation with small data. This work is motivated by a variety of design tasks in real-world engineering applications, where practitioners would replace their computationally intensive physical models (e.g., high-resolution fluid simulators) with fast-to-evaluate predictive machine learning models, so as to accelerate the engineering design processes. There are two major challenges in constructing such predictive models: (a) high-dimensional inputs (e.g., many independent design parameters) and (b) small training data, generated by running extremely time-consuming simulations. Thus, reducing the input dimension is critical to alleviate the over-fitting issue caused by data insufficiency. Existing methods, including sliced inverse regression and active subspace approaches, reduce the input dimension by learning a linear coordinate transformation; our main contribution is to extend the transformation approach to a nonlinear regime. Specifically, we exploit reversible networks (RevNets) to learn nonlinear level sets of a high-dimensional function and parameterize its level sets in low-dimensional spaces. A new loss function was designed to utilize samples of the target functions' gradient to encourage the transformed function to be sensitive to only a few transformed coordinates. The NLL approach is demonstrated by applying it to three 2D functions and two 20D functions for showing the improved approximation accuracy with the use of nonlinear transformation, as well as to an 8D composite material design problem for optimizing the buckling-resistance performance of composite shells of rocket inter-stages.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Functional Analysis},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\FXBMU3TU\\Zhang et al. - 2019 - Learning nonlinear level sets for dimensionality r.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZJDI9XFA\\1902.html}
}

@article{zhao_same_2014,
  title = {{{SAME}} but {{Different}}: {{Fast}} and {{High-Quality Gibbs Parameter Estimation}}},
  shorttitle = {{{SAME}} but {{Different}}},
  author = {Zhao, Huasha and Jiang, Biye and Canny, John},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.5402 [cs, stat]},
  eprint = {1409.5402},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gibbs sampling is a workhorse for Bayesian inference but has several limitations when used for parameter estimation, and is often much slower than non-sampling inference methods. SAME (State Augmentation for Marginal Estimation) \textbackslash cite\{Doucet99,Doucet02\} is an approach to MAP parameter estimation which gives improved parameter estimates over direct Gibbs sampling. SAME can be viewed as cooling the posterior parameter distribution and allows annealed search for the MAP parameters, often yielding very high quality (lower loss) estimates. But it does so at the expense of additional samples per iteration and generally slower performance. On the other hand, SAME dramatically increases the parallelism in the sampling schedule, and is an excellent match for modern (SIMD) hardware. In this paper we explore the application of SAME to graphical model inference on modern hardware. We show that combining SAME with factored sample representation (or approximation) gives throughput competitive with the fastest symbolic methods, but with potentially better quality. We describe experiments on Latent Dirichlet Allocation, achieving speeds similar to the fastest reported methods (online Variational Bayes) and lower cross-validated loss than other LDA implementations. The method is simple to implement and should be applicable to many other models.},
  archiveprefix = {arXiv},
  keywords = {D.1.3,K.3.2,Statistics - Machine Learning},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\3FZ6AFKT\\Zhao et al. - 2014 - SAME but Different Fast and High-Quality Gibbs Pa.pdf;C\:\\Users\\a846735\\Zotero\\storage\\7W43GP2H\\Zhao et al. - 2014 - SAME but Different Fast and High-Quality Gibbs Pa.pdf;C\:\\Users\\a846735\\Zotero\\storage\\ZPDSAHMX\\1409.html}
}

@article{zheng_model_2017,
  title = {Model {{Selection Confidence Sets}} by {{Likelihood Ratio Testing}}},
  author = {Zheng, Chao and Ferrari, Davide and Yang, Yuhong},
  year = {2017},
  month = sep,
  journal = {arXiv:1709.04342 [math, stat]},
  eprint = {1709.04342},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {The traditional activity of model selection aims at discovering a single model superior to other candidate models. In the presence of pronounced noise, however, multiple models are often found to explain the same data equally well. To resolve this model selection ambiguity, we introduce the general approach of model selection confidence sets (MSCSs) based on likelihood ratio testing. A MSCS is defined as a list of models statistically indistinguishable from the true model at a user-specified level of confidence, which extends the familiar notion of confidence intervals to the model-selection framework. Our approach guarantees asymptotically correct coverage probability of the true model when both sample size and model dimension increase. We derive conditions under which the MSCS contains all the relevant information about the true model structure. In addition, we propose natural statistics based on the MSCS to measure importance of variables in a principled way that accounts for the overall model uncertainty. When the space of feasible models is large, MSCS is implemented by an adaptive stochastic search algorithm which samples MSCS models with high probability. The MSCS methodology is illustrated through numerical experiments on synthetic data and real data examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\EY6SRTRF\\Zheng et al. - 2017 - Model Selection Confidence Sets by Likelihood Rati.pdf}
}

@article{zhuang_enhancing_2015,
  title = {Enhancing Product Robustness in Reliability-Based Design Optimization},
  author = {Zhuang, Xiaotian and Pan, Rong and Du, Xiaoping},
  year = {2015},
  month = jun,
  journal = {Reliability Engineering \& System Safety},
  volume = {138},
  pages = {145--153},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2015.01.026},
  abstract = {Different types of uncertainties need to be addressed in a product design optimization process. In this paper, the uncertainties in both product design variables and environmental noise variables are considered. The reliability-based design optimization (RBDO) is integrated with robust product design (RPD) to concurrently reduce the production cost and the long-term operation cost, including quality loss, in the process of product design. This problem leads to a multi-objective optimization with probabilistic constraints. In addition, the model uncertainties associated with a surrogate model that is derived from numerical computation methods, such as finite element analysis, is addressed. A hierarchical experimental design approach, augmented by a sequential sampling strategy, is proposed to construct the response surface of product performance function for finding optimal design solutions. The proposed method is demonstrated through an engineering example.},
  keywords = {Kriging metamodel,Robust design optimization,Sequential optimization and reliability assessment,Sequential sampling},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\VZ5TD4AD\\Zhuang et al. - 2015 - Enhancing product robustness in reliability-based .pdf;C\:\\Users\\a846735\\Zotero\\storage\\NUX5FFBN\\S0951832015000368.html}
}

@article{zupanski_preconditioning_1996,
  title = {A {{Preconditioning Algorithm}} for {{Four-Dimensional Variational Data Assimilation}}},
  author = {Zupanski, Milija},
  year = {1996},
  month = nov,
  journal = {Monthly Weather Review},
  volume = {124},
  number = {11},
  pages = {2562--2573},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/1520-0493(1996)124<2562:APAFFD>2.0.CO;2},
  abstract = {Abstract A preconditioning method suitable for use in four-dimensional variational (4DVAR) data assimilation is proposed. The method is a generalization of the preconditioning previously developed by the author, now designed to include direct observations, as well as different forms of the cost function. The original approach was based on an estimate of the ratio of the expected decrease of the cost function and of the gradient norm, derived from an approximate Taylor series expansion of the cost function. The generalized method employs only basic linear functional analysis, still preserving the efficiency of the original method. The preconditioning is tested in a realistic 4DVAR assimilation environment: the data are direct observations operationally used at the National Centers for Environmental Prediction (formerly the National Meteorological Center), the forecast model is a full-physics regional eta model, and the adjoint model includes all physics, except radiation. The results of five 4DVAR data assimilation experiments, using a memoryless quasi-Newton minimization algorithm, show a significant benefit of the new preconditioning. On average, the minimization algorithm converges in about 20\textendash 25 iterations. In particular, after only 10 iterations, about 95\% of the cost function decrease was achieved in all five cases. Especially encouraging is the fact that these results are obtained with physical processes present in the adjoint model.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {C\:\\Users\\a846735\\Zotero\\storage\\LUQQDCE9\\Zupanski - 1996 - A Preconditioning Algorithm for Four-Dimensional V.pdf;C\:\\Users\\a846735\\Zotero\\storage\\CJV87KHW\\1520-0493_1996_124_2562_apaffd_2_0_co_2.html}
}
