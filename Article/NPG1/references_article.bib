

@article{das_estimation_1991,
  author =        {Das, S. K. and Lardner, R. W.},
  journal =       {Journal of Geophysical Research},
  number =        {C8},
  pages =         {15187},
  title =         {On the Estimation of Parameters of Hydraulic Models
                   by Assimilation of Periodic Tidal Data},
  volume =        {96},
  year =          {1991},
  doi =           {10.1029/91JC01318},
  issn =          {0148-0227},
  language =      {en},
}

@article{das_variational_1992,
  author =        {Das, S. K. and Lardner, R. W.},
  journal =       {International Journal for Numerical Methods in
                   Fluids},
  month =         aug,
  number =        {3},
  pages =         {313-327},
  title =         {Variational Parameter Estimation for a
                   Two-Dimensional Numerical Tidal Model},
  volume =        {15},
  year =          {1992},
  abstract =      {It is shown that the parameters in a two-dimensional
                   (depth-averaged) numerical tidal model can be
                   estimated accurately by assimilation of data from
                   tide gauges. The tidal model considered is a
                   semi-linearized one in which kinematical
                   non-linearities are neglected but non-linear bottom
                   friction is included. The parameters to be estimated
                   (bottom friction coefficient and water depth) are
                   assumed to be position-dependent and are approximated
                   by piecewise linear interpolations between certain
                   nodal values. The numerical scheme consists of a
                   two-level leapfrog method. The adjoint scheme is
                   constructed on the assumption that a certain norm of
                   the difference between computed and observed
                   elevations at the tide gauges should be minimized. It
                   is shown that a satisfactory numerical minimization
                   can be completed using either the
                   Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton
                   algorithm or Nash's truncated Newton algorithm. On
                   the basis of a number of test problems, it is shown
                   that very effective estimation of the nodal values of
                   the parameters can be achieved provided the number of
                   data stations is sufficiently large in relation to
                   the number of nodes.},
  doi =           {10.1002/fld.1650150305},
  issn =          {1097-0363},
  language =      {en},
}

@article{kuczera_there_2010,
  author =        {Kuczera, George and Renard, Benjamin and Thyer, Mark and
                   Kavetski, Dmitri},
  journal =       {Hydrological Sciences Journal},
  month =         aug,
  number =        {6},
  pages =         {980-991},
  title =         {There Are No Hydrological Monsters, Just Models and
                   Observations with Large Uncertainties!},
  volume =        {55},
  year =          {2010},
  abstract =      {Catchments that do not behave in the way the
                   hydrologist expects, expose the frailties of
                   hydrological science, particularly its unduly
                   simplistic treatment of input and model uncertainty.
                   A conceptual rainfall\textendash{}runoff model
                   represents a highly simplified hypothesis of the
                   transformation of rainfall into runoff. Sub-grid
                   variability and mis-specification of processes
                   introduce an irreducible model error, about which
                   little is currently known. In addition, hydrological
                   observation systems are far from perfect, with the
                   principal catchment forcing (rainfall) often subject
                   to large sampling errors. When ignored or treated
                   simplistically, these errors develop into monsters
                   that destroy our ability to model certain catchments.
                   In this paper, these monsters are tackled using
                   Bayesian Total Error Analysis, a framework that
                   accounts for user-specified sources of error and
                   yields quantitative insights into how prior knowledge
                   of these uncertainties affects our ability to infer
                   models and use them for predictive purposes. A case
                   study involving a catchment with an apparent water
                   balance anomaly (a hydrological monstrosity!)
                   illustrates these concepts. It is found that, in the
                   absence of additional information, the
                   rainfall\textendash{}runoff record is insufficient to
                   explain this anomaly \textendash{} it could be due to
                   a large export of groundwater, systematic
                   overestimation of catchment rainfall of the order of
                   40\%, or a conspiracy of these factors. There is ``no
                   free lunch'' in hydrology. The
                   rainfall\textendash{}runoff record on its own is
                   insufficient to decompose the different sources of
                   uncertainty affecting calibration, testing and
                   prediction, and hydrological monstrosities will
                   persist until additional independent knowledge of
                   uncertainties is obtained. Citation Kuczera, G.,
                   Renard, B., Thyer, M. \& Kavetski, D. (2010) There
                   are no hydrological monsters, just models and
                   observations with large uncertainties! Hydrol. Sci.
                   J. 55(6), 980\textendash{}991.},
  doi =           {10.1080/02626667.2010.504677},
  issn =          {0262-6667},
}

@article{walker_defining_2003,
  author =        {Walker, Warren E. and Harremo\"es, Poul and
                   Rotmans, Jan and {van der Sluijs}, Jeroen P. and
                   {van Asselt}, Marjolein BA and Janssen, Peter and
                   {Krayer von Krauss}, Martin P.},
  journal =       {Integrated assessment},
  number =        {1},
  pages =         {5--17},
  title =         {Defining Uncertainty: A Conceptual Basis for
                   Uncertainty Management in Model-Based Decision
                   Support},
  volume =        {4},
  year =          {2003},
}

@incollection{huber_robust_2011,
  author =        {Huber, Peter J.},
  booktitle =     {International {{Encyclopedia}} of {{Statistical
                   Science}}},
  pages =         {1248--1251},
  publisher =     {{Springer}},
  title =         {Robust Statistics},
  year =          {2011},
}

@article{berger_overview_1994,
  author =        {Berger, James O. and Moreno, El\'ias and
                   Pericchi, Luis Raul and Bayarri, M. Jes\'us and
                   Bernardo, Jos\'e M. and Cano, Juan A. and
                   {De la Horra}, Juli\'an and Mart\'in, Jacinto and
                   {R\'ios-Ins\'ua}, David and Betr\`o, Bruno},
  journal =       {Test},
  number =        {1},
  pages =         {5--124},
  title =         {An Overview of Robust {{Bayesian}} Analysis},
  volume =        {3},
  year =          {1994},
}

@article{lehman_designing_2004,
  author =        {Lehman, Jeffrey S. and Santner, Thomas J. and
                   Notz, William I.},
  journal =       {Statistica Sinica},
  pages =         {571--590},
  title =         {Designing Computer Experiments to Determine Robust
                   Control Variables},
  year =          {2004},
}

@techreport{janusevskis_simultaneous_2010,
  author =        {Janusevskis, Janis and Le Riche, Rodolphe},
  month =         jul,
  title =         {Simultaneous Kriging-Based Sampling for Optimization
                   and Uncertainty Propagation},
  year =          {2010},
  abstract =      {Robust analysis and optimization is typically based
                   on repeated calls to a deterministic simulator that
                   aim at propagating uncertainties and finding optimal
                   design variables. Without loss of generality a double
                   set of simulation parameters can be assumed: x are
                   deterministic optimization variables, u are random
                   parameters of known probability density function and
                   f (x, u) is the objective function attached to the
                   simulator. Most robust optimization methods involve
                   two imbricated tasks, the u's uncertainty propagation
                   (e.g., Monte Carlo simulations, reliability index
                   calculation) which is recurcively performed inside
                   optimization iterations on the x's. In practice, f is
                   often calculated through a computationally expensive
                   software. This makes the computational cost one of
                   the principal obstacle to optimization in the
                   presence of uncertainties. This report proposes a new
                   efficient method for minimizing the mean objective
                   function, min E[f(x, U)]. The efficiency stems from
                   the simultaneous sampling of f for uncertainty
                   propagation and optimization, i.e., the hierarchical
                   imbrication is avoided. Y(x,u) ($\omega$), a kriging
                   (Gaussian process conditioned on t past calculations
                   of f) model of f (x, u) is built and the mean
                   process, Z(x) ($\omega$) = E[Y(x,U)($\omega$)], is
                   analytically derived from it. The sampling criterion
                   that yields both x and u is the one-step ahead
                   minimum variance of the mean process Z at the
                   maximizer of the expected improvement. The method is
                   compared with Monte Carlo and kriging-based
                   approaches on analytical test functions in two, four
                   and six dimensions.},
}

@inproceedings{miranda_adjoint-based_2016,
  address =       {Crete Island, Greece},
  author =        {Miranda, Joao and Kumar, Dinesh and Lacor, Chris},
  booktitle =     {Proceedings of the {{VII European Congress}} on
                   {{Computational Methods}} in {{Applied Sciences}} and
                   {{Engineering}} ({{ECCOMAS Congress}} 2016)},
  pages =         {8351-8364},
  publisher =     {{Institute of Structural Analysis and Antiseismic
                   Research School of Civil Engineering National
                   Technical University of Athens (NTUA) Greece}},
  title =         {Adjoint-Based {{Robust Optimization}} Using
                   {{Polynomial Chaos Expansions}}},
  year =          {2016},
  abstract =      {Adjoint methods are nowadays widely used to
                   efficiently perform optimization for problems with a
                   large number of design variables. However, in
                   reality, the problem at hand might be subjected to
                   uncertainties in the operational conditions or, in
                   case of optimizing geometries, the design variables
                   itself might be uncertain due to manufacturing
                   tolerances. For such applications, the optimum
                   obtained using deterministic methods might be very
                   sensitive to small variations in the uncertainties,
                   i.e. it lacks robustness. In a robust optimization,
                   the uncertainties are taken directly into account
                   during the optimization process by introducing, next
                   to the mean objective, its variance as a second
                   objective. This implies that, when using gradient
                   based optimization methods, the gradients of both
                   objectives (mean and variance) must be known. In this
                   work the Polynomial Chaos Expansion (PCE) is used in
                   combination with adjoint methods to efficiently
                   obtain both gradients. A non intrusive, regression
                   based PCE is used, requiring a new adjoint solution
                   for each sampling point in order to build the PCE of
                   the gradient. A PCE for the objective is also built
                   (at no extra cost) in order to compute the gradient
                   of the variance.},
  doi =           {10.7712/100016.2418.10873},
  isbn =          {978-618-82844-0-1},
  language =      {en},
}

