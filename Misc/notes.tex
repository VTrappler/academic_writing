\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage[francais, english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[margin=1.5cm]{geometry}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{calc}
\usepackage{caption}
\usepackage{easy-todo}
\usepackage{comment}
\usepackage{lipsum}
\usepackage{bbm}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{pdfpages}
\newcommand{\Var}{\mathbb{V}\text{ar}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\usepackage{mathrsfs}
\newcommand{\ProbGP}{\mathcal{P}}
\newcommand{\Kspace}{\mathbb{K}}
\newcommand{\Uspace}{\mathbb{U}}
\newcommand{\Xspace}{\mathbb{X}}
\newcommand{\Yspace}{\mathbb{Y}}
\newcommand{\kk}{\mathbf{k}}
\newcommand{\KK}{\mathbf{K}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\estimtxt}[2]{\hat{#1}_{\mathrm{#2}}}
\DeclareMathOperator{\Cov}{\mathbb{C}\text{ov}}
\DeclareMathOperator*{\argmin}{arg\,min \,}
\DeclareMathOperator*{\argmax}{arg\,max \,}
\DeclareMathOperator{\IMSE}{IMSE}
\newcommand{\given}{\middle|}
% \theoremstyle{definition}
% \newtheorem{example}{Example}[section]
% \newtheorem{definition}{Definition}[section]
% \newtheorem{thm}{Theorem}[section]

\usepackage{hyperref}
\usepackage[capitalize, nameinlink]{cleveref}
\usepackage{booktabs}
% \creflabelformat{equation}{Eq.(#1)}
\pagestyle{fancy}
\rhead{\rightmark}
\lhead{}
% \graphicspath{{./Figures/}}
% \newcommand{\Ex}{\operatorname{E}\expectarg}
% \DeclarePairedDelimiterX{\expectarg}[1]{[}{]}{%
%   \ifnum\currentgrouptype=16 \else\begingroup\fi
%   \activatebar#1
%   \ifnum\currentgrouptype=16 \else\endgroup\fi
% }

\newcommand{\innermid}{\nonscript\;\delimsize\vert\nonscript\;}
\newcommand{\activatebar}{%
  \begingroup\lccode`\~=`\|
  \lowercase{\endgroup\let~}\innermid
  \mathcode`|=\string"8000
}

\usepackage[framemethod=tikz]{mdframed}
\newtheoremstyle{defi}
  {\topsep}%
  {\topsep}%
  {\normalfont}%
  {}%
  {\bfseries}% 
  {:}%
  {.5em}%
  {\thmname{#1~}\thmnumber{#2}\thmnote{ -- #3}}
  %
  \theoremstyle{defi}
\mdfdefinestyle{theoremStyle}{
	hidealllines=true,
	leftline=true,
	bottomline=true,
	innertopmargin=2pt,
	innerbottommargin=6pt,
	linewidth=2.5pt,
	linecolor=gray!40,
	innerrightmargin=0pt,
}
\newcounter{thmCounter}[section]
\numberwithin{thmCounter}{section}
\newmdtheoremenv[style=theoremStyle]{theorem}[thmCounter]{Theorem}
\newmdtheoremenv[style=theoremStyle]{definition}[thmCounter]{Definition}
\newmdtheoremenv[style=theoremStyle]{conjecture}[thmCounter]{Conjecture}
\newmdtheoremenv[style=theoremStyle]{lemma}[thmCounter]{Lemma}
\newmdtheoremenv[style=theoremStyle]{remark}[thmCounter]{Remark}
\newmdtheoremenv[style=theoremStyle]{example}[thmCounter]{Example}
\newmdtheoremenv[style=theoremStyle]{proposition}[thmCounter]{Proposition}
\newmdtheoremenv[style=theoremStyle]{corollary}[thmCounter]{Corollary}

\mdfdefinestyle{alertStyle}{
backgroundcolor=red!27,
linecolor=gray,
roundcorner=6pt,
middlelinewidth=0pt
}

% Repetable theorem env
\usepackage{thmtools}
\usepackage{thm-restate}
\declaretheoremstyle
[
    preheadhook={\begin{mdframed}[style=theoremStyle]},
    postfoothook=\end{mdframed},
]{framedThmStyle}
\declaretheorem[style=framedThmStyle, title=Theorem, numberlike=thmCounter]{repenvThm}
\declaretheorem[style=framedThmStyle, title=Conjecture, numberlike=thmCounter]{repenvConj}





\begin{document}


\title{Notes}

\author{Victor Trappler \\[1cm]
  \begin{tabular}{lr}
    Directeurs de Thèse: & Arthur VIDARD (Inria) \\
                        & Élise ARNAUD (UGA)\\
                        & Laurent DEBREU (Inria)
  \end{tabular}
}

\maketitle
% \vspace{3cm}
% \includegraphics[scale=0.3]{/home/victor/logo_UGA}
% \hfill
% \includegraphics[scale=0.3]{/home/victor/ljk}
% \hfill
% \includegraphics[scale=0.3]{/home/victor/inria}
% \thispagestyle{empty} 
% \clearpage
% \tableofcontents

\showthe\columnwidth
% Columnwidth = 418.25368pt



% \subsection{Quantities derived from the likelihood}
% \subsubsection{The score function}
% \label{sec:score_function}
% Given the data $y$, the likelihood function is $\mathcal{L}(\theta ; y)$, and the log likelihood is $l(\theta;y) = \log \mathcal{L}(\theta;y)$
% The score function is defined as
% \begin{equation}
%   \label{eq:def_score_function}
%   s(\theta) = \frac{\partial \log \mathcal{L}}{\partial \theta}(\theta; y)
% \end{equation}
% and the MLE $\hat{\theta}$ verifies
% \begin{equation}
%   \label{eq:MLE}
%   s(\hat{\theta}) = 0
% \end{equation}
% For the true parameter $\bar{\theta}$, averaging over all possible information yields $0$:
% \begin{align}
%   \Ex\left[s(\bar{\theta}) \mid \bar{\theta}\right] = \Ex\left[\frac{\frac{\partial p(y|\theta)}{\partial \theta}}{p(y|\theta)} \mid \bar{\theta}\right] = \int \frac{\frac{\partial p(y|\bar\theta)}{\partial \theta}}{p(y\mid\bar\theta)} p (y \mid \bar\theta) \,\mathrm{d}y = \frac{\partial}{\partial \theta} \int p(y|\bar\theta) \,\mathrm{d}y = 0
% \end{align}
% The variance of the score is the Fisher Information matrix

% \subsubsection{Fisher Information Matrix}
% \label{sec:fisher_information_matrix}
% \begin{align}
%   \label{eq:def_fisher_information_matrix}
%   \mathcal{I}(\theta) &= \Ex\left[\left(\frac{\partial \log \mathcal{L}}{\partial \theta}\right)^2 \mid \theta\right] \\
%                         &=\Ex\left[-\frac{\partial^2 \log \mathcal{L}}{\partial \theta^2} \mid \theta\right]
% \end{align}

% \subsection{Priors}
% \subsubsection{Informative priors}
% \label{sec:informative_priors}

% \begin{align*}
%   K \sim \mathcal{U}(\mathbb{K}), \quad p(k) \\
%   U \sim \mathcal{U}(\mathbb{U}), \quad p(u)
% \end{align*}

% \subsubsection{Non-informative priors}
% \label{sec:non-info_priors}
% Non-informative priors 
% Now to Bayes' theorem
% \begin{align*}
%   p(k,u | y,\sigma^2) = \frac{p(y | k, u, \sigma^2) p(k,u)}{\iint_{\mathbb{K}\times\mathbb{U}} p(y | k, u, \sigma^2) p(k,u) \, \mathrm{d}(k,u)}
% \end{align*}
%   Let us assume an hyperprior for $\sigma^2$: $p(\sigma^2)$

% In the following, we write $\theta = (k, u)\in \Theta$ when no distinction is needed, or a general notation is needed.

\section{Model selection}
\label{sec:model_selection}

\subsection{Frequentist approach: Likelihood ratio test}
The likelihood ratio test is a useful test in the case of nested models, as described in what follows:

\subsubsection{Nested models}
\begin{definition}[Nested models]
  Let $\mathfrak{M}_1=(\mathcal{M}_1, \Theta_1)$ and $\mathfrak{M}_2=(\mathcal{M}_2, \Theta_2)$ be two models.
$\mathfrak{M}_1$ is said to be nested within $\mathfrak{M}_2$ if
\begin{equation*}
  \mathcal{M}_1 = \mathcal{M}_1 \text{ and } \Theta_1 \subset \Theta_2
\end{equation*}
\end{definition}
\begin{example}
  Let us consider two models, where $\Yspace = \mathbb{R}$
  \begin{align*}
    \mathfrak{M}_1 &= \left((a,b) \mapsto ab;\quad (a,b) \in \mathbb{R} \times [0;2]\right) \\
   \mathfrak{M}_2 &= \left((a,b) \mapsto ab;\quad (a,b) \in \mathbb{R}^+ \times \{1/\pi\}\right)
  \end{align*}
$\mathfrak{M}_2$ is nested within $\mathfrak{M}_1$
\end{example}
\begin{example}
  Now let us consider $\Yspace$ as the space of random vector of dimension $n$:
  \begin{align*}
    \mathfrak{M}_1 &: (X, A, \sigma) \mapsto AX + \sigma\epsilon, \text{ with } (X, A, \sigma)\in\mathbb{R}^n \times \mathbb{R}^{n\times n} \times \mathbb{R}^+ \text{ and } \epsilon \sim \mathcal{N}(0, I) \\
    \mathfrak{M}_2 &: (X, A, \sigma) \mapsto AX + \sigma\epsilon, \text{ with } (X, A, \sigma)\in\mathbb{R}^n \times \mathbb{R}^{n\times n} \times \{1\} \text{ and } \epsilon \sim \mathcal{N}(0, I)
  \end{align*}
Once again in this example,  $\mathfrak{M}_2$ is nested within $\mathfrak{M}_2$
\end{example}

\label{sec:lik_test}
Using the likelihood defined above, we can test for the following hypotheses:
\begin{itemize}
\item $\mathcal{H}_0$: $\theta \in \Theta_0\subset \mathbb{R}^d$
\item $\mathcal{H}_1$: $\theta \in \Theta_1 \subset \mathbb{R}^r$, and $\Theta_0 \subset \Theta_1$
\end{itemize}
Intuitively, we can see $\Theta_1$ as the more general model.
The test statistic is
\begin{equation}
  \label{eq:def_lik_ratio}
  \Lambda(y) = \frac{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta ; y)}{\sup_{\theta \in \Theta_1} \mathcal{L}(\theta ; y)}
\end{equation}
and under $\mathcal{H}_0$, the quantity 
\begin{equation}
  - 2 \log \Lambda(y) \xrightarrow[]{\mathrm{d}} \chi^2_{r-d}
\end{equation}
is asymptotically distributed as a $\chi^2_{r-d}$.
Using the log-likelihood, $-2(l(\theta_0;y) - l(\theta_1;y)) \xrightarrow{\mathrm{d}} \chi^2_{r-d}$
The asymptotic rejection region of level $\alpha$ is then
\begin{align}
  \mathrm{RejReg}_{\alpha} &= \{y \mid -2 \log \Lambda(y) > \chi^2_{1-\alpha, r-d} \} \\
                           &= \{y \mid \log \Lambda(y) < -\frac12 \chi^2_{1-\alpha, r-d} \} \\
                           &= \{ y \mid (\sup_{\theta\in\Theta_0} l(\theta;y) - \sup_{\theta\in\Theta_1} l(\theta;y)) < -\frac12 \chi^2_{1-\alpha, r-d} \} \\
                           &= \{ y \mid (\sup_{\theta\in\Theta_1} l(\theta;y) - \sup_{\theta\in\Theta_0} l(\theta;y)) > \frac12 \chi^2_{1-\alpha, r-d} \} \\
\end{align}

\subsubsection{Relative Likelihood}
\label{sec:relative_likelihood}
Relative Likelihood is defined in~\cite{kalbfleisch_probability_1985} as the ratio of the likelihood evaluated at a point $\theta$ to the maximum of the likelihood:
\begin{equation}
  R(\theta) = \frac{\mathcal{L}(\theta;y)}{\mathcal{L}(\estimtxt{\theta}{MLE};y)} = \frac{\mathcal{L}(\theta;y)}{\sup_{\theta^{\prime} \in \Theta}\mathcal{L}(\theta^{\prime};y)}
\end{equation}
In that same vein, a Likelihood interval (of level $p\in ]0,1]$) is defined as
\begin{equation}
  \mathcal{I}_{\mathrm{Lik}}(p) = \left\{\theta | R(\theta)=\frac{\mathcal{L}(\theta;y)}{\mathcal{L}(\estimtxt{\theta}{MLE};y)} \geq p\right\}
\end{equation}
\subsection{Bayesian model selection}
\label{sec:bayes_model_selection}
Let us assume that for $\mathcal{M}$ is chosen to represent the problem at stake. In this case, $\theta$ represent implicitly parameters of this model $\mathcal{M}$. Bayes' theorem gives
\begin{equation}
  \label{eq:bayes_th_BMS}
  p(\theta | \mathcal{M}, y) = \frac{p(y | \mathcal{M}, \theta)p(\theta)}{p(y | \mathcal{M})}
\end{equation}
In \cref{eq:bayes_th_BMS}, $p(y | \mathcal{M}) = \int_{\Theta}p(y | \mathcal{M}, \theta)p(\theta) \,\mathrm{d}\theta$ is called the evidence of the model $\mathcal{M}$ given the data $y$.

\subsubsection{Bayes factor}
When comparing two models $\mathcal{M}_1$ and $\mathcal{M}_2$, one can compute the Bayes factor, that is the ratio of the evidence of the two models:
\begin{equation}
  \label{eq:bayes_factor}
  \mathrm{BF}(\mathcal{M}_1, \mathcal{M}_2) = \frac{p(y | \mathcal{M}_1)}{p(y | \mathcal{M}_1)}
\end{equation}

\subsubsection{Information criteria}
Let $\mathcal{L}$ be the likelihood of the candidate model considered, $N$ the number of observations available, and $k$ its dimension
\begin{align}
  \mathrm{AIC} &= -2\log \mathcal{L} + 2 k \\
  \mathrm{BIC} &= -2\log \mathcal{L} + k\log(N) \\
  % \mathrm{AIC} &= -2\log \mathcal{L} + 2 k \\  
\end{align}
\subsection{Model selection and robust estimation}

Let us set $\theta = (k,u,\phi)$ where $\phi$ represents additional parameters in the likelihood
\begin{equation}
  \mathcal{L}(\theta; y) = \mathcal{L}(k, u, \phi ; y)
\end{equation}
Let us assume furthermore that the maximizer of the likelihood depends only on $u$ (we remove the dependence on $y$ in the notation, to declutter).
\begin{equation}
  \argmax_{k \in \Kspace} \mathcal{L}(k,u, \phi) = k^*(u) = \argmax_{k \in \Kspace} \ell(k,u, \phi)
\end{equation}

Now let us consider the ratio given a value $u$ and  
\begin{equation}
  -2\log\Lambda(u, \phi^\prime) = -2\left(\ell(k, u, \phi) - \ell(k^*(u), u, \phi^\prime) \right)
\end{equation}

Given $u$, let us define the following likelihoods
\begin{align}
  \mathcal{L}(k ; u, \sigma^2) &= \frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{J(k,u)}{2\sigma^2}\right] \\
  \mathcal{L}(k=k^*(u) ; u, \varsigma^2) &= \frac{1}{\sqrt{2\pi}\varsigma}\exp\left[-\frac{J^*(u)}{2\varsigma^2}\right] \\
\end{align}
Taking the ratio yields
\begin{align}
  \frac{\mathcal{L}(k;u,\sigma^2)}{\mathcal{L}(k^*;u,\varsigma^2)} &= \frac{\varsigma}{\sigma}\exp\left[-\frac{1}{2}\left(\frac{J(k,u)}{\sigma^2} - \frac{J^*(u)}{\varsigma^2})\right)\right] \\
                                                                   &= \frac{\varsigma}{\sigma}\exp\left[-\frac{1}{2\sigma^2}\left(J(k,u)- \frac{\sigma^2}{\varsigma^2}J^*(u))\right)\right]
\end{align}
taking twice the negative log likelihood,
\begin{equation}
  -2\log \frac{\mathcal{L}(k;u,\sigma^2)}{\mathcal{L}(k^*;u,\varsigma^2)} = \frac{1}{\sigma^2}\left(J(k,u) - \frac{\sigma^2}{\varsigma^2}J^*(u)\right) +2\log\frac{\sigma}{\varsigma}
\end{equation}
The log ratio $\varrho$ is
\begin{align}
  \varrho(k, u, \sigma, \varsigma) &= \frac{1}{\sigma^2}\left(J(k,u) - \frac{\sigma^2}{\varsigma^2}J^*(u)\right) +2\log\frac{\sigma}{\varsigma} \\
  (\text{When } \sigma=1)                  &  = \left(J(k,u) - \frac{1}{\varsigma^2}J^*(u)\right) - 2\log\varsigma
\end{align}

Problem: what are $\Theta_0$ and $\Theta_1$ ?.
Again, for a given $u$, we test for $k$, so $\Theta_0 = (\{k\}, \{2\})\subset \Kspace \times \mathbb{R}^+$.


% \subsection{The generalized normal distribution}
% \label{sec:generalized_normal_distribution}
% \subsubsection{Probability density function}
% \label{sec:generalized_normal_distribution_pdf}
% We consider a random variable $\xi(\kappa)$ with the following pdf
% \begin{equation}
%   \label{eq:pdf_GND}
%   f_{\kappa}(x, \mu, s) = \frac{\kappa}{2 s \Gamma(1/\kappa)} \exp\left[-\left(\frac{\lvert x-\mu \rvert}{s}\right)^\kappa\right]
% \end{equation}
% that depends on the parameters $\mu$ $s>0$ and $\kappa>0$ representing respectively the location, scale and the shape parameter.
% One can notice that in the particular case where $\kappa=2$, $\xi(\kappa=2)\sim \mathcal{N}(\mu, \frac{s^2}{2})$. Similarly, when $\kappa=1$, $\xi(\kappa=1)$ is distributed according to Laplace distribution.
% One important fact is that for $x \in ]\mu - s,\mu +s[$, $\frac{\lvert x-\mu \rvert}{s} < 1$ so when $\kappa \rightarrow +\infty$, $\exp\left[-\left(\frac{\lvert x-\mu \rvert}{s}\right)^\kappa\right] \rightarrow 1$ if $x \in ]\mu - s,\mu +s[$, $0$ elsewhere. This distribution converges pointwise to a uniform distribution on $]\mu - s,\mu +s[$.

% \subsubsection{Moments of the generalized normal distribution}
% \label{sec:generalized_normal_distribution_moments}
% Due to the symmetry of the pdf, one can directly conclude that the mode, median and mean are $\mu$:
% \begin{equation}
%   \Ex[\xi(\kappa)] = \mathrm{Mode}[\xi(\kappa)] = \mathrm{Median}[\xi(\kappa)] = \mu 
% \end{equation}
% In~\cite{pogany_characteristic_2010}, is also proven the following expression for the variance of $\xi(\kappa)$:
% \begin{align}
%   \Var\left[\xi(\kappa)\right] &= s^2 \frac{\Gamma(3 / \kappa)}{\Gamma(1/ \kappa)} = \frac{s^2}{3} - \frac{2s^2 \gamma}{3\kappa} + \frac{2s^2(3\gamma^2 +\pi^2)}{9\kappa^2} + \mathcal{O}\left(\frac{1}{\kappa^3}\right) \\
%                                &= \frac{(2s)^2}{12} - \frac{2s^2 \gamma}{3\kappa} + \frac{2s^2(3\gamma^2 +\pi^2)}{9\kappa^2} + \mathcal{O}\left(\frac{1}{\kappa^3}\right)
% \end{align}
% At the first order, when $\kappa\rightarrow +\infty$, the variance is the variance of a random variable on an interval of length $2s$ as expected

% \subsubsection{Loglikelihood for GND}
% \label{sec:loglik_GND}

% \begin{align}
%   \ell_{\kappa}(x, \mu, s) &= -\left(\frac{\lvert x-\mu \rvert}{s}\right)^\kappa + \log \kappa - \log(s) - \log \Gamma(1/\kappa) - \log 2 \\
%   &= -\left(\left(\frac{x-\mu}{s}\right)^2\right)^{\frac{\kappa}{2}} + \log \kappa - \log(s) - \log \Gamma(1/\kappa) - \log 2
%   \end{align}
% \subsubsection{Ratio between two GND}
% \label{sec:ratio_GND}
% Let us consider two GND distribution: the ratio between the two can be written as
% \begin{align}
%   \frac{f_{\kappa_1}(x_1, \mu_1, s_1)}{f_{\kappa_2}(x_2, \mu_2, s_2)} = \frac{\kappa_1}{\kappa_2} \frac{s_2 \Gamma(1/ \kappa_2)}{s_1 \Gamma(1/ \kappa_1)} \exp \left[\left(\frac{\lvert x_2 - \mu_2 \rvert}{s_2}\right)^{\kappa_2}-\left(\frac{\lvert x_1 - \mu_1 \rvert}{s_1}\right)^{\kappa_1} \right]
% \end{align}

% \subsection{The Profile Likelihood}
% \label{sec:prof_lik}
% The likelihood is defined as
% \begin{equation}
%   \label{eq:lik_def}
%   \mathcal{L}(k, u ; y) = p( y \mid k, u)
% \end{equation}
% Maximizing the likelihood yields the Maximum Likelihood Estimator:
% Given the observation $y$,
% \begin{equation}
%   \label{eq:MLE}
%   (k_{\mathrm{MLE}}, u_{\mathrm{MLE}}) = \max_{k,u} \mathcal{L}(k, u; y)
% \end{equation}
% The traditional profile likelihood is obtained by profiling the nuisance parameters:
% \begin{equation}
%   \label{eq:plik_def}
%   \mathcal{L}_p(k) = \sup_{u\in\Uspace}\mathcal{L}(k, u; y)
% \end{equation}
% Immediately, one can see that maximizing the profile likelihood leads to the MLE.
\newpage
\section{GP, RR-based family of estimators}
\subsection{Random processes and Gaussian Process Regression}
Let us assume that we have a map $f$ from a $p$ dimensional space to $\mathbb{R}$:
\begin{align}
  \begin{array}{rrcl}
    f: & \mathbb{X} \subset \mathbb{R}^p& \longrightarrow & \mathbb{R} \\
       & x & \longmapsto & f(x)
  \end{array}
\end{align}
This function is assumed to have been evaluated on a design of $n$ points, $\mathcal{X} \subset \mathbb{X}^n$. 
We wish to have a probabilistic modelling of this function
We introduce random processes as way to have a prior distribution on function
This uncertainty on $f$ is modelled as a random process:
\begin{equation}
  \begin{array}{rcl}
    Z: \mathbb{X} \times \Omega& \longrightarrow & \mathbb{R} \\
    (x,\omega) & \longmapsto & Z(x,\omega)
  \end{array}
\end{equation}
The $\omega$ variable will be omitted next.
\subsection{Linear Estimation}
\label{sec:linear_estimation}
A linear estimation $\hat{Z}$ of $f$ at an unobserved point $x\notin \mathcal{X}$ can be written as
\begin{equation}
  \label{eq:lin_est}
  \hat{Z}(x) =
  \begin{bmatrix}
    w_1 \dots w_n
    \end{bmatrix}
    \begin{bmatrix}
      f(x_1) \\ \vdots \\ f(x_n)
    \end{bmatrix} = \mathbf{W}^Tf(\mathcal{X}) = \sum_{i=1}^n w_i(x) f(x_i)
\end{equation}
Using those kriging weights $\mathbf{W}$, a few additional conditions must be added, in order to obtain the Best Linear Unbiased Estimator:
\begin{itemize}
\item Non-biased estimation: $\Ex[\hat{Z}(x) - Z(x)]=0$
\item Minimal variance: $\min~\Ex[(\hat{Z}(x) - Z(x))^2]$
\end{itemize}
Translating using~\cref{eq:lin_est}:
\begin{equation}
  \Ex[\hat{Z}(x) - Z(x)]=0 \iff m(\sum_{i=1}^n w_i(x)-1) = 0 \iff \sum_{i=1}^n w_i(x) = 1 \iff \mathbf{1}^T \mathbf{W} = 1
\end{equation}
For the minimum of variance, we introduce the augmented vector $\mathbf{Z}_n(x) = [Z(x_1),\dots Z(x_n), Z(x)]$ and $\mathbf{Z}_n = [Z(x_1),\dots Z(x_n)]$, and
the variance can be expressed as:
\begin{align}
  \Ex[(\hat{Z}(x) - Z(x))^2] &= \Cov\left[[\mathbf{W}^T, -1] \cdot \mathbf{Z}_n(x) \right] \\
                             &= [\mathbf{W}^T, -1] \Cov\left[\mathbf{Z}_n(x) \right] [\mathbf{W}^T, -1]^T
\end{align}
In addition, we have
\begin{equation}
  \Cov\left[\mathbf{Z}_n(x) \right] =
  \begin{bmatrix}
    \Cov\left[ \mathbf{Z}_n^T\right]
    & \Cov\left[\mathbf{Z}_n^T, Z(x) \right]
  \\
  \Cov\left[\mathbf{Z}_n^T, Z(x) \right]^T & \Var\left[Z(x)\right]
  \end{bmatrix}
\end{equation}
Once expanded, the kriging weights solve then the following optimisation problem:
\begin{align}
  \min_{\mathbf{W}} ~&~\mathbf{W}^T \Cov\left[\mathbf{Z}_n\right] \mathbf{W}+ \Var\left[Z(x)\right] \\ &-\Cov\left[\mathbf{Z}_n^T, Z(x) \right]^T \mathbf{W}- \mathbf{W}^T\Cov\left[\mathbf{Z}_n^T, Z(x) \right] \\ 
  \text{s.t. }&  \mathbf{1}^T\mathbf{W} = 1
\end{align}
This leads to
\begin{align}
  \begin{bmatrix}
    \mathbf{W} \\ m
  \end{bmatrix}
  &=
  \begin{bmatrix}
    \Cov\left[\mathbf{Z}_n\right] & \mathbf{1} \\
  \mathbf{1}^T & 0
\end{bmatrix}^{-1}
                 \begin{bmatrix}
                  \Cov\left[\mathbf{Z}_n^T, Z(x) \right]^T \\ 1 
\end{bmatrix}
  \\ &=
    \begin{bmatrix}
      C(x_1, x_1) & \cdots & C(x_1, x_n) & 1 \\
      C(x_2, x_1) & \cdots & C(x_2, x_n) & 1 \\
      \vdots & \ddots & \vdots & \vdots \\
      C(x_n, x_1) & \cdots & C(x_n, x_n)& 1 \\
      1 & \cdots & 1 & 0
    \end{bmatrix}^{-1}
                       \begin{bmatrix}
                         C(x_1, x) \\
                         C(x_2, x) \\
                         \vdots \\
                         C(x_n, x) \\
                         1
                       \end{bmatrix}
\end{align}

\subsection{Covariance functions}
\label{sec:cov_fun}
\begin{itemize}
\item Desired properties
  \begin{itemize}
  \item isotropy (?)
  \item stationarity
  \item semi-definite positiveness
  \end{itemize}
\item parametric models of covariance
\item examples
\item usual hyperparameters estimation
\end{itemize}

\subsection{General SUR strategies}
\label{sec:SUR_strat}
\subsubsection{Generalities on SUR strategies}

\subsubsection{Exploration and Space Filling objectives}

\subsubsection{Contour Estimation}
Let $Y$ be a random process over $\Xspace$, and let us follow what has been done in~\cite{bect_sequential_2012}.
Let $Y_n$ be the GP constructed using $n$ evaluations of the objective function. If the subscript $n$ is omitted, the number of evaluations is not relevant at the moment.
\subsection{GP of the penalized cost function $\Delta_{\alpha}$}
\subsubsection{GP processes}
We assume that we constructed a GP on $J$ on the joint space $\Kspace \times \Uspace$, based on a design of $n$ points $\mathcal{X} = \left\{(\kk^{(1)},\uu^{(1)}),\dots,(\kk^{(n)},\uu^{(n)}) \right\}$, denoted as $(\kk,\uu)\mapsto Y(\kk,\uu)$.

As a GP, $Y$ is described by its mean function $m_{Y}$ and its covariance function $C(\cdot, \cdot)$, while $\sigma^2_Y(\kk,\uu) = C((\kk,\uu), (\kk,\uu))$
\begin{equation}
  Y(\kk,\uu) \sim \mathcal{N}\left(m_{Y}(\kk,\uu), \sigma^2_Y(\kk,\uu) \right)
\end{equation}
Let us consider now the conditional minimiser:
\begin{align}
  J^*(\uu) = J(\kk^*(\uu),\uu) = \min_{\kk\in\Kspace} J(\kk,\uu)
\end{align}

Analogous to $J$ and $J^*$, we define $Y^*$ as
\begin{equation}
  Y^*(\uu) \sim \mathcal{N}\left(m^*_Y(\uu), \sigma^{2,*}_Y(\uu)\right)
\end{equation}
where
\begin{align}
  m^*_Y(\uu) = \min_{\kk\in\Kspace} m_Y(\kk,\uu) = m_Y(\kk^*(\uu)) \\
  \sigma^{2,*}_Y(\uu) = \sigma^{2,*}_Y(\kk^*(\uu)) 
\end{align}
The surrogate conditional minimiser is used in~\cite{ginsbourger_bayesian_2014} for instance, but other choices could be considered, such as $m_Y(\kk^*(\uu)) - \beta \sigma^{2,*}_Y(\kk^*(\uu))$. This choice for instance would lead to be more ``optimistic'' in the estimation of the minimum (i.e.\ a lower minimum), and in turn, would have a tendency to overestimate the estimated value of $\alpha$.

The $\alpha$-relaxed difference defined as  $\Delta_{\alpha} = Y - \alpha Y^*$ is a linear combination of correlated Gaussian processes. Its distribution is Gaussian and can be derived by first considering the joint distribution of $Y(\kk,\uu)$ and $Y^*(\uu) = Y(\kk^*(\uu), \uu)$:
\begin{equation}
  \begin{bmatrix}
    Y(\kk,\uu) \\
    Y^*(\uu)
  \end{bmatrix}
  \sim \mathcal{N}\left(
    \begin{bmatrix}
      m_Y(\kk,\uu) \\
      m_Y^*(\uu)
    \end{bmatrix}
    ;\,
    \begin{bmatrix}
      C\left((\kk,\uu),(\kk,\uu)\right) & C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) \\
      C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) & C\left((\kk^*(\uu),\uu),(\kk^*(\uu),\uu)\right)
    \end{bmatrix}
\right)
\end{equation}
Multiplying by the matrix $\begin{bmatrix}1 & -\alpha \end{bmatrix}$ yields

\begin{align}
  \Delta_{\alpha}(\kk,\uu) &\sim \mathcal{N}\left(m_{\Delta}(\kk,\uu); \sigma^2_{\Delta}(\kk,\uu)\right)  \label{eq:delta_GP}\\
  m_{\Delta}(\kk,\uu) &= m_Y(\kk,\uu) - \alpha m_Y^*(\uu) \label{eq:mu_delta_GP}\\
  \sigma^2_{\Delta}(\kk,\uu) &= \sigma_Y^2(\kk,\uu) + \alpha^2 \sigma_{Y^*}^2(\kk,\uu) - 2\alpha C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) \label{eq:variance_delta_GP}
\end{align}


Decomposing the variance $\sigma^2_{\Delta}$ in \cref{eq:variance_delta_GP}, 3 sources of uncertainty arise:
\begin{itemize}
\item $\sigma^2_{Y}$ is the prediction variance of the GP on $J$, that is directly reduced when additional points are evaluated
\item $\sigma^2_{Y^*}$ is the variance of the predicted value of the minimizer.
  
\item Assuming a stationary form of the covariance, the third term is directly dependent on the distance between $\kk$ and $\kk^*(\uu)$. As the covariance term can be written $C((\kk,\uu), (\kk',\uu')) = s \prod_{i\in\mathcal{I}_{\kk}}\rho_{\theta_i}(\|k_i - k'_i\|) \prod_{j\in\mathcal{I}_{\uu}} \rho_{\theta_j}(\|u_j - u'_j\|)$, substituting $\kk^*(\uu)$ for $\kk^\prime$ gives
\begin{align}
  C\left((\kk,\uu),(\kk^*(\uu),\uu)\right) &= s \prod_{i\in\mathcal{I}_{\kk}}\rho_{\theta_i}(\|k_i - k^*_i(\uu)\|)\prod_{j\in\mathcal{I}_{\uu}} \rho_{\theta_j}(0) \\
  &=s \prod_{i\in\mathcal{I}_{\kk}}\rho_{\theta_i}(\|k_i - k^*_i(\uu)\|)
\end{align}
\end{itemize}


\subsubsection{Approximation of the targeted probability using GP}

  We are going now to use a different notation for the probabilities, taken with respect to the GP: $\ProbGP$, to represent the uncertainty encompassed by the GP.
  
\paragraph{Through the probability of coverage}


For a given $\kk\in\Kspace$, the coverage probability of the $\alpha$-acceptable region, i.e.\ the probability for $\kk$ to be $\alpha$-acceptable is
\begin{align}
  \Gamma_{\alpha}(\kk) &= \Prob_{U}\left[J(\kk,\UU) \leq \alpha J^*(\UU)\right] \\
                              & =\Ex_{U}\left[\mathbbm{1}_{J(\kk,\UU) \leq \alpha J^*(\UU)}\right]
\end{align}
As $J$ is not known perfectly, it devolves into a classification problem
This classification problem can be approached with a plug-in approach in~\cref{eq:plugin_indicator}, or a probabilistic one in~\cref{eq:prob_indicator}:
\begin{align}
  \mathbbm{1}_{J(\kk,\uu) \leq \alpha J^*(\uu)} &\approx   \mathbbm{1}_{m_Y(\kk,\uu) \leq \alpha m_Y^*(\uu)} \label{eq:plugin_indicator} \\
  \mathbbm{1}_{J(\kk,\uu) \leq \alpha J^*(\uu)} &\approx   \ProbGP\left[ \Delta_{\alpha}(\kk,\uu) \leq 0 \right] = \pi_{\alpha}(\kk,\uu) \label{eq:prob_indicator}
\end{align}
Using the GPs, for a given $\kk$, $\alpha$ and $\uu$, the probability for our metamodel to verify the inequality is given by
Based on those two approximation, we can define different estimations of $\Gamma$
\begin{align}
  \hat{\Gamma}_{\alpha, n}(\kk) &= \Prob_U\left[m_Y(\kk,\uu) \leq \alpha m_Y^*(\uu) \right] \tag{plug-in} \\
  \hat{\Gamma}_{\alpha, n}(\kk) &= \Ex_U\left[ \ProbGP\left[ \Delta_{\alpha}(\kk,\uu) \leq 0\right]\right]  = \Ex_U\left[\pi_{\alpha}(\kk,\uu)\right]\tag{Probabilistic approx} \\
\end{align}

The probability of coverage for the set $\{Y - \alpha Y^*\leq 0\}$ is $\pi_{\alpha}$, and can be computed using the CDF of the standard normal distribution $\Phi$, because $\Delta_{\alpha}$ is a GP, as defined in~\cref{eq:delta_GP,eq:mu_delta_GP,eq:variance_delta_GP}
\begin{equation}
  \pi_{\alpha}(\kk,\uu) = \Phi\left(-\frac{m_{\Delta_\alpha}(\kk,\uu)}{\sigma_{\Delta_\alpha}(\kk,\uu)}\right)
\end{equation}
Finally, averaging over $\uu$ yields
\begin{equation}
  \hat{\Gamma}_{\alpha,n}(\kk) = \Ex_U\left[\pi_{\alpha}(\kk,\uu)\right]=\int_{\Uspace}\pi_{\alpha}(\kk,\uu)p(\uu) \,\mathrm{d}\uu = \int_{\Uspace}\Phi\left(-\frac{m_{\Delta_\alpha}(\kk,\uu)}{\sigma_{\Delta_\alpha}(\kk,\uu)}\right)p(\uu) \,\mathrm{d}\uu
\end{equation}

The estimation of $\Gamma$ is then maximised with respect to $\kk$ to get the candidate probability
\begin{equation}
\max_{\kk\in\Kspace}  \hat{\Gamma}_{\alpha,n}(\kk) 
\end{equation}
By tweaking the value of $\alpha$, we can get the estimate $\hat{\Gamma}$ to have its maximum equal to the targeted probability. 
As pointed out earlier the estimation of $\alpha_p$ depends on the estimation of $\hat{\Gamma}$.
\paragraph{$\alpha$ through quantiles}
Instead of maximizing for each $\alpha$ the estimated $\hat{\Gamma}$, we are now going to derive an approach based on the quantiles:
Let $Y(\kk, \uu) \sim \mathcal{N}(m_Y(\kk,\uu), \sigma^2_{Y}(\kk, \uu))$ be the GP fitted using $\mathcal{X}$. A realisation $y\sim Y$ is then a function from $\Kspace \times \Uspace$ to $\mathbb{R}^{+}_*$.
Again, the plug-in approach is to compute the ratio $m_Y(\kk, \uu) / m^*_Y(\uu)$ on a large grid for $\uu$ and for each $\kk$, look for the quantile of order $p$ with respect to $\UU$
\begin{equation}
  \alpha_{m_{Y}}(\kk) = Q_{U}\left(p;~\frac{m_Y(\kk, \UU)}{m^*_Y(\UU)}\right)
\end{equation}
the estimation of the relaxation value $\hat{\alpha}_p$ is then the minimal value of the quantiles with respect to $\kk$:
\begin{equation}
    \label{eq:def_plugin_alpha}
  \hat{\alpha}_p = \min_{\kk \in \Kspace} Q_U\left(p;~\frac{m_Y(\kk, \UU)}{m^*_Y(\UU)}\right)  \tag{plug-in}
\end{equation}
Moreover, as we can sample quite easily from the GP, we can have an idea of the uncertainty in the estimation of $\hat{\alpha}_p$.
Let us say that we sampled $N$ function from $Y$, namely $y^{(i)}$ for $1 \leq i \leq N$. For each of these samples, we can get $\alpha_{y^{(i)}}(\kk)$, shortened as $\alpha^{(i)}(\kk)$. Using Monte-Carlo, we can approximate the usual moments for $\alpha$.

\begin{equation}
 \Ex_{Y}\left[\alpha_{Y}(\kk) \right] \approx \frac{1}{N} \sum_{i=1}^N \alpha^{(i)}(\kk) = \bar{\alpha}_N(\kk)
\end{equation}
$
$




\subsubsection{SUR Strategies}
The main idea behind Stepwise Uncertainty Reduction is to define a criterion, say $\kappa_n$, that encapsulates the epistemic uncertainty, and to minimize this criterion, in order to select the next point:
\begin{equation}
  x^{n+1} = \argmax_{x\in\Xspace} \kappa_n(x)
\end{equation}
where $\kappa_n$ depends on $Y\mid \mathcal{X}_n$
This approach is suitable for step by step evaluations.

\subsubsection{Integrated Mean square criterion}
\cite{sacks_designs_1989}
Let us consider that we have a kriging model over $\Xspace$ based on a experimental design $\mathcal{X}$, that is denoted
\begin{equation}
  \label{eq:YgivenXGP}
  Y\mid \mathcal{X} \sim \mathcal{N}(m_{Y\mid\mathcal{X}}(x),\sigma^2_{Y\mid\mathcal{X}}(x))
\end{equation}
The prediction variance is directly given by $\sigma^2_{Y\mid \mathcal{X}}$, and represents the uncertainty on the Gaussian regression. To summarize this uncertainty on the whole space $\mathcal{X}$, we define the Integrated Mean Square Error (IMSE) as
\begin{equation}
  \IMSE(Y \mid \mathcal{X}) = \int_{\Xspace} \sigma_{Y\mid\mathcal{X}}^2(x)\,\mathrm{d}x
\end{equation}
For practical reasons, we can consider to integrate the MSE only on a subset $\mathfrak{X}\subset \mathcal{X}$ that yields
\begin{equation}
  \IMSE\left(Y,\mathfrak{X} \mid \mathcal{X} \right) = \int_{\mathcal{X}} \sigma^2_{Y\mid \mathcal{X}}(x)  \mathbbm{1}_{\{x \in \mathfrak{X}\}}(x)\,\mathrm{d}x = \int_{\mathfrak{X}} \sigma^2_{Y\mid \mathcal{X}}(x)\,\mathrm{d}x
\end{equation}

Unfortunately, due to the usually impossible evaluation of this integral, we will approximate this integral using a quadrature rule (or Monte-Carlo by laziness):
\begin{equation}
  \IMSE(Y,\mathfrak{X} \mid \mathcal{X}) \approx \sum_{i=1}^{n_{\mathrm{quad}}} w_i \sigma_{Y\mid\mathcal{X}}(x_i)
\end{equation}

For a given $x\in \Xspace$ and an outcome $y\in\Yspace$, the augmented design is defined as the experimental design, $\mathcal{X} \cup \left\{(x, y)\right\}$, and the IMSE of the augmented design is $\IMSE\left(Y \mid \mathcal{X} \cup \left\{(x, y)\right\} \right)$. Before the actual experiment though, $y$ is unknown, but we can model it by its distribution given by the GP (per \eqref{eq:YgivenXGP}). So for a given candidate $x$, the mean prediction error we will get when evaluating $x$ is given by
\begin{equation}
  \label{eq:IMSE_augmented}
  \Ex_{Y(x)}\Big[\IMSE\big(Y \mid \mathcal{X} \cup \left\{(x, Y(x))\right\} \big)\Big]
\end{equation}
where the expectation is to be taken with respect to different realisations of $Y(x)$. As each scenario requires to fit a GP, and to compute the IMSE, a precise evaluation is quite expensive. A strategy ffound for instance in~\cite{villemonteix_informational_2006} is to take $M$ possible outcomes for $Y(x)$, corresponding to evenly spaced quantiles of the its distribution.
It is maybe important to note that the hyperparameters of the GP should not be reevaluated when augmenting the design, in order to get comparable values for the IMSE.

To enrich the design with the best point, that reduces the most the prediction error, a simple $1$-step strategy is to minimize the expectation of~\eqref{eq:IMSE_augmented}.
\begin{equation}
  x^{n+1} = \argmin_{x\in \Xspace}\Ex_{Y(x)}\Big[\IMSE\big(Y \mid \mathcal{X} \cup \left\{(x, Y(x))\right\} \big)\Big]
\end{equation}

\subsubsection{Weighted IMSE}
The IMSE presented above is not objective driven, as the IMSE (integrated implicitly on the whole space $\Xspace$) aims solely at improving the prediction (though that can be the objective in itself).

To include a more precise objective than the enrichment of the design, one can add a weight function to the integral, giving the weighted.
\begin{equation}
  \label{eq:w-imse}
  w\IMSE(Y\mid \mathcal{X}) = \int_{\Xspace} \sigma_{Y\mid\mathcal{X}}^2(x)w(x)\,\mathrm{d}x
\end{equation}

In order to increase the accuracy of the surrogate model around some region of interest, the $w-\IMSE$ can be transformed into
\begin{equation}
  w\IMSE(Y\mid \mathcal{X}) = \int_{\Xspace} \sigma_{Y\mid\mathcal{X}}^2(x)\ProbGP\left[x \in \mathbb{M}_{\eta}\right]\,\mathrm{d}x
\end{equation}
where $\mathbb{M}_{\eta}$ is the $\eta$-margin of uncertainty.



\subsection{Sources, quantification of uncertainties, and SUR strategy?}
Formally, for a given point $(\kk,\uu)$, the event ``the point is $\alpha$-acceptable'' has probability $\pi_{\alpha}(\kk,\uu)$ and variance $\pi_{\alpha}(\kk,\uu) (1-\pi_{\alpha}(\kk,\uu))$. Obviously, the points with the highest uncertainty have the highest variance, so have a coverage probability around $0.5$.

\subsubsection{Random sets}
Let us start by introducing diverse tools based around Vorob'ev expectation of closed sets (\cite{el_amri_analyse_2019},~\cite{heinrich_level_2012}). 


Let us consider $A$, a random closed set, such that its realizations are subsets of $\Xspace$, and $p$ is its coverage probability, that is
\begin{equation}
  p(\theta) = \Prob\left[\theta\in A\right], \theta\in\Xspace
\end{equation}
For $\eta \in [0, 1]$, we define the $\eta$-level set of $p$,
\begin{equation}
  Q_{\eta} = \{x\in\Xspace \mid p(x) \geq \eta \}
\end{equation}
It may seem trivial, but let us still note that those sets are decreasing:
\begin{equation}
  0\leq \eta \leq \xi \leq 1 \implies Q_{\xi} \subseteq Q_{\eta}
\end{equation}

Let $\mu$ be a Borel $\sigma$-finite measure on $\Xspace$. We define Vorob'ev expectation, as the $\eta^*$-level set of $A$ verifying
\begin{equation}
  \forall \beta < \eta^* \quad \mu(Q_{\beta}) \leq \Ex[\mu(A)] \leq \mu(Q_{\eta^*})
\end{equation}
that is the level set of $p$, that has the volume of the mean of the volume of the random set $A$.

\subsubsection{Margin of uncertainty}
\label{sec:margin_of_uncertainty}
Using the quantiles of this level set, we can construct the $\eta$-margin of uncertainty, as~\cite{dubourg_reliability-based_2011}.
Setting the classical level $\eta=0.05$ for instance, $Q_{1-\frac{\eta}{2}}=Q_{0.975}$ is the set of points whose probability of coverage is higher than $0.975$, while $Q_{\frac{\eta}{2}}=Q_{0.025}$ is the set of points whose probability of coverage is higher than $0.025$. Obviously, $Q_{1-\frac{\eta}{2}} \subset Q_{\frac{\eta}{2}}$. The complement of $Q_{\frac{\eta}{2}}$ in $\Xspace$, denoted by $Q_{\frac{\eta}{2}}^C$ is the set of points whose probability of coverage is lower than $0.025$. The $\eta$-margin of uncertainty $\mathbb{M}_{\eta}$ is defined as the sets of points whose coverage probability is between $0.025$ and $0.975$.
\begin{equation*}
  \mathbb{M}_{\eta} = \left(Q_{1-\frac{\eta}{2}} \cup Q^C_{\frac{\eta}{2}} \right)^C = Q_{1-\frac{\eta}{2}}^C \cap Q_{\frac{\eta}{2}} = Q_{\frac{\eta}{2}} \setminus Q_{1-\frac{\eta}{2}}
\end{equation*}


Recalling the objective, it gives upper bounds and lower bounds of the confidence interval of level $\eta$ on the probability for each $\kk$:
\begin{align}
  \hat{\Gamma}_{\alpha}^{U}(\kk) &= \Prob_\uu\left[\theta=(\kk,\uu) \in Q_{1-\frac{\eta}{2}}\right] \\
  \hat{\Gamma}_{\alpha}^{L}(\kk) &= \Prob_\uu\left[\theta=(\kk,\uu) \in Q_{\frac{\eta}{2}}\right]
\end{align}



\subsubsection{UB-LB for $(p, \alpha_p, \kk_p)$}
Let us assume that we have set a probability $p\in [0,1]$. Let us recall that the triplet $(p, \alpha_p, \kk_p)$ verifies
\begin{align}
  \max_{\kk} \Gamma_{\alpha_p}(\kk) = \Gamma_{\alpha_p}(\kk_p) = \Prob_{\uu}\left[J(\kk_p,\uu) \leq \alpha_p J^*(\uu)\mid \uu = \uu\right] = p
\end{align}
Let us say that $\bar{\Gamma}$ is the $\eta$-upper-bound, while $\underline{\Gamma}$ is the $\eta$-lower bounds, so
\begin{equation}
  \ProbGP\left[\underline{\Gamma}(\kk) \leq \Gamma_n(\kk) \leq \bar{\Gamma}(\kk)\right] = \eta
\end{equation}
\begin{itemize}
\item If $\underline{\Gamma}(\kk)>p$, we are too permissive, so we should decrease $\alpha$
  \begin{itemize}
  \item by how much ?
  \end{itemize}
\item If $\bar{\Gamma}(\kk)<p$, we are too conservative, so we should increase $\alpha$
  \begin{itemize}
  \item by how much again ?
  \end{itemize}
 \item If $\underline{\Gamma}(\kk)<p<\bar{\Gamma}(\kk)$, reduce uncertainty on $\kk_p$
\end{itemize}
Changing the value of $\alpha$ does not require any further evaluation of the objective function, so can be increased until $\max \hat{\Gamma} = p$ ? by dichotomy for instance. This $\hat{\kk}_p$ is then the candidate.

Criterion: stepwise reduction of the variance of the estimation of $\hat{\Gamma}(\hat{\kk}_p) = \max_{\kk}\hat{\Gamma}(\hat{\kk})$

For a fixed $p\in (0, 1]$, and an initial design $\mathcal{X}$. Set an initial value for $\alpha \geq 1$. 
\begin{itemize}
\item Define $\Delta_{\alpha}$, using $Y \mid \mathcal{X}$
\item Update $\alpha$ such that $\max \hat{\Gamma}_{\alpha,n} = p$
\item Compute measure of uncertainty that we want to reduce:
  \begin{itemize}
  \item $\bar{\Gamma}_{\alpha,n}(\kk) - \underline{\Gamma}_{\alpha,n}(\kk)$
  \item $\pi_{\alpha}(\kk,\uu)(1-\pi_{\alpha}(\kk,\uu))$
  \end{itemize}
\end{itemize}

\subsubsection{Sampling based criterion}
\label{sec:sampling_based_criterion}
This technique is described in~\cite{dubourg_reliability-based_2011}
Let assume that we derived a criterion $\kappa$. And let $f(x) = \frac{\kappa(x)}{\int_{\Xspace}\kappa(u)\,\mathrm{d}u}$. $f$ can be seen as a density.
  Using an appropriate sampler, we can generate $N$ iid samples from this criterion $\{x_i\}_{1\leq i \leq N}$
  
However, as $N$ should be large, there is no point in evaluating all the samples $x_i$. This goes by the statistical reduction of the samples

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{/home/victor/acadwriting/sampling_estimation_Meta.pdf}
  \caption{\label{fig:label} }
\end{figure}


\section{Application to CROCO}
\label{sec:croco_application}

\begin{figure}[!h]
  \centering
  \scalebox{0.6}{\input{/home/victor/acadwriting/ocean_floor.pgf}}
  \caption{Ocean floor depth}
\end{figure}
\bibliographystyle{abbrvnat}
\bibliography{/home/victor/acadwriting/bibzotero}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
